

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>1.11. Métodos combinados &mdash; documentación de scikit-learn - 0.24.2</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/ensemble.html" />

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../index.html">
        <img
          class="sk-brand-img"
          src="../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../install.html">Instalación</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">Manual de Usuario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Ejemplos</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../getting_started.html">¿Cómo empezar?</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../whats_new/v0.24.html">Novedades</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../glossary.html">Glosario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../developers/index.html">Desarrollo</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../support.html">Soporte</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../related_projects.html">Paquetes relacionados</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../roadmap.html">Hoja de ruta</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../about.html">Sobre nosotros</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Más</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../getting_started.html">¿Cómo empezar?</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../whats_new/v0.24.html">Novedades</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../glossary.html">Glosario</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../developers/index.html">Desarrollo</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../support.html">Soporte</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../related_projects.html">Paquetes relacionados</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../roadmap.html">Hoja de ruta</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../about.html">Sobre nosotros</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Ir a" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Alternar menú</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../index.html">
            <img
              class="sk-brand-img"
              src="../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="tree.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.10. Árboles de decisión">Prev</a><a href="../supervised_learning.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1. Aprendizaje supervisado">Arriba</a>
            <a href="multiclass.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.12. Algoritmos multiclase y multisalida">Sig.</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.24.2</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Otras versiones</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Por favor <a class="font-weight-bold" href="../about.html#citing-scikit-learn"><string>cítanos</string></a> si usas el software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">1.11. Métodos combinados</a><ul>
<li><a class="reference internal" href="#bagging-meta-estimator">1.11.1. Meta estimador de bagging</a></li>
<li><a class="reference internal" href="#forests-of-randomized-trees">1.11.2. Bosques de árboles aleatorios</a><ul>
<li><a class="reference internal" href="#random-forests">1.11.2.1. Bosques Aleatorios</a></li>
<li><a class="reference internal" href="#extremely-randomized-trees">1.11.2.2. Árboles extremadamente aleatorizados</a></li>
<li><a class="reference internal" href="#parameters">1.11.2.3. Parámetros</a></li>
<li><a class="reference internal" href="#parallelization">1.11.2.4. Paralelización</a></li>
<li><a class="reference internal" href="#feature-importance-evaluation">1.11.2.5. Evaluación de importancia de características</a></li>
<li><a class="reference internal" href="#totally-random-trees-embedding">1.11.2.6. Incrustación de Arboles Totalmente Aleatorios</a></li>
</ul>
</li>
<li><a class="reference internal" href="#adaboost">1.11.3. AdaBoost</a><ul>
<li><a class="reference internal" href="#usage">1.11.3.1. Uso</a></li>
</ul>
</li>
<li><a class="reference internal" href="#gradient-tree-boosting">1.11.4. Gradient Tree Boosting</a><ul>
<li><a class="reference internal" href="#classification">1.11.4.1. Clasificación</a></li>
<li><a class="reference internal" href="#regression">1.11.4.2. Regresión</a></li>
<li><a class="reference internal" href="#fitting-additional-weak-learners">1.11.4.3. Ajustando aprendices débiles adicionales</a></li>
<li><a class="reference internal" href="#controlling-the-tree-size">1.11.4.4. Controlando el tamaño del árbol</a></li>
<li><a class="reference internal" href="#mathematical-formulation">1.11.4.5. Formulación matemática</a><ul>
<li><a class="reference internal" href="#id16">1.11.4.5.1. Regresión</a></li>
<li><a class="reference internal" href="#id17">1.11.4.5.2. Clasificación</a></li>
</ul>
</li>
<li><a class="reference internal" href="#loss-functions">1.11.4.6. Funciones de pérdida</a></li>
<li><a class="reference internal" href="#shrinkage-via-learning-rate">1.11.4.7. Reducción a través de la tasa de aprendizaje</a></li>
<li><a class="reference internal" href="#subsampling">1.11.4.8. Submuestreo</a></li>
<li><a class="reference internal" href="#interpretation-with-feature-importance">1.11.4.9. Interpretación con importancia de característica</a></li>
</ul>
</li>
<li><a class="reference internal" href="#histogram-based-gradient-boosting">1.11.5. Boosting por gradientes basado en Histogramas</a><ul>
<li><a class="reference internal" href="#id25">1.11.5.1. Uso</a></li>
<li><a class="reference internal" href="#missing-values-support">1.11.5.2. Soporte para valores faltantes</a></li>
<li><a class="reference internal" href="#sample-weight-support">1.11.5.3. Soporte para ponderado de muestras</a></li>
<li><a class="reference internal" href="#categorical-features-support">1.11.5.4. Soporte de características categóricas</a></li>
<li><a class="reference internal" href="#monotonic-constraints">1.11.5.5. Restricciones monotónicas</a></li>
<li><a class="reference internal" href="#low-level-parallelism">1.11.5.6. Paralelismo de bajo nivel</a></li>
<li><a class="reference internal" href="#why-it-s-faster">1.11.5.7. Porqué es más rápido</a></li>
</ul>
</li>
<li><a class="reference internal" href="#voting-classifier">1.11.6. Clasificador de voto</a><ul>
<li><a class="reference internal" href="#majority-class-labels-majority-hard-voting">1.11.6.1. Etiquetas de clase mayoritarias (mayoría/voto duro)</a></li>
<li><a class="reference internal" href="#id29">1.11.6.2. Uso</a></li>
<li><a class="reference internal" href="#weighted-average-probabilities-soft-voting">1.11.6.3. Promedio ponderado de probabilidades (Votación blanda)</a></li>
<li><a class="reference internal" href="#using-the-votingclassifier-with-gridsearchcv">1.11.6.4. Usando el <code class="docutils literal notranslate"><span class="pre">VotingClassifier</span></code> con <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code></a></li>
<li><a class="reference internal" href="#id30">1.11.6.5. Uso</a></li>
</ul>
</li>
<li><a class="reference internal" href="#voting-regressor">1.11.7. Regresor de voto</a><ul>
<li><a class="reference internal" href="#id32">1.11.7.1. Uso</a></li>
</ul>
</li>
<li><a class="reference internal" href="#stacked-generalization">1.11.8. Generalización apilada</a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="ensemble-methods">
<span id="ensemble"></span><h1><span class="section-number">1.11. </span>Métodos combinados<a class="headerlink" href="#ensemble-methods" title="Enlazar permanentemente con este título">¶</a></h1>
<p>Los <strong>métodos combinados</strong> se utilizan para unir las predicciones de varios estimadores base construidos desde un algoritmo de aprendizaje dado con el propósito de tener una generalización / robustez mayor que la de un solo estimador.</p>
<p>Se suelen distinguir dos familias de métodos combinados:</p>
<ul>
<li><p>En los <strong>métodos de promedio</strong>, el principio guía es la creación de varios estimadores independientes para después promediar sus predicciones. En promedio, el estimador combinado es mejor que cualquiera de los estimadores base individuales porque su varianza se reduce.</p>
<p><strong>Ejemplos:</strong> <a class="reference internal" href="#bagging"><span class="std std-ref">Métodos de bagging</span></a>, <a class="reference internal" href="#forest"><span class="std std-ref">Bosques de árboles aleatorios</span></a>, …</p>
</li>
<li><p>En cambio, en los <strong>métodos de boosting</strong>, los estimadores base se construyen de manera secuencial y uno intenta reducir el sesgo del estimador combinado. La intención es producir un conjunto potente a partir de varios modelos débiles.</p>
<p><strong>Ejemplos:</strong> <a class="reference internal" href="#adaboost"><span class="std std-ref">AdaBoost</span></a>, <a class="reference internal" href="#gradient-boosting"><span class="std std-ref">Gradient Tree Boosting</span></a>, …</p>
</li>
</ul>
<section id="bagging-meta-estimator">
<span id="bagging"></span><h2><span class="section-number">1.11.1. </span>Meta estimador de bagging<a class="headerlink" href="#bagging-meta-estimator" title="Enlazar permanentemente con este título">¶</a></h2>
<p>En los algoritmos de conjunto, los métodos de bagging forman una clase de algoritmos que construyen varias instancias de un estimador de caja negra en subconjuntos aleatorios del conjunto de entrenamiento original y luego agregan sus predicciones individuales para realizar una predicción final. Estos métodos introducen la aleatorización en la construcción de un estimador base (por ejemplo, un árbol de decisión) y crean un conjunto después para así reducir su varianza. En muchos casos, los métodos de bagging son una manera muy sencilla de mejorar con respecto a un único modelo, sin hacer necesario el adaptar el algoritmo base subyacente. Como proporcionan una forma de reducir el sobreajuste, los métodos de bagging funcionan mejor con modelos fuertes y complejos (por ejemplo, árboles de decisión completamente desarrollados), al contrario que los métodos de boosting, los cuales suelen funcionar mejor con modelos débiles (por ejemplo, árboles de decisión con poca profundidad).</p>
<p>Los métodos de bagging vienen en muchos sabores, pero por lo general difieren uno del otro por la forma en la cual agarran subconjuntos aleatorios del conjunto de entrenamiento:</p>
<blockquote>
<div><ul class="simple">
<li><p>Cuando se escogen subconjuntos aleatorios del conjunto de datos como subconjuntos aleatorios de las muestras, este algoritmo es conocido como Pasting <a class="reference internal" href="#b1999" id="id1"><span>[B1999]</span></a>.</p></li>
<li><p>Cuando las muestras son escogidas con reemplazo, entonces el método es conocido como Bagging <a class="reference internal" href="#b1996" id="id2"><span>[B1996]</span></a>.</p></li>
<li><p>Cuando son escogidos subconjuntos aleatorios del conjunto de datos como subconjuntos aleatorios de las características, entonces el método se conoce como Random Subspaces <a class="reference internal" href="#h1998" id="id3"><span>[H1998]</span></a>.</p></li>
<li><p>Finalmente, cuando los estimadores base son construidos en subconjuntos tanto de características como muestras, entonces el método es conocido como Random Patches <a class="reference internal" href="#lg2012" id="id4"><span>[LG2012]</span></a>.</p></li>
</ul>
</div></blockquote>
<p>En scikit-learn, los métodos de bagging son ofrecidos como un metaestimador unificado <a class="reference internal" href="generated/sklearn.ensemble.BaggingClassifier.html#sklearn.ensemble.BaggingClassifier" title="sklearn.ensemble.BaggingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaggingClassifier</span></code></a> (resp. <a class="reference internal" href="generated/sklearn.ensemble.BaggingRegressor.html#sklearn.ensemble.BaggingRegressor" title="sklearn.ensemble.BaggingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">BaggingRegressor</span></code></a>) tomando como entrada un estimador base especificado por el usuario y parámetros especificando la estrategia a utilizar para escoger subconjuntos aleatorios. En particular. <code class="docutils literal notranslate"><span class="pre">max_samples</span></code> y <code class="docutils literal notranslate"><span class="pre">max_features</span></code> controlan el tamaño de los subconjuntos (en términos de muestras y características), mientras que <code class="docutils literal notranslate"><span class="pre">bootstrap</span></code> and <code class="docutils literal notranslate"><span class="pre">bootstrap_features</span></code> controlan si las muestras y características son escogidas con o sin reemplazo. Cuando es utilizado un subconjunto de las muestras disponibles, la exactitud de la generalización puede ser estimada con las muestras out-of-bag estableciendo que <code class="docutils literal notranslate"><span class="pre">oob_score=True</span></code>. Como un ejemplo, el fragmento de código abajo ilustra como instanciar un conjunto de bagging de <code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code> estimadores base, cada uno construido con subconjuntos aleatorios de 50% de las muestras y 50% de las características.</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">BaggingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">bagging</span> <span class="o">=</span> <span class="n">BaggingClassifier</span><span class="p">(</span><span class="n">KNeighborsClassifier</span><span class="p">(),</span>
<span class="gp">... </span>                            <span class="n">max_samples</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_bias_variance.html#sphx-glr-auto-examples-ensemble-plot-bias-variance-py"><span class="std std-ref">Estimador único frente a empaquetado (bagging): descomposición sesgo-varianza</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias</p>
<dl class="citation">
<dt class="label" id="b1999"><span class="brackets"><a class="fn-backref" href="#id1">B1999</a></span></dt>
<dd><p>L. Breiman, «Pasting small votes for classification in large
databases and on-line», Machine Learning, 36(1), 85-103, 1999.</p>
</dd>
<dt class="label" id="b1996"><span class="brackets"><a class="fn-backref" href="#id2">B1996</a></span></dt>
<dd><p>L. Breiman, «Bagging predictors», Machine Learning, 24(2),
123-140, 1996.</p>
</dd>
<dt class="label" id="h1998"><span class="brackets"><a class="fn-backref" href="#id3">H1998</a></span></dt>
<dd><p>T. Ho, «The random subspace method for constructing decision
forests», Pattern Analysis and Machine Intelligence, 20(8), 832-844,
1998.</p>
</dd>
<dt class="label" id="lg2012"><span class="brackets"><a class="fn-backref" href="#id4">LG2012</a></span></dt>
<dd><p>G. Louppe and P. Geurts, «Ensembles on Random Patches»,
Machine Learning and Knowledge Discovery in Databases, 346-361, 2012.</p>
</dd>
</dl>
</div>
</section>
<section id="forests-of-randomized-trees">
<span id="forest"></span><h2><span class="section-number">1.11.2. </span>Bosques de árboles aleatorios<a class="headerlink" href="#forests-of-randomized-trees" title="Enlazar permanentemente con este título">¶</a></h2>
<p>El módulo <a class="reference internal" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> incluye dos algoritmos de promedio basados en <a class="reference internal" href="tree.html#tree"><span class="std std-ref">arboles de decisión</span></a>: el algoritmo RandomForest y el método Extra-Trees. Ambos algoritmos son técnicas de perturbar-y-combinar <a class="reference internal" href="#b1998" id="id5"><span>[B1998]</span></a> específicamente diseñadas para árboles. Esto significa que un diverso conjunto de clasificadores es creado introduciendo aleatoriedad en la construcción de los clasificadores. La predicción del conjunto es dada como la predicción promediada de los clasificadores individuales.</p>
<p>Como otros clasificadores, los clasificadores de árboles tienen que estar provistos con dos arreglos: un arreglo X denso o disperso de forma <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code>, sosteniendo las muestras de entrenamiento, y un arreglo Y de forma <code class="docutils literal notranslate"><span class="pre">(n_samples,)</span></code>, sosteniendo los valores objetivo (etiquetas de clase) para las muestras de entrenamiento:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Como los <a class="reference internal" href="tree.html#tree"><span class="std std-ref">arboles de decisión</span></a>, los bosques de arboles también se extienden a <a class="reference internal" href="tree.html#tree-multioutput"><span class="std std-ref">problemas de salida múltiple</span></a> (si Y es un array de forma <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_outputs)</span></code>).</p>
<section id="random-forests">
<h3><span class="section-number">1.11.2.1. </span>Bosques Aleatorios<a class="headerlink" href="#random-forests" title="Enlazar permanentemente con este título">¶</a></h3>
<p>En los bosques aleatorios (ver las clases <a class="reference internal" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.RandomForestRegressor.html#sklearn.ensemble.RandomForestRegressor" title="sklearn.ensemble.RandomForestRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestRegressor</span></code></a>), cada árbol en el conjunto es construido desde una muestra escogida con reemplazo (es decir, una muestra de bootstrap) desde el conjunto de entrenamiento.</p>
<p>Además, cuando se divide cada nodo durante la construcción de un árbol, se encuentra la mejor división desde todas las características de entrada o bien desde un subconjunto aleatorio de tamaño <code class="docutils literal notranslate"><span class="pre">max_features</span></code>. (Ver las <a class="reference internal" href="#random-forest-parameters"><span class="std std-ref">pautas de ajuste de parámetros</span></a> para más detalles).</p>
<p>Estas dos fuentes de aleatoriedad son utilizadas para disminuir la varianza del estimador de árbol. En efecto, los árboles de decisión únicos suelen exhibir varianza alta y tienden a sobreajustarse. Al inyectar aleatoriedad en los bosques, se obtienen árboles con errores de predicción algo desacoplados. Algunos de estos errores se pueden cancelar tomando un promedio de estas predicciones. Los bosques aleatorios consiguen una varianza reducida mediante la combinación de árboles diversos, a veces al costo de un ligero aumento en el sesgo. En la practica, la reducción de varianza suele ser significativa, por lo que se obtiene un mejor modelo en general.</p>
<p>A diferencia de la publicación original <a class="reference internal" href="#b2001" id="id6"><span>[B2001]</span></a>, la implementación de scikit-learn combina clasificadores al promediar su predicción probabilística, en lugar de dejar que cada clasificador vote por una única clase.</p>
</section>
<section id="extremely-randomized-trees">
<h3><span class="section-number">1.11.2.2. </span>Árboles extremadamente aleatorizados<a class="headerlink" href="#extremely-randomized-trees" title="Enlazar permanentemente con este título">¶</a></h3>
<p>En árboles extremadamente aleatorizados (ver las clases <a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtraTreesClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesRegressor.html#sklearn.ensemble.ExtraTreesRegressor" title="sklearn.ensemble.ExtraTreesRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtraTreesRegressor</span></code></a>), la aleatoriedad va un paso más allá en la forma en la que se calculan las divisiones. Como en los bosques aleatorios, un subconjunto aleatorio de características candidato son utilizadas, pero en lugar de buscar por los umbrales más discriminatorios, los umbrales son escogidos de forma aleatoria por cada característica candidato, y se escoge el mejor de estos umbrales generados al azar como la regla según la cual se realizaran las divisiones. Esto usualmente permite reducir la varianza del modelo un poco más, al costo de un ligeramente mayor aumento en el sesgo:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">0.98...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">0.999...</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">ExtraTreesClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mf">0.999</span>
<span class="go">True</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_forest_iris.html"><img alt="../_images/sphx_glr_plot_forest_iris_001.png" src="../_images/sphx_glr_plot_forest_iris_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
</section>
<section id="parameters">
<span id="random-forest-parameters"></span><h3><span class="section-number">1.11.2.3. </span>Parámetros<a class="headerlink" href="#parameters" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Los parámetros principales a ajustar cuando se utilizan estos métodos son <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> y <code class="docutils literal notranslate"><span class="pre">max_features</span></code>. El primero es el número de árboles en el bosque. Mientras más grande, mejor, pero tardará también más tiempo en calcular. Además, ten en cuenta que los resultados dejarán de mejorar significativamente después de un número crítico de árboles. El segundo es el tamaño de los subconjuntos aleatorios de características a considerar cuando se divide un nodo. Reduce la varianza mientras sea más bajo, pero también aumenta el sesgo. Empíricamente, <code class="docutils literal notranslate"><span class="pre">max_features=None</span></code> (considerar siempre todas las características en lugar de un subconjunto aleatorio), y <code class="docutils literal notranslate"><span class="pre">max_features=&quot;sqrt&quot;</span></code> (usar un subconjunto aleatorio de tamaño <code class="docutils literal notranslate"><span class="pre">sqrt(n_features)</span></code>) son buenos valores por defecto para problemas de regresión, y tareas de clasificación (donde <code class="docutils literal notranslate"><span class="pre">n_features</span></code> es el número de características en los datos), respectivamente. Buenos resultados se suelen obtener al establecer que <code class="docutils literal notranslate"><span class="pre">max_depth=None</span></code> en combinación con <code class="docutils literal notranslate"><span class="pre">min_samples_split=2</span></code> (es decir, cuando se desarrollan los árboles por completo). Sin embargo, ten en cuenta que estos valores no suelen ser óptimos, y quizás resulten en modelos que consumen mucha RAM. Los mejores valores de parámetros siempre deben ser validados de forma cruzada. Además, ten en cuenta que en bosques aleatorios, se usan por defecto muestras de bootstrap (<code class="docutils literal notranslate"><span class="pre">bootstrap=True</span></code>), mientras que usar todo el conjunto de datos (<code class="docutils literal notranslate"><span class="pre">bootstrap=False</span></code>) es la estrategia por defecto de extra-trees. Cuando se utiliza el muestreo por bootstrap, la exactitud de la generalización puede ser estimada en las muestras que sobren o que queden fuera-de-bolsa (out-of-bag). Esto se puede habilitar estableciendo que <code class="docutils literal notranslate"><span class="pre">oob_score=True</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Con los parámetros predeterminados, el tamaño del modelo es <span class="math notranslate nohighlight">\(O( M * N * log (N) )\)</span>. donde <span class="math notranslate nohighlight">\(M\)</span> es el número de árboles y <span class="math notranslate nohighlight">\(N\)</span> es el número de muestras. Si se desea reducir el tamaño del modelo, se pueden cambiar estos parámetros:
<code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>, <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>, <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> y <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>.</p>
</div>
</section>
<section id="parallelization">
<h3><span class="section-number">1.11.2.4. </span>Paralelización<a class="headerlink" href="#parallelization" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Finalmente, este módulo también permite la construcción en paralelo de los árboles y el cálculo en paralelo de las predicciones mediante el parámetro <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>. Si <code class="docutils literal notranslate"><span class="pre">n_jobs=k</span></code>, entonces los cálculos se dividen en <code class="docutils literal notranslate"><span class="pre">k</span></code> trabajos, y se ejecutan en <code class="docutils literal notranslate"><span class="pre">k</span></code> núcleos de la máquina. Si <code class="docutils literal notranslate"><span class="pre">n_jobs=-1</span></code>, entonces todos los núcleos disponibles en la máquina serán utilizados. Ten en cuenta que, debido a la sobrecarga de la comunicación entre procesos, es posible que la aceleración no será lineal (es decir, usar <code class="docutils literal notranslate"><span class="pre">k</span></code> trabajos desafortunadamente no sera <code class="docutils literal notranslate"><span class="pre">k</span></code> veces más rápido). Sin embargo, la aceleración igualmente puede ser significativa cuando se construye un número grande de árboles, o cuando se construye un solo árbol que requiere una buena cantidad de tiempo (por ejemplo, en conjuntos de datos grandes).</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_forest_iris.html#sphx-glr-auto-examples-ensemble-plot-forest-iris-py"><span class="std std-ref">Graficar las superficies de decisión de los ensembles de árboles en el conjunto de datos iris</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Importancia de los píxeles con un bosque paralelo de árboles</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py"><span class="std std-ref">Finalización facial con estimadores de salida múltiple</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias</p>
<dl class="citation">
<dt class="label" id="b2001"><span class="brackets"><a class="fn-backref" href="#id6">B2001</a></span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, «Random Forests», Machine Learning, 45(1), 5-32, 2001.</p></li>
</ol>
</dd>
<dt class="label" id="b1998"><span class="brackets"><a class="fn-backref" href="#id5">B1998</a></span></dt>
<dd><ol class="upperalpha simple" start="12">
<li><p>Breiman, «Arcing Classifiers», Annals of Statistics 1998.</p></li>
</ol>
</dd>
</dl>
<ul class="simple">
<li><p>P. Geurts, D. Ernst., and L. Wehenkel, «Extremely randomized
trees», Machine Learning, 63(1), 3-42, 2006.</p></li>
</ul>
</div>
</section>
<section id="feature-importance-evaluation">
<span id="random-forest-feature-importance"></span><h3><span class="section-number">1.11.2.5. </span>Evaluación de importancia de características<a class="headerlink" href="#feature-importance-evaluation" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El rango relativo (es decir, la profundidad) de una característica usada como nodo de decisión en un árbol puede ser utilizada para estimar la importancia relativa de esa característica con respecto a la previsibilidad de la variable objetivo. Las características usadas en la cima del árbol contribuyen a la decisión final de predicción en una fracción grande de las muestras de entrada. La <strong>fracción esperada de las muestras</strong> a la cual contribuyen puede entonces ser usada como un estimado de la <strong>importancia relativa de las características</strong>. En scikit-learn, la fracción de muestras a la cual una característica contribuye es combinada con la reducción en impureza que produce su división para crear una estimación normalizada del poder predictivo de esa característica.</p>
<p>Mediante la <strong>promediación</strong> de los estimados de habilidad predictiva sobre varios árboles aleatorizados, uno puede <strong>reducir la varianza</strong> de dicho estimado y usarlo para seleccionar características. Esto se conoce como la disminución media de la impureza, o DMI (MDI, en ingles). Ver <a class="reference internal" href="#l2014" id="id7"><span>[L2014]</span></a> para más información sobre la DMI y la evaluación de importancia de características con Bosques Aleatorios.</p>
<div class="admonition warning">
<p class="admonition-title">Advertencia</p>
<p>Las importancias de característica basadas en impureza calculadas en modelos basados en árboles sufren de dos defectos clave que pueden llevar a conclusiones engañosas. Primero, son calculados en estadísticas derivadas del conjunto de datos de entrenamiento y por lo tanto <strong>no necesariamente nos informan acerca de cuáles características son las más importantes para hacer buenas predicciones en los conjuntos de datos retenidos</strong>. Segundo, <strong>prefieren características de alta cardinalidad</strong>, es decir, características con muchos valores únicos. <a class="reference internal" href="permutation_importance.html#permutation-importance"><span class="std std-ref">Importancia de la característica de permutación</span></a> es una alternativa a la importancia de característica basada en impureza que no sufre de estos defectos. Estos dos métodos para obtener la importancia de características se exploran en: <a class="reference internal" href="../auto_examples/inspection/plot_permutation_importance.html#sphx-glr-auto-examples-inspection-plot-permutation-importance-py"><span class="std std-ref">Importancia de la Permutación vs la Importancia de las Características del Bosque Aleatorio (MDI)</span></a>.</p>
</div>
<p>El siguiente ejemplo presenta una representación codificada en color de la importancia relativa de cada pixel para una tarea de reconocimiento de caras utilizando un modelo <a class="reference internal" href="generated/sklearn.ensemble.ExtraTreesClassifier.html#sklearn.ensemble.ExtraTreesClassifier" title="sklearn.ensemble.ExtraTreesClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">ExtraTreesClassifier</span></code></a>.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_forest_importances_faces.html"><img alt="../_images/sphx_glr_plot_forest_importances_faces_001.png" src="../_images/sphx_glr_plot_forest_importances_faces_001.png" style="width: 360.0px; height: 360.0px;" /></a>
</figure>
<p>En la práctica, estos estimados se almacenan como un atributo llamado <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code> en el modelo ajustado. Este es un arreglo de forma <code class="docutils literal notranslate"><span class="pre">(n_features,)</span></code> cuyos valores son positivos y cuya suma resulta en 1.0. Mientras más grande sea el valor, más importante va a ser la contribución de la característica correspondiente a la función de predicción.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py"><span class="std std-ref">Importancia de los píxeles con un bosque paralelo de árboles</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py"><span class="std std-ref">Importancia de las características con los bosques de árboles</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias</p>
<dl class="citation">
<dt class="label" id="l2014"><span class="brackets"><a class="fn-backref" href="#id7">L2014</a></span></dt>
<dd><p>G. Louppe,
«Understanding Random Forests: From Theory to Practice»,
PhD Thesis, U. of Liege, 2014.</p>
</dd>
</dl>
</div>
</section>
<section id="totally-random-trees-embedding">
<span id="random-trees-embedding"></span><h3><span class="section-number">1.11.2.6. </span>Incrustación de Arboles Totalmente Aleatorios<a class="headerlink" href="#totally-random-trees-embedding" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code></a> implementa una transformación no supervisada de los datos. Usando un bosque de árboles completamente aleatorios, <a class="reference internal" href="generated/sklearn.ensemble.RandomTreesEmbedding.html#sklearn.ensemble.RandomTreesEmbedding" title="sklearn.ensemble.RandomTreesEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomTreesEmbedding</span></code></a> codifica los datos mediante los índices de las hojas en las que termina un punto de datos. Este índice es entonces codificado de una manera one-of-K, dando lugar a una codificación binaria dispersa y de alta  dimensión. Esta codificación puede ser eficientemente calculada y después ser usada como una base para otras tareas de aprendizaje. El tamaño y la dispersión del codigo puede ser influenciada eligiendo el número de árboles y la profundidad máxima por árbol. Por cada árbol en el conjunto, la codificación contiene una entrada de uno. El tamaño de la codificación es a lo mas <code class="docutils literal notranslate"><span class="pre">n_estimators</span> <span class="pre">*</span> <span class="pre">2</span> <span class="pre">**</span> <span class="pre">max_depth</span></code>, el número máximo de hojas en el bosque.</p>
<p>Ya que los puntos de datos vecinos son mas probables a estar dentro de la misma hoja de un árbol, la transformación realiza una estimación de densidad implícita y no paramétrica.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_random_forest_embedding.html#sphx-glr-auto-examples-ensemble-plot-random-forest-embedding-py"><span class="std std-ref">Transformación de rasgos de hashing mediante Árboles Totalmente Aleatorios</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Aprendizaje múltiple sobre dígitos manuscritos: Incrustación local lineal, Isomap…</span></a> compares non-linear
dimensionality reduction techniques on handwritten digits.</p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_feature_transformation.html#sphx-glr-auto-examples-ensemble-plot-feature-transformation-py"><span class="std std-ref">Transformaciones de características con grupos (ensembles) de árboles</span></a> compares
supervised and unsupervised tree based feature transformations.</p></li>
</ul>
</div>
<div class="admonition seealso">
<p class="admonition-title">Ver también</p>
<p><a class="reference internal" href="manifold.html#manifold"><span class="std std-ref">Aprendizaje múltiple</span></a> techniques can also be useful to derive non-linear
representations of feature space, also these approaches focus also on
dimensionality reduction.</p>
</div>
</section>
</section>
<section id="adaboost">
<span id="id8"></span><h2><span class="section-number">1.11.3. </span>AdaBoost<a class="headerlink" href="#adaboost" title="Enlazar permanentemente con este título">¶</a></h2>
<p>El módulo <a class="reference internal" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> incluye el popular algoritmo de boosting AdaBoost, introducido en 1995 por Freund y Schapire <a class="reference internal" href="#fs1995" id="id9"><span>[FS1995]</span></a>.</p>
<p>El principio básico de AdaBoost es encajar una secuencia de aprendices débiles (es decir, modelos que solo son ligeramente mejores que adivinar al azar, como los árboles de decisión pequeños) en versiones repetidamente modificadas de los datos. Las predicciones de todos ellos son entonces combinados mediante un voto por mayoría ponderado (o suma) para producir la última predicción. Las modificaciones de datos en cada iteración de boosting consisten en aplicar los ponderados <span class="math notranslate nohighlight">\(w_1\)</span>, <span class="math notranslate nohighlight">\(w_2\)</span>, …, <span class="math notranslate nohighlight">\(w_N\)</span> a cada una de las muestras de entrenamiento. Inicialmente, esos ponderados son todos establecidos como <span class="math notranslate nohighlight">\(w_i = 1/N\)</span>, para que el primer paso simplemente entrene a un aprendiz débil con los datos originales. Para cada iteración sucesiva, los ponderados de la muestra son modificados individualmente y el algoritmo de aprendizaje es aplicado de nuevo a los datos reponderados. En un paso determinado, se incrementan los ponderados de aquellos ejemplos de entrenamiento que fueron predichos incorrectamente por el modelo con boosting inducido en el paso anterior, mientras que los ponderados son disminuidos para aquellos que fueron predichos correctamente. Con cada iteración, los ejemplos que fueron difíciles de predecir reciben influencia cada vez mayor. Cada aprendedor débil subsecuente es entonces forzado a concentrarse en los ejemplos que fueron omitidos por aquellos anteriores en la secuencia <a class="reference internal" href="#htf" id="id10"><span>[HTF]</span></a>.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_adaboost_hastie_10_2.html"><img alt="../_images/sphx_glr_plot_adaboost_hastie_10_2_001.png" src="../_images/sphx_glr_plot_adaboost_hastie_10_2_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>AdaBoost puede ser utilizado tanto para problemas de clasificación como de regresión:</p>
<blockquote>
<div><ul class="simple">
<li><p>Para clasificación multiclase, <a class="reference internal" href="generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a> implementa AdaBoost-SAMME y AdaBoost-SAMME.R <a class="reference internal" href="#zzrh2009" id="id11"><span>[ZZRH2009]</span></a>.</p></li>
<li><p>Para la regresión, <a class="reference internal" href="generated/sklearn.ensemble.AdaBoostRegressor.html#sklearn.ensemble.AdaBoostRegressor" title="sklearn.ensemble.AdaBoostRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaBoostRegressor</span></code></a> implementa AdaBoost.R2 <a class="reference internal" href="#d1997" id="id12"><span>[D1997]</span></a>.</p></li>
</ul>
</div></blockquote>
<section id="usage">
<h3><span class="section-number">1.11.3.1. </span>Uso<a class="headerlink" href="#usage" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El siguiente ejemplo muestra cómo ajustar un clasificador AdaBoost con 100 aprendices débiles:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
<span class="go">0.9...</span>
</pre></div>
</div>
<p>El número de algoritmos de aprendizaje débiles es controlado por el parámetro <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>. El parámetro <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> controla la contribución de los algoritmos de aprendizaje débiles en la última combinación. Por defecto, los algoritmos de aprendizaje débiles son tocones de decisión (decision stumps, en inglés). Diferentes algoritmos de aprendizaje débiles pueden ser especificados a través del parámetro <code class="docutils literal notranslate"><span class="pre">base_estimator</span></code>. Los principales parámetros a ajustar para obtener buenos resultados son <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> y la complejidad de los estimadores base (por ejemplo, su profundidad <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> o el número mínimo requerido de muestras para considerar una división <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code>).</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_hastie_10_2.html#sphx-glr-auto-examples-ensemble-plot-adaboost-hastie-10-2-py"><span class="std std-ref">AdaBoost Discreto versus Real</span></a> compara el error de clasificación de un tope de decisión, árbol de decisión, y un tope de decisión con boosting usando AdaBoost-SAMME y AdaBoost-SAMME.R.</p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_multiclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-multiclass-py"><span class="std std-ref">Árboles de decisión multiclase AdaBoosted</span></a> muestra el rendimiento de AdaBoost-SAMME y AdaBoost-SAMME.R en un problema de multiclase.</p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_twoclass.html#sphx-glr-auto-examples-ensemble-plot-adaboost-twoclass-py"><span class="std std-ref">AdaBoost de dos clases</span></a> muestra el límite de decisión y los valores de la función de decisión para un problema de dos clases separables no linealmente usando AdaBoost-SAMME.</p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_adaboost_regression.html#sphx-glr-auto-examples-ensemble-plot-adaboost-regression-py"><span class="std std-ref">Regresión en árbol de decisión con AdaBoost</span></a> demuestra regresión con el algoritmo AdaBoost.R2.</p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias</p>
<dl class="citation">
<dt class="label" id="fs1995"><span class="brackets"><a class="fn-backref" href="#id9">FS1995</a></span></dt>
<dd><p>Y. Freund, and R. Schapire, «A Decision-Theoretic Generalization of
On-Line Learning and an Application to Boosting», 1997.</p>
</dd>
<dt class="label" id="zzrh2009"><span class="brackets"><a class="fn-backref" href="#id11">ZZRH2009</a></span></dt>
<dd><p>J. Zhu, H. Zou, S. Rosset, T. Hastie. «Multi-class AdaBoost»,
2009.</p>
</dd>
<dt class="label" id="d1997"><span class="brackets"><a class="fn-backref" href="#id12">D1997</a></span></dt>
<dd><ol class="upperalpha simple" start="8">
<li><p>Drucker. «Improving Regressors using Boosting Techniques», 1997.</p></li>
</ol>
</dd>
<dt class="label" id="htf"><span class="brackets">HTF</span><span class="fn-backref">(<a href="#id10">1</a>,<a href="#id20">2</a>,<a href="#id34">3</a>)</span></dt>
<dd><p>T. Hastie, R. Tibshirani and J. Friedman, «Elements of
Statistical Learning Ed. 2», Springer, 2009.</p>
</dd>
</dl>
</div>
</section>
</section>
<section id="gradient-tree-boosting">
<span id="gradient-boosting"></span><h2><span class="section-number">1.11.4. </span>Gradient Tree Boosting<a class="headerlink" href="#gradient-tree-boosting" title="Enlazar permanentemente con este título">¶</a></h2>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Gradient_boosting">Gradient Tree Boosting</a> o Gradient Boosted Decision Trees (GBDT) es una generalización del boosting a funciones de pérdida diferenciable arbitrarias. GBDT es un procedimiento preciso y efectivo que puede ser usado tanto para problemas de regresión como de clasificación en una variedad de areas, incluyendo la ecología y el ranking de búsquedas Web.</p>
<p>El módulo <a class="reference internal" href="classes.html#module-sklearn.ensemble" title="sklearn.ensemble"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.ensemble</span></code></a> proporciona métodos para clasificación y regresión mediante árboles de decisión con boosting por gradiente.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Scikit-learn 0.21 introduce dos nuevas implementaciones y experimentales de árboles con boosting por gradientes, principalmente <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a>, inspirado por <a class="reference external" href="https://github.com/Microsoft/LightGBM">LightGBM</a> (Ver <a class="reference internal" href="#lightgbm" id="id14"><span>[LightGBM]</span></a>).</p>
<p>Estos estimadores basados en histogramas pueden ser <strong>órdenes de magnitud más rápidos</strong> que <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> cuando el número de muestras es mayor que decenas de miles de muestras.</p>
<p>También tienen soporte incorporado para valores faltantes, lo que evita la necesidad de un imputador.</p>
<p>Estos estimadores son descritos con más detalle a continuación en <a class="reference internal" href="#histogram-based-gradient-boosting"><span class="std std-ref">Boosting por gradientes basado en Histogramas</span></a>.</p>
<p>La siguiente guía se enfoca en <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>, lo que podría ser preferido para tamaños pequeños de muestra ya que el binning quizás lleve a puntos de división que son demasiado aproximados con esta configuración.</p>
</div>
<p>A continuación se describe el uso y los parámetros de <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>. Los 2 parámetros más importantes de estos estimadores son <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> y <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p>
<section id="classification">
<h3><span class="section-number">1.11.4.1. </span>Clasificación<a class="headerlink" href="#classification" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> soporta tanto clasificación binaria como multiclase. El siguiente ejemplo muestra como ajustar un clasificador de boosting por gradiente con 100 topes de decisión como aprendices débiles:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.913...</span>
</pre></div>
</div>
<p>El número de aprendices débiles (por ejemplo, árboles de regresión) es controlado por el parametro <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>; <a class="reference internal" href="#gradient-boosting-tree-size"><span class="std std-ref">El tamaño de cada árbol</span></a> puede ser controlado tanto estableciendo la profundidad del árbol mediante <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> o estableciendo el número de nodos de hojas mediante <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>. El <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> es ún hiperparámetro en el rango (0.0, 1.0] que controla el sobreajuste mediante el <a class="reference internal" href="#gradient-boosting-shrinkage"><span class="std std-ref">encogimiento</span></a> .</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>La clasificación con más de 2 clases requiere la inducción de <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> árboles de regresión en cada iteración, por lo que el número total de árboles inducidos es igual a <code class="docutils literal notranslate"><span class="pre">n_clases</span> <span class="pre">*</span> <span class="pre">n_estimators</span></code>. Para conjuntos de datos con un número grande de clases recomendamos utilizar <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> como una alternativa a <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> .</p>
</div>
</section>
<section id="regression">
<h3><span class="section-number">1.11.4.2. </span>Regresión<a class="headerlink" href="#regression" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> soporta un número de <a class="reference internal" href="#gradient-boosting-loss"><span class="std std-ref">diferentes funciones de pérdida</span></a> para regresión que pueden ser especificadas mediante el argumento <code class="docutils literal notranslate"><span class="pre">loss</span></code>; la función de pérdida por defecto para la regresión es la de mínimos cuadrados (<code class="docutils literal notranslate"><span class="pre">'ls'</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_friedman1</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_friedman1</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1200</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">200</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">200</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">est</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;ls&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="go">5.00...</span>
</pre></div>
</div>
<p>La siguiente figura muestra los resultados de aplicar <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> con pérdida de mínimos cuadrados y 500 aprendices base al conjunto de datos de diabetes (<a class="reference internal" href="generated/sklearn.datasets.load_diabetes.html#sklearn.datasets.load_diabetes" title="sklearn.datasets.load_diabetes"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.datasets.load_diabetes</span></code></a>). La gráfica a la izquierda muestra el error de prueba y entrenamiento en cada iteración. El error de entrenamiento en cada iteración se guarda en el atributo <code class="xref py py-attr docutils literal notranslate"><span class="pre">train_score_</span></code> del modelo de boosting por gradientes. El error de prueba en cada iteración se puede obtener mediante el método <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor.staged_predict" title="sklearn.ensemble.GradientBoostingRegressor.staged_predict"><code class="xref py py-meth docutils literal notranslate"><span class="pre">staged_predict</span></code></a> que devuelve un generador que da las predicciones en cada etapa. Gráficos como estos pueden ser utilizados para determinar el número óptimo de árboles (por ejemplo, <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>) por parada anticipada.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html"><img alt="../_images/sphx_glr_plot_gradient_boosting_regression_001.png" src="../_images/sphx_glr_plot_gradient_boosting_regression_001.png" style="width: 450.0px; height: 450.0px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Regresión de Potenciación de Gradiente</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Estimados Out-of-Bag de Potenciación de Gradiente</span></a></p></li>
</ul>
</div>
</section>
<section id="fitting-additional-weak-learners">
<span id="gradient-boosting-warm-start"></span><h3><span class="section-number">1.11.4.3. </span>Ajustando aprendices débiles adicionales<a class="headerlink" href="#fitting-additional-weak-learners" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Tanto <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> como <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> soportan <code class="docutils literal notranslate"><span class="pre">warm_start=True</span></code>, el cual te permite añadir mas estimadores a un modelo ya ajustado.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span> <span class="n">warm_start</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  <span class="c1"># set warm_start and new nr of trees</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">_</span> <span class="o">=</span> <span class="n">est</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c1"># fit additional 100 trees to est</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">est</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">))</span>
<span class="go">3.84...</span>
</pre></div>
</div>
</section>
<section id="controlling-the-tree-size">
<span id="gradient-boosting-tree-size"></span><h3><span class="section-number">1.11.4.4. </span>Controlando el tamaño del árbol<a class="headerlink" href="#controlling-the-tree-size" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El tamaño de los aprendices del árbol de regresión base define el nivel de interacciones variable que pueden ser capturadas por el modelo de boosting por gradiente. En general, un árbol de profundidad <code class="docutils literal notranslate"><span class="pre">h</span></code> puede capturar interacciones de orden <code class="docutils literal notranslate"><span class="pre">h</span></code>. Hay dos formas en la que el tamaño de los árboles de regresión individuales puede ser controlado.</p>
<p>Si tu especificas <code class="docutils literal notranslate"><span class="pre">max_depth=h</span></code>, entonces se crecerán árboles binarios completos de profundidad <code class="docutils literal notranslate"><span class="pre">h</span></code>. Tales árboles tendrán (como máximo) <code class="docutils literal notranslate"><span class="pre">2**h</span></code> nodos de hoja y <code class="docutils literal notranslate"><span class="pre">2**h</span> <span class="pre">-</span> <span class="pre">1</span></code> nodos divididos.</p>
<p>Alternativamente, puedes controlar el tamaño del árbol especificando el número de nodos de hoja mediante el parámetro <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>. En este caso, los árboles crecerán usando una búsqueda best-first donde los nodos con la mayor mejora en la impureza se expandirán primero. Un árbol con <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=k</span></code> tiene <code class="docutils literal notranslate"><span class="pre">k</span> <span class="pre">-</span> <span class="pre">1</span></code> nodos divididos y por lo tanto puede modelar interacciones de hasta el orden <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=k</span></code> .</p>
<p>Encontramos que <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes=k</span></code> da resultados comparables a <code class="docutils literal notranslate"><span class="pre">max_depth=k-1</span></code> pero es significativamente mas rapido de entrenar al costo de un error de entrenamiento ligeramente mayor. El parámetro <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code> corresponde a la variable <code class="docutils literal notranslate"><span class="pre">J</span></code> en el capítulo sobre boosting por gradiente en <a class="reference internal" href="model_evaluation.html#f2001" id="id15"><span>[F2001]</span></a> y es relacionado al parámetro <code class="docutils literal notranslate"><span class="pre">interaction.depth</span></code> en el paquete gbm de R donde <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span> <span class="pre">==</span> <span class="pre">interaction.depth</span> <span class="pre">+1</span></code> .</p>
</section>
<section id="mathematical-formulation">
<h3><span class="section-number">1.11.4.5. </span>Formulación matemática<a class="headerlink" href="#mathematical-formulation" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Primero presentamos GBRT para la regresión, y luego detallamos el caso por clasificación.</p>
<section id="id16">
<h4><span class="section-number">1.11.4.5.1. </span>Regresión<a class="headerlink" href="#id16" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Los regresores GBRT son modelos aditivos cuya predicción <span class="math notranslate nohighlight">\(y_i\)</span> para una entrada dada <span class="math notranslate nohighlight">\(x_i\)</span> es de la siguiente forma:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[\hat{y_i} = F_M(x_i) = \sum_{m=1}^{M} h_m(x_i)\]</div>
</div></blockquote>
<p>donde los <span class="math notranslate nohighlight">\(h_m\)</span> son estimadores llamados <em>aprendices débiles</em> en el contexto del boosting. Gradient Tree Boosting utiliza <a class="reference internal" href="tree.html#tree"><span class="std std-ref">regresores de árboles de decisión</span></a> de tamaño fijo como aprendices débiles. La constante M corresponde al parámetro <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>.</p>
<p>Similar a otros algoritmos de boosting, un GBRT se construye de manera codiciosa:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + h_m(x),\]</div>
</div></blockquote>
<p>donde el árbol recién añadido <span class="math notranslate nohighlight">\(h_m\)</span> es ajustado para minimizar una suma de pérdidas <span class="math notranslate nohighlight">\(L_m\)</span>, dado el conjunto anterior <span class="math notranslate nohighlight">\(F_{m-1}\)</span>:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[h_m =  \arg\min_{h} L_m = \arg\min_{h} \sum_{i=1}^{n}
l(y_i, F_{m-1}(x_i) + h(x_i)),\]</div>
</div></blockquote>
<p>donde <span class="math notranslate nohighlight">\(l(y_i, F(x_i))\)</span> es definido por el parámetro <code class="docutils literal notranslate"><span class="pre">loss</span></code>, detallado en la siguiente sección.</p>
<p>Por defecto, el modelo inicial :math: <code class="docutils literal notranslate"><span class="pre">F_{0}</span></code> es elegido como la constante que minimiza la pérdida: para una pérdida de mínimos cuadrados, esta es la media empírica de los valores objetivo. El modelo inicial también puede ser especificado mediante el arugmento <code class="docutils literal notranslate"><span class="pre">ìnit`</span></code>.</p>
<p>Utilizando una aproximación de Taylor de primer orden, el valor de <span class="math notranslate nohighlight">\(l\)</span> puede aproximarse de la siguiente manera:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[l(y_i, F_{m-1}(x_i) + h_m(x_i)) \approx
l(y_i, F_{m-1}(x_i))
+ h_m(x_i)
\left[ \frac{\partial l(y_i, F(x_i))}{\partial F(x_i)} \right]_{F=F_{m - 1}}.\]</div>
</div></blockquote>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>En pocas palabras, una aproximación de Taylor de primer orden dice que <span class="math notranslate nohighlight">\(l(z) \approx l(a) + (z - a) \frac{\partial l(a)}{\partial a}\)</span>. Aquí, <span class="math notranslate nohighlight">\(z\)</span> corresponde a <span class="math notranslate nohighlight">\(F_{m - 1}(x_i) + h_m(x_i)\)</span>, y <span class="math notranslate nohighlight">\(a\)</span> corresponde a <span class="math notranslate nohighlight">\(F_{m-1}(x_i)\)</span></p>
</div>
<p>La cantidad <span class="math notranslate nohighlight">\(\left[ \frac{\parcial l(y_i, F(x_i))}{\parcial F(x_i)} \right]_{F=F_{m - 1}}\)</span> es la derivada de la pérdida con respecto a su segundo parámetro, evaluado en <span class="math notranslate nohighlight">\(F_{m-1}(x)\)</span>. Es fácil calcular por cualquier <span class="math notranslate nohighlight">\(F_{m - 1}(x_i)\)</span> dado en una forma cerrada, ya que la pérdida es diferenciable. Lo denotaremos por <span class="math notranslate nohighlight">\(g_i\)</span>.</p>
<p>Eliminando los términos constantes, tenemos:</p>
<blockquote>
<div><div class="math notranslate nohighlight">
\[h_m \approx \arg\min_{h} \sum_{i=1}^{n} h(x_i) g_i\]</div>
</div></blockquote>
<p>Esto es minimizado si <span class="math notranslate nohighlight">\(h(x_i)\)</span> es ajustado para predecir un valor que es proporcional al gradiente negativo <span class="math notranslate nohighlight">\(-g_i\)</span>. Por lo tanto, en cada iteración, <strong>el estimador</strong> <span class="math notranslate nohighlight">\(h_m\)</span> <strong>es ajustado para predecir los gradientes negativos de las muestras</strong>. Los gradientes son actualizados en cada iteración. Esto se puede considerar como alguna especie de descenso por gradiente en un espacio funcional.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Para algunas pérdidas, por ejemplo, la mínima desviación absoluta (MDA, o LAD en inglés) donde los gradientes son <span class="math notranslate nohighlight">\(\pm 1\)</span>, los valores predichos por un <span class="math notranslate nohighlight">\(h_m\)</span> ajustado no son lo suficientemente precisos: el árbol sólo puede generar valores enteros. Como resultado, los valores de las hojas del árbol <span class="math notranslate nohighlight">\(h_m\)</span> se modifican una vez que el árbol es ajustado, de tal forma que los valores de las hojas minimizan la pérdida <span class="math notranslate nohighlight">\(L_m\)</span>. Su actualización depende de la pérdida: para la pérdida LAD, el valor de una hoja se actualiza a la mediana de las muestras en esa hoja.</p>
</div>
</section>
<section id="id17">
<h4><span class="section-number">1.11.4.5.2. </span>Clasificación<a class="headerlink" href="#id17" title="Enlazar permanentemente con este título">¶</a></h4>
<p>El boosting por gradiente para la clasificación es muy similar al caso de regresión. Sin embargo, la suma de los árboles <span class="math notranslate nohighlight">\(F_M(x_i) = \sum_m h_m(x_i)\)</span> no es homogénea a una predicción: no puede ser una clase, ya que los árboles predicen valores continuos.</p>
<p>El mapeo desde el valor <span class="math notranslate nohighlight">\(F_M(x_i)\)</span> a una clase o una probabilidad es dependiente de pérdidas. Para la desviación (o log-loss, es decir, perdida de logistica), la probabilidad de que <span class="math notranslate nohighlight">\(x_i\)</span> pertenezca a la clase positiva está modelada como <span class="math notranslate nohighlight">\(p(y_i = 1 | x_i) = \sigma(F_M(x_i)\)</span> donde <span class="math notranslate nohighlight">\(\sigma\)</span> es la función sigmoid.</p>
<p>Para la clasificación multiclase, los árboles K (para clases K) se construyen en cada una de las iteraciones <span class="math notranslate nohighlight">\(M\)</span>. La probabilidad de que <span class="math notranslate nohighlight">\(x_i\)</span> pertenezca a la clase k se modela como un softmax de los valores <span class="math notranslate nohighlight">\(F_{M,k}(x_i)\)</span>.</p>
<p>Tenga en cuenta que incluso para una tarea de clasificación, el sub-estimador <span class="math notranslate nohighlight">\(h_m\)</span> sigue siendo un regresor, no un clasificador. Esto es debido a que los sub-estimadores estan entrenados para predecir <em>gradientes</em> (negativos), los cuales siempre son cantidades continuas.</p>
</section>
</section>
<section id="loss-functions">
<span id="gradient-boosting-loss"></span><h3><span class="section-number">1.11.4.6. </span>Funciones de pérdida<a class="headerlink" href="#loss-functions" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Las siguientes funciones de pérdida son soportadas y se pueden especificar usando el parámetro <code class="docutils literal notranslate"><span class="pre">loss</span></code>:</p>
<blockquote>
<div><ul class="simple">
<li><p>Regresión</p>
<ul>
<li><p>Mínimos cuadrados (<code class="docutils literal notranslate"><span class="pre">'ls'</span></code>): La elección natural para la regresión debido a sus propiedades computacionales superiores. El modelo inicial es dado por la media de los valores objetivo.</p></li>
<li><p>Menor desviación absoluta (<code class="docutils literal notranslate"><span class="pre">'lad'</span></code>): Una función de pérdida robusta para la regresión. El modelo inicial es dado por la mediana de los valores objetivo.</p></li>
<li><p>Huber (<code class="docutils literal notranslate"><span class="pre">'huber'</span></code>): Otra función de pérdida robusta que combina mínimos cuadrados y menor desviación absoluta; utilice <code class="docutils literal notranslate"><span class="pre">alpha</span></code> para controlar la sensibilidad con respecto a los valores atípicos (ver <a class="reference internal" href="model_evaluation.html#f2001" id="id18"><span>[F2001]</span></a> para más detalles).</p></li>
<li><p>Quantile (<code class="docutils literal notranslate"><span class="pre">'quantile'</span></code>): Una función de pérdida para regresión de cuantiles. Utilice <code class="docutils literal notranslate"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">alfa</span> <span class="pre">&lt;</span> <span class="pre">1</span></code> para especificar el cuantil. Esta función de pérdida puede utilizarse para crear intervalos de predicción (ver <a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_quantile.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-quantile-py"><span class="std std-ref">Intervalos de predicción para la Regresión con Potenciación de Gradiente</span></a>).</p></li>
</ul>
</li>
<li><p>Clasificación</p>
<ul>
<li><p>Desviación binomial (<code class="docutils literal notranslate"><span class="pre">'deviance'</span></code>): La función de pérdida de probabilidad logarítmica binomial negativa para la clasificación binaria (provee estimados de probabilidad). El modelo inicial viene dado por el logaritmo de la razón de momios.</p></li>
<li><p>Desviación multinomial (<code class="docutils literal notranslate"><span class="pre">'deviance'</span></code>): La función de pérdida de probabilidad logarítmica multinomial negativa para clasificación multiclase con <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> clases mutuamente exclusivas. Proporciona estimados de probabilidad. El modelo inicial es dado por la anterior probabilidad de cada clase. En cada iteración <code class="docutils literal notranslate"><span class="pre">n_classes</span></code> los árboles de regresión deben ser construidos, lo que hace que GBRT sea algo ineficiente para conjuntos de datos con un gran número de clases.</p></li>
<li><p>Pérdida exponencial (<code class="docutils literal notranslate"><span class="pre">'exponential'</span></code>): La misma función de perdida que <a class="reference internal" href="generated/sklearn.ensemble.AdaBoostClassifier.html#sklearn.ensemble.AdaBoostClassifier" title="sklearn.ensemble.AdaBoostClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">AdaBoostClassifier</span></code></a>. Menos robusta para los ejemplos mal etiquetados que <code class="docutils literal notranslate"><span class="pre">'deviance'</span></code>, solo puede ser usada para la clasificación binaria.</p></li>
</ul>
</li>
</ul>
</div></blockquote>
</section>
<section id="shrinkage-via-learning-rate">
<span id="gradient-boosting-shrinkage"></span><h3><span class="section-number">1.11.4.7. </span>Reducción a través de la tasa de aprendizaje<a class="headerlink" href="#shrinkage-via-learning-rate" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="model_evaluation.html#f2001" id="id19"><span>[F2001]</span></a> propuso una sencilla estrategia de regularización que escala la contribución de cada aprendiz débil por un factor constante <span class="math notranslate nohighlight">\(\nu\)</span>:</p>
<div class="math notranslate nohighlight">
\[F_m(x) = F_{m-1}(x) + \nu h_m(x)\]</div>
<p>El parámetro <span class="math notranslate nohighlight">\(\nu\)</span> es también llamado la <strong>tasa de aprendizaje</strong> porque escala la longitud de paso del procedimiento de descenso de gradiente; se puede establecer a través del parámetro <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code>.</p>
<p>El parámetro <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> interactúa fuertemente con el parámetro <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, el número de algoritmos de aprendizaje débiles a ajustar. Valores mas pequeños de <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> requieren de grandes números de algoritmos de aprendizaje débiles para mantener un error de entrenamiento constante. La evidencia empírica sugiere que valores pequeños de <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> favorecen un mejor error de pruebas. <a class="reference internal" href="#htf" id="id20"><span>[HTF]</span></a> recomiendan establecer la tasa de aprendizaje en una constante pequeña (por ejemplo, <code class="docutils literal notranslate"><span class="pre">learning_rate</span> <span class="pre">&lt;=</span> <span class="pre">0.1</span></code>), y escoger <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> mediante una parada anticipada. Para una discusión mas detallada de la interacción entre <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> y <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code> consulta <a class="reference internal" href="#r2007" id="id21"><span>[R2007]</span></a>.</p>
</section>
<section id="subsampling">
<h3><span class="section-number">1.11.4.8. </span>Submuestreo<a class="headerlink" href="#subsampling" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="#f1999" id="id22"><span>[F1999]</span></a> propone boosting por gradiente estocástico, lo cual combina el boosting por gradiente con el promediado de bootstrap (bagging). En cada iteración el clasificador base es entrenado en una fracción <code class="docutils literal notranslate"><span class="pre">subsamples</span></code> de los datos de entrenamiento disponibles. La submuestra es escogida sin reemplazo. Un valor típico de <code class="docutils literal notranslate"><span class="pre">subsamples</span></code> es 0.5.</p>
<p>La figura a continuación ilustra el efecto de la reducción y el submuestreo sobre la calidad del ajuste del modelo. Podemos ver claramente que el rendimiento es mayor con reducción que sín reducción. El submuestreo con reducción puede aumentar aún mas la precisión del modelo. Sin embargo, el submuestreo sín reducción tiene un resultado pobre.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html"><img alt="../_images/sphx_glr_plot_gradient_boosting_regularization_001.png" src="../_images/sphx_glr_plot_gradient_boosting_regularization_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>Otra estrategia para reducir la varianza es mediante el submuestreo de las características análogas a las divisiones aleatorias en <a class="reference internal" href="generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier" title="sklearn.ensemble.RandomForestClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RandomForestClassifier</span></code></a> . El número de características submuestreadas puede ser controlado mediante el parámetro <code class="docutils literal notranslate"><span class="pre">max_features</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Usar un valor <code class="docutils literal notranslate"><span class="pre">max_features</span></code> pequeño puede reducir significativamente el tiempo de ejecución.</p>
</div>
<p>El boosting por gradiente stocástico permite la computación de estimados fuera-de-bolsa de la desviación prueba mediante el calculo de la mejore en desviación de los ejemplos que no son incluidos en la muestra de bootstrap (es decir, los ejemplos fuera-de-bolsa). Las mejoras son almacenadas en el atributo <code class="xref py py-attr docutils literal notranslate"><span class="pre">oob_improvement_</span></code>. <code class="docutils literal notranslate"><span class="pre">oob_improvement_[i]</span></code> guarda la mejora en terminos de la pérdida en las muestras fuera-de-bolsa (OOB) si añades la i-ésima etapa a las predicciones actuales. Los estimados fuera-de-bolsa pueden ser utilizados para la selección de modelos, por ejemplo, para determinar el número de iteraciones original. Los estimados OOB suelen ser bastante pesimistas, por lo que recomendamos recurrir a ella solamente si la validación cruzada utiliza demasiado tiempo.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regularization.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regularization-py"><span class="std std-ref">Regularización del Potenciador de Gradiente (Gradient Boosting)</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_oob.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-oob-py"><span class="std std-ref">Estimados Out-of-Bag de Potenciación de Gradiente</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py"><span class="std std-ref">Errores OOB para bosques aleatorios (random forests)</span></a></p></li>
</ul>
</div>
</section>
<section id="interpretation-with-feature-importance">
<h3><span class="section-number">1.11.4.9. </span>Interpretación con importancia de característica<a class="headerlink" href="#interpretation-with-feature-importance" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Los árboles de decisión individuales se pueden interpretar fácilmente si se visualiza la estructura del árbol. Los modelos de boosting por gradiente, sin embargo, comprenden cientos de árboles de regresión, por lo que no pueden ser fácilmente interpretados por inspección visual de los árboles individuales. Afortunadamente, una diversidad de técnicas han sido propuestas para resumir e interpretar modelos de boosting por gradiente.</p>
<p>A menudo las características no contribuyen de manera equitativa para predecir la respuesta objetivo; en muchas situaciones la mayoría de las características, son de hecho irrelevantes. Cuando se interpreta un modelo, la primera pregunta suele ser: ¿cuáles son estas características importantes? y ¿cómo contribuyen a predecir la respuesta objetivo?</p>
<p>Los árboles de decisión individuales realizan de manera intrínseca una selección de características al elegir puntos de división apropiados. Esta información puede ser utilizada para medir la importancia de cada característica; la idea fundamental es: mientras mas seguido se utilice una característica en los puntos de división de un árbol mas importante sera esta característica. Esta noción de importancia puede ser extendida a los conjuntos de árboles de decisión si simplemente se promedia la importancia de característica basadas en la impureza de cada árbol (see <a class="reference internal" href="#random-forest-feature-importance"><span class="std std-ref">Evaluación de importancia de características</span></a> for more details).</p>
<p>Los puntajes de importancia de característica de un modelo de boosting por gradientes ya ajustado puede ser accedido mediante la propiedad <code class="docutils literal notranslate"><span class="pre">feature_importances_</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">GradientBoostingClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">max_depth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">feature_importances_</span>
<span class="go">array([0.10..., 0.10..., 0.11..., ...</span>
</pre></div>
</div>
<p>Ten en cuenta que este cálculo de la importancia de características está basado en la entropía, y es distinto a <a class="reference internal" href="generated/sklearn.inspection.permutation_importance.html#sklearn.inspection.permutation_importance" title="sklearn.inspection.permutation_importance"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.inspection.permutation_importance</span></code></a> el cual está basado en la permutación de las características.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_regression.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-regression-py"><span class="std std-ref">Regresión de Potenciación de Gradiente</span></a></p></li>
</ul>
</div>
</section>
</section>
<section id="histogram-based-gradient-boosting">
<span id="id23"></span><h2><span class="section-number">1.11.5. </span>Boosting por gradientes basado en Histogramas<a class="headerlink" href="#histogram-based-gradient-boosting" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Scikit-learn 0.21 introduce dos nuevas implementaciones experimentales de árboles de boosting por gradientes, <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a>, inspirado por <a class="reference external" href="https://github.com/Microsoft/LightGBM">LightGBM</a> (Ver <a class="reference internal" href="#lightgbm" id="id24"><span>[LightGBM]</span></a>).</p>
<p>Estos estimadores basados en histogramas pueden ser <strong>órdenes de magnitud más rápidos</strong> que <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> cuando el número de muestras es mayor que decenas de miles de muestras.</p>
<p>También tienen soporte incorporado para valores faltantes, lo que evita la necesidad de un imputador.</p>
<p>Estos estimadores rápidos primero recolectan las muestras de entrada <code class="docutils literal notranslate"><span class="pre">X</span></code> en intervalos de valores enteros (usualmente 256 intervalos) lo cual reduce tremendamente el número de puntos de división a considerar, y permite al algoritmo aprovechar estructuras de datos basadas en enteros (histogramas) en lugar de confiar en valores continuos ordenados cuando se construyen los árboles. La API de estos estimadores es ligeramente distinto, y algunas de las funciones de <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a> aún no son soportadas, por ejemplo algunas funciones de pérdida.</p>
<p>Estos estimadores aún son <strong>experimentales</strong>: sus predicciones y su API quizás tenga que cambiar sin ningún ciclo de depreciación. Para utilizarlas, tienes que importar de manera explicita <code class="docutils literal notranslate"><span class="pre">enable_hist_gradient_boosting</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># explicitly require this experimental feature</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.experimental</span> <span class="kn">import</span> <span class="n">enable_hist_gradient_boosting</span>  <span class="c1"># noqa</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># now you can import normally from ensemble</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/inspection/plot_partial_dependence.html#sphx-glr-auto-examples-inspection-plot-partial-dependence-py"><span class="std std-ref">Gráficos de Dependencia Parcial y de Expectativa Condicional Individual</span></a></p></li>
</ul>
</div>
<section id="id25">
<h3><span class="section-number">1.11.5.1. </span>Uso<a class="headerlink" href="#id25" title="Enlazar permanentemente con este título">¶</a></h3>
<p>La mayoría de los parámetros no cambian de <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>. Una excepción es el parámetro <code class="docutils literal notranslate"><span class="pre">max_iter</span></code> que sustituye a <code class="docutils literal notranslate"><span class="pre">n_estimators</span></code>, y controla el número de iteraciones del proceso de boosting:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.experimental</span> <span class="kn">import</span> <span class="n">enable_hist_gradient_boosting</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_hastie_10_2</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_hastie_10_2</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="mi">2000</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="mi">2000</span><span class="p">:]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="go">0.8965</span>
</pre></div>
</div>
<p>Las pérdidas disponibles para la regresión son “least_squares”, “least_absolute_deviation”, que es menos sensible a los valores atípicos, y “poisson”, que se adapta bien a las cuentas de modelos y frecuencias. Para la clasificación, “binary_crossentropy” se utiliza para la clasificación binaria y “categorical_crossentropy” se utiliza para la clasificación multiclase. Por defecto, la pérdida es “auto” y seleccionará la pérdida apropiada dependiendo de <a class="reference internal" href="../glossary.html#term-y"><span class="xref std std-term">y</span></a> pasado a <a class="reference internal" href="../glossary.html#term-fit"><span class="xref std std-term">fit</span></a>.</p>
<p>El tamaño de los árboles puede controlarse a través de los parámetros <code class="docutils literal notranslate"><span class="pre">max_leaf_nodes</span></code>, <code class="docutils literal notranslate"><span class="pre">max_depth</span></code>, y <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>.</p>
<p>El número de contenedores usados para recolectar los datos se controla con el parámetro <code class="docutils literal notranslate"><span class="pre">max_bins</span></code>. Usar menos contenedores actúa como una forma de regularización. Generalmente se recomienda utilizar el mayor número posible de contenedores, el cual es el valor predeterminado.</p>
<p>El parámetro <code class="docutils literal notranslate"><span class="pre">l2_regularization</span></code> es un regularizador en la función de pérdida y corresponde a <span class="math notranslate nohighlight">\(\lambda\)</span> en la ecuación (2) de <a class="reference internal" href="#xgboost" id="id26"><span>[XGBoost]</span></a>.</p>
<p>Ten en cuenta que <strong>el parado temprano esta habilitado por defecto si el número de muestras es mayor que 10,000</strong>. El comportamiento del parado temprano es controlado mediante los parámetros <code class="docutils literal notranslate"><span class="pre">validation_fraction</span></code>, <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code>, y <code class="docutils literal notranslate"><span class="pre">tol</span></code>. Es posible parar temprano usando un <a class="reference internal" href="../glossary.html#term-scorer"><span class="xref std std-term">scorer</span></a> arbitrario, o simplemente la perdida de entrenamiento o validación. Sin embargo, por razones técnicas, usar un scorer es significativamente mas lento que utilizar la perdida. Por defecto, se utiliza el parado temprano si hay por lo menos 10,000 muestras en el conjunto de entrenamiento, utilizando la pérdida por validación.</p>
</section>
<section id="missing-values-support">
<h3><span class="section-number">1.11.5.2. </span>Soporte para valores faltantes<a class="headerlink" href="#missing-values-support" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a> tienen soporte integrado para valores faltantes (NaNs).</p>
<p>Durante el entrenamiento, el cultivador de árboles aprende en cada punto de división si las muestras con valores faltantes deberían ir al hijo de la izquierda o la derecha, basado en la ganancia potencial. Cuando se realiza la predicción, las muestras con valores faltantes son asignadas consecuentemente al hijo de la izquierda o la derecha:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.experimental</span> <span class="kn">import</span> <span class="n">enable_hist_gradient_boosting</span>  <span class="c1"># noqa</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 0, 1, 1])</span>
</pre></div>
</div>
<p>Cuando el patrón de falta es predictivo, las divisiones pueden ser realizadas dependiendo sí falta el valor de característica o no:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">nan</span><span class="p">])</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                                      <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
<span class="gp">... </span>                                      <span class="n">learning_rate</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>                                      <span class="n">max_iter</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="go">array([0, 1, 0, 0, 1])</span>
</pre></div>
</div>
<p>Si no se encontraron valores faltantes para una característica determinada durante el entrenamiento, entonces las muestras con valores faltantes se mapean al hijo que tenga más muestras.</p>
</section>
<section id="sample-weight-support">
<span id="sw-hgbdt"></span><h3><span class="section-number">1.11.5.3. </span>Soporte para ponderado de muestras<a class="headerlink" href="#sample-weight-support" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a> soportan ponderados durante <a class="reference internal" href="../glossary.html#term-fit"><span class="xref std std-term">fit</span></a>.</p>
<p>El siguiente ejemplo de juguete demuestra cómo el modelo ignora las muestras con cero ponderado de muestra:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
<span class="gp">... </span>     <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># ignore the first 2 training samples by setting their weight to 0</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">sample_weight</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gb</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gb</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">)</span>
<span class="go">HistGradientBoostingClassifier(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gb</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])</span>
<span class="go">array([1])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gb</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">]])[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="go">0.99...</span>
</pre></div>
</div>
<p>Como puedes ver, el <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">0]</span></code> se clasifica cómodamente como <code class="docutils literal notranslate"><span class="pre">1</span></code> ya que las dos primeras muestras son ignoradas debido a sus ponderados de muestra.</p>
<p>Detalle de la implementación: tomar en cuenta los ponderados de las muestras equivale a multiplicar los gradientes (y los hessians) por los ponderados de las muestras. Note que la etapa de binning (específicamente el cálculo de cuantías) no toma en cuenta los ponderados.</p>
</section>
<section id="categorical-features-support">
<span id="categorical-support-gbdt"></span><h3><span class="section-number">1.11.5.4. </span>Soporte de características categóricas<a class="headerlink" href="#categorical-features-support" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a> tienen soporte nativo para características categóricas: las dos pueden considerar divisiones en datos categóricos no ordenados.</p>
<p>Para conjuntos de datos con características categóricas, usar el soporte nativo categórico suele ser mejor que depender de la codificación one-hot (<a class="reference internal" href="generated/sklearn.preprocessing.OneHotEncoder.html#sklearn.preprocessing.OneHotEncoder" title="sklearn.preprocessing.OneHotEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">OneHotEncoder</span></code></a>), ya que la codificación one-hot requiere más profundidad del árbol para obtener divisiones equivalentes. También generalmente es mejor depender en el soporte categórico nativo que tomar las características categóricas como continuas (ordinal), lo cual ocurre para datos categóricos codificados en ordinal, ya que las categorías son cantidades nominales donde el orden no importa.</p>
<p>Para activar el soporte categórico, se puede pasar una máscara booleana al parámetro <code class="docutils literal notranslate"><span class="pre">categorical_features</span></code>, indicando qué característica es categórica. En lo siguiente, la primera característica será tomada como categórica y la segunda característica como numérica:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="p">[</span><span class="kc">True</span><span class="p">,</span> <span class="kc">False</span><span class="p">])</span>
</pre></div>
</div>
<p>Equivalentemente, uno puede pasar una lista de enteros indicando los índices de las características categóricas:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingClassifier</span><span class="p">(</span><span class="n">categorical_features</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>La cardinalidad de cada característica categórica debe ser menor que el parámetro <code class="docutils literal notranslate"><span class="pre">max_bins</span></code>, y se espera que cada característica categórica este codificada en <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">max_bins</span> <span class="pre">-</span> <span class="pre">1]</span></code>. Para este fin, quizás sea útil pre-procesar los datos con un <a class="reference internal" href="generated/sklearn.preprocessing.OrdinalEncoder.html#sklearn.preprocessing.OrdinalEncoder" title="sklearn.preprocessing.OrdinalEncoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">OrdinalEncoder</span></code></a> como se hace en <a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_categorical.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-categorical-py"><span class="std std-ref">Soporte de características categóricas en Potenciación del Gradiente (Gradient Boosting)</span></a>.</p>
<p>Si hay valores faltantes durante el entrenamiento, estos serán tratados como una categoría adecuada. Si no hay valores faltantes durante el entrenamiento, entonces en el tiempo de predicción, los valores faltantes son mapeados al nodo hijo que tenga el mayor número de muestras (igual que en la características continuas). Durante la predicción, las categorías que no fueron vistas durante el tiempo de ajuste serán tratadas como valores faltantes.</p>
<p><strong>Búsqueda de divisiones con características categóricas</strong>: La manera canónica de considerar divisiones categóricas en un árbol es considerar todas las particiones <span class="math notranslate nohighlight">\(2^{K - 1} - 1\)</span>, donde <span class="math notranslate nohighlight">\(K\)</span> es el número de categorías. Esto puede volverse prohibitivo rápidamente si <span class="math notranslate nohighlight">\(K\)</span> es grande. Afortunadamente, ya que los árboles de potenciación de gradiente son siempre árboles de regresión (incluso para problemas de clasificación), existe una estrategia más rápida que puede dar divisiones equivalentes. Primero, las categorías de una característica son ordenadas de acuerdo a la varianza del objetivo, por cada categoría <code class="docutils literal notranslate"><span class="pre">k</span></code>. Después de que las categorías sean ordenadas, uno puede considerar <em>particiones continuas</em>, es decir, tratar las categorías como si fueran valores continuos ordenados (ver Fisher <a class="reference internal" href="#fisher1958" id="id27"><span>[Fisher1958]</span></a> para una prueba formal). Como resultado, solo <span class="math notranslate nohighlight">\(K - 1\)</span> divisiones deben ser consideradas en lugar de <span class="math notranslate nohighlight">\(2^{K - 1} - 1\)</span>. El sorteo inicial es una operación <span class="math notranslate nohighlight">\(\mathcal{O}(K \log(K))\)</span>, lo cual lleva a una complejidad total de <span class="math notranslate nohighlight">\(\mathcal{O}(K \log(K) + K)\)</span>, en lugar de <span class="math notranslate nohighlight">\(\mathcal{O}(2^K)\)</span>.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_gradient_boosting_categorical.html#sphx-glr-auto-examples-ensemble-plot-gradient-boosting-categorical-py"><span class="std std-ref">Soporte de características categóricas en Potenciación del Gradiente (Gradient Boosting)</span></a></p></li>
</ul>
</div>
</section>
<section id="monotonic-constraints">
<span id="monotonic-cst-gbdt"></span><h3><span class="section-number">1.11.5.5. </span>Restricciones monotónicas<a class="headerlink" href="#monotonic-constraints" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Dependiendo del problema en cuestión, quizás tengas conocimientos previos indicando que una característica particular debería tener, por lo general, un efecto positivo (o negativo) en el valor objetivo. Por ejemplo, si todo lo demás es igual, una mayor calificación de solvencia debería incrementar la probabilidad de que se apruebe un préstamo. Las restricciones monotónicas te permiten incorporar estos conocimientos previos dentro del modelo.</p>
<p>Una restricción monotónica positiva es una restricción de la forma:</p>
<p><span class="math notranslate nohighlight">\(x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2)\)</span>, donde <span class="math notranslate nohighlight">\(F\)</span> es el predictor con dos características.</p>
<p>De la misma manera, una restricción monotónica negativa es de la forma:</p>
<p><span class="math notranslate nohighlight">\(x_1 \leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)\)</span>.</p>
<p>Tenga en cuenta que las restricciones monotónicas solo restringen la salida «todo lo demás siendo igual». En efecto, la siguiente relación <strong>no es forzada</strong> por una restricción positiva: <span class="math notranslate nohighlight">\(x_1 \leq x_1' \implies F(x_1, x_2) \leq F(x_1', x_2')\)</span>.</p>
<p>Puedes especificar una restricción monotónica en cada característica usando el parámetro <code class="docutils literal notranslate"><span class="pre">monotonic_cst</span></code>. Para cada característica, un valor de 0 indica que no hay ninguna restricción, mientras que -1 y 1 indica una restricción negativa y positiva, respectivamente:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.experimental</span> <span class="kn">import</span> <span class="n">enable_hist_gradient_boosting</span>  <span class="c1"># noqa</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">HistGradientBoostingRegressor</span>

<span class="gp">... </span><span class="c1"># positive, negative, and no constraint on the 3 features</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gbdt</span> <span class="o">=</span> <span class="n">HistGradientBoostingRegressor</span><span class="p">(</span><span class="n">monotonic_cst</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">])</span>
</pre></div>
</div>
<p>En el contexto de una clasificación binaria, imponer una restricción monotónica significa que la característica debe tener un efecto positivo / negativo en la probabilidad de pertenecer a la clase positiva. Las restricciones monotónicas no están soportadas en un contexto multiclase.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Ya que las categorías son cantidades no ordenadas, no es posible imponer restricciones monotónicas sobre características categóricas.</p>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_monotonic_constraints.html#sphx-glr-auto-examples-ensemble-plot-monotonic-constraints-py"><span class="std std-ref">Restricciones monotónicas</span></a></p></li>
</ul>
</div>
</section>
<section id="low-level-parallelism">
<h3><span class="section-number">1.11.5.6. </span>Paralelismo de bajo nivel<a class="headerlink" href="#low-level-parallelism" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a> tienen implementaciones que usan OpenMP para la paralelización a través de Cython. Para más detalles sobre cómo controlar el número de hilos, por favor consulte nuestras notas de <a class="reference internal" href="../computing/parallelism.html#parallelism"><span class="std std-ref">Paralelismo</span></a>.</p>
<p>Las siguientes partes son paralelizadas:</p>
<ul class="simple">
<li><p>el mapeado de muestras desde valores reales a contenedores de valores enteros (sin embargo, encontrar los umbrales de los contenedores es secuencial)</p></li>
<li><p>la construcción de histogramas está paralelizada sobre características</p></li>
<li><p>la búsqueda del mejor punto de división en un nodo es paralelizado sobre características</p></li>
<li><p>durante el ajuste, el mapeo de muestras a los hijos de izquierda y derecha es paralelizado sobre muestras</p></li>
<li><p>los cálculos de gradiente y hessians son paralelizados sobre muestras</p></li>
<li><p>la predicción es paralelizada sobre muestras</p></li>
</ul>
</section>
<section id="why-it-s-faster">
<h3><span class="section-number">1.11.5.7. </span>Porqué es más rápido<a class="headerlink" href="#why-it-s-faster" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El cuello de botella de un procedimiento de boosting por gradientes es la construcción de los árboles de decisión. Construir un árbol de decisión tradicional (como en los otros GBDT, <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> and <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>) requiere ordenar las muestras en cada nodo (por cada característica). Es necesario ordenar para que cada aumento potencial de un punto de división pueda ser calculado eficientemente. Dividir un único nodo tiene entonces una complejidad de <span class="math notranslate nohighlight">\(\mathcal{O}(n_\text{features} \times n \log(n))\)</span> donde <span class="math notranslate nohighlight">\(n\)</span> es el número de muestras en el nodo.</p>
<p><a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a>, en contraste, no requieren ordenar los valores de las características y en su lugar usar una estructura de datos llamada histograma, donde las muestras están ordenadas implícitamente. Construir un histograma tiene una complejidad <span class="math notranslate nohighlight">\(\mathcal{O}(n)\)</span>, así que el procedimiento de división del nodo tiene una complejidad de <span class="math notranslate nohighlight">\(\mathcal{O}(n_\text{features} \times n)\)</span>, mucho más pequeño que el anterior. Además, en lugar de considerar <span class="math notranslate nohighlight">\(n\)</span> puntos divididos, aquí consideramos sólo <code class="docutils literal notranslate"><span class="pre">max_bins</span></code> puntos de división, que es mucho más pequeño.</p>
<p>Para construir histogramas, los datos de entrada <code class="docutils literal notranslate"><span class="pre">X</span></code> necesitan ser recolectados en contenedores de valores enteros. Este procedimiento de binning si requiere ordenar los valores de característica, pero sólo ocurre una vez al principio del proceso de boosting (no en cada nodo, como en <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingClassifier.html#sklearn.ensemble.GradientBoostingClassifier" title="sklearn.ensemble.GradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.GradientBoostingRegressor.html#sklearn.ensemble.GradientBoostingRegressor" title="sklearn.ensemble.GradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">GradientBoostingRegressor</span></code></a>).</p>
<p>Finalmente, muchas partes de la implementación de <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingClassifier.html#sklearn.ensemble.HistGradientBoostingClassifier" title="sklearn.ensemble.HistGradientBoostingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.HistGradientBoostingRegressor.html#sklearn.ensemble.HistGradientBoostingRegressor" title="sklearn.ensemble.HistGradientBoostingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">HistGradientBoostingRegressor</span></code></a> son paralelizadas.</p>
<div class="topic">
<p class="topic-title">Referencias</p>
<dl class="citation">
<dt class="label" id="f1999"><span class="brackets"><a class="fn-backref" href="#id22">F1999</a></span></dt>
<dd><p>Friedmann, Jerome H., 2007, <a class="reference external" href="https://statweb.stanford.edu/~jhf/ftp/stobst.pdf">«Stochastic Gradient Boosting»</a></p>
</dd>
<dt class="label" id="r2007"><span class="brackets"><a class="fn-backref" href="#id21">R2007</a></span></dt>
<dd><p>G. Ridgeway, «Generalized Boosted Models: A guide to the gbm
package», 2007</p>
</dd>
<dt class="label" id="xgboost"><span class="brackets"><a class="fn-backref" href="#id26">XGBoost</a></span></dt>
<dd><p>Tianqi Chen, Carlos Guestrin, <a class="reference external" href="https://arxiv.org/abs/1603.02754">«XGBoost: A Scalable Tree
Boosting System»</a></p>
</dd>
<dt class="label" id="lightgbm"><span class="brackets">LightGBM</span><span class="fn-backref">(<a href="#id14">1</a>,<a href="#id24">2</a>)</span></dt>
<dd><p>Ke et. al. <a class="reference external" href="https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree">«LightGBM: A Highly Efficient Gradient
BoostingDecision Tree»</a></p>
</dd>
<dt class="label" id="fisher1958"><span class="brackets"><a class="fn-backref" href="#id27">Fisher1958</a></span></dt>
<dd><p>Walter D. Fisher. <a class="reference external" href="http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf">«On Grouping for Maximum Homogeneity»</a></p>
</dd>
</dl>
</div>
</section>
</section>
<section id="voting-classifier">
<span id="id28"></span><h2><span class="section-number">1.11.6. </span>Clasificador de voto<a class="headerlink" href="#voting-classifier" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La idea detrás de <a class="reference internal" href="generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" title="sklearn.ensemble.VotingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a> es combinar clasificadores de aprendizaje automático conceptualmente diferentes y usar un voto por mayoría o el promedio de probabilidades predichas (voto blando) para predecir las etiquetas de clase. Este tipo de clasificador puede ser útil para un conjunto de modelos de igual rendimiento a fin de equilibrar sus debilidades individuales.</p>
<section id="majority-class-labels-majority-hard-voting">
<h3><span class="section-number">1.11.6.1. </span>Etiquetas de clase mayoritarias (mayoría/voto duro)<a class="headerlink" href="#majority-class-labels-majority-hard-voting" title="Enlazar permanentemente con este título">¶</a></h3>
<p>En la votación por mayoría, la etiqueta de clase predicha para una muestra en particular es aquella etiqueta de clase que representa la mayoría (modo) de las etiquetas de clases predichas por cada clasificador individual.</p>
<p>Por ejemplo, si la predicción de una muestra dada es</p>
<ul class="simple">
<li><p>clasificador 1 -&gt; clase 1</p></li>
<li><p>clasificador 2 -&gt; clase 1</p></li>
<li><p>clasificador 3 -&gt; clase 2</p></li>
</ul>
<p>el VotingClassifier (con <code class="docutils literal notranslate"><span class="pre">voting='hard'</span></code>) clasificaría la muestra como «clase 1» basada en la etiqueta de clase mayoritaria.</p>
<p>En el caso de un empate, el <a class="reference internal" href="generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" title="sklearn.ensemble.VotingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a> seleccionará la clase basada en el orden de clasificación ascendente. Por ejemplo, en el siguiente escenario</p>
<ul class="simple">
<li><p>clasificador 1 -&gt; clase 2</p></li>
<li><p>clasificador 2 -&gt; clase 1</p></li>
</ul>
<p>la etiqueta de clase 1 será asignada a la muestra.</p>
</section>
<section id="id29">
<h3><span class="section-number">1.11.6.2. </span>Uso<a class="headerlink" href="#id29" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El siguiente ejemplo muestra cómo ajustar el clasificador por regla de mayoría:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.naive_bayes</span> <span class="kn">import</span> <span class="n">GaussianNB</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="mi">3</span><span class="p">],</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">clf</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">([</span><span class="n">clf1</span><span class="p">,</span> <span class="n">clf2</span><span class="p">,</span> <span class="n">clf3</span><span class="p">,</span> <span class="n">eclf</span><span class="p">],</span> <span class="p">[</span><span class="s1">&#39;Logistic Regression&#39;</span><span class="p">,</span> <span class="s1">&#39;Random Forest&#39;</span><span class="p">,</span> <span class="s1">&#39;naive Bayes&#39;</span><span class="p">,</span> <span class="s1">&#39;Ensemble&#39;</span><span class="p">]):</span>
<span class="gp">... </span>    <span class="n">scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;accuracy&#39;</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s2"> (+/- </span><span class="si">%0.2f</span><span class="s2">) [</span><span class="si">%s</span><span class="s2">]&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">(),</span> <span class="n">label</span><span class="p">))</span>
<span class="go">Accuracy: 0.95 (+/- 0.04) [Logistic Regression]</span>
<span class="go">Accuracy: 0.94 (+/- 0.04) [Random Forest]</span>
<span class="go">Accuracy: 0.91 (+/- 0.04) [naive Bayes]</span>
<span class="go">Accuracy: 0.95 (+/- 0.04) [Ensemble]</span>
</pre></div>
</div>
</section>
<section id="weighted-average-probabilities-soft-voting">
<h3><span class="section-number">1.11.6.3. </span>Promedio ponderado de probabilidades (Votación blanda)<a class="headerlink" href="#weighted-average-probabilities-soft-voting" title="Enlazar permanentemente con este título">¶</a></h3>
<p>En contraste con la votación por mayoría (votación dura), el voto blando devuelve la etiqueta de clase como argmax de la suma de probabilidades predichas.</p>
<p>Se pueden asignar ponderados específicos a cada clasificador mediante el parámetro <code class="docutils literal notranslate"><span class="pre">weights</span></code>. Cuando se proporcionan ponderados, las probabilidades de clase para cada clasificador son recolectadas, multiplicadas por el ponderado del clasificador, y promediadas. La etiqueta de clase final se deriva entonces de la etiqueta de clase con la probabilidad media más alta.</p>
<p>Para ilustrar esto con un ejemplo sencillo, supongamos que tengamos 3 clasificadores y un problema de clasificación de 3 clases donde asignamos ponderados iguales a todos los clasificadores: w1=1, w2=1, w3=1.</p>
<p>Las probabilidades medias ponderadas para una muestra entonces se calcularían de la siguiente forma:</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 35%" />
<col style="width: 22%" />
<col style="width: 22%" />
<col style="width: 22%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>clasificador</p></th>
<th class="head"><p>clase 1</p></th>
<th class="head"><p>clase 2</p></th>
<th class="head"><p>clase 3</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>clasificador 1</p></td>
<td><p>w1 * 0.2</p></td>
<td><p>w1 * 0.5</p></td>
<td><p>w1 * 0.3</p></td>
</tr>
<tr class="row-odd"><td><p>clasificador 2</p></td>
<td><p>w2 * 0.6</p></td>
<td><p>w2 * 0.3</p></td>
<td><p>w2 * 0.1</p></td>
</tr>
<tr class="row-even"><td><p>clasificador 3</p></td>
<td><p>w3 * 0.3</p></td>
<td><p>w3 * 0.4</p></td>
<td><p>w3 * 0.3</p></td>
</tr>
<tr class="row-odd"><td><p>promedio ponderado</p></td>
<td><p>0.37</p></td>
<td><p>0.4</p></td>
<td><p>0.23</p></td>
</tr>
</tbody>
</table>
<p>Aquí, la etiqueta de clase predicha es 2, ya que tiene la probabilidad media más alta.</p>
<p>El siguiente ejemplo ilustra como las regiones de decisión quizás cambien cuando un <a class="reference internal" href="generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" title="sklearn.ensemble.VotingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a> blando se utilice basado en una maquína de vectores de apoyo lineal, un árbol de decisiones y un clasificador de vecinos K-nearest:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">itertools</span> <span class="kn">import</span> <span class="n">product</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingClassifier</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading some example data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">[:,</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training classifiers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">7</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;rbf&#39;</span><span class="p">,</span> <span class="n">probability</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;dt&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>                        <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">clf1</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">clf2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">clf3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">eclf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_voting_decision_regions.html"><img alt="../_images/sphx_glr_plot_voting_decision_regions_001.png" src="../_images/sphx_glr_plot_voting_decision_regions_001.png" style="width: 750.0px; height: 600.0px;" /></a>
</figure>
</section>
<section id="using-the-votingclassifier-with-gridsearchcv">
<h3><span class="section-number">1.11.6.4. </span>Usando el <code class="docutils literal notranslate"><span class="pre">VotingClassifier</span></code> con <code class="docutils literal notranslate"><span class="pre">GridSearchCV</span></code><a class="headerlink" href="#using-the-votingclassifier-with-gridsearchcv" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El <a class="reference internal" href="generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" title="sklearn.ensemble.VotingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a> también puede ser usado junto con <a class="reference internal" href="generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV" title="sklearn.model_selection.GridSearchCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">GridSearchCV</span></code></a> para afinar los hiperparámetros de los estimadores individuales:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">GridSearchCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf1</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf2</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf3</span> <span class="o">=</span> <span class="n">GaussianNB</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span>
<span class="gp">... </span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">params</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;lr__C&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">100.0</span><span class="p">],</span> <span class="s1">&#39;rf__n_estimators&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">20</span><span class="p">,</span> <span class="mi">200</span><span class="p">]}</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">GridSearchCV</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">eclf</span><span class="p">,</span> <span class="n">param_grid</span><span class="o">=</span><span class="n">params</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">grid</span> <span class="o">=</span> <span class="n">grid</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id30">
<h3><span class="section-number">1.11.6.5. </span>Uso<a class="headerlink" href="#id30" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Para predecir las etiquetas de clase basadas en las probabilidades de clase predichas (los estimadores de scikit-learn en el VotingClassifier deben soportar el método <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>Opcionalmente, se pueden proporcionar ponderados para clasificadores individuales:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">eclf</span> <span class="o">=</span> <span class="n">VotingClassifier</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">clf1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">clf2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;gnb&#39;</span><span class="p">,</span> <span class="n">clf3</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">voting</span><span class="o">=</span><span class="s1">&#39;soft&#39;</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">]</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="voting-regressor">
<span id="id31"></span><h2><span class="section-number">1.11.7. </span>Regresor de voto<a class="headerlink" href="#voting-regressor" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La idea detrás del <a class="reference internal" href="generated/sklearn.ensemble.VotingClassifier.html#sklearn.ensemble.VotingClassifier" title="sklearn.ensemble.VotingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">VotingClassifier</span></code></a> es combinar regresores de aprendizaje automático conceptualmente distintos y devolver los valores promedio predichos. Este tipo de regresor puede ser útil para un conjunto de modelos de igual rendimiento a fin de equilibrar sus debilidades individuales.</p>
<section id="id32">
<h3><span class="section-number">1.11.7.1. </span>Uso<a class="headerlink" href="#id32" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El siguiente ejemplo muestra cómo ajustar el VotingRegressor:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">VotingRegressor</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Loading some example data</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training classifiers</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg1</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg2</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg3</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ereg</span> <span class="o">=</span> <span class="n">VotingRegressor</span><span class="p">(</span><span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;gb&#39;</span><span class="p">,</span> <span class="n">reg1</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">reg2</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;lr&#39;</span><span class="p">,</span> <span class="n">reg3</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ereg</span> <span class="o">=</span> <span class="n">ereg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/ensemble/plot_voting_regressor.html"><img alt="../_images/sphx_glr_plot_voting_regressor_001.png" src="../_images/sphx_glr_plot_voting_regressor_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/ensemble/plot_voting_regressor.html#sphx-glr-auto-examples-ensemble-plot-voting-regressor-py"><span class="std std-ref">Graficar las predicciones de la regresión individual y de la votación</span></a></p></li>
</ul>
</div>
</section>
</section>
<section id="stacked-generalization">
<span id="stacking"></span><h2><span class="section-number">1.11.8. </span>Generalización apilada<a class="headerlink" href="#stacked-generalization" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La generalización apilada es un método combinar estimadores y así reducir sus sesgos <a class="reference internal" href="#w1992" id="id33"><span>[W1992]</span></a> <a class="reference internal" href="#htf" id="id34"><span>[HTF]</span></a>. Para ser mas precisos, las predicciones de cada estimador individual se apilan juntos y se usan como entrada a un estimador final para calcular la predicción. Este ultimo estimador es entrenado a través de la validación cruzada.</p>
<p>Los <a class="reference internal" href="generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier" title="sklearn.ensemble.StackingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor" title="sklearn.ensemble.StackingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingRegressor</span></code></a> proporcionan tales estrategias, las cuales se pueden aplicar a problemas tanto de clasificación como de regresión.</p>
<p>El parámetro <code class="docutils literal notranslate"><span class="pre">estimators</span></code> corresponde a la lista de estimadores que se apilan juntos en paralelo en los datos de entrada. Debe ser dado como una lista de nombres y estimadores:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span><span class="p">,</span> <span class="n">LassoCV</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimators</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
<span class="gp">... </span>              <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<span class="gp">... </span>              <span class="p">(</span><span class="s1">&#39;knr&#39;</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                                          <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))]</span>
</pre></div>
</div>
<p>El <code class="docutils literal notranslate"><span class="pre">final_estimator</span></code> usará las predicciones de los <code class="docutils literal notranslate"><span class="pre">estimators</span></code> como entrada. Debe ser un clasificador o un regresor cuando se utiliza <a class="reference internal" href="generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier" title="sklearn.ensemble.StackingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingClassifier</span></code></a> o <a class="reference internal" href="generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor" title="sklearn.ensemble.StackingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingRegressor</span></code></a>, respectivamente:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">GradientBoostingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">StackingRegressor</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">final_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">subsample</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="n">estimators</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">final_estimator</span><span class="p">)</span>
</pre></div>
</div>
<p>Para entrenar a <code class="docutils literal notranslate"><span class="pre">estimators</span></code> y <code class="docutils literal notranslate"><span class="pre">final_estimator</span></code>, el método <code class="docutils literal notranslate"><span class="pre">fit</span></code> necesita ser llamado en los datos de entrenamiento:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_diabetes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_diabetes</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<span class="gp">... </span>                                                    <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">StackingRegressor(...)</span>
</pre></div>
</div>
<p>Durante el entrenamiento, los <code class="docutils literal notranslate"><span class="pre">estimators</span></code> se ajustan en todos los datos de entrenamiento <code class="docutils literal notranslate"><span class="pre">X_train</span></code>. Serán utilizados al llamar a <code class="docutils literal notranslate"><span class="pre">predict</span></code> o <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>. Para generalizar y evitar sobreajustes, el <code class="docutils literal notranslate"><span class="pre">final_estimator</span></code> es entrenado en muestras externas usando <a class="reference internal" href="generated/sklearn.model_selection.cross_val_predict.html#sklearn.model_selection.cross_val_predict" title="sklearn.model_selection.cross_val_predict"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.model_selection.cross_val_predict</span></code></a> internamente.</p>
<p>Para <a class="reference internal" href="generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier" title="sklearn.ensemble.StackingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingClassifier</span></code></a>, tenga en cuenta que la salida de los <code class="docutils literal notranslate"><span class="pre">estimators</span></code> está controlada por el parámetro <code class="docutils literal notranslate"><span class="pre">stack_method</span></code> y es llamada por cada estimador. Este parámetro puede ser tanto una cadena, con los nombres de métodos de estimador, o <code class="docutils literal notranslate"><span class="pre">'auto'</span></code> que identificará automáticamente un método disponible dependiendo de la disponibilidad, probado en el siguiente orden de preferencia: <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, <code class="docutils literal notranslate"><span class="pre">decision_function</span></code> y <code class="docutils literal notranslate"><span class="pre">predict</span></code>.</p>
<p>Un <a class="reference internal" href="generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor" title="sklearn.ensemble.StackingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingRegressor</span></code></a> y un <a class="reference internal" href="generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier" title="sklearn.ensemble.StackingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingClassifier</span></code></a> pueden ser usados como cualquier otro regresor o clasificador, exponiendo los métodos <code class="docutils literal notranslate"><span class="pre">predict</span></code>, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>, y <code class="docutils literal notranslate"><span class="pre">decision_function</span></code>, por ejemplo.:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span> <span class="o">=</span> <span class="n">reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 score: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">r2_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">)))</span>
<span class="go">R2 score: 0.53</span>
</pre></div>
</div>
<p>Ten en cuenta que también es posible obtener la salida de los <code class="docutils literal notranslate"><span class="pre">estimators</span></code> apilados utilizando el método <code class="docutils literal notranslate"><span class="pre">transform</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">reg</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_test</span><span class="p">[:</span><span class="mi">5</span><span class="p">])</span>
<span class="go">array([[142..., 138..., 146...],</span>
<span class="go">       [179..., 182..., 151...],</span>
<span class="go">       [139..., 132..., 158...],</span>
<span class="go">       [286..., 292..., 225...],</span>
<span class="go">       [126..., 124..., 164...]])</span>
</pre></div>
</div>
<p>En la práctica, un predictor de apilado predice tan bien como el mejor predictor de la capa base e inclusive a veces lo supera combinando las diferentes fortalezas de estos predictores. Sin embargo, entrenar un predictor apilado es costoso computacionalmente.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Para <a class="reference internal" href="generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier" title="sklearn.ensemble.StackingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingClassifier</span></code></a>, al usar <code class="docutils literal notranslate"><span class="pre">stack_method_='predict_proba'</span></code>, la primera columna se elimina cuando el problema es un problema de clasificación binaria. En efecto, ambas columnas de probabilidad predichas por cada estimador son perfectamente colineales.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Múltiples capas de apilación se pueden conseguir asignando <code class="docutils literal notranslate"><span class="pre">final_estimator</span></code> a un <a class="reference internal" href="generated/sklearn.ensemble.StackingClassifier.html#sklearn.ensemble.StackingClassifier" title="sklearn.ensemble.StackingClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingClassifier</span></code></a> o <a class="reference internal" href="generated/sklearn.ensemble.StackingRegressor.html#sklearn.ensemble.StackingRegressor" title="sklearn.ensemble.StackingRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">StackingRegressor</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">final_layer_rfr</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">final_layer_gbr</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">final_layer</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;rf&#39;</span><span class="p">,</span> <span class="n">final_layer_rfr</span><span class="p">),</span>
<span class="gp">... </span>                <span class="p">(</span><span class="s1">&#39;gbrt&#39;</span><span class="p">,</span> <span class="n">final_layer_gbr</span><span class="p">)],</span>
<span class="gp">... </span>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">RidgeCV</span><span class="p">()</span>
<span class="gp">... </span>    <span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multi_layer_regressor</span> <span class="o">=</span> <span class="n">StackingRegressor</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">estimators</span><span class="o">=</span><span class="p">[(</span><span class="s1">&#39;ridge&#39;</span><span class="p">,</span> <span class="n">RidgeCV</span><span class="p">()),</span>
<span class="gp">... </span>                <span class="p">(</span><span class="s1">&#39;lasso&#39;</span><span class="p">,</span> <span class="n">LassoCV</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)),</span>
<span class="gp">... </span>                <span class="p">(</span><span class="s1">&#39;knr&#39;</span><span class="p">,</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="gp">... </span>                                            <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">))],</span>
<span class="gp">... </span>    <span class="n">final_estimator</span><span class="o">=</span><span class="n">final_layer</span>
<span class="gp">... </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">multi_layer_regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">StackingRegressor(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s1">&#39;R2 score: </span><span class="si">{:.2f}</span><span class="s1">&#39;</span>
<span class="gp">... </span>      <span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">multi_layer_regressor</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)))</span>
<span class="go">R2 score: 0.53</span>
</pre></div>
</div>
</div>
<div class="topic">
<p class="topic-title">Referencias</p>
<dl class="citation">
<dt class="label" id="w1992"><span class="brackets"><a class="fn-backref" href="#id33">W1992</a></span></dt>
<dd><p>Wolpert, David H. «Stacked generalization.» Neural networks 5.2
(1992): 241-259.</p>
</dd>
</dl>
</div>
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../_sources/modules/ensemble.rst.txt" rel="nofollow">Mostrar la fuente de esta página</a>
      </footer>
    </div>
  </div>
</div>
<script src="../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>