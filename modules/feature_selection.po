# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/feature_selection.rst:7
msgid "Feature selection"
msgstr ""

#: ../modules/feature_selection.rst:10
msgid ""
"The classes in the :mod:`sklearn.feature_selection` module can be used "
"for feature selection/dimensionality reduction on sample sets, either to "
"improve estimators' accuracy scores or to boost their performance on very"
" high-dimensional datasets."
msgstr ""

#: ../modules/feature_selection.rst:19
msgid "Removing features with low variance"
msgstr ""

#: ../modules/feature_selection.rst:21
msgid ""
":class:`VarianceThreshold` is a simple baseline approach to feature "
"selection. It removes all features whose variance doesn't meet some "
"threshold. By default, it removes all zero-variance features, i.e. "
"features that have the same value in all samples."
msgstr ""

#: ../modules/feature_selection.rst:26
#, python-format
msgid ""
"As an example, suppose that we have a dataset with boolean features, and "
"we want to remove all features that are either one or zero (on or off) in"
" more than 80% of the samples. Boolean features are Bernoulli random "
"variables, and the variance of such variables is given by"
msgstr ""

#: ../modules/feature_selection.rst:32
msgid ""
"\\mathrm{Var}[X] = p(1 - p)\n"
"\n"
msgstr ""

#: ../modules/feature_selection.rst:34
msgid "so we can select using the threshold ``.8 * (1 - .8)``::"
msgstr ""

#: ../modules/feature_selection.rst:47
msgid ""
"As expected, ``VarianceThreshold`` has removed the first column, which "
"has a probability :math:`p = 5/6 > .8` of containing a zero."
msgstr ""

#: ../modules/feature_selection.rst:53
msgid "Univariate feature selection"
msgstr ""

#: ../modules/feature_selection.rst:55
msgid ""
"Univariate feature selection works by selecting the best features based "
"on univariate statistical tests. It can be seen as a preprocessing step "
"to an estimator. Scikit-learn exposes feature selection routines as "
"objects that implement the ``transform`` method:"
msgstr ""

#: ../modules/feature_selection.rst:60
msgid ""
":class:`SelectKBest` removes all but the :math:`k` highest scoring "
"features"
msgstr ""

#: ../modules/feature_selection.rst:62
msgid ""
":class:`SelectPercentile` removes all but a user-specified highest "
"scoring percentage of features"
msgstr ""

#: ../modules/feature_selection.rst:65
msgid ""
"using common univariate statistical tests for each feature: false "
"positive rate :class:`SelectFpr`, false discovery rate "
":class:`SelectFdr`, or family wise error :class:`SelectFwe`."
msgstr ""

#: ../modules/feature_selection.rst:69
msgid ""
":class:`GenericUnivariateSelect` allows to perform univariate feature "
"selection with a configurable strategy. This allows to select the best "
"univariate selection strategy with hyper-parameter search estimator."
msgstr ""

#: ../modules/feature_selection.rst:73
msgid ""
"For instance, we can perform a :math:`\\chi^2` test to the samples to "
"retrieve only the two best features as follows:"
msgstr ""

#: ../modules/feature_selection.rst:86
msgid ""
"These objects take as input a scoring function that returns univariate "
"scores and p-values (or only scores for :class:`SelectKBest` and "
":class:`SelectPercentile`):"
msgstr ""

#: ../modules/feature_selection.rst:90
msgid "For regression: :func:`f_regression`, :func:`mutual_info_regression`"
msgstr ""

#: ../modules/feature_selection.rst:92
msgid ""
"For classification: :func:`chi2`, :func:`f_classif`, "
":func:`mutual_info_classif`"
msgstr ""

#: ../modules/feature_selection.rst:94
msgid ""
"The methods based on F-test estimate the degree of linear dependency "
"between two random variables. On the other hand, mutual information "
"methods can capture any kind of statistical dependency, but being "
"nonparametric, they require more samples for accurate estimation."
msgstr ""

msgid "Feature selection with sparse data"
msgstr ""

#: ../modules/feature_selection.rst:101
msgid ""
"If you use sparse data (i.e. data represented as sparse matrices), "
":func:`chi2`, :func:`mutual_info_regression`, :func:`mutual_info_classif`"
" will deal with the data without making it dense."
msgstr ""

#: ../modules/feature_selection.rst:107
msgid ""
"Beware not to use a regression scoring function with a classification "
"problem, you will get useless results."
msgstr ""

#: ../modules/feature_selection.rst:112
msgid ":ref:`sphx_glr_auto_examples_feature_selection_plot_feature_selection.py`"
msgstr ""

#: ../modules/feature_selection.rst:114
msgid ":ref:`sphx_glr_auto_examples_feature_selection_plot_f_test_vs_mi.py`"
msgstr ""

#: ../modules/feature_selection.rst:119
msgid "Recursive feature elimination"
msgstr ""

#: ../modules/feature_selection.rst:121
msgid ""
"Given an external estimator that assigns weights to features (e.g., the "
"coefficients of a linear model), the goal of recursive feature "
"elimination (:class:`RFE`) is to select features by recursively "
"considering smaller and smaller sets of features. First, the estimator is"
" trained on the initial set of features and the importance of each "
"feature is obtained either through any specific attribute (such as "
"``coef_``, ``feature_importances_``) or callable. Then, the least "
"important features are pruned from current set of features. That "
"procedure is recursively repeated on the pruned set until the desired "
"number of features to select is eventually reached."
msgstr ""

#: ../modules/feature_selection.rst:131
msgid ""
":class:`RFECV` performs RFE in a cross-validation loop to find the "
"optimal number of features."
msgstr ""

#: ../modules/feature_selection.rst:136
msgid ""
":ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_digits.py`: A "
"recursive feature elimination example showing the relevance of pixels in "
"a digit classification task."
msgstr ""

#: ../modules/feature_selection.rst:139
msgid ""
":ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`:"
" A recursive feature elimination example with automatic tuning of the "
"number of features selected with cross-validation."
msgstr ""

#: ../modules/feature_selection.rst:146
msgid "Feature selection using SelectFromModel"
msgstr ""

#: ../modules/feature_selection.rst:148
msgid ""
":class:`SelectFromModel` is a meta-transformer that can be used along "
"with any estimator that importance of each feature through a specific "
"attribute (such as ``coef_``, ``feature_importances_``) or callable after"
" fitting. The features are considered unimportant and removed, if the "
"corresponding importance of the feature values are below the provided "
"``threshold`` parameter. Apart from specifying the threshold numerically,"
" there are built-in heuristics for finding a threshold using a string "
"argument. Available heuristics are \"mean\", \"median\" and float "
"multiples of these like \"0.1*mean\". In combination with the `threshold`"
" criteria, one can use the `max_features` parameter to set a limit on the"
" number of features to select."
msgstr ""

#: ../modules/feature_selection.rst:159
msgid "For examples on how it is to be used refer to the sections below."
msgstr ""

#: ../modules/feature_selection.rst:163 ../modules/feature_selection.rst:304
msgid ":ref:`sphx_glr_auto_examples_feature_selection_plot_select_from_model_diabetes.py`"
msgstr ""

#: ../modules/feature_selection.rst:168
msgid "L1-based feature selection"
msgstr ""

#: ../modules/feature_selection.rst:172
msgid ""
":ref:`Linear models <linear_model>` penalized with the L1 norm have "
"sparse solutions: many of their estimated coefficients are zero. When the"
" goal is to reduce the dimensionality of the data to use with another "
"classifier, they can be used along with "
":class:`~feature_selection.SelectFromModel` to select the non-zero "
"coefficients. In particular, sparse estimators useful for this purpose "
"are the :class:`~linear_model.Lasso` for regression, and of "
":class:`~linear_model.LogisticRegression` and :class:`~svm.LinearSVC` for"
" classification::"
msgstr ""

#: ../modules/feature_selection.rst:193
msgid ""
"With SVMs and logistic-regression, the parameter C controls the sparsity:"
" the smaller C the fewer features selected. With Lasso, the higher the "
"alpha parameter, the fewer features selected."
msgstr ""

#: ../modules/feature_selection.rst:199
msgid ""
":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`:"
" Comparison of different algorithms for document classification including"
" L1-based feature selection."
msgstr ""

msgid "**L1-recovery and compressive sensing**"
msgstr ""

#: ../modules/feature_selection.rst:207
msgid ""
"For a good choice of alpha, the :ref:`lasso` can fully recover the exact "
"set of non-zero variables using only few observations, provided certain "
"specific conditions are met. In particular, the number of samples should "
"be \"sufficiently large\", or L1 models will perform at random, where "
"\"sufficiently large\" depends on the number of non-zero coefficients, "
"the logarithm of the number of features, the amount of noise, the "
"smallest absolute value of non-zero coefficients, and the structure of "
"the design matrix X. In addition, the design matrix must display certain "
"specific properties, such as not being too correlated."
msgstr ""

#: ../modules/feature_selection.rst:217
msgid ""
"There is no general rule to select an alpha parameter for recovery of "
"non-zero coefficients. It can by set by cross-validation "
"(:class:`LassoCV` or :class:`LassoLarsCV`), though this may lead to "
"under-penalized models: including a small number of non-relevant "
"variables is not detrimental to prediction score. BIC "
"(:class:`LassoLarsIC`) tends, on the opposite, to set high values of "
"alpha."
msgstr ""

#: ../modules/feature_selection.rst:225
msgid ""
"**Reference** Richard G. Baraniuk \"Compressive Sensing\", IEEE Signal "
"Processing Magazine [120] July 2007 "
"http://users.isr.ist.utl.pt/~aguiar/CS_notes.pdf"
msgstr ""

#: ../modules/feature_selection.rst:231
msgid "Tree-based feature selection"
msgstr ""

#: ../modules/feature_selection.rst:233
msgid ""
"Tree-based estimators (see the :mod:`sklearn.tree` module and forest of "
"trees in the :mod:`sklearn.ensemble` module) can be used to compute "
"impurity-based feature importances, which in turn can be used to discard "
"irrelevant features (when coupled with the "
":class:`~feature_selection.SelectFromModel` meta-transformer)::"
msgstr ""

#: ../modules/feature_selection.rst:256
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances.py`: "
"example on synthetic data showing the recovery of the actually meaningful"
" features."
msgstr ""

#: ../modules/feature_selection.rst:260
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py`: "
"example on face recognition data."
msgstr ""

#: ../modules/feature_selection.rst:266
msgid "Sequential Feature Selection"
msgstr ""

#: ../modules/feature_selection.rst:268
msgid ""
"Sequential Feature Selection [sfs]_ (SFS) is available in the "
":class:`~sklearn.feature_selection.SequentialFeatureSelector` "
"transformer. SFS can be either forward or backward:"
msgstr ""

#: ../modules/feature_selection.rst:272
msgid ""
"Forward-SFS is a greedy procedure that iteratively finds the best new "
"feature to add to the set of selected features. Concretely, we initially "
"start with zero feature and find the one feature that maximizes a cross-"
"validated score when an estimator is trained on this single feature. Once"
" that first feature is selected, we repeat the procedure by adding a new "
"feature to the set of selected features. The procedure stops when the "
"desired number of selected features is reached, as determined by the "
"`n_features_to_select` parameter."
msgstr ""

#: ../modules/feature_selection.rst:280
msgid ""
"Backward-SFS follows the same idea but works in the opposite direction: "
"instead of starting with no feature and greedily adding features, we "
"start with *all* the features and greedily *remove* features from the "
"set. The `direction` parameter controls whether forward or backward SFS "
"is used."
msgstr ""

#: ../modules/feature_selection.rst:285
msgid ""
"In general, forward and backward selection do not yield equivalent "
"results. Also, one may be much faster than the other depending on the "
"requested number of selected features: if we have 10 features and ask for"
" 7 selected features, forward selection would need to perform 7 "
"iterations while backward selection would only need to perform 3."
msgstr ""

#: ../modules/feature_selection.rst:291
msgid ""
"SFS differs from :class:`~sklearn.feature_selection.RFE` and "
":class:`~sklearn.feature_selection.SelectFromModel` in that it does not "
"require the underlying model to expose a `coef_` or "
"`feature_importances_` attribute. It may however be slower considering "
"that more models need to be evaluated, compared to the other approaches. "
"For example in backward selection, the iteration going from `m` features "
"to `m - 1` features using k-fold cross-validation requires fitting `m * "
"k` models, while :class:`~sklearn.feature_selection.RFE` would require "
"only a single fit, and "
":class:`~sklearn.feature_selection.SelectFromModel` always just does a "
"single fit and requires no iterations."
msgstr ""

#: ../modules/feature_selection.rst:308
msgid ""
"Ferri et al, `Comparative study of techniques for large-scale feature "
"selection "
"<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.24.4369&rep=rep1&type=pdf>`_."
msgstr ""

#: ../modules/feature_selection.rst:313
msgid "Feature selection as part of a pipeline"
msgstr ""

#: ../modules/feature_selection.rst:315
msgid ""
"Feature selection is usually used as a pre-processing step before doing "
"the actual learning. The recommended way to do this in scikit-learn is to"
" use a :class:`~pipeline.Pipeline`::"
msgstr ""

#: ../modules/feature_selection.rst:325
msgid ""
"In this snippet we make use of a :class:`~svm.LinearSVC` coupled with "
":class:`~feature_selection.SelectFromModel` to evaluate feature "
"importances and select the most relevant features. Then, a "
":class:`~ensemble.RandomForestClassifier` is trained on the transformed "
"output, i.e. using only relevant features. You can perform similar "
"operations with the other feature selection methods and also classifiers "
"that provide a way to evaluate feature importances of course. See the "
":class:`~pipeline.Pipeline` examples for more details."
msgstr ""

