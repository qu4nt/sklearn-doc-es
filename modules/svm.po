# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/svm.rst:5
msgid "Support Vector Machines"
msgstr ""

#: ../modules/svm.rst:12
msgid ""
"**Support vector machines (SVMs)** are a set of supervised learning "
"methods used for :ref:`classification <svm_classification>`, "
":ref:`regression <svm_regression>` and :ref:`outliers detection "
"<svm_outlier_detection>`."
msgstr ""

#: ../modules/svm.rst:17
msgid "The advantages of support vector machines are:"
msgstr ""

#: ../modules/svm.rst:19
msgid "Effective in high dimensional spaces."
msgstr ""

#: ../modules/svm.rst:21
msgid ""
"Still effective in cases where number of dimensions is greater than the "
"number of samples."
msgstr ""

#: ../modules/svm.rst:24
msgid ""
"Uses a subset of training points in the decision function (called support"
" vectors), so it is also memory efficient."
msgstr ""

#: ../modules/svm.rst:27
msgid ""
"Versatile: different :ref:`svm_kernels` can be specified for the decision"
" function. Common kernels are provided, but it is also possible to "
"specify custom kernels."
msgstr ""

#: ../modules/svm.rst:31
msgid "The disadvantages of support vector machines include:"
msgstr ""

#: ../modules/svm.rst:33
msgid ""
"If the number of features is much greater than the number of samples, "
"avoid over-fitting in choosing :ref:`svm_kernels` and regularization term"
" is crucial."
msgstr ""

#: ../modules/svm.rst:37
msgid ""
"SVMs do not directly provide probability estimates, these are calculated "
"using an expensive five-fold cross-validation (see :ref:`Scores and "
"probabilities <scores_probabilities>`, below)."
msgstr ""

#: ../modules/svm.rst:41
msgid ""
"The support vector machines in scikit-learn support both dense "
"(``numpy.ndarray`` and convertible to that by ``numpy.asarray``) and "
"sparse (any ``scipy.sparse``) sample vectors as input. However, to use an"
" SVM to make predictions for sparse data, it must have been fit on such "
"data. For optimal performance, use C-ordered ``numpy.ndarray`` (dense) or"
" ``scipy.sparse.csr_matrix`` (sparse) with ``dtype=float64``."
msgstr ""

#: ../modules/svm.rst:52
msgid "Classification"
msgstr ""

#: ../modules/svm.rst:54
msgid ""
":class:`SVC`, :class:`NuSVC` and :class:`LinearSVC` are classes capable "
"of performing binary and multi-class classification on a dataset."
msgstr ""

#: ../modules/svm.rst:63
msgid ""
":class:`SVC` and :class:`NuSVC` are similar methods, but accept slightly "
"different sets of parameters and have different mathematical formulations"
" (see section :ref:`svm_mathematical_formulation`). On the other hand, "
":class:`LinearSVC` is another (faster) implementation of Support Vector "
"Classification for the case of a linear kernel. Note that "
":class:`LinearSVC` does not accept parameter ``kernel``, as this is "
"assumed to be linear. It also lacks some of the attributes of "
":class:`SVC` and :class:`NuSVC`, like ``support_``."
msgstr ""

#: ../modules/svm.rst:72
msgid ""
"As other classifiers, :class:`SVC`, :class:`NuSVC` and :class:`LinearSVC`"
" take as input two arrays: an array `X` of shape `(n_samples, "
"n_features)` holding the training samples, and an array `y` of class "
"labels (strings or integers), of shape `(n_samples)`::"
msgstr ""

#: ../modules/svm.rst:85
msgid "After being fitted, the model can then be used to predict new values::"
msgstr ""

#: ../modules/svm.rst:90
msgid ""
"SVMs decision function (detailed in the "
":ref:`svm_mathematical_formulation`) depends on some subset of the "
"training data, called the support vectors. Some properties of these "
"support vectors can be found in attributes ``support_vectors_``, "
"``support_`` and ``n_support_``::"
msgstr ""

#: ../modules/svm.rst:108
msgid ":ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane.py`,"
msgstr ""

#: ../modules/svm.rst:109 ../modules/svm.rst:509
msgid ":ref:`sphx_glr_auto_examples_svm_plot_svm_nonlinear.py`"
msgstr ""

#: ../modules/svm.rst:110
msgid ":ref:`sphx_glr_auto_examples_svm_plot_svm_anova.py`,"
msgstr ""

#: ../modules/svm.rst:115
msgid "Multi-class classification"
msgstr ""

#: ../modules/svm.rst:117
msgid ""
":class:`SVC` and :class:`NuSVC` implement the \"one-versus-one\" approach"
" for multi-class classification. In total, ``n_classes * (n_classes - 1) "
"/ 2`` classifiers are constructed and each one trains data from two "
"classes. To provide a consistent interface with other classifiers, the "
"``decision_function_shape`` option allows to monotonically transform the "
"results of the \"one-versus-one\" classifiers to a \"one-vs-rest\" "
"decision function of shape ``(n_samples, n_classes)``."
msgstr ""

#: ../modules/svm.rst:139
msgid ""
"On the other hand, :class:`LinearSVC` implements \"one-vs-the-rest\" "
"multi-class strategy, thus training `n_classes` models."
msgstr ""

#: ../modules/svm.rst:149
msgid ""
"See :ref:`svm_mathematical_formulation` for a complete description of the"
" decision function."
msgstr ""

#: ../modules/svm.rst:152
msgid ""
"Note that the :class:`LinearSVC` also implements an alternative multi-"
"class strategy, the so-called multi-class SVM formulated by Crammer and "
"Singer [#8]_, by using the option ``multi_class='crammer_singer'``. In "
"practice, one-vs-rest classification is usually preferred, since the "
"results are mostly similar, but the runtime is significantly less."
msgstr ""

#: ../modules/svm.rst:158
msgid ""
"For \"one-vs-rest\" :class:`LinearSVC` the attributes ``coef_`` and "
"``intercept_`` have the shape ``(n_classes, n_features)`` and "
"``(n_classes,)`` respectively. Each row of the coefficients corresponds "
"to one of the ``n_classes`` \"one-vs-rest\" classifiers and similar for "
"the intercepts, in the order of the \"one\" class."
msgstr ""

#: ../modules/svm.rst:164
msgid ""
"In the case of \"one-vs-one\" :class:`SVC` and :class:`NuSVC`, the layout"
" of the attributes is a little more involved. In the case of a linear "
"kernel, the attributes ``coef_`` and ``intercept_`` have the shape "
"``(n_classes * (n_classes - 1) / 2, n_features)`` and ``(n_classes * "
"(n_classes - 1) / 2)`` respectively. This is similar to the layout for "
":class:`LinearSVC` described above, with each row now corresponding to a "
"binary classifier. The order for classes 0 to n is \"0 vs 1\", \"0 vs 2\""
" , ... \"0 vs n\", \"1 vs 2\", \"1 vs 3\", \"1 vs n\", . . . \"n-1 vs "
"n\"."
msgstr ""

#: ../modules/svm.rst:174
msgid ""
"The shape of ``dual_coef_`` is ``(n_classes-1, n_SV)`` with a somewhat "
"hard to grasp layout. The columns correspond to the support vectors "
"involved in any of the ``n_classes * (n_classes - 1) / 2`` \"one-vs-one\""
" classifiers. Each of the support vectors is used in ``n_classes - 1`` "
"classifiers. The ``n_classes - 1`` entries in each row correspond to the "
"dual coefficients for these classifiers."
msgstr ""

#: ../modules/svm.rst:182
msgid ""
"This might be clearer with an example: consider a three class problem "
"with class 0 having three support vectors :math:`v^{0}_0, v^{1}_0, "
"v^{2}_0` and class 1 and 2 having two support vectors :math:`v^{0}_1, "
"v^{1}_1` and :math:`v^{0}_2, v^{1}_2` respectively.  For each support "
"vector :math:`v^{j}_i`, there are two dual coefficients.  Let's call the "
"coefficient of support vector :math:`v^{j}_i` in the classifier between "
"classes :math:`i` and :math:`k` :math:`\\alpha^{j}_{i,k}`. Then "
"``dual_coef_`` looks like this:"
msgstr ""

#: ../modules/svm.rst:192
msgid ":math:`\\alpha^{0}_{0,1}`"
msgstr ""

#: ../modules/svm.rst:192
msgid ":math:`\\alpha^{0}_{0,2}`"
msgstr ""

#: ../modules/svm.rst:192
msgid "Coefficients for SVs of class 0"
msgstr ""

#: ../modules/svm.rst:194
msgid ":math:`\\alpha^{1}_{0,1}`"
msgstr ""

#: ../modules/svm.rst:194
msgid ":math:`\\alpha^{1}_{0,2}`"
msgstr ""

#: ../modules/svm.rst:196
msgid ":math:`\\alpha^{2}_{0,1}`"
msgstr ""

#: ../modules/svm.rst:196
msgid ":math:`\\alpha^{2}_{0,2}`"
msgstr ""

#: ../modules/svm.rst:198
msgid ":math:`\\alpha^{0}_{1,0}`"
msgstr ""

#: ../modules/svm.rst:198
msgid ":math:`\\alpha^{0}_{1,2}`"
msgstr ""

#: ../modules/svm.rst:198
msgid "Coefficients for SVs of class 1"
msgstr ""

#: ../modules/svm.rst:200
msgid ":math:`\\alpha^{1}_{1,0}`"
msgstr ""

#: ../modules/svm.rst:200
msgid ":math:`\\alpha^{1}_{1,2}`"
msgstr ""

#: ../modules/svm.rst:202
msgid ":math:`\\alpha^{0}_{2,0}`"
msgstr ""

#: ../modules/svm.rst:202
msgid ":math:`\\alpha^{0}_{2,1}`"
msgstr ""

#: ../modules/svm.rst:202
msgid "Coefficients for SVs of class 2"
msgstr ""

#: ../modules/svm.rst:204
msgid ":math:`\\alpha^{1}_{2,0}`"
msgstr ""

#: ../modules/svm.rst:204
msgid ":math:`\\alpha^{1}_{2,1}`"
msgstr ""

#: ../modules/svm.rst:209
msgid ":ref:`sphx_glr_auto_examples_svm_plot_iris_svc.py`,"
msgstr ""

#: ../modules/svm.rst:214
msgid "Scores and probabilities"
msgstr ""

#: ../modules/svm.rst:216
msgid ""
"The ``decision_function`` method of :class:`SVC` and :class:`NuSVC` gives"
" per-class scores for each sample (or a single score per sample in the "
"binary case). When the constructor option ``probability`` is set to "
"``True``, class membership probability estimates (from the methods "
"``predict_proba`` and ``predict_log_proba``) are enabled. In the binary "
"case, the probabilities are calibrated using Platt scaling [#1]_: "
"logistic regression on the SVM's scores, fit by an additional cross-"
"validation on the training data. In the multiclass case, this is extended"
" as per [#2]_."
msgstr ""

#: ../modules/svm.rst:227
msgid ""
"The same probability calibration procedure is available for all "
"estimators via the :class:`~sklearn.calibration.CalibratedClassifierCV` "
"(see :ref:`calibration`). In the case of :class:`SVC` and :class:`NuSVC`,"
" this procedure is builtin in `libsvm`_ which is used under the hood, so "
"it does not rely on scikit-learn's "
":class:`~sklearn.calibration.CalibratedClassifierCV`."
msgstr ""

#: ../modules/svm.rst:234
msgid ""
"The cross-validation involved in Platt scaling is an expensive operation "
"for large datasets. In addition, the probability estimates may be "
"inconsistent with the scores:"
msgstr ""

#: ../modules/svm.rst:238
msgid "the \"argmax\" of the scores may not be the argmax of the probabilities"
msgstr ""

#: ../modules/svm.rst:239
msgid ""
"in binary classification, a sample may be labeled by ``predict`` as "
"belonging to the positive class even if the output of `predict_proba` is "
"less than 0.5; and similarly, it could be labeled as negative even if the"
" output of `predict_proba` is more than 0.5."
msgstr ""

#: ../modules/svm.rst:244
msgid ""
"Platt's method is also known to have theoretical issues. If confidence "
"scores are required, but these do not have to be probabilities, then it "
"is advisable to set ``probability=False`` and use ``decision_function`` "
"instead of ``predict_proba``."
msgstr ""

#: ../modules/svm.rst:249
msgid ""
"Please note that when ``decision_function_shape='ovr'`` and ``n_classes >"
" 2``, unlike ``decision_function``, the ``predict`` method does not try "
"to break ties by default. You can set ``break_ties=True`` for the output "
"of ``predict`` to be the same as ``np.argmax(clf.decision_function(...), "
"axis=1)``, otherwise the first class among the tied classes will always "
"be returned; but have in mind that it comes with a computational cost. "
"See :ref:`sphx_glr_auto_examples_svm_plot_svm_tie_breaking.py` for an "
"example on tie breaking."
msgstr ""

#: ../modules/svm.rst:259
msgid "Unbalanced problems"
msgstr ""

#: ../modules/svm.rst:261
msgid ""
"In problems where it is desired to give more importance to certain "
"classes or certain individual samples, the parameters ``class_weight`` "
"and ``sample_weight`` can be used."
msgstr ""

#: ../modules/svm.rst:265
msgid ""
":class:`SVC` (but not :class:`NuSVC`) implements the parameter "
"``class_weight`` in the ``fit`` method. It's a dictionary of the form "
"``{class_label : value}``, where value is a floating point number > 0 "
"that sets the parameter ``C`` of class ``class_label`` to ``C * value``. "
"The figure below illustrates the decision boundary of an unbalanced "
"problem, with and without weight correction."
msgstr ""

#: ../modules/svm.rst:278
msgid ""
":class:`SVC`, :class:`NuSVC`, :class:`SVR`, :class:`NuSVR`, "
":class:`LinearSVC`, :class:`LinearSVR` and :class:`OneClassSVM` implement"
" also weights for individual samples in the `fit` method through the "
"``sample_weight`` parameter. Similar to ``class_weight``, this sets the "
"parameter ``C`` for the i-th example to ``C * sample_weight[i]``, which "
"will encourage the classifier to get these samples right. The figure "
"below illustrates the effect of sample weighting on the decision "
"boundary. The size of the circles is proportional to the sample weights:"
msgstr ""

#: ../modules/svm.rst:294
msgid ":ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py`"
msgstr ""

#: ../modules/svm.rst:295
msgid ":ref:`sphx_glr_auto_examples_svm_plot_weighted_samples.py`,"
msgstr ""

#: ../modules/svm.rst:301
msgid "Regression"
msgstr ""

#: ../modules/svm.rst:303
msgid ""
"The method of Support Vector Classification can be extended to solve "
"regression problems. This method is called Support Vector Regression."
msgstr ""

#: ../modules/svm.rst:306
msgid ""
"The model produced by support vector classification (as described above) "
"depends only on a subset of the training data, because the cost function "
"for building the model does not care about training points that lie "
"beyond the margin. Analogously, the model produced by Support Vector "
"Regression depends only on a subset of the training data, because the "
"cost function ignores samples whose prediction is close to their target."
msgstr ""

#: ../modules/svm.rst:314
msgid ""
"There are three different implementations of Support Vector Regression: "
":class:`SVR`, :class:`NuSVR` and :class:`LinearSVR`. :class:`LinearSVR` "
"provides a faster implementation than :class:`SVR` but only considers the"
" linear kernel, while :class:`NuSVR` implements a slightly different "
"formulation than :class:`SVR` and :class:`LinearSVR`. See "
":ref:`svm_implementation_details` for further details."
msgstr ""

#: ../modules/svm.rst:321
msgid ""
"As with classification classes, the fit method will take as argument "
"vectors X, y, only that in this case y is expected to have floating point"
" values instead of integer values::"
msgstr ""

#: ../modules/svm.rst:337
msgid ":ref:`sphx_glr_auto_examples_svm_plot_svm_regression.py`"
msgstr ""

#: ../modules/svm.rst:342
msgid "Density estimation, novelty detection"
msgstr ""

#: ../modules/svm.rst:344
msgid ""
"The class :class:`OneClassSVM` implements a One-Class SVM which is used "
"in outlier detection."
msgstr ""

#: ../modules/svm.rst:347
msgid "See :ref:`outlier_detection` for the description and usage of OneClassSVM."
msgstr ""

#: ../modules/svm.rst:350
msgid "Complexity"
msgstr ""

#: ../modules/svm.rst:352
msgid ""
"Support Vector Machines are powerful tools, but their compute and storage"
" requirements increase rapidly with the number of training vectors. The "
"core of an SVM is a quadratic programming problem (QP), separating "
"support vectors from the rest of the training data. The QP solver used by"
" the `libsvm`_-based implementation scales between :math:`O(n_{features} "
"\\times n_{samples}^2)` and :math:`O(n_{features} \\times n_{samples}^3)`"
" depending on how efficiently the `libsvm`_ cache is used in practice "
"(dataset dependent). If the data is very sparse :math:`n_{features}` "
"should be replaced by the average number of non-zero features in a sample"
" vector."
msgstr ""

#: ../modules/svm.rst:363
msgid ""
"For the linear case, the algorithm used in :class:`LinearSVC` by the "
"`liblinear`_ implementation is much more efficient than its "
"`libsvm`_-based :class:`SVC` counterpart and can scale almost linearly to"
" millions of samples and/or features."
msgstr ""

#: ../modules/svm.rst:370
msgid "Tips on Practical Use"
msgstr ""

#: ../modules/svm.rst:373
msgid ""
"**Avoiding data copy**: For :class:`SVC`, :class:`SVR`, :class:`NuSVC` "
"and :class:`NuSVR`, if the data passed to certain methods is not "
"C-ordered contiguous and double precision, it will be copied before "
"calling the underlying C implementation. You can check whether a given "
"numpy array is C-contiguous by inspecting its ``flags`` attribute."
msgstr ""

#: ../modules/svm.rst:379
msgid ""
"For :class:`LinearSVC` (and :class:`LogisticRegression "
"<sklearn.linear_model.LogisticRegression>`) any input passed as a numpy "
"array will be copied and converted to the `liblinear`_ internal sparse "
"data representation (double precision floats and int32 indices of non-"
"zero components). If you want to fit a large-scale linear classifier "
"without copying a dense numpy C-contiguous double precision array as "
"input, we suggest to use the :class:`SGDClassifier "
"<sklearn.linear_model.SGDClassifier>` class instead.  The objective "
"function can be configured to be almost the same as the "
":class:`LinearSVC` model."
msgstr ""

#: ../modules/svm.rst:390
msgid ""
"**Kernel cache size**: For :class:`SVC`, :class:`SVR`, :class:`NuSVC` and"
" :class:`NuSVR`, the size of the kernel cache has a strong impact on run "
"times for larger problems.  If you have enough RAM available, it is "
"recommended to set ``cache_size`` to a higher value than the default of "
"200(MB), such as 500(MB) or 1000(MB)."
msgstr ""

#: ../modules/svm.rst:397
msgid ""
"**Setting C**: ``C`` is ``1`` by default and it's a reasonable default "
"choice.  If you have a lot of noisy observations you should decrease it: "
"decreasing C corresponds to more regularization."
msgstr ""

#: ../modules/svm.rst:401
msgid ""
":class:`LinearSVC` and :class:`LinearSVR` are less sensitive to ``C`` "
"when it becomes large, and prediction results stop improving after a "
"certain threshold. Meanwhile, larger ``C`` values will take more time to "
"train, sometimes up to 10 times longer, as shown in [#3]_."
msgstr ""

#: ../modules/svm.rst:406
msgid ""
"Support Vector Machine algorithms are not scale invariant, so **it is "
"highly recommended to scale your data**. For example, scale each "
"attribute on the input vector X to [0,1] or [-1,+1], or standardize it to"
" have mean 0 and variance 1. Note that the *same* scaling must be applied"
" to the test vector to obtain meaningful results. This can be done easily"
" by using a :class:`~sklearn.pipeline.Pipeline`::"
msgstr ""

#: ../modules/svm.rst:419
msgid ""
"See section :ref:`preprocessing` for more details on scaling and "
"normalization."
msgstr ""

#: ../modules/svm.rst:424
msgid ""
"Regarding the `shrinking` parameter, quoting [#4]_: *We found that if the"
" number of iterations is large, then shrinking can shorten the training "
"time. However, if we loosely solve the optimization problem (e.g., by "
"using a large stopping tolerance), the code without using shrinking may "
"be much faster*"
msgstr ""

#: ../modules/svm.rst:430
msgid ""
"Parameter ``nu`` in :class:`NuSVC`/:class:`OneClassSVM`/:class:`NuSVR` "
"approximates the fraction of training errors and support vectors."
msgstr ""

#: ../modules/svm.rst:433
msgid ""
"In :class:`SVC`, if the data is unbalanced (e.g. many positive and few "
"negative), set ``class_weight='balanced'`` and/or try different penalty "
"parameters ``C``."
msgstr ""

#: ../modules/svm.rst:437
msgid ""
"**Randomness of the underlying implementations**: The underlying "
"implementations of :class:`SVC` and :class:`NuSVC` use a random number "
"generator only to shuffle the data for probability estimation (when "
"``probability`` is set to ``True``). This randomness can be controlled "
"with the ``random_state`` parameter. If ``probability`` is set to "
"``False`` these estimators are not random and ``random_state`` has no "
"effect on the results. The underlying :class:`OneClassSVM` implementation"
" is similar to the ones of :class:`SVC` and :class:`NuSVC`. As no "
"probability estimation is provided for :class:`OneClassSVM`, it is not "
"random."
msgstr ""

#: ../modules/svm.rst:447
msgid ""
"The underlying :class:`LinearSVC` implementation uses a random number "
"generator to select features when fitting the model with a dual "
"coordinate descent (i.e when ``dual`` is set to ``True``). It is thus not"
" uncommon to have slightly different results for the same input data. If "
"that happens, try with a smaller `tol` parameter. This randomness can "
"also be controlled with the ``random_state`` parameter. When ``dual`` is "
"set to ``False`` the underlying implementation of :class:`LinearSVC` is "
"not random and ``random_state`` has no effect on the results."
msgstr ""

#: ../modules/svm.rst:456
msgid ""
"Using L1 penalization as provided by ``LinearSVC(penalty='l1', "
"dual=False)`` yields a sparse solution, i.e. only a subset of feature "
"weights is different from zero and contribute to the decision function. "
"Increasing ``C`` yields a more complex model (more features are "
"selected). The ``C`` value that yields a \"null\" model (all weights "
"equal to zero) can be calculated using :func:`l1_min_c`."
msgstr ""

#: ../modules/svm.rst:467
msgid "Kernel functions"
msgstr ""

#: ../modules/svm.rst:469
msgid "The *kernel function* can be any of the following:"
msgstr ""

#: ../modules/svm.rst:471
msgid "linear: :math:`\\langle x, x'\\rangle`."
msgstr ""

#: ../modules/svm.rst:473
msgid ""
"polynomial: :math:`(\\gamma \\langle x, x'\\rangle + r)^d`, where "
":math:`d` is specified by parameter ``degree``, :math:`r` by ``coef0``."
msgstr ""

#: ../modules/svm.rst:476
msgid ""
"rbf: :math:`\\exp(-\\gamma \\|x-x'\\|^2)`, where :math:`\\gamma` is "
"specified by parameter ``gamma``, must be greater than 0."
msgstr ""

#: ../modules/svm.rst:479
msgid ""
"sigmoid :math:`\\tanh(\\gamma \\langle x,x'\\rangle + r)`, where "
":math:`r` is specified by ``coef0``."
msgstr ""

#: ../modules/svm.rst:482
msgid "Different kernels are specified by the `kernel` parameter::"
msgstr ""

#: ../modules/svm.rst:492
msgid "Parameters of the RBF Kernel"
msgstr ""

#: ../modules/svm.rst:494
msgid ""
"When training an SVM with the *Radial Basis Function* (RBF) kernel, two "
"parameters must be considered: ``C`` and ``gamma``.  The parameter ``C``,"
" common to all SVM kernels, trades off misclassification of training "
"examples against simplicity of the decision surface. A low ``C`` makes "
"the decision surface smooth, while a high ``C`` aims at classifying all "
"training examples correctly.  ``gamma`` defines how much influence a "
"single training example has. The larger ``gamma`` is, the closer other "
"examples must be to be affected."
msgstr ""

#: ../modules/svm.rst:502
msgid ""
"Proper choice of ``C`` and ``gamma`` is critical to the SVM's "
"performance.  One is advised to use "
":class:`~sklearn.model_selection.GridSearchCV` with ``C`` and ``gamma`` "
"spaced exponentially far apart to choose good values."
msgstr ""

#: ../modules/svm.rst:508
msgid ":ref:`sphx_glr_auto_examples_svm_plot_rbf_parameters.py`"
msgstr ""

#: ../modules/svm.rst:513
msgid "Custom Kernels"
msgstr ""

#: ../modules/svm.rst:515
msgid ""
"You can define your own kernels by either giving the kernel as a python "
"function or by precomputing the Gram matrix."
msgstr ""

#: ../modules/svm.rst:518
msgid ""
"Classifiers with custom kernels behave the same way as any other "
"classifiers, except that:"
msgstr ""

#: ../modules/svm.rst:521
msgid ""
"Field ``support_vectors_`` is now empty, only indices of support vectors "
"are stored in ``support_``"
msgstr ""

#: ../modules/svm.rst:524
msgid ""
"A reference (and not a copy) of the first argument in the ``fit()`` "
"method is stored for future reference. If that array changes between the "
"use of ``fit()`` and ``predict()`` you will have unexpected results."
msgstr ""

#: ../modules/svm.rst:530
msgid "Using Python functions as kernels"
msgstr ""

#: ../modules/svm.rst:532
msgid ""
"You can use your own defined kernels by passing a function to the "
"``kernel`` parameter."
msgstr ""

#: ../modules/svm.rst:535
msgid ""
"Your kernel must take as arguments two matrices of shape ``(n_samples_1, "
"n_features)``, ``(n_samples_2, n_features)`` and return a kernel matrix "
"of shape ``(n_samples_1, n_samples_2)``."
msgstr ""

#: ../modules/svm.rst:539
msgid ""
"The following code defines a linear kernel and creates a classifier "
"instance that will use that kernel::"
msgstr ""

#: ../modules/svm.rst:551
msgid ":ref:`sphx_glr_auto_examples_svm_plot_custom_kernel.py`."
msgstr ""

#: ../modules/svm.rst:554
msgid "Using the Gram matrix"
msgstr ""

#: ../modules/svm.rst:556
msgid ""
"You can pass pre-computed kernels by using the ``kernel='precomputed'`` "
"option. You should then pass Gram matrix instead of X to the `fit` and "
"`predict` methods. The kernel values between *all* training vectors and "
"the test vectors must be provided:"
msgstr ""

#: ../modules/svm.rst:581
msgid "Mathematical formulation"
msgstr ""

#: ../modules/svm.rst:583
msgid ""
"A support vector machine constructs a hyper-plane or set of hyper-planes "
"in a high or infinite dimensional space, which can be used for "
"classification, regression or other tasks. Intuitively, a good separation"
" is achieved by the hyper-plane that has the largest distance to the "
"nearest training data points of any class (so-called functional margin), "
"since in general the larger the margin the lower the generalization error"
" of the classifier. The figure below shows the decision function for a "
"linearly separable problem, with three samples on the margin boundaries, "
"called \"support vectors\":"
msgstr ""

#: ../modules/svm.rst:597
msgid ""
"In general, when the problem isn't linearly separable, the support "
"vectors are the samples *within* the margin boundaries."
msgstr ""

#: ../modules/svm.rst:600
msgid ""
"We recommend [#5]_ and [#6]_ as good references for the theory and "
"practicalities of SVMs."
msgstr ""

#: ../modules/svm.rst:604
msgid "SVC"
msgstr ""

#: ../modules/svm.rst:606
msgid ""
"Given training vectors :math:`x_i \\in \\mathbb{R}^p`, i=1,..., n, in two"
" classes, and a vector :math:`y \\in \\{1, -1\\}^n`, our goal is to find "
":math:`w \\in \\mathbb{R}^p` and :math:`b \\in \\mathbb{R}` such that the"
" prediction given by :math:`\\text{sign} (w^T\\phi(x) + b)` is correct "
"for most samples."
msgstr ""

#: ../modules/svm.rst:611
msgid "SVC solves the following primal problem:"
msgstr ""

#: ../modules/svm.rst:613
msgid ""
"\\min_ {w, b, \\zeta} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} \\zeta_i\n"
"\n"
"\\textrm {subject to } & y_i (w^T \\phi (x_i) + b) \\geq 1 - "
"\\zeta_i,\\\\\n"
"& \\zeta_i \\geq 0, i=1, ..., n"
msgstr ""

#: ../modules/svm.rst:620
msgid ""
"Intuitively, we're trying to maximize the margin (by minimizing "
":math:`||w||^2 = w^Tw`), while incurring a penalty when a sample is "
"misclassified or within the margin boundary. Ideally, the value "
":math:`y_i (w^T \\phi (x_i) + b)` would be :math:`\\geq 1` for all "
"samples, which indicates a perfect prediction. But problems are usually "
"not always perfectly separable with a hyperplane, so we allow some "
"samples to be at a distance :math:`\\zeta_i` from their correct margin "
"boundary. The penalty term `C` controls the strengh of this penalty, and "
"as a result, acts as an inverse regularization parameter (see note "
"below)."
msgstr ""

#: ../modules/svm.rst:630
msgid "The dual problem to the primal is"
msgstr ""

#: ../modules/svm.rst:632
msgid ""
"\\min_{\\alpha} \\frac{1}{2} \\alpha^T Q \\alpha - e^T \\alpha\n"
"\n"
"\n"
"\\textrm {subject to } & y^T \\alpha = 0\\\\\n"
"& 0 \\leq \\alpha_i \\leq C, i=1, ..., n"
msgstr ""

#: ../modules/svm.rst:640
msgid ""
"where :math:`e` is the vector of all ones, and :math:`Q` is an :math:`n` "
"by :math:`n` positive semidefinite matrix, :math:`Q_{ij} \\equiv y_i y_j "
"K(x_i, x_j)`, where :math:`K(x_i, x_j) = \\phi (x_i)^T \\phi (x_j)` is "
"the kernel. The terms :math:`\\alpha_i` are called the dual coefficients,"
" and they are upper-bounded by :math:`C`. This dual representation "
"highlights the fact that training vectors are implicitly mapped into a "
"higher (maybe infinite) dimensional space by the function :math:`\\phi`: "
"see `kernel trick <https://en.wikipedia.org/wiki/Kernel_method>`_."
msgstr ""

#: ../modules/svm.rst:650
msgid ""
"Once the optimization problem is solved, the output of "
":term:`decision_function` for a given sample :math:`x` becomes:"
msgstr ""

#: ../modules/svm.rst:653
msgid ""
"\\sum_{i\\in SV} y_i \\alpha_i K(x_i, x) + b,\n"
"\n"
msgstr ""

#: ../modules/svm.rst:655
msgid ""
"and the predicted class correspond to its sign. We only need to sum over "
"the support vectors (i.e. the samples that lie within the margin) because"
" the dual coefficients :math:`\\alpha_i` are zero for the other samples."
msgstr ""

#: ../modules/svm.rst:659
msgid ""
"These parameters can be accessed through the attributes ``dual_coef_`` "
"which holds the product :math:`y_i \\alpha_i`, ``support_vectors_`` which"
" holds the support vectors, and ``intercept_`` which holds the "
"independent term :math:`b`"
msgstr ""

#: ../modules/svm.rst:666
msgid ""
"While SVM models derived from `libsvm`_ and `liblinear`_ use ``C`` as "
"regularization parameter, most other estimators use ``alpha``. The exact "
"equivalence between the amount of regularization of two models depends on"
" the exact objective function optimized by the model. For example, when "
"the estimator used is :class:`~sklearn.linear_model.Ridge` regression, "
"the relation between them is given as :math:`C = \\frac{1}{alpha}`."
msgstr ""

#: ../modules/svm.rst:674
msgid "LinearSVC"
msgstr ""

#: ../modules/svm.rst:676 ../modules/svm.rst:756
msgid "The primal problem can be equivalently formulated as"
msgstr ""

#: ../modules/svm.rst:678
msgid ""
"\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, y_i (w^T "
"\\phi(x_i) + b)),"
msgstr ""

#: ../modules/svm.rst:682
msgid ""
"where we make use of the `hinge loss "
"<https://en.wikipedia.org/wiki/Hinge_loss>`_. This is the form that is "
"directly optimized by :class:`LinearSVC`, but unlike the dual form, this "
"one does not involve inner products between samples, so the famous kernel"
" trick cannot be applied. This is why only the linear kernel is supported"
" by :class:`LinearSVC` (:math:`\\phi` is the identity function)."
msgstr ""

#: ../modules/svm.rst:692
msgid "NuSVC"
msgstr ""

#: ../modules/svm.rst:694
msgid ""
"The :math:`\\nu`-SVC formulation [#7]_ is a reparameterization of the "
":math:`C`-SVC and therefore mathematically equivalent."
msgstr ""

#: ../modules/svm.rst:697
msgid ""
"We introduce a new parameter :math:`\\nu` (instead of :math:`C`) which "
"controls the number of support vectors and *margin errors*: :math:`\\nu "
"\\in (0, 1]` is an upper bound on the fraction of margin errors and a "
"lower bound of the fraction of support vectors. A margin error "
"corresponds to a sample that lies on the wrong side of its margin "
"boundary: it is either misclassified, or it is correctly classified but "
"does not lie beyond the margin."
msgstr ""

#: ../modules/svm.rst:707
msgid "SVR"
msgstr ""

#: ../modules/svm.rst:709
msgid ""
"Given training vectors :math:`x_i \\in \\mathbb{R}^p`, i=1,..., n, and a "
"vector :math:`y \\in \\mathbb{R}^n` :math:`\\varepsilon`-SVR solves the "
"following primal problem:"
msgstr ""

#: ../modules/svm.rst:713
msgid ""
"\\min_ {w, b, \\zeta, \\zeta^*} \\frac{1}{2} w^T w + C \\sum_{i=1}^{n} "
"(\\zeta_i + \\zeta_i^*)\n"
"\n"
"\n"
"\n"
"\\textrm {subject to } & y_i - w^T \\phi (x_i) - b \\leq \\varepsilon + "
"\\zeta_i,\\\\\n"
"                      & w^T \\phi (x_i) + b - y_i \\leq \\varepsilon + "
"\\zeta_i^*,\\\\\n"
"                      & \\zeta_i, \\zeta_i^* \\geq 0, i=1, ..., n"
msgstr ""

#: ../modules/svm.rst:723
msgid ""
"Here, we are penalizing samples whose prediction is at least "
":math:`\\varepsilon` away from their true target. These samples penalize "
"the objective by :math:`\\zeta_i` or :math:`\\zeta_i^*`, depending on "
"whether their predictions lie above or below the :math:`\\varepsilon` "
"tube."
msgstr ""

#: ../modules/svm.rst:728
msgid "The dual problem is"
msgstr ""

#: ../modules/svm.rst:730
msgid ""
"\\min_{\\alpha, \\alpha^*} \\frac{1}{2} (\\alpha - \\alpha^*)^T Q "
"(\\alpha - \\alpha^*) + \\varepsilon e^T (\\alpha + \\alpha^*) - y^T "
"(\\alpha - \\alpha^*)\n"
"\n"
"\n"
"\\textrm {subject to } & e^T (\\alpha - \\alpha^*) = 0\\\\\n"
"& 0 \\leq \\alpha_i, \\alpha_i^* \\leq C, i=1, ..., n"
msgstr ""

#: ../modules/svm.rst:738
msgid ""
"where :math:`e` is the vector of all ones, :math:`Q` is an :math:`n` by "
":math:`n` positive semidefinite matrix, :math:`Q_{ij} \\equiv K(x_i, x_j)"
" = \\phi (x_i)^T \\phi (x_j)` is the kernel. Here training vectors are "
"implicitly mapped into a higher (maybe infinite) dimensional space by the"
" function :math:`\\phi`."
msgstr ""

#: ../modules/svm.rst:744
msgid "The prediction is:"
msgstr ""

#: ../modules/svm.rst:746
msgid ""
"\\sum_{i \\in SV}(\\alpha_i - \\alpha_i^*) K(x_i, x) + b\n"
"\n"
msgstr ""

#: ../modules/svm.rst:748
msgid ""
"These parameters can be accessed through the attributes ``dual_coef_`` "
"which holds the difference :math:`\\alpha_i - \\alpha_i^*`, "
"``support_vectors_`` which holds the support vectors, and ``intercept_`` "
"which holds the independent term :math:`b`"
msgstr ""

#: ../modules/svm.rst:754
msgid "LinearSVR"
msgstr ""

#: ../modules/svm.rst:758
msgid ""
"\\min_ {w, b} \\frac{1}{2} w^T w + C \\sum_{i=1}\\max(0, |y_i - (w^T "
"\\phi(x_i) + b)| - \\varepsilon),"
msgstr ""

#: ../modules/svm.rst:762
msgid ""
"where we make use of the epsilon-insensitive loss, i.e. errors of less "
"than :math:`\\varepsilon` are ignored. This is the form that is directly "
"optimized by :class:`LinearSVR`."
msgstr ""

#: ../modules/svm.rst:769
msgid "Implementation details"
msgstr ""

#: ../modules/svm.rst:771
msgid ""
"Internally, we use `libsvm`_ [#4]_ and `liblinear`_ [#3]_ to handle all "
"computations. These libraries are wrapped using C and Cython. For a "
"description of the implementation and details of the algorithms used, "
"please refer to their respective papers."
msgstr ""

#: ../modules/svm.rst:782
msgid ""
"Platt `\"Probabilistic outputs for SVMs and comparisons to regularized "
"likelihood methods\" "
"<https://www.cs.colorado.edu/~mozer/Teaching/syllabi/6622/papers/Platt1999.pdf>`_."
msgstr ""

#: ../modules/svm.rst:786
msgid ""
"Wu, Lin and Weng, `\"Probability estimates for multi-class classification"
" by pairwise coupling\" "
"<https://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf>`_, JMLR "
"5:975-1005, 2004."
msgstr ""

#: ../modules/svm.rst:791
msgid ""
"Fan, Rong-En, et al., `\"LIBLINEAR: A library for large linear "
"classification.\" "
"<https://www.csie.ntu.edu.tw/~cjlin/papers/liblinear.pdf>`_, Journal of "
"machine learning research 9.Aug (2008): 1871-1874."
msgstr ""

#: ../modules/svm.rst:796
msgid ""
"Chang and Lin, `LIBSVM: A Library for Support Vector Machines "
"<https://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf>`_."
msgstr ""

#: ../modules/svm.rst:799
msgid ""
"Bishop, `Pattern recognition and machine learning "
"<https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-"
"Pattern-Recognition-and-Machine-Learning-2006.pdf>`_, chapter 7 Sparse "
"Kernel Machines"
msgstr ""

#: ../modules/svm.rst:803
msgid ""
"`\"A Tutorial on Support Vector Regression\" "
"<http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.114.4288>`_, "
"Alex J. Smola, Bernhard Schölkopf - Statistics and Computing archive "
"Volume 14 Issue 3, August 2004, p. 199-222."
msgstr ""

#: ../modules/svm.rst:808
msgid ""
"Schölkopf et. al `New Support Vector Algorithms "
"<https://www.stat.purdue.edu/~yuzhu/stat598m3/Papers/NewSVM.pdf>`_"
msgstr ""

#: ../modules/svm.rst:811
msgid ""
"Crammer and Singer `On the Algorithmic Implementation ofMulticlass "
"Kernel-based Vector Machines "
"<http://jmlr.csail.mit.edu/papers/volume2/crammer01a/crammer01a.pdf>`_, "
"JMLR 2001."
msgstr ""

