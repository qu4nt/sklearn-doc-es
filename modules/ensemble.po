# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/ensemble.rst:5
msgid "Ensemble methods"
msgstr ""

#: ../modules/ensemble.rst:9
msgid ""
"The goal of **ensemble methods** is to combine the predictions of several"
" base estimators built with a given learning algorithm in order to "
"improve generalizability / robustness over a single estimator."
msgstr ""

#: ../modules/ensemble.rst:13
msgid "Two families of ensemble methods are usually distinguished:"
msgstr ""

#: ../modules/ensemble.rst:15
msgid ""
"In **averaging methods**, the driving principle is to build several "
"estimators independently and then to average their predictions. On "
"average, the combined estimator is usually better than any of the single "
"base estimator because its variance is reduced."
msgstr ""

#: ../modules/ensemble.rst:20
msgid ""
"**Examples:** :ref:`Bagging methods <bagging>`, :ref:`Forests of "
"randomized trees <forest>`, ..."
msgstr ""

#: ../modules/ensemble.rst:22
msgid ""
"By contrast, in **boosting methods**, base estimators are built "
"sequentially and one tries to reduce the bias of the combined estimator. "
"The motivation is to combine several weak models to produce a powerful "
"ensemble."
msgstr ""

#: ../modules/ensemble.rst:26
msgid ""
"**Examples:** :ref:`AdaBoost <adaboost>`, :ref:`Gradient Tree Boosting "
"<gradient_boosting>`, ..."
msgstr ""

#: ../modules/ensemble.rst:32
msgid "Bagging meta-estimator"
msgstr ""

#: ../modules/ensemble.rst:34
msgid ""
"In ensemble algorithms, bagging methods form a class of algorithms which "
"build several instances of a black-box estimator on random subsets of the"
" original training set and then aggregate their individual predictions to"
" form a final prediction. These methods are used as a way to reduce the "
"variance of a base estimator (e.g., a decision tree), by introducing "
"randomization into its construction procedure and then making an ensemble"
" out of it. In many cases, bagging methods constitute a very simple way "
"to improve with respect to a single model, without making it necessary to"
" adapt the underlying base algorithm. As they provide a way to reduce "
"overfitting, bagging methods work best with strong and complex models "
"(e.g., fully developed decision trees), in contrast with boosting methods"
" which usually work best with weak models (e.g., shallow decision trees)."
msgstr ""

#: ../modules/ensemble.rst:47
msgid ""
"Bagging methods come in many flavours but mostly differ from each other "
"by the way they draw random subsets of the training set:"
msgstr ""

#: ../modules/ensemble.rst:50
msgid ""
"When random subsets of the dataset are drawn as random subsets of the "
"samples, then this algorithm is known as Pasting [B1999]_."
msgstr ""

#: ../modules/ensemble.rst:53
msgid ""
"When samples are drawn with replacement, then the method is known as "
"Bagging [B1996]_."
msgstr ""

#: ../modules/ensemble.rst:56
msgid ""
"When random subsets of the dataset are drawn as random subsets of the "
"features, then the method is known as Random Subspaces [H1998]_."
msgstr ""

#: ../modules/ensemble.rst:59
msgid ""
"Finally, when base estimators are built on subsets of both samples and "
"features, then the method is known as Random Patches [LG2012]_."
msgstr ""

#: ../modules/ensemble.rst:62
#, python-format
msgid ""
"In scikit-learn, bagging methods are offered as a unified "
":class:`BaggingClassifier` meta-estimator  (resp. "
":class:`BaggingRegressor`), taking as input a user-specified base "
"estimator along with parameters specifying the strategy to draw random "
"subsets. In particular, ``max_samples`` and ``max_features`` control the "
"size of the subsets (in terms of samples and features), while "
"``bootstrap`` and ``bootstrap_features`` control whether samples and "
"features are drawn with or without replacement. When using a subset of "
"the available samples the generalization accuracy can be estimated with "
"the out-of-bag samples by setting ``oob_score=True``. As an example, the "
"snippet below illustrates how to instantiate a bagging ensemble of "
":class:`KNeighborsClassifier` base estimators, each built on random "
"subsets of 50% of the samples and 50% of the features."
msgstr ""

#: ../modules/ensemble.rst:82
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_bias_variance.py`"
msgstr ""

#: ../modules/ensemble.rst:86
msgid ""
"L. Breiman, \"Pasting small votes for classification in large databases "
"and on-line\", Machine Learning, 36(1), 85-103, 1999."
msgstr ""

#: ../modules/ensemble.rst:89
msgid ""
"L. Breiman, \"Bagging predictors\", Machine Learning, 24(2), 123-140, "
"1996."
msgstr ""

#: ../modules/ensemble.rst:92
msgid ""
"T. Ho, \"The random subspace method for constructing decision forests\", "
"Pattern Analysis and Machine Intelligence, 20(8), 832-844, 1998."
msgstr ""

#: ../modules/ensemble.rst:96
msgid ""
"G. Louppe and P. Geurts, \"Ensembles on Random Patches\", Machine "
"Learning and Knowledge Discovery in Databases, 346-361, 2012."
msgstr ""

#: ../modules/ensemble.rst:102
msgid "Forests of randomized trees"
msgstr ""

#: ../modules/ensemble.rst:104
msgid ""
"The :mod:`sklearn.ensemble` module includes two averaging algorithms "
"based on randomized :ref:`decision trees <tree>`: the RandomForest "
"algorithm and the Extra-Trees method. Both algorithms are perturb-and-"
"combine techniques [B1998]_ specifically designed for trees. This means a"
" diverse set of classifiers is created by introducing randomness in the "
"classifier construction.  The prediction of the ensemble is given as the "
"averaged prediction of the individual classifiers."
msgstr ""

#: ../modules/ensemble.rst:112
msgid ""
"As other classifiers, forest classifiers have to be fitted with two "
"arrays: a sparse or dense array X of shape ``(n_samples, n_features)`` "
"holding the training samples, and an array Y of shape ``(n_samples,)`` "
"holding the target values (class labels) for the training samples::"
msgstr ""

#: ../modules/ensemble.rst:123
msgid ""
"Like :ref:`decision trees <tree>`, forests of trees also extend to :ref"
":`multi-output problems <tree_multioutput>`  (if Y is an array of shape "
"``(n_samples, n_outputs)``)."
msgstr ""

#: ../modules/ensemble.rst:128
msgid "Random Forests"
msgstr ""

#: ../modules/ensemble.rst:130
msgid ""
"In random forests (see :class:`RandomForestClassifier` and "
":class:`RandomForestRegressor` classes), each tree in the ensemble is "
"built from a sample drawn with replacement (i.e., a bootstrap sample) "
"from the training set."
msgstr ""

#: ../modules/ensemble.rst:135
msgid ""
"Furthermore, when splitting each node during the construction of a tree, "
"the best split is found either from all input features or a random subset"
" of size ``max_features``. (See the :ref:`parameter tuning guidelines "
"<random_forest_parameters>` for more details)."
msgstr ""

#: ../modules/ensemble.rst:140
msgid ""
"The purpose of these two sources of randomness is to decrease the "
"variance of the forest estimator. Indeed, individual decision trees "
"typically exhibit high variance and tend to overfit. The injected "
"randomness in forests yield decision trees with somewhat decoupled "
"prediction errors. By taking an average of those predictions, some errors"
" can cancel out. Random forests achieve a reduced variance by combining "
"diverse trees, sometimes at the cost of a slight increase in bias. In "
"practice the variance reduction is often significant hence yielding an "
"overall better model."
msgstr ""

#: ../modules/ensemble.rst:149
msgid ""
"In contrast to the original publication [B2001]_, the scikit-learn "
"implementation combines classifiers by averaging their probabilistic "
"prediction, instead of letting each classifier vote for a single class."
msgstr ""

#: ../modules/ensemble.rst:154
msgid "Extremely Randomized Trees"
msgstr ""

#: ../modules/ensemble.rst:156
msgid ""
"In extremely randomized trees (see :class:`ExtraTreesClassifier` and "
":class:`ExtraTreesRegressor` classes), randomness goes one step further "
"in the way splits are computed. As in random forests, a random subset of "
"candidate features is used, but instead of looking for the most "
"discriminative thresholds, thresholds are drawn at random for each "
"candidate feature and the best of these randomly-generated thresholds is "
"picked as the splitting rule. This usually allows to reduce the variance "
"of the model a bit more, at the expense of a slightly greater increase in"
" bias::"
msgstr ""

#: ../modules/ensemble.rst:201
msgid "Parameters"
msgstr ""

#: ../modules/ensemble.rst:203
msgid ""
"The main parameters to adjust when using these methods is "
"``n_estimators`` and ``max_features``. The former is the number of trees "
"in the forest. The larger the better, but also the longer it will take to"
" compute. In addition, note that results will stop getting significantly "
"better beyond a critical number of trees. The latter is the size of the "
"random subsets of features to consider when splitting a node. The lower "
"the greater the reduction of variance, but also the greater the increase "
"in bias. Empirical good default values are ``max_features=None`` (always "
"considering all features instead of a random subset) for regression "
"problems, and ``max_features=\"sqrt\"`` (using a random subset of size "
"``sqrt(n_features)``) for classification tasks (where ``n_features`` is "
"the number of features in the data). Good results are often achieved when"
" setting ``max_depth=None`` in combination with ``min_samples_split=2`` "
"(i.e., when fully developing the trees). Bear in mind though that these "
"values are usually not optimal, and might result in models that consume a"
" lot of RAM. The best parameter values should always be cross-validated. "
"In addition, note that in random forests, bootstrap samples are used by "
"default (``bootstrap=True``) while the default strategy for extra-trees "
"is to use the whole dataset (``bootstrap=False``). When using bootstrap "
"sampling the generalization accuracy can be estimated on the left out or "
"out-of-bag samples. This can be enabled by setting ``oob_score=True``."
msgstr ""

#: ../modules/ensemble.rst:226
msgid ""
"The size of the model with the default parameters is :math:`O( M * N * "
"log (N) )`, where :math:`M` is the number of trees and :math:`N` is the "
"number of samples. In order to reduce the size of the model, you can "
"change these parameters: ``min_samples_split``, ``max_leaf_nodes``, "
"``max_depth`` and ``min_samples_leaf``."
msgstr ""

#: ../modules/ensemble.rst:232
msgid "Parallelization"
msgstr ""

#: ../modules/ensemble.rst:234
msgid ""
"Finally, this module also features the parallel construction of the trees"
" and the parallel computation of the predictions through the ``n_jobs`` "
"parameter. If ``n_jobs=k`` then computations are partitioned into ``k`` "
"jobs, and run on ``k`` cores of the machine. If ``n_jobs=-1`` then all "
"cores available on the machine are used. Note that because of inter-"
"process communication overhead, the speedup might not be linear (i.e., "
"using ``k`` jobs will unfortunately not be ``k`` times as fast). "
"Significant speedup can still be achieved though when building a large "
"number of trees, or when building a single tree requires a fair amount of"
" time (e.g., on large datasets)."
msgstr ""

#: ../modules/ensemble.rst:247
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_iris.py`"
msgstr ""

#: ../modules/ensemble.rst:248 ../modules/ensemble.rst:312
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances_faces.py`"
msgstr ""

#: ../modules/ensemble.rst:249
msgid ":ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`"
msgstr ""

#: ../modules/ensemble.rst:253
msgid "Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001."
msgstr ""

#: ../modules/ensemble.rst:255
msgid "Breiman, \"Arcing Classifiers\", Annals of Statistics 1998."
msgstr ""

#: ../modules/ensemble.rst:257
msgid ""
"P. Geurts, D. Ernst., and L. Wehenkel, \"Extremely randomized trees\", "
"Machine Learning, 63(1), 3-42, 2006."
msgstr ""

#: ../modules/ensemble.rst:263
msgid "Feature importance evaluation"
msgstr ""

#: ../modules/ensemble.rst:265
msgid ""
"The relative rank (i.e. depth) of a feature used as a decision node in a "
"tree can be used to assess the relative importance of that feature with "
"respect to the predictability of the target variable. Features used at "
"the top of the tree contribute to the final prediction decision of a "
"larger fraction of the input samples. The **expected fraction of the "
"samples** they contribute to can thus be used as an estimate of the "
"**relative importance of the features**. In scikit-learn, the fraction of"
" samples a feature contributes to is combined with the decrease in "
"impurity from splitting them to create a normalized estimate of the "
"predictive power of that feature."
msgstr ""

#: ../modules/ensemble.rst:276
msgid ""
"By **averaging** the estimates of predictive ability over several "
"randomized trees one can **reduce the variance** of such an estimate and "
"use it for feature selection. This is known as the mean decrease in "
"impurity, or MDI. Refer to [L2014]_ for more information on MDI and "
"feature importance evaluation with Random Forests."
msgstr ""

#: ../modules/ensemble.rst:284
msgid ""
"The impurity-based feature importances computed on tree-based models "
"suffer from two flaws that can lead to misleading conclusions. First they"
" are computed on statistics derived from the training dataset and "
"therefore **do not necessarily inform us on which features are most "
"important to make good predictions on held-out dataset**. Secondly, "
"**they favor high cardinality features**, that is features with many "
"unique values. :ref:`permutation_importance` is an alternative to "
"impurity-based feature importance that does not suffer from these flaws. "
"These two methods of obtaining feature importance are explored in: "
":ref:`sphx_glr_auto_examples_inspection_plot_permutation_importance.py`."
msgstr ""

#: ../modules/ensemble.rst:295
msgid ""
"The following example shows a color-coded representation of the relative "
"importances of each individual pixel for a face recognition task using a "
":class:`ExtraTreesClassifier` model."
msgstr ""

#: ../modules/ensemble.rst:304
msgid ""
"In practice those estimates are stored as an attribute named "
"``feature_importances_`` on the fitted model. This is an array with shape"
" ``(n_features,)`` whose values are positive and sum to 1.0. The higher "
"the value, the more important is the contribution of the matching feature"
" to the prediction function."
msgstr ""

#: ../modules/ensemble.rst:313
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_forest_importances.py`"
msgstr ""

#: ../modules/ensemble.rst:317
msgid ""
"G. Louppe, \"Understanding Random Forests: From Theory to Practice\", PhD"
" Thesis, U. of Liege, 2014."
msgstr ""

#: ../modules/ensemble.rst:324
msgid "Totally Random Trees Embedding"
msgstr ""

#: ../modules/ensemble.rst:326
msgid ""
":class:`RandomTreesEmbedding` implements an unsupervised transformation "
"of the data.  Using a forest of completely random trees, "
":class:`RandomTreesEmbedding` encodes the data by the indices of the "
"leaves a data point ends up in.  This index is then encoded in a one-of-K"
" manner, leading to a high dimensional, sparse binary coding. This coding"
" can be computed very efficiently and can then be used as a basis for "
"other learning tasks. The size and sparsity of the code can be influenced"
" by choosing the number of trees and the maximum depth per tree. For each"
" tree in the ensemble, the coding contains one entry of one. The size of "
"the coding is at most ``n_estimators * 2 ** max_depth``, the maximum "
"number of leaves in the forest."
msgstr ""

#: ../modules/ensemble.rst:338
msgid ""
"As neighboring data points are more likely to lie within the same leaf of"
" a tree, the transformation performs an implicit, non-parametric density "
"estimation."
msgstr ""

#: ../modules/ensemble.rst:344
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_random_forest_embedding.py`"
msgstr ""

#: ../modules/ensemble.rst:346
msgid ""
":ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py` compares non-"
"linear dimensionality reduction techniques on handwritten digits."
msgstr ""

#: ../modules/ensemble.rst:349
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_feature_transformation.py` "
"compares supervised and unsupervised tree based feature transformations."
msgstr ""

#: ../modules/ensemble.rst:354
msgid ""
":ref:`manifold` techniques can also be useful to derive non-linear "
"representations of feature space, also these approaches focus also on "
"dimensionality reduction."
msgstr ""

#: ../modules/ensemble.rst:362
msgid "AdaBoost"
msgstr ""

#: ../modules/ensemble.rst:364
msgid ""
"The module :mod:`sklearn.ensemble` includes the popular boosting "
"algorithm AdaBoost, introduced in 1995 by Freund and Schapire [FS1995]_."
msgstr ""

#: ../modules/ensemble.rst:367
msgid ""
"The core principle of AdaBoost is to fit a sequence of weak learners "
"(i.e., models that are only slightly better than random guessing, such as"
" small decision trees) on repeatedly modified versions of the data. The "
"predictions from all of them are then combined through a weighted "
"majority vote (or sum) to produce the final prediction. The data "
"modifications at each so-called boosting iteration consist of applying "
"weights :math:`w_1`, :math:`w_2`, ..., :math:`w_N` to each of the "
"training samples. Initially, those weights are all set to :math:`w_i = "
"1/N`, so that the first step simply trains a weak learner on the original"
" data. For each successive iteration, the sample weights are individually"
" modified and the learning algorithm is reapplied to the reweighted data."
" At a given step, those training examples that were incorrectly predicted"
" by the boosted model induced at the previous step have their weights "
"increased, whereas the weights are decreased for those that were "
"predicted correctly. As iterations proceed, examples that are difficult "
"to predict receive ever-increasing influence. Each subsequent weak "
"learner is thereby forced to concentrate on the examples that are missed "
"by the previous ones in the sequence [HTF]_."
msgstr ""

#: ../modules/ensemble.rst:390
msgid "AdaBoost can be used both for classification and regression problems:"
msgstr ""

#: ../modules/ensemble.rst:392
msgid ""
"For multi-class classification, :class:`AdaBoostClassifier` implements "
"AdaBoost-SAMME and AdaBoost-SAMME.R [ZZRH2009]_."
msgstr ""

#: ../modules/ensemble.rst:395
msgid ""
"For regression, :class:`AdaBoostRegressor` implements AdaBoost.R2 "
"[D1997]_."
msgstr ""

#: ../modules/ensemble.rst:398 ../modules/ensemble.rst:934
#: ../modules/ensemble.rst:1267 ../modules/ensemble.rst:1384
#: ../modules/ensemble.rst:1413
msgid "Usage"
msgstr ""

#: ../modules/ensemble.rst:400
msgid ""
"The following example shows how to fit an AdaBoost classifier with 100 "
"weak learners::"
msgstr ""

#: ../modules/ensemble.rst:413
msgid ""
"The number of weak learners is controlled by the parameter "
"``n_estimators``. The ``learning_rate`` parameter controls the "
"contribution of the weak learners in the final combination. By default, "
"weak learners are decision stumps. Different weak learners can be "
"specified through the ``base_estimator`` parameter. The main parameters "
"to tune to obtain good results are ``n_estimators`` and the complexity of"
" the base estimators (e.g., its depth ``max_depth`` or minimum required "
"number of samples to consider a split ``min_samples_split``)."
msgstr ""

#: ../modules/ensemble.rst:423
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_hastie_10_2.py` "
"compares the classification error of a decision stump, decision tree, and"
" a boosted decision stump using AdaBoost-SAMME and AdaBoost-SAMME.R."
msgstr ""

#: ../modules/ensemble.rst:427
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_multiclass.py` shows "
"the performance of AdaBoost-SAMME and AdaBoost-SAMME.R on a multi-class "
"problem."
msgstr ""

#: ../modules/ensemble.rst:430
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_twoclass.py` shows "
"the decision boundary and decision function values for a non-linearly "
"separable two-class problem using AdaBoost-SAMME."
msgstr ""

#: ../modules/ensemble.rst:434
msgid ""
":ref:`sphx_glr_auto_examples_ensemble_plot_adaboost_regression.py` "
"demonstrates regression with the AdaBoost.R2 algorithm."
msgstr ""

#: ../modules/ensemble.rst:439
msgid ""
"Y. Freund, and R. Schapire, \"A Decision-Theoretic Generalization of On-"
"Line Learning and an Application to Boosting\", 1997."
msgstr ""

#: ../modules/ensemble.rst:442
msgid "J. Zhu, H. Zou, S. Rosset, T. Hastie. \"Multi-class AdaBoost\", 2009."
msgstr ""

#: ../modules/ensemble.rst:445
msgid "Drucker. \"Improving Regressors using Boosting Techniques\", 1997."
msgstr ""

#: ../modules/ensemble.rst:447
msgid ""
"T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical "
"Learning Ed. 2\", Springer, 2009."
msgstr ""

#: ../modules/ensemble.rst:454
msgid "Gradient Tree Boosting"
msgstr ""

#: ../modules/ensemble.rst:456
msgid ""
"`Gradient Tree Boosting "
"<https://en.wikipedia.org/wiki/Gradient_boosting>`_ or Gradient Boosted "
"Decision Trees (GBDT) is a generalization of boosting to arbitrary "
"differentiable loss functions. GBDT is an accurate and effective off-the-"
"shelf procedure that can be used for both regression and classification "
"problems in a variety of areas including Web search ranking and ecology."
msgstr ""

#: ../modules/ensemble.rst:464
msgid ""
"The module :mod:`sklearn.ensemble` provides methods for both "
"classification and regression via gradient boosted decision trees."
msgstr ""

#: ../modules/ensemble.rst:470
msgid ""
"Scikit-learn 0.21 introduces two new experimental implementations of "
"gradient boosting trees, namely :class:`HistGradientBoostingClassifier` "
"and :class:`HistGradientBoostingRegressor`, inspired by `LightGBM "
"<https://github.com/Microsoft/LightGBM>`__ (See [LightGBM]_)."
msgstr ""

#: ../modules/ensemble.rst:475 ../modules/ensemble.rst:903
msgid ""
"These histogram-based estimators can be **orders of magnitude faster** "
"than :class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor` when the number of samples is larger "
"than tens of thousands of samples."
msgstr ""

#: ../modules/ensemble.rst:480 ../modules/ensemble.rst:908
msgid ""
"They also have built-in support for missing values, which avoids the need"
" for an imputer."
msgstr ""

#: ../modules/ensemble.rst:483
msgid ""
"These estimators are described in more detail below in "
":ref:`histogram_based_gradient_boosting`."
msgstr ""

#: ../modules/ensemble.rst:486
msgid ""
"The following guide focuses on :class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor`, which might be preferred for small "
"sample sizes since binning may lead to split points that are too "
"approximate in this setting."
msgstr ""

#: ../modules/ensemble.rst:492
msgid ""
"The usage and the parameters of :class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor` are described below. The 2 most "
"important parameters of these estimators are `n_estimators` and "
"`learning_rate`."
msgstr ""

#: ../modules/ensemble.rst:497 ../modules/ensemble.rst:713
#: ../modules/ensemble.rst:759
msgid "Classification"
msgstr ""

#: ../modules/ensemble.rst:499
msgid ""
":class:`GradientBoostingClassifier` supports both binary and multi-class "
"classification. The following example shows how to fit a gradient "
"boosting classifier with 100 decision stumps as weak learners::"
msgstr ""

#: ../modules/ensemble.rst:516
msgid ""
"The number of weak learners (i.e. regression trees) is controlled by the "
"parameter ``n_estimators``; :ref:`The size of each tree "
"<gradient_boosting_tree_size>` can be controlled either by setting the "
"tree depth via ``max_depth`` or by setting the number of leaf nodes via "
"``max_leaf_nodes``. The ``learning_rate`` is a hyper-parameter in the "
"range (0.0, 1.0] that controls overfitting via :ref:`shrinkage "
"<gradient_boosting_shrinkage>` ."
msgstr ""

#: ../modules/ensemble.rst:526
msgid ""
"Classification with more than 2 classes requires the induction of "
"``n_classes`` regression trees at each iteration, thus, the total number "
"of induced trees equals ``n_classes * n_estimators``. For datasets with a"
" large number of classes we strongly recommend to use "
":class:`HistGradientBoostingClassifier` as an alternative to "
":class:`GradientBoostingClassifier` ."
msgstr ""

#: ../modules/ensemble.rst:535 ../modules/ensemble.rst:630
#: ../modules/ensemble.rst:742
msgid "Regression"
msgstr ""

#: ../modules/ensemble.rst:537
msgid ""
":class:`GradientBoostingRegressor` supports a number of :ref:`different "
"loss functions <gradient_boosting_loss>` for regression which can be "
"specified via the argument ``loss``; the default loss function for "
"regression is least squares (``'ls'``)."
msgstr ""

#: ../modules/ensemble.rst:557
msgid ""
"The figure below shows the results of applying "
":class:`GradientBoostingRegressor` with least squares loss and 500 base "
"learners to the diabetes dataset "
"(:func:`sklearn.datasets.load_diabetes`). The plot on the left shows the "
"train and test error at each iteration. The train error at each iteration"
" is stored in the :attr:`~GradientBoostingRegressor.train_score_` "
"attribute of the gradient boosting model. The test error at each "
"iterations can be obtained via the "
":meth:`~GradientBoostingRegressor.staged_predict` method which returns a "
"generator that yields the predictions at each stage. Plots like these can"
" be used to determine the optimal number of trees (i.e. ``n_estimators``)"
" by early stopping."
msgstr ""

#: ../modules/ensemble.rst:575 ../modules/ensemble.rst:891
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regression.py`"
msgstr ""

#: ../modules/ensemble.rst:576 ../modules/ensemble.rst:845
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_oob.py`"
msgstr ""

#: ../modules/ensemble.rst:581
msgid "Fitting additional weak-learners"
msgstr ""

#: ../modules/ensemble.rst:583
msgid ""
"Both :class:`GradientBoostingRegressor` and "
":class:`GradientBoostingClassifier` support ``warm_start=True`` which "
"allows you to add more estimators to an already fitted model."
msgstr ""

#: ../modules/ensemble.rst:597
msgid "Controlling the tree size"
msgstr ""

#: ../modules/ensemble.rst:599
msgid ""
"The size of the regression tree base learners defines the level of "
"variable interactions that can be captured by the gradient boosting "
"model. In general, a tree of depth ``h`` can capture interactions of "
"order ``h`` . There are two ways in which the size of the individual "
"regression trees can be controlled."
msgstr ""

#: ../modules/ensemble.rst:605
msgid ""
"If you specify ``max_depth=h`` then complete binary trees of depth ``h`` "
"will be grown. Such trees will have (at most) ``2**h`` leaf nodes and "
"``2**h - 1`` split nodes."
msgstr ""

#: ../modules/ensemble.rst:609
msgid ""
"Alternatively, you can control the tree size by specifying the number of "
"leaf nodes via the parameter ``max_leaf_nodes``. In this case, trees will"
" be grown using best-first search where nodes with the highest "
"improvement in impurity will be expanded first. A tree with "
"``max_leaf_nodes=k`` has ``k - 1`` split nodes and thus can model "
"interactions of up to order ``max_leaf_nodes - 1`` ."
msgstr ""

#: ../modules/ensemble.rst:616
msgid ""
"We found that ``max_leaf_nodes=k`` gives comparable results to "
"``max_depth=k-1`` but is significantly faster to train at the expense of "
"a slightly higher training error. The parameter ``max_leaf_nodes`` "
"corresponds to the variable ``J`` in the chapter on gradient boosting in "
"[F2001]_ and is related to the parameter ``interaction.depth`` in R's gbm"
" package where ``max_leaf_nodes == interaction.depth + 1`` ."
msgstr ""

#: ../modules/ensemble.rst:624
msgid "Mathematical formulation"
msgstr ""

#: ../modules/ensemble.rst:626
msgid ""
"We first present GBRT for regression, and then detail the classification "
"case."
msgstr ""

#: ../modules/ensemble.rst:632
msgid ""
"GBRT regressors are additive models whose prediction :math:`y_i` for a "
"given input :math:`x_i` is of the following form:"
msgstr ""

#: ../modules/ensemble.rst:635
msgid "\\hat{y_i} = F_M(x_i) = \\sum_{m=1}^{M} h_m(x_i)"
msgstr ""

#: ../modules/ensemble.rst:639
msgid ""
"where the :math:`h_m` are estimators called *weak learners* in the "
"context of boosting. Gradient Tree Boosting uses :ref:`decision tree "
"regressors <tree>` of fixed size as weak learners. The constant M "
"corresponds to the `n_estimators` parameter."
msgstr ""

#: ../modules/ensemble.rst:644
msgid "Similar to other boosting algorithms, a GBRT is built in a greedy fashion:"
msgstr ""

#: ../modules/ensemble.rst:646
msgid "F_m(x) = F_{m-1}(x) + h_m(x),"
msgstr ""

#: ../modules/ensemble.rst:650
msgid ""
"where the newly added tree :math:`h_m` is fitted in order to minimize a "
"sum of losses :math:`L_m`, given the previous ensemble :math:`F_{m-1}`:"
msgstr ""

#: ../modules/ensemble.rst:653
msgid ""
"h_m =  \\arg\\min_{h} L_m = \\arg\\min_{h} \\sum_{i=1}^{n}\n"
"l(y_i, F_{m-1}(x_i) + h(x_i)),"
msgstr ""

#: ../modules/ensemble.rst:658
msgid ""
"where :math:`l(y_i, F(x_i))` is defined by the `loss` parameter, detailed"
" in the next section."
msgstr ""

#: ../modules/ensemble.rst:661
msgid ""
"By default, the initial model :math:`F_{0}` is chosen as the constant "
"that minimizes the loss: for a least-squares loss, this is the empirical "
"mean of the target values. The initial model can also be specified via "
"the ``init`` argument."
msgstr ""

#: ../modules/ensemble.rst:666
msgid ""
"Using a first-order Taylor approximation, the value of :math:`l` can be "
"approximated as follows:"
msgstr ""

#: ../modules/ensemble.rst:669
msgid ""
"l(y_i, F_{m-1}(x_i) + h_m(x_i)) \\approx\n"
"l(y_i, F_{m-1}(x_i))\n"
"+ h_m(x_i)\n"
"\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial F(x_i)} "
"\\right]_{F=F_{m - 1}}."
msgstr ""

#: ../modules/ensemble.rst:678
msgid ""
"Briefly, a first-order Taylor approximation says that :math:`l(z) "
"\\approx l(a) + (z - a) \\frac{\\partial l(a)}{\\partial a}`. Here, "
":math:`z` corresponds to :math:`F_{m - 1}(x_i) + h_m(x_i)`, and :math:`a`"
" corresponds to :math:`F_{m-1}(x_i)`"
msgstr ""

#: ../modules/ensemble.rst:683
msgid ""
"The quantity :math:`\\left[ \\frac{\\partial l(y_i, F(x_i))}{\\partial "
"F(x_i)} \\right]_{F=F_{m - 1}}` is the derivative of the loss with "
"respect to its second parameter, evaluated at :math:`F_{m-1}(x)`. It is "
"easy to compute for any given :math:`F_{m - 1}(x_i)` in a closed form "
"since the loss is differentiable. We will denote it by :math:`g_i`."
msgstr ""

#: ../modules/ensemble.rst:689
msgid "Removing the constant terms, we have:"
msgstr ""

#: ../modules/ensemble.rst:691
msgid "h_m \\approx \\arg\\min_{h} \\sum_{i=1}^{n} h(x_i) g_i"
msgstr ""

#: ../modules/ensemble.rst:695
msgid ""
"This is minimized if :math:`h(x_i)` is fitted to predict a value that is "
"proportional to the negative gradient :math:`-g_i`. Therefore, at each "
"iteration, **the estimator** :math:`h_m` **is fitted to predict the "
"negative gradients of the samples**. The gradients are updated at each "
"iteration. This can be considered as some kind of gradient descent in a "
"functional space."
msgstr ""

#: ../modules/ensemble.rst:704
msgid ""
"For some losses, e.g. the least absolute deviation (LAD) where the "
"gradients are :math:`\\pm 1`, the values predicted by a fitted "
":math:`h_m` are not accurate enough: the tree can only output integer "
"values. As a result, the leaves values of the tree :math:`h_m` are "
"modified once the tree is fitted, such that the leaves values minimize "
"the loss :math:`L_m`. The update is loss-dependent: for the LAD loss, the"
" value of a leaf is updated to the median of the samples in that leaf."
msgstr ""

#: ../modules/ensemble.rst:715
msgid ""
"Gradient boosting for classification is very similar to the regression "
"case. However, the sum of the trees :math:`F_M(x_i) = \\sum_m h_m(x_i)` "
"is not homogeneous to a prediction: it cannot be a class, since the trees"
" predict continuous values."
msgstr ""

#: ../modules/ensemble.rst:720
msgid ""
"The mapping from the value :math:`F_M(x_i)` to a class or a probability "
"is loss-dependent. For the deviance (or log-loss), the probability that "
":math:`x_i` belongs to the positive class is modeled as :math:`p(y_i = 1 "
"| x_i) = \\sigma(F_M(x_i))` where :math:`\\sigma` is the sigmoid "
"function."
msgstr ""

#: ../modules/ensemble.rst:725
msgid ""
"For multiclass classification, K trees (for K classes) are built at each "
"of the :math:`M` iterations. The probability that :math:`x_i` belongs to "
"class k is modeled as a softmax of the :math:`F_{M,k}(x_i)` values."
msgstr ""

#: ../modules/ensemble.rst:729
msgid ""
"Note that even for a classification task, the :math:`h_m` sub-estimator "
"is still a regressor, not a classifier. This is because the sub-"
"estimators are trained to predict (negative) *gradients*, which are "
"always continuous quantities."
msgstr ""

#: ../modules/ensemble.rst:737
msgid "Loss Functions"
msgstr ""

#: ../modules/ensemble.rst:739
msgid ""
"The following loss functions are supported and can be specified using the"
" parameter ``loss``:"
msgstr ""

#: ../modules/ensemble.rst:744
msgid ""
"Least squares (``'ls'``): The natural choice for regression due to its "
"superior computational properties. The initial model is given by the mean"
" of the target values."
msgstr ""

#: ../modules/ensemble.rst:747
msgid ""
"Least absolute deviation (``'lad'``): A robust loss function for "
"regression. The initial model is given by the median of the target "
"values."
msgstr ""

#: ../modules/ensemble.rst:750
msgid ""
"Huber (``'huber'``): Another robust loss function that combines least "
"squares and least absolute deviation; use ``alpha`` to control the "
"sensitivity with regards to outliers (see [F2001]_ for more details)."
msgstr ""

#: ../modules/ensemble.rst:754
msgid ""
"Quantile (``'quantile'``): A loss function for quantile regression. Use "
"``0 < alpha < 1`` to specify the quantile. This loss function can be used"
" to create prediction intervals (see "
":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_quantile.py`)."
msgstr ""

#: ../modules/ensemble.rst:761
msgid ""
"Binomial deviance (``'deviance'``): The negative binomial log-likelihood "
"loss function for binary classification (provides probability estimates)."
"  The initial model is given by the log odds-ratio."
msgstr ""

#: ../modules/ensemble.rst:765
msgid ""
"Multinomial deviance (``'deviance'``): The negative multinomial log-"
"likelihood loss function for multi-class classification with "
"``n_classes`` mutually exclusive classes. It provides probability "
"estimates.  The initial model is given by the prior probability of each "
"class. At each iteration ``n_classes`` regression trees have to be "
"constructed which makes GBRT rather inefficient for data sets with a "
"large number of classes."
msgstr ""

#: ../modules/ensemble.rst:772
msgid ""
"Exponential loss (``'exponential'``): The same loss function as "
":class:`AdaBoostClassifier`. Less robust to mislabeled examples than "
"``'deviance'``; can only be used for binary classification."
msgstr ""

#: ../modules/ensemble.rst:780
msgid "Shrinkage via learning rate"
msgstr ""

#: ../modules/ensemble.rst:782
msgid ""
"[F2001]_ proposed a simple regularization strategy that scales the "
"contribution of each weak learner by a constant factor :math:`\\nu`:"
msgstr ""

#: ../modules/ensemble.rst:785
msgid "F_m(x) = F_{m-1}(x) + \\nu h_m(x)"
msgstr ""

#: ../modules/ensemble.rst:789
msgid ""
"The parameter :math:`\\nu` is also called the **learning rate** because "
"it scales the step length the gradient descent procedure; it can be set "
"via the ``learning_rate`` parameter."
msgstr ""

#: ../modules/ensemble.rst:793
msgid ""
"The parameter ``learning_rate`` strongly interacts with the parameter "
"``n_estimators``, the number of weak learners to fit. Smaller values of "
"``learning_rate`` require larger numbers of weak learners to maintain a "
"constant training error. Empirical evidence suggests that small values of"
" ``learning_rate`` favor better test error. [HTF]_ recommend to set the "
"learning rate to a small constant (e.g. ``learning_rate <= 0.1``) and "
"choose ``n_estimators`` by early stopping. For a more detailed discussion"
" of the interaction between ``learning_rate`` and ``n_estimators`` see "
"[R2007]_."
msgstr ""

#: ../modules/ensemble.rst:804
msgid "Subsampling"
msgstr ""

#: ../modules/ensemble.rst:806
msgid ""
"[F1999]_ proposed stochastic gradient boosting, which combines gradient "
"boosting with bootstrap averaging (bagging). At each iteration the base "
"classifier is trained on a fraction ``subsample`` of the available "
"training data. The subsample is drawn without replacement. A typical "
"value of ``subsample`` is 0.5."
msgstr ""

#: ../modules/ensemble.rst:812
msgid ""
"The figure below illustrates the effect of shrinkage and subsampling on "
"the goodness-of-fit of the model. We can clearly see that shrinkage "
"outperforms no-shrinkage. Subsampling with shrinkage can further increase"
" the accuracy of the model. Subsampling without shrinkage, on the other "
"hand, does poorly."
msgstr ""

#: ../modules/ensemble.rst:823
msgid ""
"Another strategy to reduce the variance is by subsampling the features "
"analogous to the random splits in :class:`RandomForestClassifier` . The "
"number of subsampled features can be controlled via the ``max_features`` "
"parameter."
msgstr ""

#: ../modules/ensemble.rst:828
msgid ""
"Using a small ``max_features`` value can significantly decrease the "
"runtime."
msgstr ""

#: ../modules/ensemble.rst:830
msgid ""
"Stochastic gradient boosting allows to compute out-of-bag estimates of "
"the test deviance by computing the improvement in deviance on the "
"examples that are not included in the bootstrap sample (i.e. the out-of-"
"bag examples). The improvements are stored in the attribute "
":attr:`~GradientBoostingRegressor.oob_improvement_`. "
"``oob_improvement_[i]`` holds the improvement in terms of the loss on the"
" OOB samples if you add the i-th stage to the current predictions. Out-"
"of-bag estimates can be used for model selection, for example to "
"determine the optimal number of iterations. OOB estimates are usually "
"very pessimistic thus we recommend to use cross-validation instead and "
"only use OOB if cross-validation is too time consuming."
msgstr ""

#: ../modules/ensemble.rst:844
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_regularization.py`"
msgstr ""

#: ../modules/ensemble.rst:846
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_ensemble_oob.py`"
msgstr ""

#: ../modules/ensemble.rst:849
msgid "Interpretation with feature importance"
msgstr ""

#: ../modules/ensemble.rst:851
msgid ""
"Individual decision trees can be interpreted easily by simply visualizing"
" the tree structure. Gradient boosting models, however, comprise hundreds"
" of regression trees thus they cannot be easily interpreted by visual "
"inspection of the individual trees. Fortunately, a number of techniques "
"have been proposed to summarize and interpret gradient boosting models."
msgstr ""

#: ../modules/ensemble.rst:858
msgid ""
"Often features do not contribute equally to predict the target response; "
"in many situations the majority of the features are in fact irrelevant. "
"When interpreting a model, the first question usually is: what are those "
"important features and how do they contributing in predicting the target "
"response?"
msgstr ""

#: ../modules/ensemble.rst:865
msgid ""
"Individual decision trees intrinsically perform feature selection by "
"selecting appropriate split points. This information can be used to "
"measure the importance of each feature; the basic idea is: the more often"
" a feature is used in the split points of a tree the more important that "
"feature is. This notion of importance can be extended to decision tree "
"ensembles by simply averaging the impurity-based feature importance of "
"each tree (see :ref:`random_forest_feature_importance` for more details)."
msgstr ""

#: ../modules/ensemble.rst:873
msgid ""
"The feature importance scores of a fit gradient boosting model can be "
"accessed via the ``feature_importances_`` property::"
msgstr ""

#: ../modules/ensemble.rst:885
msgid ""
"Note that this computation of feature importance is based on entropy, and"
" it is distinct from :func:`sklearn.inspection.permutation_importance` "
"which is based on permutation of the features."
msgstr ""

#: ../modules/ensemble.rst:896
msgid "Histogram-Based Gradient Boosting"
msgstr ""

#: ../modules/ensemble.rst:898
msgid ""
"Scikit-learn 0.21 introduced two new experimental implementations of "
"gradient boosting trees, namely :class:`HistGradientBoostingClassifier` "
"and :class:`HistGradientBoostingRegressor`, inspired by `LightGBM "
"<https://github.com/Microsoft/LightGBM>`__ (See [LightGBM]_)."
msgstr ""

#: ../modules/ensemble.rst:911
msgid ""
"These fast estimators first bin the input samples ``X`` into integer-"
"valued bins (typically 256 bins) which tremendously reduces the number of"
" splitting points to consider, and allows the algorithm to leverage "
"integer-based data structures (histograms) instead of relying on sorted "
"continuous values when building the trees. The API of these estimators is"
" slightly different, and some of the features from "
":class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor` are not yet supported, for instance "
"some loss functions."
msgstr ""

#: ../modules/ensemble.rst:920
msgid ""
"These estimators are still **experimental**: their predictions and their "
"API might change without any deprecation cycle. To use them, you need to "
"explicitly import ``enable_hist_gradient_boosting``::"
msgstr ""

#: ../modules/ensemble.rst:931
msgid ":ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`"
msgstr ""

#: ../modules/ensemble.rst:936
msgid ""
"Most of the parameters are unchanged from "
":class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor`. One exception is the ``max_iter`` "
"parameter that replaces ``n_estimators``, and controls the number of "
"iterations of the boosting process::"
msgstr ""

#: ../modules/ensemble.rst:953
msgid ""
"Available losses for regression are 'least_squares', "
"'least_absolute_deviation', which is less sensitive to outliers, and "
"'poisson', which is well suited to model counts and frequencies. For "
"classification, 'binary_crossentropy' is used for binary classification "
"and 'categorical_crossentropy' is used for multiclass classification. By "
"default the loss is 'auto' and will select the appropriate loss depending"
" on :term:`y` passed to :term:`fit`."
msgstr ""

#: ../modules/ensemble.rst:961
msgid ""
"The size of the trees can be controlled through the ``max_leaf_nodes``, "
"``max_depth``, and ``min_samples_leaf`` parameters."
msgstr ""

#: ../modules/ensemble.rst:964
msgid ""
"The number of bins used to bin the data is controlled with the "
"``max_bins`` parameter. Using less bins acts as a form of regularization."
" It is generally recommended to use as many bins as possible, which is "
"the default."
msgstr ""

#: ../modules/ensemble.rst:968
msgid ""
"The ``l2_regularization`` parameter is a regularizer on the loss function"
" and corresponds to :math:`\\lambda` in equation (2) of [XGBoost]_."
msgstr ""

#: ../modules/ensemble.rst:971
msgid ""
"Note that **early-stopping is enabled by default if the number of samples"
" is larger than 10,000**. The early-stopping behaviour is controlled via "
"the ``early-stopping``, ``scoring``, ``validation_fraction``, "
"``n_iter_no_change``, and ``tol`` parameters. It is possible to early-"
"stop using an arbitrary :term:`scorer`, or just the training or "
"validation loss. Note that for technical reasons, using a scorer is "
"significantly slower than using the loss. By default, early-stopping is "
"performed if there are at least 10,000 samples in the training set, using"
" the validation loss."
msgstr ""

#: ../modules/ensemble.rst:981
msgid "Missing values support"
msgstr ""

#: ../modules/ensemble.rst:983
msgid ""
":class:`HistGradientBoostingClassifier` and "
":class:`HistGradientBoostingRegressor` have built-in support for missing "
"values (NaNs)."
msgstr ""

#: ../modules/ensemble.rst:987
msgid ""
"During training, the tree grower learns at each split point whether "
"samples with missing values should go to the left or right child, based "
"on the potential gain. When predicting, samples with missing values are "
"assigned to the left or right child consequently::"
msgstr ""

#: ../modules/ensemble.rst:1003
msgid ""
"When the missingness pattern is predictive, the splits can be done on "
"whether the feature value is missing or not::"
msgstr ""

#: ../modules/ensemble.rst:1015
msgid ""
"If no missing values were encountered for a given feature during "
"training, then samples with missing values are mapped to whichever child "
"has the most samples."
msgstr ""

#: ../modules/ensemble.rst:1022
msgid "Sample weight support"
msgstr ""

#: ../modules/ensemble.rst:1024
msgid ""
":class:`HistGradientBoostingClassifier` and "
":class:`HistGradientBoostingRegressor` sample support weights during "
":term:`fit`."
msgstr ""

#: ../modules/ensemble.rst:1028
msgid ""
"The following toy example demonstrates how the model ignores the samples "
"with zero sample weights:"
msgstr ""

#: ../modules/ensemble.rst:1046
msgid ""
"As you can see, the `[1, 0]` is comfortably classified as `1` since the "
"first two samples are ignored due to their sample weights."
msgstr ""

#: ../modules/ensemble.rst:1049
msgid ""
"Implementation detail: taking sample weights into account amounts to "
"multiplying the gradients (and the hessians) by the sample weights. Note "
"that the binning stage (specifically the quantiles computation) does not "
"take the weights into account."
msgstr ""

#: ../modules/ensemble.rst:1057
msgid "Categorical Features Support"
msgstr ""

#: ../modules/ensemble.rst:1059
msgid ""
":class:`HistGradientBoostingClassifier` and "
":class:`HistGradientBoostingRegressor` have native support for "
"categorical features: they can consider splits on non-ordered, "
"categorical data."
msgstr ""

#: ../modules/ensemble.rst:1063
msgid ""
"For datasets with categorical features, using the native categorical "
"support is often better than relying on one-hot encoding "
"(:class:`~sklearn.preprocessing.OneHotEncoder`), because one-hot encoding"
" requires more tree depth to achieve equivalent splits. It is also "
"usually better to rely on the native categorical support rather than to "
"treat categorical features as continuous (ordinal), which happens for "
"ordinal-encoded categorical data, since categories are nominal quantities"
" where order does not matter."
msgstr ""

#: ../modules/ensemble.rst:1072
msgid ""
"To enable categorical support, a boolean mask can be passed to the "
"`categorical_features` parameter, indicating which feature is "
"categorical. In the following, the first feature will be treated as "
"categorical and the second feature as numerical::"
msgstr ""

#: ../modules/ensemble.rst:1079
msgid ""
"Equivalently, one can pass a list of integers indicating the indices of "
"the categorical features::"
msgstr ""

#: ../modules/ensemble.rst:1084
msgid ""
"The cardinality of each categorical feature should be less than the "
"`max_bins` parameter, and each categorical feature is expected to be "
"encoded in `[0, max_bins - 1]`. To that end, it might be useful to pre-"
"process the data with an :class:`~sklearn.preprocessing.OrdinalEncoder` "
"as done in "
":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`."
msgstr ""

#: ../modules/ensemble.rst:1090
msgid ""
"If there are missing values during training, the missing values will be "
"treated as a proper category. If there are no missing values during "
"training, then at prediction time, missing values are mapped to the child"
" node that has the most samples (just like for continuous features). When"
" predicting, categories that were not seen during fit time will be "
"treated as missing values."
msgstr ""

#: ../modules/ensemble.rst:1097
msgid ""
"**Split finding with categorical features**: The canonical way of "
"considering categorical splits in a tree is to consider all of the "
":math:`2^{K - 1} - 1` partitions, where :math:`K` is the number of "
"categories. This can quickly become prohibitive when :math:`K` is large. "
"Fortunately, since gradient boosting trees are always regression trees "
"(even for classification problems), there exist a faster strategy that "
"can yield equivalent splits. First, the categories of a feature are "
"sorted according to the variance of the target, for each category `k`. "
"Once the categories are sorted, one can consider *continuous partitions*,"
" i.e. treat the categories as if they were ordered continuous values (see"
" Fisher [Fisher1958]_ for a formal proof). As a result, only :math:`K - "
"1` splits need to be considered instead of :math:`2^{K - 1} - 1`. The "
"initial sorting is a :math:`\\mathcal{O}(K \\log(K))` operation, leading "
"to a total complexity of :math:`\\mathcal{O}(K \\log(K) + K)`, instead of"
" :math:`\\mathcal{O}(2^K)`."
msgstr ""

#: ../modules/ensemble.rst:1114
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_gradient_boosting_categorical.py`"
msgstr ""

#: ../modules/ensemble.rst:1119
msgid "Monotonic Constraints"
msgstr ""

#: ../modules/ensemble.rst:1121
msgid ""
"Depending on the problem at hand, you may have prior knowledge indicating"
" that a given feature should in general have a positive (or negative) "
"effect on the target value. For example, all else being equal, a higher "
"credit score should increase the probability of getting approved for a "
"loan. Monotonic constraints allow you to incorporate such prior knowledge"
" into the model."
msgstr ""

#: ../modules/ensemble.rst:1128
msgid "A positive monotonic constraint is a constraint of the form:"
msgstr ""

#: ../modules/ensemble.rst:1130
msgid ""
":math:`x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq F(x_1', x_2)`, where "
":math:`F` is the predictor with two features."
msgstr ""

#: ../modules/ensemble.rst:1133
msgid "Similarly, a negative monotonic constraint is of the form:"
msgstr ""

#: ../modules/ensemble.rst:1135
msgid ":math:`x_1 \\leq x_1' \\implies F(x_1, x_2) \\geq F(x_1', x_2)`."
msgstr ""

#: ../modules/ensemble.rst:1137
msgid ""
"Note that monotonic constraints only constraint the output \"all else "
"being equal\". Indeed, the following relation **is not enforced** by a "
"positive constraint: :math:`x_1 \\leq x_1' \\implies F(x_1, x_2) \\leq "
"F(x_1', x_2')`."
msgstr ""

#: ../modules/ensemble.rst:1141
msgid ""
"You can specify a monotonic constraint on each feature using the "
"`monotonic_cst` parameter. For each feature, a value of 0 indicates no "
"constraint, while -1 and 1 indicate a negative and positive constraint, "
"respectively::"
msgstr ""

#: ../modules/ensemble.rst:1152
msgid ""
"In a binary classification context, imposing a monotonic constraint means"
" that the feature is supposed to have a positive / negative effect on the"
" probability to belong to the positive class. Monotonic constraints are "
"not supported for multiclass context."
msgstr ""

#: ../modules/ensemble.rst:1158
msgid ""
"Since categories are unordered quantities, it is not possible to enforce "
"monotonic constraints on categorical features."
msgstr ""

#: ../modules/ensemble.rst:1163
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_monotonic_constraints.py`"
msgstr ""

#: ../modules/ensemble.rst:1166
msgid "Low-level parallelism"
msgstr ""

#: ../modules/ensemble.rst:1168
msgid ""
":class:`HistGradientBoostingClassifier` and "
":class:`HistGradientBoostingRegressor` have implementations that use "
"OpenMP for parallelization through Cython. For more details on how to "
"control the number of threads, please refer to our :ref:`parallelism` "
"notes."
msgstr ""

#: ../modules/ensemble.rst:1173
msgid "The following parts are parallelized:"
msgstr ""

#: ../modules/ensemble.rst:1175
msgid ""
"mapping samples from real values to integer-valued bins (finding the bin "
"thresholds is however sequential)"
msgstr ""

#: ../modules/ensemble.rst:1177
msgid "building histograms is parallelized over features"
msgstr ""

#: ../modules/ensemble.rst:1178
msgid "finding the best split point at a node is parallelized over features"
msgstr ""

#: ../modules/ensemble.rst:1179
msgid ""
"during fit, mapping samples into the left and right children is "
"parallelized over samples"
msgstr ""

#: ../modules/ensemble.rst:1181
msgid "gradient and hessians computations are parallelized over samples"
msgstr ""

#: ../modules/ensemble.rst:1182
msgid "predicting is parallelized over samples"
msgstr ""

#: ../modules/ensemble.rst:1185
msgid "Why it's faster"
msgstr ""

#: ../modules/ensemble.rst:1187
msgid ""
"The bottleneck of a gradient boosting procedure is building the decision "
"trees. Building a traditional decision tree (as in the other GBDTs "
":class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor`) requires sorting the samples at each "
"node (for each feature). Sorting is needed so that the potential gain of "
"a split point can be computed efficiently. Splitting a single node has "
"thus a complexity of :math:`\\mathcal{O}(n_\\text{features} \\times n "
"\\log(n))` where :math:`n` is the number of samples at the node."
msgstr ""

#: ../modules/ensemble.rst:1196
msgid ""
":class:`HistGradientBoostingClassifier` and "
":class:`HistGradientBoostingRegressor`, in contrast, do not require "
"sorting the feature values and instead use a data-structure called a "
"histogram, where the samples are implicitly ordered. Building a histogram"
" has a :math:`\\mathcal{O}(n)` complexity, so the node splitting "
"procedure has a :math:`\\mathcal{O}(n_\\text{features} \\times n)` "
"complexity, much smaller than the previous one. In addition, instead of "
"considering :math:`n` split points, we here consider only ``max_bins`` "
"split points, which is much smaller."
msgstr ""

#: ../modules/ensemble.rst:1206
msgid ""
"In order to build histograms, the input data `X` needs to be binned into "
"integer-valued bins. This binning procedure does require sorting the "
"feature values, but it only happens once at the very beginning of the "
"boosting process (not at each node, like in "
":class:`GradientBoostingClassifier` and "
":class:`GradientBoostingRegressor`)."
msgstr ""

#: ../modules/ensemble.rst:1212
msgid ""
"Finally, many parts of the implementation of "
":class:`HistGradientBoostingClassifier` and "
":class:`HistGradientBoostingRegressor` are parallelized."
msgstr ""

#: ../modules/ensemble.rst:1218
msgid ""
"Friedmann, Jerome H., 2007, `\"Stochastic Gradient Boosting\" "
"<https://statweb.stanford.edu/~jhf/ftp/stobst.pdf>`_"
msgstr ""

#: ../modules/ensemble.rst:1220
msgid ""
"G. Ridgeway, \"Generalized Boosted Models: A guide to the gbm package\", "
"2007"
msgstr ""

#: ../modules/ensemble.rst:1222
msgid ""
"Tianqi Chen, Carlos Guestrin, `\"XGBoost: A Scalable Tree Boosting "
"System\" <https://arxiv.org/abs/1603.02754>`_"
msgstr ""

#: ../modules/ensemble.rst:1224
msgid ""
"Ke et. al. `\"LightGBM: A Highly Efficient Gradient BoostingDecision "
"Tree\" <https://papers.nips.cc/paper/ 6907-lightgbm-a-highly-efficient-"
"gradient-boosting-decision-tree>`_"
msgstr ""

#: ../modules/ensemble.rst:1227
msgid ""
"Walter D. Fisher. `\"On Grouping for Maximum Homogeneity\" "
"<http://www.csiss.org/SPACE/workshops/2004/SAC/files/fisher.pdf>`_"
msgstr ""

#: ../modules/ensemble.rst:1233
msgid "Voting Classifier"
msgstr ""

#: ../modules/ensemble.rst:1235
msgid ""
"The idea behind the :class:`VotingClassifier` is to combine conceptually "
"different machine learning classifiers and use a majority vote or the "
"average predicted probabilities (soft vote) to predict the class labels. "
"Such a classifier can be useful for a set of equally well performing "
"model in order to balance out their individual weaknesses."
msgstr ""

#: ../modules/ensemble.rst:1243
msgid "Majority Class Labels (Majority/Hard Voting)"
msgstr ""

#: ../modules/ensemble.rst:1245
msgid ""
"In majority voting, the predicted class label for a particular sample is "
"the class label that represents the majority (mode) of the class labels "
"predicted by each individual classifier."
msgstr ""

#: ../modules/ensemble.rst:1249
msgid "E.g., if the prediction for a given sample is"
msgstr ""

#: ../modules/ensemble.rst:1251
msgid "classifier 1 -> class 1"
msgstr ""

#: ../modules/ensemble.rst:1252 ../modules/ensemble.rst:1262
msgid "classifier 2 -> class 1"
msgstr ""

#: ../modules/ensemble.rst:1253
msgid "classifier 3 -> class 2"
msgstr ""

#: ../modules/ensemble.rst:1255
msgid ""
"the VotingClassifier (with ``voting='hard'``) would classify the sample "
"as \"class 1\" based on the majority class label."
msgstr ""

#: ../modules/ensemble.rst:1258
msgid ""
"In the cases of a tie, the :class:`VotingClassifier` will select the "
"class based on the ascending sort order. E.g., in the following scenario"
msgstr ""

#: ../modules/ensemble.rst:1261
msgid "classifier 1 -> class 2"
msgstr ""

#: ../modules/ensemble.rst:1264
msgid "the class label 1 will be assigned to the sample."
msgstr ""

#: ../modules/ensemble.rst:1269
msgid "The following example shows how to fit the majority rule classifier::"
msgstr ""

#: ../modules/ensemble.rst:1299
msgid "Weighted Average Probabilities (Soft Voting)"
msgstr ""

#: ../modules/ensemble.rst:1301
msgid ""
"In contrast to majority voting (hard voting), soft voting returns the "
"class label as argmax of the sum of predicted probabilities."
msgstr ""

#: ../modules/ensemble.rst:1304
msgid ""
"Specific weights can be assigned to each classifier via the ``weights`` "
"parameter. When weights are provided, the predicted class probabilities "
"for each classifier are collected, multiplied by the classifier weight, "
"and averaged. The final class label is then derived from the class label "
"with the highest average probability."
msgstr ""

#: ../modules/ensemble.rst:1310
msgid ""
"To illustrate this with a simple example, let's assume we have 3 "
"classifiers and a 3-class classification problems where we assign equal "
"weights to all classifiers: w1=1, w2=1, w3=1."
msgstr ""

#: ../modules/ensemble.rst:1314
msgid ""
"The weighted average probabilities for a sample would then be calculated "
"as follows:"
msgstr ""

#: ../modules/ensemble.rst:1318
msgid "classifier"
msgstr ""

#: ../modules/ensemble.rst:1318
msgid "class 1"
msgstr ""

#: ../modules/ensemble.rst:1318
msgid "class 2"
msgstr ""

#: ../modules/ensemble.rst:1318
msgid "class 3"
msgstr ""

#: ../modules/ensemble.rst:1320
msgid "classifier 1"
msgstr ""

#: ../modules/ensemble.rst:1320
msgid "w1 * 0.2"
msgstr ""

#: ../modules/ensemble.rst:1320
msgid "w1 * 0.5"
msgstr ""

#: ../modules/ensemble.rst:1320
msgid "w1 * 0.3"
msgstr ""

#: ../modules/ensemble.rst:1321
msgid "classifier 2"
msgstr ""

#: ../modules/ensemble.rst:1321
msgid "w2 * 0.6"
msgstr ""

#: ../modules/ensemble.rst:1321
msgid "w2 * 0.3"
msgstr ""

#: ../modules/ensemble.rst:1321
msgid "w2 * 0.1"
msgstr ""

#: ../modules/ensemble.rst:1322
msgid "classifier 3"
msgstr ""

#: ../modules/ensemble.rst:1322
msgid "w3 * 0.3"
msgstr ""

#: ../modules/ensemble.rst:1322
msgid "w3 * 0.4"
msgstr ""

#: ../modules/ensemble.rst:1323
msgid "weighted average"
msgstr ""

#: ../modules/ensemble.rst:1323
msgid "0.37"
msgstr ""

#: ../modules/ensemble.rst:1323
msgid "0.4"
msgstr ""

#: ../modules/ensemble.rst:1323
msgid "0.23"
msgstr ""

#: ../modules/ensemble.rst:1326
msgid ""
"Here, the predicted class label is 2, since it has the highest average "
"probability."
msgstr ""

#: ../modules/ensemble.rst:1329
msgid ""
"The following example illustrates how the decision regions may change "
"when a soft :class:`VotingClassifier` is used based on an linear Support "
"Vector Machine, a Decision Tree, and a K-nearest neighbor classifier::"
msgstr ""

#: ../modules/ensemble.rst:1363
msgid "Using the `VotingClassifier` with `GridSearchCV`"
msgstr ""

#: ../modules/ensemble.rst:1365
msgid ""
"The :class:`VotingClassifier` can also be used together with "
":class:`~sklearn.model_selection.GridSearchCV` in order to tune the "
"hyperparameters of the individual estimators::"
msgstr ""

#: ../modules/ensemble.rst:1386
msgid ""
"In order to predict the class labels based on the predicted class-"
"probabilities (scikit-learn estimators in the VotingClassifier must "
"support ``predict_proba`` method)::"
msgstr ""

#: ../modules/ensemble.rst:1395
msgid "Optionally, weights can be provided for the individual classifiers::"
msgstr ""

#: ../modules/ensemble.rst:1405
msgid "Voting Regressor"
msgstr ""

#: ../modules/ensemble.rst:1407
msgid ""
"The idea behind the :class:`VotingRegressor` is to combine conceptually "
"different machine learning regressors and return the average predicted "
"values. Such a regressor can be useful for a set of equally well "
"performing models in order to balance out their individual weaknesses."
msgstr ""

#: ../modules/ensemble.rst:1415
msgid "The following example shows how to fit the VotingRegressor::"
msgstr ""

#: ../modules/ensemble.rst:1440
msgid ":ref:`sphx_glr_auto_examples_ensemble_plot_voting_regressor.py`"
msgstr ""

#: ../modules/ensemble.rst:1445
msgid "Stacked generalization"
msgstr ""

#: ../modules/ensemble.rst:1447
msgid ""
"Stacked generalization is a method for combining estimators to reduce "
"their biases [W1992]_ [HTF]_. More precisely, the predictions of each "
"individual estimator are stacked together and used as input to a final "
"estimator to compute the prediction. This final estimator is trained "
"through cross-validation."
msgstr ""

#: ../modules/ensemble.rst:1453
msgid ""
"The :class:`StackingClassifier` and :class:`StackingRegressor` provide "
"such strategies which can be applied to classification and regression "
"problems."
msgstr ""

#: ../modules/ensemble.rst:1456
msgid ""
"The `estimators` parameter corresponds to the list of the estimators "
"which are stacked together in parallel on the input data. It should be "
"given as a list of names and estimators::"
msgstr ""

#: ../modules/ensemble.rst:1467
msgid ""
"The `final_estimator` will use the predictions of the `estimators` as "
"input. It needs to be a classifier or a regressor when using "
":class:`StackingClassifier` or :class:`StackingRegressor`, respectively::"
msgstr ""

#: ../modules/ensemble.rst:1480
msgid ""
"To train the `estimators` and `final_estimator`, the `fit` method needs "
"to be called on the training data::"
msgstr ""

#: ../modules/ensemble.rst:1491
msgid ""
"During training, the `estimators` are fitted on the whole training data "
"`X_train`. They will be used when calling `predict` or `predict_proba`. "
"To generalize and avoid over-fitting, the `final_estimator` is trained on"
" out-samples using :func:`sklearn.model_selection.cross_val_predict` "
"internally."
msgstr ""

#: ../modules/ensemble.rst:1496
msgid ""
"For :class:`StackingClassifier`, note that the output of the "
"``estimators`` is controlled by the parameter `stack_method` and it is "
"called by each estimator. This parameter is either a string, being "
"estimator method names, or `'auto'` which will automatically identify an "
"available method depending on the availability, tested in the order of "
"preference: `predict_proba`, `decision_function` and `predict`."
msgstr ""

#: ../modules/ensemble.rst:1503
msgid ""
"A :class:`StackingRegressor` and :class:`StackingClassifier` can be used "
"as any other regressor or classifier, exposing a `predict`, "
"`predict_proba`, and `decision_function` methods, e.g.::"
msgstr ""

#: ../modules/ensemble.rst:1512
msgid ""
"Note that it is also possible to get the output of the stacked "
"`estimators` using the `transform` method::"
msgstr ""

#: ../modules/ensemble.rst:1522
msgid ""
"In practice, a stacking predictor predicts as good as the best predictor "
"of the base layer and even sometimes outperforms it by combining the "
"different strengths of the these predictors. However, training a stacking"
" predictor is computationally expensive."
msgstr ""

#: ../modules/ensemble.rst:1528
msgid ""
"For :class:`StackingClassifier`, when using "
"`stack_method_='predict_proba'`, the first column is dropped when the "
"problem is a binary classification problem. Indeed, both probability "
"columns predicted by each estimator are perfectly collinear."
msgstr ""

#: ../modules/ensemble.rst:1534
msgid ""
"Multiple stacking layers can be achieved by assigning `final_estimator` "
"to a :class:`StackingClassifier` or :class:`StackingRegressor`::"
msgstr ""

#: ../modules/ensemble.rst:1561
msgid ""
"Wolpert, David H. \"Stacked generalization.\" Neural networks 5.2 (1992):"
" 241-259."
msgstr ""

