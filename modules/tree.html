

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>1.10. Árboles de decisión &mdash; documentación de scikit-learn - 0.24.1</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/tree.html" />

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../index.html">
        <img
          class="sk-brand-img"
          src="../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../install.html">Instalación</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">Manual de Usuario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Ejemplos</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../getting_started.html">¿Cómo empezar?</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../whats_new/v0.24.html">Novedades</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../glossary.html">Glosario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../developers/index.html">Desarrollo</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../support.html">Soporte</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../related_projects.html">Paquetes relacionados</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../roadmap.html">Hoja de ruta</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../about.html">Sobre nosotros</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Más</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../getting_started.html">¿Cómo empezar?</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../whats_new/v0.24.html">Novedades</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../glossary.html">Glosario</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../developers/index.html">Desarrollo</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../support.html">Soporte</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../related_projects.html">Paquetes relacionados</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../roadmap.html">Hoja de ruta</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../about.html">Sobre nosotros</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Ir a" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Alternar menú</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../index.html">
            <img
              class="sk-brand-img"
              src="../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="naive_bayes.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.9. Bayesiano ingenuo">Prev</a><a href="../supervised_learning.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1. Aprendizaje supervisado">Arriba</a>
            <a href="ensemble.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.11. Métodos combinados">Sig.</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.24.1</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Otras versiones</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Por favor <a class="font-weight-bold" href="../about.html#citing-scikit-learn"><string>cítanos</string></a> si usas el software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">1.10. Árboles de decisión</a><ul>
<li><a class="reference internal" href="#classification">1.10.1. Clasificación</a></li>
<li><a class="reference internal" href="#regression">1.10.2. Regresión</a></li>
<li><a class="reference internal" href="#multi-output-problems">1.10.3. Problemas de salida múltiple</a></li>
<li><a class="reference internal" href="#complexity">1.10.4. Complejidad</a></li>
<li><a class="reference internal" href="#tips-on-practical-use">1.10.5. Consejos sobre uso práctico</a></li>
<li><a class="reference internal" href="#tree-algorithms-id3-c4-5-c5-0-and-cart">1.10.6. Algoritmos de árbol: ID3, C4.5, C5.0 y CART</a></li>
<li><a class="reference internal" href="#mathematical-formulation">1.10.7. Formulación matemática</a><ul>
<li><a class="reference internal" href="#classification-criteria">1.10.7.1. Criterios de clasificación</a></li>
<li><a class="reference internal" href="#regression-criteria">1.10.7.2. Criterios de Regresión</a></li>
</ul>
</li>
<li><a class="reference internal" href="#minimal-cost-complexity-pruning">1.10.8. Poda de Coste-Complejidad Mínima</a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="decision-trees">
<span id="tree"></span><h1><span class="section-number">1.10. </span>Árboles de decisión<a class="headerlink" href="#decision-trees" title="Enlazar permanentemente con este título">¶</a></h1>
<p><strong>Los árboles de decisión (DTs por sus siglas en inglés)</strong> son un método de aprendizaje supervisado no paramétrico utilizado para <a class="reference internal" href="#tree-classification"><span class="std std-ref">clasificación</span></a> y <a class="reference internal" href="#tree-regression"><span class="std std-ref">regresión</span></a>. El objetivo es crear un modelo que prediga el valor de una variable objetivo mediante el aprendizaje de reglas de decisión simples inferidas a partir de las características de los datos. Un árbol puede ser visto como una aproximación constante a trozos.</p>
<p>En el siguiente ejemplo, los árboles de decisión aprenden de los datos para aproximar una curva sinusoidal con un conjunto de reglas de decisión si-entonces-en otro caso (if-then-else). Cuanto más profundo sea el árbol, más complejas serán las reglas de decisión y más ajustado será el modelo.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/tree/plot_tree_regression.html"><img alt="../_images/sphx_glr_plot_tree_regression_001.png" src="../_images/sphx_glr_plot_tree_regression_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>Algunas ventajas de los árboles de decisión son:</p>
<blockquote>
<div><ul class="simple">
<li><p>Sencillo de entender e interpretar. Los árboles pueden ser visualizados.</p></li>
<li><p>Requiere poca preparación de los datos. Otras técnicas a menudo requieren la normalización de datos, la creación de variables dummy y la eliminación de valores en blanco. Sin embargo, ten en cuenta que este módulo no admite valores faltantes.</p></li>
<li><p>El costo de usar el árbol (es decir, de predecir datos) es logarítmico en el número de puntos de datos utilizados para entrenar el árbol.</p></li>
<li><p>Capaz de manejar datos tanto numéricos como categóricos. Sin embargo, la implementación de scikit-learn no admite variables categóricas por ahora. Otras técnicas suelen estar especializadas en el análisis de conjuntos de datos que sólo tienen un tipo de variable. Ver <a class="reference internal" href="#tree-algorithms"><span class="std std-ref">algoritmos</span></a> para más información.</p></li>
<li><p>Capaz de manejar problemas de salida múltiple.</p></li>
<li><p>Utiliza un modelo de caja blanca —white box model—. Si una situación dada es observable en un modelo, la explicación de la condición se explica fácilmente por lógica booleana. Por el contrario, en un modelo de caja negra —black box model— (por ejemplo, en una red neuronal artificial), los resultados pueden ser más difíciles de interpretar.</p></li>
<li><p>Es posible validar un modelo utilizando pruebas estadísticas. Eso permite tener en cuenta la fiabilidad del modelo.</p></li>
<li><p>Funciona bien incluso si sus supuestos son violados de alguna manera por el verdadero modelo a partir del cual se generaron los datos.</p></li>
</ul>
</div></blockquote>
<p>Las desventajas de los árboles de decisión incluyen:</p>
<blockquote>
<div><ul class="simple">
<li><p>Los algoritmos de aprendizaje de árboles de decisión pueden crear árboles demasiado complejos que no generalicen bien los datos. Esto se denomina sobreajuste. Para evitar este problema son necesarios mecanismos como la poda, establecer el número mínimo de muestras necesarias en un nodo de la hoja o establecer la profundidad máxima del árbol.</p></li>
<li><p>Los árboles de decisión pueden ser inestables porque pequeñas variaciones en los datos pueden resultar en la generación de un árbol completamente diferente. Este problema es mitigado mediante el uso de árboles de decisión dentro de un conjunto.</p></li>
<li><p>Las predicciones de los árboles de decisión no son suaves ni continuas, sino aproximaciones constantes a trozos, como se ve en la figura anterior. Por lo tanto, no son buenos para la extrapolación.</p></li>
<li><p>Se sabe que el problema del aprendizaje de un árbol de decisión óptimo es NP-completo bajo varios aspectos de optimalidad e incluso para conceptos simples. En consecuencia, los algoritmos prácticos de aprendizaje de árboles de decisión se basan en algoritmos heurísticos como el algoritmo codicioso, donde se toman decisiones localmente óptimas en cada nodo. Tales algoritmos no pueden garantizar que devuelvan el árbol de decisión globalmente óptimo. Esto puede mitigarse entrenando múltiples árboles en un algoritmo de aprendizaje de conjunto, donde las características y las muestras se muestrean aleatoriamente con reemplazo.</p></li>
<li><p>Hay conceptos difíciles de aprender porque los árboles de decisión no los expresan fácilmente, como los problemas de XOR, paridad o multiplexores.</p></li>
<li><p>Los algoritmos de aprendizaje de árboles de decisión crean árboles sesgados si algunas clases dominan. Por lo tanto, se recomienda equilibrar el conjunto de datos antes de ajustarlo con el árbol de decisión.</p></li>
</ul>
</div></blockquote>
<section id="classification">
<span id="tree-classification"></span><h2><span class="section-number">1.10.1. </span>Clasificación<a class="headerlink" href="#classification" title="Enlazar permanentemente con este título">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a> es una clase capaz de realizar una clasificación multiclase en un conjunto de datos.</p>
<p>Al igual que otros clasificadores, <a class="reference internal" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a> toma como entrada dos arreglos: un arreglo X, disperso o denso, de forma <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_features)</span></code> que contiene las muestras de entrenamiento, y un arreglo Y de valores enteros, de forma <code class="docutils literal notranslate"><span class="pre">(n_samples,)</span></code>, que contiene las etiquetas de clase para las muestras de entrenamiento:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
</pre></div>
</div>
<p>Después de ser ajustado, el modelo puede ser utilizado para predecir la clase de muestras:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
<p>En caso de que haya múltiples clases con la misma y más alta probabilidad, el clasificador predecirá la clase con el índice más bajo entre esas clases.</p>
<p>Como alternativa a la salida de una clase específica, se puede predecir la probabilidad de cada clase, que es la fracción de las muestras de entrenamiento de la clase en una hoja:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict_proba</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([[0., 1.]])</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a> es capaz de realizar tanto una clasificación binaria (donde las etiquetas son [-1, 1]) como una clasificación multiclase (donde las etiquetas son [0, …, K-1]).</p>
<p>Usando el conjunto de datos Iris, podemos construir un árbol como sigue:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeClassifier</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
<p>Una vez entrenado, puedes graficar el árbol con la función <a class="reference internal" href="generated/sklearn.tree.plot_tree.html#sklearn.tree.plot_tree" title="sklearn.tree.plot_tree"><code class="xref py py-func docutils literal notranslate"><span class="pre">plot_tree</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">tree</span><span class="o">.</span><span class="n">plot_tree</span><span class="p">(</span><span class="n">clf</span><span class="p">)</span> 
</pre></div>
</div>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/tree/plot_iris_dtc.html"><img alt="../_images/sphx_glr_plot_iris_dtc_002.png" src="../_images/sphx_glr_plot_iris_dtc_002.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>También podemos exportar el árbol en formato <a class="reference external" href="https://www.graphviz.org/">Graphviz</a> usando el exportador <a class="reference internal" href="generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz" title="sklearn.tree.export_graphviz"><code class="xref py py-func docutils literal notranslate"><span class="pre">export_graphviz</span></code></a>. Si utilizas el gestor de paquetes <a class="reference external" href="https://conda.io">conda</a>, los binarios de graphviz y el paquete python pueden instalarse con <code class="docutils literal notranslate"><span class="pre">conda</span> <span class="pre">install</span> <span class="pre">python-graphviz</span></code>.</p>
<p>Alternativamente, los binarios para graphviz pueden ser descargados desde la página web del proyecto graphviz, y la capa de Python instalada desde pypi con <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">graphviz</span></code>.</p>
<p>A continuación se muestra un ejemplo de exportación en graphviz del árbol anterior entrenado en todo el conjunto de datos iris; los resultados se guardan en un archivo de salida <code class="docutils literal notranslate"><span class="pre">iris.pdf</span></code>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">graphviz</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">dot_data</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="o">=</span> <span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span> 
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span><span class="o">.</span><span class="n">render</span><span class="p">(</span><span class="s2">&quot;iris&quot;</span><span class="p">)</span> 
</pre></div>
</div>
<p>El exportador <a class="reference internal" href="generated/sklearn.tree.export_graphviz.html#sklearn.tree.export_graphviz" title="sklearn.tree.export_graphviz"><code class="xref py py-func docutils literal notranslate"><span class="pre">export_graphviz</span></code></a> también soporta una variedad de opciones estéticas, como colorear los nodos por su clase (o valor para la regresión) y usar nombres explícitos de variables y clases si así lo deseas. Los cuadernos de Jupyter también representan estos gráficos dentro de la línea automáticamente:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dot_data</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">export_graphviz</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">out_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> 
<span class="gp">... </span>                     <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">feature_names</span><span class="p">,</span>  
<span class="gp">... </span>                     <span class="n">class_names</span><span class="o">=</span><span class="n">iris</span><span class="o">.</span><span class="n">target_names</span><span class="p">,</span>  
<span class="gp">... </span>                     <span class="n">filled</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">rounded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>  
<span class="gp">... </span>                     <span class="n">special_characters</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> <span class="o">=</span> <span class="n">graphviz</span><span class="o">.</span><span class="n">Source</span><span class="p">(</span><span class="n">dot_data</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">graph</span> 
</pre></div>
</div>
<figure class="align-center">
<img alt="../_images/iris.svg" src="../_images/iris.svg" /></figure>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/tree/plot_iris_dtc.html"><img alt="../_images/sphx_glr_plot_iris_dtc_001.png" src="../_images/sphx_glr_plot_iris_dtc_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>Alternativamente, el árbol también puede exportarse en formato de texto con la función <a class="reference internal" href="generated/sklearn.tree.export_text.html#sklearn.tree.export_text" title="sklearn.tree.export_text"><code class="xref py py-func docutils literal notranslate"><span class="pre">export_text</span></code></a>. Este método no requiere la instalación de bibliotecas externas y es más compacto:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">export_text</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decision_tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decision_tree</span> <span class="o">=</span> <span class="n">decision_tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">r</span> <span class="o">=</span> <span class="n">export_text</span><span class="p">(</span><span class="n">decision_tree</span><span class="p">,</span> <span class="n">feature_names</span><span class="o">=</span><span class="n">iris</span><span class="p">[</span><span class="s1">&#39;feature_names&#39;</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">r</span><span class="p">)</span>
<span class="go">|--- petal width (cm) &lt;= 0.80</span>
<span class="go">|   |--- class: 0</span>
<span class="go">|--- petal width (cm) &gt;  0.80</span>
<span class="go">|   |--- petal width (cm) &lt;= 1.75</span>
<span class="go">|   |   |--- class: 1</span>
<span class="go">|   |--- petal width (cm) &gt;  1.75</span>
<span class="go">|   |   |--- class: 2</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/tree/plot_iris_dtc.html#sphx-glr-auto-examples-tree-plot-iris-dtc-py"><span class="std std-ref">Trazar la superficie de decisión de un árbol de decisiones en el conjunto de datos iris</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"><span class="std std-ref">Comprensión de la estructura del árbol de decisiones</span></a></p></li>
</ul>
</div>
</section>
<section id="regression">
<span id="tree-regression"></span><h2><span class="section-number">1.10.2. </span>Regresión<a class="headerlink" href="#regression" title="Enlazar permanentemente con este título">¶</a></h2>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/tree/plot_tree_regression.html"><img alt="../_images/sphx_glr_plot_tree_regression_001.png" src="../_images/sphx_glr_plot_tree_regression_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>Los árboles de decisión también pueden ser aplicados en problemas de regresión, utilizando la clase <a class="reference internal" href="generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a>.</p>
<p>Como en la configuración de la clasificación, el método de ajuste tomará como argumento los arreglos X e y, sólo que en este caso se espera que y tenga valores de punto flotante en lugar de valores enteros:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">tree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">DecisionTreeRegressor</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([0.5])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/tree/plot_tree_regression.html#sphx-glr-auto-examples-tree-plot-tree-regression-py"><span class="std std-ref">Regresión del árbol de decisión</span></a></p></li>
</ul>
</div>
</section>
<section id="multi-output-problems">
<span id="tree-multioutput"></span><h2><span class="section-number">1.10.3. </span>Problemas de salida múltiple<a class="headerlink" href="#multi-output-problems" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Un problema de salida múltiple es un problema de aprendizaje supervisado con varias salidas a predecir, que es cuando Y es un arreglo 2d de la forma <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_outputs)</span></code>.</p>
<p>Cuando no hay correlación entre las salidas, una forma muy sencilla de resolver este tipo de problema es construir n modelos independientes, es decir, uno para cada salida, y luego utilizar esos modelos para predecir independientemente cada una de las n salidas. Sin embargo, dado que es probable que los valores de salida relacionados con la misma entrada estén correlacionados, una forma a menudo mejor es construir un único modelo capaz de predecir simultáneamente todas las n salidas. En primer lugar, requiere menos tiempo de entrenamiento, ya que sólo se construye un único estimador. En segundo lugar, la precisión de la generalización del estimador resultante a menudo puede aumentar.</p>
<p>Con respecto a los árboles de decisión, esta estrategia puede utilizarse fácilmente para soportar los problemas de salida múltiple. Esto requiere los siguientes cambios:</p>
<blockquote>
<div><ul class="simple">
<li><p>Almacena n valores de salida en hojas, en lugar de 1;</p></li>
<li><p>Utiliza criterios de separación que calculen la reducción promedio en todas las n salidas.</p></li>
</ul>
</div></blockquote>
<p>Este módulo ofrece soporte para problemas de salida múltiple implementando esta estrategia tanto en <a class="reference internal" href="generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier" title="sklearn.tree.DecisionTreeClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeClassifier</span></code></a> como en <a class="reference internal" href="generated/sklearn.tree.DecisionTreeRegressor.html#sklearn.tree.DecisionTreeRegressor" title="sklearn.tree.DecisionTreeRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">DecisionTreeRegressor</span></code></a>. Si un árbol de decisión se ajusta a un arreglo de salida Y de la forma <code class="docutils literal notranslate"><span class="pre">(n_samples,</span> <span class="pre">n_outputs)</span></code> entonces el estimador resultante será:</p>
<blockquote>
<div><ul class="simple">
<li><p>Valores de salida n_output en <code class="docutils literal notranslate"><span class="pre">predict</span></code>;</p></li>
<li><p>Da salida a una lista de arreglos n_output de probabilidades de clase sobre <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code>.</p></li>
</ul>
</div></blockquote>
<p>El uso de árboles de salida múltiple para regresión se demuestra en <a class="reference internal" href="../auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py"><span class="std std-ref">Regresión del Árbol de Decisión con salida múltiple</span></a>. En este ejemplo, la entrada X es un único valor real y las salidas Y son el seno y el coseno de X.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/tree/plot_tree_regression_multioutput.html"><img alt="../_images/sphx_glr_plot_tree_regression_multioutput_001.png" src="../_images/sphx_glr_plot_tree_regression_multioutput_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>El uso de árboles de salida múltiple para la clasificación se demuestra en <a class="reference internal" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py"><span class="std std-ref">Finalización facial con estimadores de salida múltiple</span></a>. En este ejemplo, las entradas X son los píxeles de la mitad superior de las caras y las salidas Y son los píxeles de la mitad inferior de esas caras.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html"><img alt="../_images/sphx_glr_plot_multioutput_face_completion_001.png" src="../_images/sphx_glr_plot_multioutput_face_completion_001.png" style="width: 750.0px; height: 847.5px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/tree/plot_tree_regression_multioutput.html#sphx-glr-auto-examples-tree-plot-tree-regression-multioutput-py"><span class="std std-ref">Regresión del Árbol de Decisión con salida múltiple</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py"><span class="std std-ref">Finalización facial con estimadores de salida múltiple</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p>M. Dumont et al,  <a class="reference external" href="http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf">Fast multi-class image annotation with random subwindows
and multiple output randomized trees</a>, International Conference on
Computer Vision Theory and Applications 2009</p></li>
</ul>
</div>
</section>
<section id="complexity">
<span id="tree-complexity"></span><h2><span class="section-number">1.10.4. </span>Complejidad<a class="headerlink" href="#complexity" title="Enlazar permanentemente con este título">¶</a></h2>
<p>En general el costo de tiempo de ejecución para construir un árbol binario balanceado es <span class="math notranslate nohighlight">\(O(n_{samples}n_{features}\log(n_{samples}))\)</span> y el tiempo de consulta <span class="math notranslate nohighlight">\(O(\log(n_{samples}))\)</span>. Aunque el algoritmo de construcción de árboles intenta generar árboles equilibrados, no siempre lo estarán. Asumiendo que los subárboles permanecen aproximadamente equilibrados, el costo en cada nodo consiste en buscar a través de <span class="math notranslate nohighlight">\(O(n_{features})\)</span> para encontrar la característica que ofrece la mayor reducción de entropía. Esto tiene un costo de <span class="math notranslate nohighlight">\(O(n_{features}n_{samples}\log(n_{samples}))\)</span> en cada nodo, lo que lleva a un costo total sobre todos los árboles (sumando el costo en cada nodo) de <span class="math notranslate nohighlight">\(O(n_{features}n_{samples}^{2}\log(n_{samples}))\)</span>.</p>
</section>
<section id="tips-on-practical-use">
<h2><span class="section-number">1.10.5. </span>Consejos sobre uso práctico<a class="headerlink" href="#tips-on-practical-use" title="Enlazar permanentemente con este título">¶</a></h2>
<blockquote>
<div><ul>
<li><p>Los árboles de decisión tienden a sobreajustarse en datos con un gran número de características. Es importante obtener la proporción adecuada de muestras con respecto al número de características, ya que un árbol con pocas muestras en un espacio de alta dimensión es muy probable que se sobreajuste.</p></li>
<li><p>Considera la posibilidad de realizar una reducción de la dimensionalidad (<a class="reference internal" href="decomposition.html#pca"><span class="std std-ref">PCA</span></a>, <a class="reference internal" href="decomposition.html#ica"><span class="std std-ref">ICA</span></a>, o <a class="reference internal" href="feature_selection.html#feature-selection"><span class="std std-ref">Selección de características</span></a>) de antemano para dar a tu árbol una mejor oportunidad de encontrar características que sean discriminatorias.</p></li>
<li><p><a class="reference internal" href="../auto_examples/tree/plot_unveil_tree_structure.html#sphx-glr-auto-examples-tree-plot-unveil-tree-structure-py"><span class="std std-ref">Comprensión de la estructura del árbol de decisiones</span></a> ayudará a obtener más información sobre cómo el árbol de decisión hace las predicciones, lo cual es importante para comprender las características importantes de los datos.</p></li>
<li><p>Visualiza tu árbol mientras estás entrenando usando la función <code class="docutils literal notranslate"><span class="pre">export</span></code>. Usa <code class="docutils literal notranslate"><span class="pre">max_depth=3</span></code> como profundidad inicial del árbol para tener una idea de cómo se ajusta a tus datos, y luego aumenta la profundidad.</p></li>
<li><p>Recuerda que el número de muestras requeridas para poblar el árbol se duplica por cada nivel adicional al que crece el árbol. Usa <code class="docutils literal notranslate"><span class="pre">max_depth</span></code> para controlar el tamaño del árbol para prevenir el sobreajuste.</p></li>
<li><p>Utiliza <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code> o <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> para asegurarte de que las muestras múltiples informan cada decisión en el árbol, controlando qué divisiones serán consideradas. Un número muy pequeño normalmente significa que el árbol se sobreajustará, mientras que un número grande impedirá que el árbol aprenda de los datos. Prueba <code class="docutils literal notranslate"><span class="pre">min_samples_leaf=5</span></code> como un valor inicial. Si el tamaño de la muestra varía considerablemente, se puede utilizar un número de punto flotante como porcentaje en estos dos parámetros. Mientras que <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code> puede crear hojas arbitrariamente pequeñas, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code> garantiza que cada hoja tiene un tamaño mínimo, evitando nodos hoja de baja varianza y sobreajuste en problemas de regresión.  Para la clasificación con pocas clases, <code class="docutils literal notranslate"><span class="pre">min_samples_leaf=1</span></code> es a menudo la mejor opción.</p>
<p>Ten en cuenta que <code class="docutils literal notranslate"><span class="pre">min_samples_split</span></code> considera las muestras directamente e independientemente de <code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>, si se proporciona (por ejemplo, un nodo con m muestras ponderadas se sigue tratando como si tuviera exactamente m muestras). Considera <code class="docutils literal notranslate"><span class="pre">min_weight_fraction_leaf</span></code> o <code class="docutils literal notranslate"><span class="pre">min_impurity_decrease</span></code> si se deben tener en cuenta los pesos de las muestras en las divisiones.</p>
</li>
<li><p>Equilibra tu conjunto de datos antes del entrenamiento para evitar que el árbol esté sesgado hacia las clases que son dominantes. El equilibrio de las clases puede hacerse muestreando un número igual de muestras de cada clase, o preferiblemente normalizando la suma de las ponderaciones de las muestras (<code class="docutils literal notranslate"><span class="pre">sample_weight</span></code>) para cada clase al mismo valor. También ten en cuenta que los criterios de pre-poda basados en ponderaciones, tales como <code class="docutils literal notranslate"><span class="pre">min_weight_fraction_leaf</span></code>, entonces estarán menos sesgados hacia las clases dominantes que los criterios que no tienen en cuenta las ponderaciones de las muestras, como <code class="docutils literal notranslate"><span class="pre">min_samples_leaf</span></code>.</p></li>
<li><p>Si las muestras están ponderadas, será más fácil optimizar la estructura del árbol usando un criterio de pre-poda basado en ponderaciones como <code class="docutils literal notranslate"><span class="pre">min_weight_fraction_leaf</span></code>, que asegura que los nodos hoja contengan al menos una fracción de la suma total de las ponderaciones de las muestras.</p></li>
<li><p>Todos los árboles de decisión usan internamente arreglos <code class="docutils literal notranslate"><span class="pre">np.float32</span></code>. Si los datos de entrenamiento no están en este formato, se hará una copia del conjunto de datos.</p></li>
<li><p>Si la matriz de entrada X es muy dispersa, se recomienda convertirla a dispersa <code class="docutils literal notranslate"><span class="pre">csc_matrix</span></code> antes de llamar a fit y dispersa <code class="docutils literal notranslate"><span class="pre">csr_matrix</span></code> antes de llamar a predict. El tiempo de entrenamiento pueden ser órdenes de magnitud más rápido para una matriz de entrada dispersa en comparación con una matriz densa cuando las características tienen valores cero en la mayoría de las muestras.</p></li>
</ul>
</div></blockquote>
</section>
<section id="tree-algorithms-id3-c4-5-c5-0-and-cart">
<span id="tree-algorithms"></span><h2><span class="section-number">1.10.6. </span>Algoritmos de árbol: ID3, C4.5, C5.0 y CART<a class="headerlink" href="#tree-algorithms-id3-c4-5-c5-0-and-cart" title="Enlazar permanentemente con este título">¶</a></h2>
<p>¿Cuáles son los diferentes algoritmos de árboles de decisión y cómo se diferencian unos de otros? ¿Cuál está implementado en scikit-learn?</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/ID3_algorithm">ID3</a> (Iterative Dichotomiser 3, Dicotomizador iterativo 3) fue desarrollado en 1986 por Ross Quinlan. El algoritmo crea un árbol multidireccional, encontrando para cada nodo (es decir, de forma codiciosa) la característica categórica que producirá la mayor ganancia de información para los objetivos categóricos. Los árboles crecen hasta su tamaño máximo y luego, se suele aplicar un paso de poda para mejorar la capacidad del árbol de generalizar a los datos no vistos.</p>
<p>C4.5 es el sucesor de ID3 y elimina la restricción de que las características sean categóricas definiendo dinámicamente un atributo discreto (basado en variables numéricas) que particiona el valor del atributo continuo en un conjunto discreto de intervalos. C4.5 convierte los árboles entrenados (es decir, la salida del algoritmo ID3) en conjuntos de reglas if-then (si-entonces). Esa precisión de cada regla se evalúa para determinar el orden en que deben aplicarse. La poda se realiza eliminando la condición previa de una regla si la precisión de ésta mejora sin ella.</p>
<p>C5.0 es la última versión publicada por Quinlan bajo una licencia propietaria. Utiliza menos memoria y construye conjuntos de reglas más pequeños que C4.5, a la vez que es más preciso.</p>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Predictive_analytics#Classification_and_regression_trees_.28CART.29">CART</a> (Classification and Regression Trees, Árboles de Clasificación y Regresión) es muy similar a C4.5, pero difiere en que soporta variables objetivo numéricas (regresión) y no calcula conjuntos de reglas. CART construye árboles binarios usando la característica y el umbral que producen la mayor ganancia de información en cada nodo.</p>
<p>scikit-learn utiliza una versión optimizada del algoritmo CART; sin embargo, la implementación de scikit-learn no soporta variables categóricas por ahora.</p>
</section>
<section id="mathematical-formulation">
<span id="tree-mathematical-formulation"></span><h2><span class="section-number">1.10.7. </span>Formulación matemática<a class="headerlink" href="#mathematical-formulation" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Dados los vectores de entrenamiento <span class="math notranslate nohighlight">\(x_i \in R^n\)</span>, i=1,…, l y un vector de etiquetas <span class="math notranslate nohighlight">\(y \in R^l\)</span>, un árbol de decisión particiona recursivamente el espacio de características de tal manera que las muestras con las mismas etiquetas o valores objetivo similares se agrupan.</p>
<p>Deja que los datos en el nodo <span class="math notranslate nohighlight">\(m\)</span> sean representados por <span class="math notranslate nohighlight">\(Q_m\)</span> con <span class="math notranslate nohighlight">\(N_m\)</span> muestras. Para cada división candidata <span class="math notranslate nohighlight">\(\theta = (j, t_m)\)</span> que consiste en una característica <span class="math notranslate nohighlight">\(j\)</span> y un umbral <span class="math notranslate nohighlight">\(t_m\)</span>, particiona los datos en subconjuntos <span class="math notranslate nohighlight">\(Q_m^{left}(\theta)\)</span> y <span class="math notranslate nohighlight">\(Q_m^{right}(\theta)\)</span></p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}Q_m^{left}(\theta) = \{(x, y) | x_j &lt;= t_m\}\\Q_m^{right}(\theta) = Q_m \setminus Q_m^{left}(\theta)\end{aligned}\end{align} \]</div>
<p>La calidad de una división candidata del nodo <span class="math notranslate nohighlight">\(m\)</span> se calcula entonces utilizando una función de impureza o función de pérdida <span class="math notranslate nohighlight">\(H()\)</span>, cuya elección depende de la tarea que se esté resolviendo (clasificación o regresión)</p>
<div class="math notranslate nohighlight">
\[G(Q_m, \theta) = \frac{N_m^{left}}{N_m} H(Q_m^{left}(\theta))
+ \frac{N_m^{right}}{N_m} H(Q_m^{right}(\theta))\]</div>
<p>Selecciona los parámetros que minimizan la impureza</p>
<div class="math notranslate nohighlight">
\[\theta^* = \operatorname{argmin}_\theta  G(Q_m, \theta)\]</div>
<p>Recurre a los subconjuntos <span class="math notranslate nohighlight">\(Q_m^{left}(\theta^*)\)</span> y <span class="math notranslate nohighlight">\(Q_m^{right}(\theta^*)\)</span> hasta alcanzar la profundidad máxima permitida, <span class="math notranslate nohighlight">\(N_m &lt; \min_{samples}\)</span> o <span class="math notranslate nohighlight">\(N_m = 1\)</span>.</p>
<section id="classification-criteria">
<h3><span class="section-number">1.10.7.1. </span>Criterios de clasificación<a class="headerlink" href="#classification-criteria" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Si un objetivo es un resultado de clasificación que toma valores 0,1,…,K-1, para el nodo <span class="math notranslate nohighlight">\(m\)</span>, sea</p>
<div class="math notranslate nohighlight">
\[p_{mk} = 1/ N_m \sum_{y \in Q_m} I(y = k)\]</div>
<p>la proporción de observaciones de la clase k en el nodo <span class="math notranslate nohighlight">\(m\)</span>. Si <span class="math notranslate nohighlight">\(m\)</span> es un nodo terminal, <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> para esta región se establece en <span class="math notranslate nohighlight">\(p_{mk}\)</span>. Las medidas comunes de impureza son las siguientes.</p>
<p>Gini:</p>
<div class="math notranslate nohighlight">
\[H(Q_m) = \sum_k p_{mk} (1 - p_{mk})\]</div>
<p>Entropía:</p>
<div class="math notranslate nohighlight">
\[H(Q_m) = - \sum_k p_{mk} \log(p_{mk})\]</div>
<p>Clasificación errónea:</p>
<div class="math notranslate nohighlight">
\[H(Q_m) = 1 - \max(p_{mk})\]</div>
</section>
<section id="regression-criteria">
<h3><span class="section-number">1.10.7.2. </span>Criterios de Regresión<a class="headerlink" href="#regression-criteria" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Si el objetivo es un valor continuo, entonces para el nodo <span class="math notranslate nohighlight">\(m\)</span>, los criterios comunes a minimizar para determinar las ubicaciones de las futuras divisiones son el Error Cuadrático Medio (MSE o error L2), la desviación de Poisson así como el Error medio absoluto (MAE o error L1). El MSE y la desviación de Poisson establecen el valor predicho de los nodos terminales al valor medio aprendido <span class="math notranslate nohighlight">\(\bar{y}_m\)</span> del nodo mientras que el MAE establece el valor predicho de los nodos terminales a la mediana <span class="math notranslate nohighlight">\(median(y)_m\)</span>.</p>
<p>Error Cuadrático Medio:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}\bar{y}_m = \frac{1}{N_m} \sum_{y \in Q_m} y\\H(Q_m) = \frac{1}{N_m} \sum_{y \in Q_m} (y - \bar{y}_m)^2\end{aligned}\end{align} \]</div>
<p>Desviación media de Poisson:</p>
<div class="math notranslate nohighlight">
\[H(Q_m) = \frac{1}{N_m} \sum_{y \in Q_m} (y \log\frac{y}{\bar{y}_m}
- y + \bar{y}_m)\]</div>
<p>Establecer <code class="docutils literal notranslate"><span class="pre">criterion=&quot;poisson&quot;</span></code> puede ser una buena opción si tu objetivo es un conteo o una frecuencia (conteo por alguna unidad). En cualquier caso, <span class="math notranslate nohighlight">\(y &gt;= 0\)</span> es una condición necesaria para utilizar este criterio. Ten en cuenta que se ajusta mucho más lento que el criterio MSE.</p>
<p>Error medio absoluto:</p>
<div class="math notranslate nohighlight">
\[ \begin{align}\begin{aligned}median(y)_m = \underset{y \in Q_m}{\mathrm{median}}(y)\\H(Q_m) = \frac{1}{N_m} \sum_{y \in Q_m} |y - median(y)_m|\end{aligned}\end{align} \]</div>
<p>Ten en cuenta que se ajusta mucho más lento que el criterio del MSE.</p>
</section>
</section>
<section id="minimal-cost-complexity-pruning">
<span id="id1"></span><h2><span class="section-number">1.10.8. </span>Poda de Coste-Complejidad Mínima<a class="headerlink" href="#minimal-cost-complexity-pruning" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La poda de coste-complejidad mínima es un algoritmo utilizado para podar un árbol para evitar el sobreajuste, descrito en el capítulo 3 de <a class="reference internal" href="#bre" id="id2"><span>[BRE]</span></a>. Este algoritmo está parametrizado por <span class="math notranslate nohighlight">\(\alpha\ge0\)</span> conocido como el parámetro de complejidad. El parámetro de complejidad se utiliza para definir la medida de coste-complejidad, <span class="math notranslate nohighlight">\(R_\alpha(T)\)</span> de un árbol <span class="math notranslate nohighlight">\(T\)</span> dado:</p>
<div class="math notranslate nohighlight">
\[R_\alpha(T) = R(T) + \alpha|\widetilde{T}|\]</div>
<p>donde <span class="math notranslate nohighlight">\(|\widetilde{T}|\)</span> es el número de nodos terminales en <span class="math notranslate nohighlight">\(T\)</span> y <span class="math notranslate nohighlight">\(R(T)\)</span> se define tradicionalmente como la tasa total de clasificación errónea de los nodos terminales. Alternativamente, scikit-learn utiliza la impureza total ponderada de la muestra de los nodos terminales para <span class="math notranslate nohighlight">\(R(T)\)</span>. Como se muestra arriba, la impureza de un nodo depende del criterio. La poda de coste-complejidad mínima encuentra el subárbol de <span class="math notranslate nohighlight">\(T\)</span> que minimiza <span class="math notranslate nohighlight">\(R_alpha(T)\)</span>.</p>
<p>La medida de costo-complejidad de un único nodo es <span class="math notranslate nohighlight">\(R_\alpha(t)=R(t)+\alpha\)</span>. La rama, <span class="math notranslate nohighlight">\(T_t\)</span>, se define como un árbol donde el nodo <span class="math notranslate nohighlight">\(t\)</span> es su raíz. En general, la impureza de un nodo es mayor que la suma de impurezas de sus nodos terminales, <span class="math notranslate nohighlight">\(R(T_t)&lt;R(t)\)</span>. Sin embargo, la medida de costo-complejidad de un nodo, <span class="math notranslate nohighlight">\(t\)</span>, y su rama, <span class="math notranslate nohighlight">\(T_t\)</span>, pueden ser iguales dependiendo de <span class="math notranslate nohighlight">\(alpha\)</span>. Definimos el <span class="math notranslate nohighlight">\(alpha\)</span> efectivo de un nodo como el valor donde son iguales, <span class="math notranslate nohighlight">\(R_\alpha(T_t)=R_\alpha(t)\)</span> o <span class="math notranslate nohighlight">\(\alpha_{eff}(t)=\frac{R(t)-R(T_t)}{|T|-1}\)</span>. Un nodo no terminal con el valor más pequeño de <span class="math notranslate nohighlight">\(\alpha_{eff}\)</span> es el enlace más débil y será podado. Este proceso se detiene cuando el valor mínimo de <span class="math notranslate nohighlight">\(\alpha_{eff}\)</span> del árbol podado es mayor que el parámetro <code class="docutils literal notranslate"><span class="pre">ccp_alpha</span></code>.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/tree/plot_cost_complexity_pruning.html#sphx-glr-auto-examples-tree-plot-cost-complexity-pruning-py"><span class="std std-ref">Árboles de decisión con poda de complejidad de costes</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<dl class="citation">
<dt class="label" id="bre"><span class="brackets"><a class="fn-backref" href="#id2">BRE</a></span></dt>
<dd><p>L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification
and Regression Trees. Wadsworth, Belmont, CA, 1984.</p>
</dd>
</dl>
<ul class="simple">
<li><p><a class="reference external" href="https://es.wikipedia.org/wiki/Aprendizaje_basado_en_%C3%A1rboles_de_decisi%C3%B3n">https://es.wikipedia.org/wiki/Aprendizaje_basado_en_%C3%A1rboles_de_decisi%C3%B3n</a></p></li>
<li><p><a class="reference external" href="https://es.wikipedia.org/wiki/An%C3%A1lisis_predictivo">https://es.wikipedia.org/wiki/An%C3%A1lisis_predictivo</a></p></li>
<li><p>J.R. Quinlan. C4. 5: programs for machine learning. Morgan
Kaufmann, 1993.</p></li>
<li><p>T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical
Learning, Springer, 2009.</p></li>
</ul>
</div>
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../_sources/modules/tree.rst.txt" rel="nofollow">Mostrar la fuente de esta página</a>
      </footer>
    </div>
  </div>
</div>
<script src="../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>