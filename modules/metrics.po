# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/metrics.rst:4
msgid "Pairwise metrics, Affinities and Kernels"
msgstr ""

#: ../modules/metrics.rst:6
msgid ""
"The :mod:`sklearn.metrics.pairwise` submodule implements utilities to "
"evaluate pairwise distances or affinity of sets of samples."
msgstr ""

#: ../modules/metrics.rst:9
msgid ""
"This module contains both distance metrics and kernels. A brief summary "
"is given on the two here."
msgstr ""

#: ../modules/metrics.rst:12
msgid ""
"Distance metrics are functions ``d(a, b)`` such that ``d(a, b) < d(a, "
"c)`` if objects ``a`` and ``b`` are considered \"more similar\" than "
"objects ``a`` and ``c``. Two objects exactly alike would have a distance "
"of zero. One of the most popular examples is Euclidean distance. To be a "
"'true' metric, it must obey the following four conditions::"
msgstr ""

#: ../modules/metrics.rst:23
msgid ""
"Kernels are measures of similarity, i.e. ``s(a, b) > s(a, c)`` if objects"
" ``a`` and ``b`` are considered \"more similar\" than objects ``a`` and "
"``c``. A kernel must also be positive semi-definite."
msgstr ""

#: ../modules/metrics.rst:27
msgid ""
"There are a number of ways to convert between a distance metric and a "
"similarity measure, such as a kernel. Let ``D`` be the distance, and "
"``S`` be the kernel:"
msgstr ""

#: ../modules/metrics.rst:31
msgid ""
"``S = np.exp(-D * gamma)``, where one heuristic for choosing ``gamma`` is"
" ``1 / num_features``"
msgstr ""

#: ../modules/metrics.rst:33
msgid "``S = 1. / (D / np.max(D))``"
msgstr ""

#: ../modules/metrics.rst:38
msgid ""
"The distances between the row vectors of ``X`` and the row vectors of "
"``Y`` can be evaluated using :func:`pairwise_distances`. If ``Y`` is "
"omitted the pairwise distances of the row vectors of ``X`` are "
"calculated. Similarly, :func:`pairwise.pairwise_kernels` can be used to "
"calculate the kernel between `X` and `Y` using different kernel "
"functions. See the API reference for more details."
msgstr ""

#: ../modules/metrics.rst:69
msgid "Cosine similarity"
msgstr ""

#: ../modules/metrics.rst:70
msgid ""
":func:`cosine_similarity` computes the L2-normalized dot product of "
"vectors. That is, if :math:`x` and :math:`y` are row vectors, their "
"cosine similarity :math:`k` is defined as:"
msgstr ""

#: ../modules/metrics.rst:74
msgid "k(x, y) = \\frac{x y^\\top}{\\|x\\| \\|y\\|}"
msgstr ""

#: ../modules/metrics.rst:78
msgid ""
"This is called cosine similarity, because Euclidean (L2) normalization "
"projects the vectors onto the unit sphere, and their dot product is then "
"the cosine of the angle between the points denoted by the vectors."
msgstr ""

#: ../modules/metrics.rst:83
msgid ""
"This kernel is a popular choice for computing the similarity of documents"
" represented as tf-idf vectors. :func:`cosine_similarity` accepts "
"``scipy.sparse`` matrices. (Note that the tf-idf functionality in "
"``sklearn.feature_extraction.text`` can produce normalized vectors, in "
"which case :func:`cosine_similarity` is equivalent to "
":func:`linear_kernel`, only slower.)"
msgstr ""

#: ../modules/metrics.rst:92
msgid ""
"C.D. Manning, P. Raghavan and H. Sch√ºtze (2008). Introduction to "
"Information Retrieval. Cambridge University Press. "
"https://nlp.stanford.edu/IR-book/html/htmledition/the-vector-space-model-"
"for-scoring-1.html"
msgstr ""

#: ../modules/metrics.rst:99
msgid "Linear kernel"
msgstr ""

#: ../modules/metrics.rst:100
msgid ""
"The function :func:`linear_kernel` computes the linear kernel, that is, a"
" special case of :func:`polynomial_kernel` with ``degree=1`` and "
"``coef0=0`` (homogeneous). If ``x`` and ``y`` are column vectors, their "
"linear kernel is:"
msgstr ""

#: ../modules/metrics.rst:104
msgid "k(x, y) = x^\\top y"
msgstr ""

#: ../modules/metrics.rst:111
msgid "Polynomial kernel"
msgstr ""

#: ../modules/metrics.rst:112
msgid ""
"The function :func:`polynomial_kernel` computes the degree-d polynomial "
"kernel between two vectors. The polynomial kernel represents the "
"similarity between two vectors. Conceptually, the polynomial kernels "
"considers not only the similarity between vectors under the same "
"dimension, but also across dimensions. When used in machine learning "
"algorithms, this allows to account for feature interaction."
msgstr ""

#: ../modules/metrics.rst:118
msgid "The polynomial kernel is defined as:"
msgstr ""

#: ../modules/metrics.rst:120
msgid "k(x, y) = (\\gamma x^\\top y +c_0)^d"
msgstr ""

#: ../modules/metrics.rst:124 ../modules/metrics.rst:144
msgid "where:"
msgstr ""

#: ../modules/metrics.rst:126 ../modules/metrics.rst:146
msgid "``x``, ``y`` are the input vectors"
msgstr ""

#: ../modules/metrics.rst:127
msgid "``d`` is the kernel degree"
msgstr ""

#: ../modules/metrics.rst:129
msgid "If :math:`c_0 = 0` the kernel is said to be homogeneous."
msgstr ""

#: ../modules/metrics.rst:134
msgid "Sigmoid kernel"
msgstr ""

#: ../modules/metrics.rst:135
msgid ""
"The function :func:`sigmoid_kernel` computes the sigmoid kernel between "
"two vectors. The sigmoid kernel is also known as hyperbolic tangent, or "
"Multilayer Perceptron (because, in the neural network field, it is often "
"used as neuron activation function). It is defined as:"
msgstr ""

#: ../modules/metrics.rst:140
msgid "k(x, y) = \\tanh( \\gamma x^\\top y + c_0)"
msgstr ""

#: ../modules/metrics.rst:147
msgid ":math:`\\gamma` is known as slope"
msgstr ""

#: ../modules/metrics.rst:148
msgid ":math:`c_0` is known as intercept"
msgstr ""

#: ../modules/metrics.rst:153
msgid "RBF kernel"
msgstr ""

#: ../modules/metrics.rst:154
msgid ""
"The function :func:`rbf_kernel` computes the radial basis function (RBF) "
"kernel between two vectors. This kernel is defined as:"
msgstr ""

#: ../modules/metrics.rst:157
msgid "k(x, y) = \\exp( -\\gamma \\| x-y \\|^2)"
msgstr ""

#: ../modules/metrics.rst:161
msgid ""
"where ``x`` and ``y`` are the input vectors. If :math:`\\gamma = "
"\\sigma^{-2}` the kernel is known as the Gaussian kernel of variance "
":math:`\\sigma^2`."
msgstr ""

#: ../modules/metrics.rst:167
msgid "Laplacian kernel"
msgstr ""

#: ../modules/metrics.rst:168
msgid ""
"The function :func:`laplacian_kernel` is a variant on the radial basis "
"function kernel defined as:"
msgstr ""

#: ../modules/metrics.rst:171
msgid "k(x, y) = \\exp( -\\gamma \\| x-y \\|_1)"
msgstr ""

#: ../modules/metrics.rst:175
msgid ""
"where ``x`` and ``y`` are the input vectors and :math:`\\|x-y\\|_1` is "
"the Manhattan distance between the input vectors."
msgstr ""

#: ../modules/metrics.rst:178
msgid ""
"It has proven useful in ML applied to noiseless data. See e.g. `Machine "
"learning for quantum mechanics in a nutshell "
"<https://onlinelibrary.wiley.com/doi/10.1002/qua.24954/abstract/>`_."
msgstr ""

#: ../modules/metrics.rst:185
msgid "Chi-squared kernel"
msgstr ""

#: ../modules/metrics.rst:186
msgid ""
"The chi-squared kernel is a very popular choice for training non-linear "
"SVMs in computer vision applications. It can be computed using "
":func:`chi2_kernel` and then passed to an :class:`~sklearn.svm.SVC` with "
"``kernel=\"precomputed\"``::"
msgstr ""

#: ../modules/metrics.rst:206
msgid "It can also be directly used as the ``kernel`` argument::"
msgstr ""

#: ../modules/metrics.rst:213
msgid "The chi squared kernel is given by"
msgstr ""

#: ../modules/metrics.rst:215
msgid ""
"k(x, y) = \\exp \\left (-\\gamma \\sum_i \\frac{(x[i] - y[i]) ^ 2}{x[i] +"
" y[i]} \\right )"
msgstr ""

#: ../modules/metrics.rst:219
msgid ""
"The data is assumed to be non-negative, and is often normalized to have "
"an L1-norm of one. The normalization is rationalized with the connection "
"to the chi squared distance, which is a distance between discrete "
"probability distributions."
msgstr ""

#: ../modules/metrics.rst:223
msgid ""
"The chi squared kernel is most commonly used on histograms (bags) of "
"visual words."
msgstr ""

#: ../modules/metrics.rst:227
msgid ""
"Zhang, J. and Marszalek, M. and Lazebnik, S. and Schmid, C. Local "
"features and kernels for classification of texture and object categories:"
" A comprehensive study International Journal of Computer Vision 2007 "
"https://research.microsoft.com/en-us/um/people/manik/projects/trade-"
"off/papers/ZhangIJCV06.pdf"
msgstr ""

