# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/generated/sklearn.metrics.average_precision_score.rst:2
msgid ":mod:`sklearn.metrics`.average_precision_score"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:2
msgid "Compute average precision (AP) from prediction scores."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:4
msgid ""
"AP summarizes a precision-recall curve as the weighted mean of precisions"
" achieved at each threshold, with the increase in recall from the "
"previous threshold used as the weight:"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:8
msgid ""
"\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\n"
"\n"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:11
msgid ""
"where :math:`P_n` and :math:`R_n` are the precision and recall at the nth"
" threshold [Rcdf8f32d7f9d-1]_. This implementation is not interpolated "
"and is different from computing the area under the precision-recall curve"
" with the trapezoidal rule, which uses linear interpolation and can be "
"too optimistic."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:17
msgid ""
"Note: this implementation is restricted to the binary classification task"
" or multilabel classification task."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:20
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score
msgid "Parameters"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:25
msgid "**y_true**"
msgstr ""

#: of
msgid "ndarray of shape (n_samples,) or (n_samples, n_classes)"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:25
msgid "True binary labels or binary label indicators."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:30
msgid "**y_score**"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:28
msgid ""
"Target scores, can either be probability estimates of the positive class,"
" confidence values, or non-thresholded measure of decisions (as returned "
"by :term:`decision_function` on some classifiers)."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:48
msgid "**average**"
msgstr ""

#: of
msgid ""
"{'micro', 'samples', 'weighted', 'macro'} or None,             "
"default='macro'"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:33
msgid ""
"If ``None``, the scores for each class are returned. Otherwise, this "
"determines the type of averaging performed on the data:"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:37
msgid "``'micro'``:"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:37
msgid ""
"Calculate metrics globally by considering each element of the label "
"indicator matrix as a label."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:40
msgid "``'macro'``:"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:40
msgid ""
"Calculate metrics for each label, and find their unweighted mean.  This "
"does not take label imbalance into account."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:43
msgid "``'weighted'``:"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:43
msgid ""
"Calculate metrics for each label, and find their average, weighted by "
"support (the number of true instances for each label)."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:46
msgid "``'samples'``:"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:46
msgid "Calculate metrics for each instance, and find their average."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:48
msgid "Will be ignored when ``y_true`` is binary."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:52
msgid "**pos_label**"
msgstr ""

#: of
msgid "int or str, default=1"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:51
msgid ""
"The label of the positive class. Only applied to binary ``y_true``. For "
"multilabel-indicator ``y_true``, ``pos_label`` is fixed to 1."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:55
msgid "**sample_weight**"
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:55
msgid "Sample weights."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score
msgid "Returns"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:66
msgid "**average_precision**"
msgstr ""

#: of
msgid "float"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:71
msgid ":obj:`roc_auc_score`"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:72
msgid "Compute the area under the ROC curve."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:73
msgid ":obj:`precision_recall_curve`"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:74
msgid "Compute precision-recall pairs for different probability thresholds."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:78
msgid "Notes"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:79
msgid ""
"Instead of linearly interpolating between operating points, precisions "
"are weighted by the change in recall since the last operating point."
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:84
msgid "References"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:85
msgid ""
"`Wikipedia entry for the Average precision "
"<https://en.wikipedia.org/w/index.php?title=Information_retrieval& "
"oldid=793358396#Average_precision>`_"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:91
msgid "[Rcdf8f32d7f9d-1]_"
msgstr ""

#: of sklearn.metrics._ranking.average_precision_score:94
msgid "Examples"
msgstr ""

#: ../modules/generated/sklearn.metrics.average_precision_score.examples:4
msgid "Examples using ``sklearn.metrics.average_precision_score``"
msgstr ""

#: ../modules/generated/sklearn.metrics.average_precision_score.examples:15
#: ../modules/generated/sklearn.metrics.average_precision_score.examples:23
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`"
msgstr ""

