# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/generated/sklearn.linear_model.ElasticNet.rst:2
msgid ":mod:`sklearn.linear_model`.ElasticNet"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:2
msgid "Linear regression with combined L1 and L2 priors as regularizer."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:4
msgid "Minimizes the objective function::"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:10
msgid ""
"If you are interested in controlling the L1 and L2 penalty separately, "
"keep in mind that this is equivalent to::"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:15
msgid "where::"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:19
msgid ""
"The parameter l1_ratio corresponds to alpha in the glmnet R package while"
" alpha corresponds to the lambda parameter in glmnet. Specifically, "
"l1_ratio = 1 is the lasso penalty. Currently, l1_ratio <= 0.01 is not "
"reliable, unless you supply your own sequence of alpha."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:24
#: sklearn.linear_model._coordinate_descent.enet_path:24
msgid "Read more in the :ref:`User Guide <elastic_net>`."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params
#: sklearn.base.BaseEstimator.set_params sklearn.base.RegressorMixin.score
#: sklearn.linear_model._base.LinearModel.predict
#: sklearn.linear_model._coordinate_descent.ElasticNet
#: sklearn.linear_model._coordinate_descent.ElasticNet.fit
#: sklearn.linear_model._coordinate_descent.enet_path
msgid "Parameters"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:34
msgid "**alpha**"
msgstr ""

#: of
msgid "float, default=1.0"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:29
msgid ""
"Constant that multiplies the penalty terms. Defaults to 1.0. See the "
"notes for the exact mathematical meaning of this parameter. ``alpha = 0``"
" is equivalent to an ordinary least square, solved by the "
":class:`LinearRegression` object. For numerical reasons, using ``alpha = "
"0`` with the ``Lasso`` object is not advised. Given this, you should use "
"the :class:`LinearRegression` object."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:40
#: sklearn.linear_model._coordinate_descent.enet_path:38
msgid "**l1_ratio**"
msgstr ""

#: of
msgid "float, default=0.5"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:37
msgid ""
"The ElasticNet mixing parameter, with ``0 <= l1_ratio <= 1``. For "
"``l1_ratio = 0`` the penalty is an L2 penalty. ``For l1_ratio = 1`` it is"
" an L1 penalty.  For ``0 < l1_ratio < 1``, the penalty is a combination "
"of L1 and L2."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:44
msgid "**fit_intercept**"
msgstr ""

#: of
msgid "bool, default=True"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:43
msgid ""
"Whether the intercept should be estimated or not. If ``False``, the data "
"is assumed to be already centered."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:52
msgid "**normalize**"
msgstr ""

#: of
msgid "bool, default=False"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:47
msgid ""
"This parameter is ignored when ``fit_intercept`` is set to False. If "
"True, the regressors X will be normalized before regression by "
"subtracting the mean and dividing by the l2-norm. If you wish to "
"standardize, please use :class:`~sklearn.preprocessing.StandardScaler` "
"before calling ``fit`` on an estimator with ``normalize=False``."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:57
#: sklearn.linear_model._coordinate_descent.enet_path:54
msgid "**precompute**"
msgstr ""

#: of
msgid ""
"bool or array-like of shape (n_features, n_features),                 "
"default=False"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:55
msgid ""
"Whether to use a precomputed Gram matrix to speed up calculations. The "
"Gram matrix can also be passed as argument. For sparse input this option "
"is always ``True`` to preserve sparsity."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:60
msgid "**max_iter**"
msgstr ""

#: of
msgid "int, default=1000"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:60
msgid "The maximum number of iterations."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:63
#: sklearn.linear_model._coordinate_descent.enet_path:61
msgid "**copy_X**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:63
#: sklearn.linear_model._coordinate_descent.enet_path:61
msgid "If ``True``, X will be copied; else, it may be overwritten."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:69
msgid "**tol**"
msgstr ""

#: of
msgid "float, default=1e-4"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:66
msgid ""
"The tolerance for the optimization: if the updates are smaller than "
"``tol``, the optimization code checks the dual gap for optimality and "
"continues until it is smaller than ``tol``."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:74
msgid "**warm_start**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:72
msgid ""
"When set to ``True``, reuse the solution of the previous call to fit as "
"initialization, otherwise, just erase the previous solution. See "
":term:`the Glossary <warm_start>`."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:77
#: sklearn.linear_model._coordinate_descent.enet_path:74
msgid "**positive**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:77
msgid "When set to ``True``, forces the coefficients to be positive."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:83
msgid "**random_state**"
msgstr ""

#: of
msgid "int, RandomState instance, default=None"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:80
msgid ""
"The seed of the pseudo random number generator that selects a random "
"feature to update. Used when ``selection`` == 'random'. Pass an int for "
"reproducible output across multiple function calls. See :term:`Glossary "
"<random_state>`."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:92
msgid "**selection**"
msgstr ""

#: of
msgid "{'cyclic', 'random'}, default='cyclic'"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:86
msgid ""
"If set to 'random', a random coefficient is updated every iteration "
"rather than looping over features sequentially by default. This (setting "
"to 'random') often leads to significantly faster convergence especially "
"when tol is higher than 1e-4."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet
msgid "Attributes"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:97
msgid "**coef_**"
msgstr ""

#: of
msgid "ndarray of shape (n_features,) or (n_targets, n_features)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:97
msgid "Parameter vector (w in the cost function formula)."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:100
msgid ":obj:`sparse_coef_ <sparse_coef_>`"
msgstr ""

#: of
msgid "sparse matrix of shape (n_features,) or             (n_tasks, n_features)"
msgstr ""

#: of sklearn.linear_model.ElasticNet.sparse_coef_:2
#: sklearn.linear_model._coordinate_descent.ElasticNet:100
msgid "Sparse representation of the fitted `coef_`."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:103
msgid "**intercept_**"
msgstr ""

#: of
msgid "float or ndarray of shape (n_targets,)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:103
msgid "Independent term in decision function."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:107
msgid "**n_iter_**"
msgstr ""

#: of
msgid "list of int"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:106
msgid ""
"Number of iterations run by the coordinate descent solver to reach the "
"specified tolerance."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:114
msgid "**dual_gap_**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:110
msgid ""
"Given param alpha, the dual gaps at the end of the optimization, same "
"shape as each observation of y."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:119
#: sklearn.linear_model._coordinate_descent.enet_path:115
msgid ":obj:`ElasticNetCV`"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:120
msgid "Elastic net model with best model selection by cross-validation."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:121
msgid ":obj:`SGDRegressor`"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:122
msgid "Implements elastic net regression with incremental training."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:123
msgid ":obj:`SGDClassifier`"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:124
msgid ""
"Implements logistic regression with elastic net penalty "
"(``SGDClassifier(loss=\"log\", penalty=\"elasticnet\")``)."
msgstr ""

#: of sklearn.base.RegressorMixin.score:41
#: sklearn.linear_model._coordinate_descent.ElasticNet:128
#: sklearn.linear_model._coordinate_descent.ElasticNet.fit:31
#: sklearn.linear_model._coordinate_descent.enet_path:119
msgid "Notes"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:129
msgid ""
"To avoid unnecessary memory duplication the X argument of the fit method "
"should be directly passed as a Fortran-contiguous numpy array."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:134
msgid "Examples"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:150
msgid "Methods"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid ":obj:`fit <sklearn.linear_model.ElasticNet.fit>`\\"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:2
#: sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid "Fit model with coordinate descent."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid ":obj:`get_params <sklearn.linear_model.ElasticNet.get_params>`\\"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:2
#: sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid "Get parameters for this estimator."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid ":obj:`path <sklearn.linear_model.ElasticNet.path>`\\"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
#: sklearn.linear_model._coordinate_descent.enet_path:2
msgid "Compute elastic net path with coordinate descent."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid ":obj:`predict <sklearn.linear_model.ElasticNet.predict>`\\"
msgstr ""

#: of sklearn.linear_model._base.LinearModel.predict:2
#: sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid "Predict using the linear model."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid ":obj:`score <sklearn.linear_model.ElasticNet.score>`\\"
msgstr ""

#: of sklearn.base.RegressorMixin.score:2
#: sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid "Return the coefficient of determination :math:`R^2` of the prediction."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid ":obj:`set_params <sklearn.linear_model.ElasticNet.set_params>`\\"
msgstr ""

#: of sklearn.base.BaseEstimator.set_params:2
#: sklearn.linear_model._coordinate_descent.ElasticNet:159:<autosummary>:1
msgid "Set the parameters of this estimator."
msgstr ""

#: of sklearn.base.RegressorMixin.score:20
#: sklearn.linear_model._base.LinearModel.predict:8
#: sklearn.linear_model._coordinate_descent.ElasticNet.fit:8
#: sklearn.linear_model._coordinate_descent.enet_path:31
msgid "**X**"
msgstr ""

#: of
msgid "{ndarray, sparse matrix} of (n_samples, n_features)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:8
msgid "Data."
msgstr ""

#: of sklearn.base.RegressorMixin.score:23
#: sklearn.linear_model._coordinate_descent.ElasticNet.fit:11
#: sklearn.linear_model._coordinate_descent.enet_path:34
msgid "**y**"
msgstr ""

#: of
msgid ""
"{ndarray, sparse matrix} of shape (n_samples,) or             (n_samples,"
" n_targets)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:11
msgid "Target. Will be cast to X's dtype if necessary."
msgstr ""

#: of sklearn.base.RegressorMixin.score:26
#: sklearn.linear_model._coordinate_descent.ElasticNet.fit:16
msgid "**sample_weight**"
msgstr ""

#: of
msgid "float or array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:14
msgid "Sample weight."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:28
#: sklearn.linear_model._coordinate_descent.enet_path:79
msgid "**check_input**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:19
msgid ""
"Allow to bypass several input checking. Don't use this parameter unless "
"you know what you do."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:32
msgid ""
"Coordinate descent is an algorithm that considers each column of data at "
"a time hence it will automatically convert the X input as a Fortran-"
"contiguous numpy array if necessary."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.ElasticNet.fit:36
msgid ""
"To avoid memory re-allocation it is advised to allocate the initial data "
"in memory directly using that format."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:9
msgid "**deep**"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:8
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params
#: sklearn.base.BaseEstimator.set_params sklearn.base.RegressorMixin.score
#: sklearn.linear_model._base.LinearModel.predict
#: sklearn.linear_model._coordinate_descent.enet_path
msgid "Returns"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:25
msgid "**params**"
msgstr ""

#: of
msgid "dict"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:14
msgid "Parameter names mapped to their values."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:4
msgid "The elastic net optimization function varies for mono and multi-outputs."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:6
msgid "For mono-output tasks it is::"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:12
msgid "For multi-output tasks it is::"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:18
msgid "Where::"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:22
msgid "i.e. the sum of norm of each row."
msgstr ""

#: of
msgid "{array-like, sparse matrix} of shape (n_samples, n_features)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:29
msgid ""
"Training data. Pass directly as Fortran-contiguous data to avoid "
"unnecessary memory duplication. If ``y`` is mono-output then ``X`` can be"
" sparse."
msgstr ""

#: of
msgid ""
"{array-like, sparse matrix} of shape (n_samples,) or         (n_samples, "
"n_outputs)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:34
msgid "Target values."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:37
msgid ""
"Number between 0 and 1 passed to elastic net (scaling between l1 and l2 "
"penalties). ``l1_ratio=1`` corresponds to the Lasso."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:42
msgid "**eps**"
msgstr ""

#: of
msgid "float, default=1e-3"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:41
msgid ""
"Length of the path. ``eps=1e-3`` means that ``alpha_min / alpha_max = "
"1e-3``."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:45
msgid "**n_alphas**"
msgstr ""

#: of
msgid "int, default=100"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:45
msgid "Number of alphas along the regularization path."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:49
#: sklearn.linear_model._coordinate_descent.enet_path:87
msgid "**alphas**"
msgstr ""

#: of
msgid "ndarray, default=None"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:48
msgid ""
"List of alphas where to compute the models. If None alphas are set "
"automatically."
msgstr ""

#: of
msgid ""
"'auto', bool or array-like of shape (n_features, n_features),"
"                 default='auto'"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:52
msgid ""
"Whether to use a precomputed Gram matrix to speed up calculations. If set"
" to ``'auto'`` let us decide. The Gram matrix can also be passed as "
"argument."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:58
msgid "**Xy**"
msgstr ""

#: of
msgid ""
"array-like of shape (n_features,) or (n_features, n_outputs),         "
"default=None"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:57
msgid ""
"Xy = np.dot(X.T, y) that can be precomputed. It is useful only when the "
"Gram matrix is precomputed."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:64
msgid "**coef_init**"
msgstr ""

#: of
msgid "ndarray of shape (n_features, ), default=None"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:64
msgid "The initial values of the coefficients."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:67
msgid "**verbose**"
msgstr ""

#: of
msgid "bool or int, default=False"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:67
msgid "Amount of verbosity."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:70
msgid "**return_n_iter**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:70
msgid "Whether to return the number of iterations or not."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:73
msgid ""
"If set to True, forces coefficients to be positive. (Only allowed when "
"``y.ndim == 1``)."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:77
msgid ""
"If set to False, the input validation checks are skipped (including the "
"Gram matrix when provided). It is assumed that they are handled by the "
"caller."
msgstr ""

#: of sklearn.base.BaseEstimator.set_params:12
#: sklearn.linear_model._coordinate_descent.enet_path:82
msgid "**\\*\\*params**"
msgstr ""

#: of
msgid "kwargs"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:82
msgid "Keyword arguments passed to the coordinate descent solver."
msgstr ""

#: of
msgid "ndarray of shape (n_alphas,)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:87
msgid "The alphas along the path where models are computed."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:90
msgid "**coefs**"
msgstr ""

#: of
msgid ""
"ndarray of shape (n_features, n_alphas) or             (n_outputs, "
"n_features, n_alphas)"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:90
msgid "Coefficients along the path."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:93
msgid "**dual_gaps**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:93
msgid "The dual gaps at the end of the optimization for each alpha."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:104
msgid "**n_iters**"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:96
msgid ""
"The number of iterations taken by the coordinate descent optimizer to "
"reach the specified tolerance for each alpha. (Is returned when "
"``return_n_iter`` is set to True)."
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:109
msgid ":obj:`MultiTaskElasticNet`"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:111
msgid ":obj:`MultiTaskElasticNetCV`"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:113
msgid ":obj:`ElasticNet`"
msgstr ""

#: of sklearn.linear_model._coordinate_descent.enet_path:120
msgid ""
"For an example, see "
":ref:`examples/linear_model/plot_lasso_coordinate_descent_path.py "
"<sphx_glr_auto_examples_linear_model_plot_lasso_coordinate_descent_path.py>`."
msgstr ""

#: of
msgid "array-like or sparse matrix, shape (n_samples, n_features)"
msgstr ""

#: of sklearn.linear_model._base.LinearModel.predict:8
msgid "Samples."
msgstr ""

#: of sklearn.linear_model._base.LinearModel.predict:24
msgid "**C**"
msgstr ""

#: of
msgid "array, shape (n_samples,)"
msgstr ""

#: of sklearn.linear_model._base.LinearModel.predict:13
msgid "Returns predicted values."
msgstr ""

#: of sklearn.base.RegressorMixin.score:5
msgid ""
"The coefficient :math:`R^2` is defined as :math:`(1 - \\frac{u}{v})`, "
"where :math:`u` is the residual sum of squares ``((y_true - y_pred) ** "
"2).sum()`` and :math:`v` is the total sum of squares ``((y_true - "
"y_true.mean()) ** 2).sum()``. The best possible score is 1.0 and it can "
"be negative (because the model can be arbitrarily worse). A constant "
"model that always predicts the expected value of `y`, disregarding the "
"input features, would get a :math:`R^2` score of 0.0."
msgstr ""

#: of
msgid "array-like of shape (n_samples, n_features)"
msgstr ""

#: of sklearn.base.RegressorMixin.score:17
msgid ""
"Test samples. For some estimators this may be a precomputed kernel matrix"
" or a list of generic objects instead with shape ``(n_samples, "
"n_samples_fitted)``, where ``n_samples_fitted`` is the number of samples "
"used in the fitting for the estimator."
msgstr ""

#: of
msgid "array-like of shape (n_samples,) or (n_samples, n_outputs)"
msgstr ""

#: of sklearn.base.RegressorMixin.score:23
msgid "True values for `X`."
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.base.RegressorMixin.score:26
msgid "Sample weights."
msgstr ""

#: of sklearn.base.RegressorMixin.score:38
msgid "**score**"
msgstr ""

#: of
msgid "float"
msgstr ""

#: of sklearn.base.RegressorMixin.score:31
msgid ":math:`R^2` of ``self.predict(X)`` wrt. `y`."
msgstr ""

#: of sklearn.base.RegressorMixin.score:42
msgid ""
"The :math:`R^2` score used when calling ``score`` on a regressor uses "
"``multioutput='uniform_average'`` from version 0.23 to keep consistent "
"with default value of :func:`~sklearn.metrics.r2_score`. This influences "
"the ``score`` method of all the multioutput regressors (except for "
":class:`~sklearn.multioutput.MultiOutputRegressor`)."
msgstr ""

#: of sklearn.base.BaseEstimator.set_params:4
msgid ""
"The method works on simple estimators as well as on nested objects (such "
"as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of "
"the form ``<component>__<parameter>`` so that it's possible to update "
"each component of a nested object."
msgstr ""

#: of sklearn.base.BaseEstimator.set_params:12
msgid "Estimator parameters."
msgstr ""

#: of sklearn.base.BaseEstimator.set_params:28
msgid "**self**"
msgstr ""

#: of
msgid "estimator instance"
msgstr ""

#: of sklearn.base.BaseEstimator.set_params:17
msgid "Estimator instance."
msgstr ""

