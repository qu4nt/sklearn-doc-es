

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>sklearn.linear_model.SGDClassifier &mdash; documentación de scikit-learn - 0.24.1</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html" />

  
  <link rel="shortcut icon" href="../../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
<script src="../../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../../index.html">
        <img
          class="sk-brand-img"
          src="../../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../install.html">Instalación</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../user_guide.html">Manual de Usuario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../../auto_examples/index.html">Ejemplos</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../getting_started.html">¿Cómo empezar?</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../whats_new/v0.24.html">Novedades</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../glossary.html">Glosario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../developers/index.html">Desarrollo</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../support.html">Soporte</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../related_projects.html">Paquetes relacionados</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../roadmap.html">Hoja de ruta</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../../about.html">Sobre nosotros</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Más</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../../getting_started.html">¿Cómo empezar?</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../whats_new/v0.24.html">Novedades</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../glossary.html">Glosario</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../developers/index.html">Desarrollo</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../support.html">Soporte</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../related_projects.html">Paquetes relacionados</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../roadmap.html">Hoja de ruta</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../../about.html">Sobre nosotros</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Ir a" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Alternar menú</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../../index.html">
            <img
              class="sk-brand-img"
              src="../../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="sklearn.linear_model.RidgeClassifierCV.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="sklearn.linear_model.RidgeClassifierCV">Prev</a><a href="../classes.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="Referencia de la API">Arriba</a>
            <a href="sklearn.linear_model.LinearRegression.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="sklearn.linear_model.LinearRegression">Sig.</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.24.1</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Otras versiones</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Por favor <a class="font-weight-bold" href="../../about.html#citing-scikit-learn"><string>cítanos</string></a> si usas el software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code>.SGDClassifier</a><ul>
<li><a class="reference internal" href="#examples-using-sklearn-linear-model-sgdclassifier">Ejemplos usando <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDClassifier</span></code></a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="sklearn-linear-model-sgdclassifier">
<h1><a class="reference internal" href="../classes.html#module-sklearn.linear_model" title="sklearn.linear_model"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.linear_model</span></code></a>.SGDClassifier<a class="headerlink" href="#sklearn-linear-model-sgdclassifier" title="Enlazar permanentemente con este título">¶</a></h1>
<dl class="py class">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier">
<em class="property"><span class="pre">class</span> </em><span class="sig-prename descclassname"><span class="pre">sklearn.linear_model.</span></span><span class="sig-name descname"><span class="pre">SGDClassifier</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Clasificadores lineales (SVM, regresión logística, etc.) con entrenamiento SGD.</p>
<p>Este estimador implementa modelos lineales regularizados con aprendizaje de descenso de gradiente estocástico (SGD): el gradiente de la pérdida se estima cada muestra a la vez y el modelo se actualiza a lo largo del camino con un programa de fuerza decreciente (aka tasa de aprendizaje). El SGD permite el aprendizaje por minilotes (en línea y fuera del núcleo) mediante el método <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>. Para obtener los mejores resultados con la tasa de aprendizaje predeterminada, los datos deben tener media cero y varianza unitaria.</p>
<p>Esta implementación funciona con datos representados como arreglos densos o dispersos de valores de punto flotante para las características. El modelo que ajusta se puede controlar con el parámetro de pérdida; por defecto, ajusta una máquina de vectores de soporte lineal (SVM).</p>
<p>El regularizador es una penalización añadida a la función de pérdida que encoge los parámetros del modelo hacia el vector cero utilizando la norma euclidiana cuadrada L2 o la norma absoluta L1 o una combinación de ambas (red elástica). Si la actualización de los parámetros cruza el valor 0.0 debido al regularizador, la actualización se trunca a 0.0 para permitir el aprendizaje de modelos dispersos y lograr la selección de características en línea.</p>
<p>Más información en el <a class="reference internal" href="../sgd.html#sgd"><span class="std std-ref">Manual de usuario</span></a>.</p>
<dl class="field-list">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl>
<dt><strong>loss</strong><span class="classifier">cadena de caracteres, default=”hinge”</span></dt><dd><p>La función de pérdida a utilizar. Por defecto es “hinge”, que da un SVM lineal.</p>
<p>Las opciones posibles son “hinge”, “log”, “modified_huber”, “squared_hinge”, “perceptron”, o una pérdida de regresión: “squared_loss”, “huber”, “epsilon_insensitive”, o “squared_epsilon_insensitive”.</p>
<p>La pérdida “log” proporciona una regresión logística, un clasificador probabilístico. “modified_huber” es otra pérdida suave que aporta tolerancia a los valores atípicos, así como a las estimaciones de probabilidad. “squared_hinge” es como hinge pero se penaliza cuadráticamente. “perceptron” es la pérdida lineal utilizada por el algoritmo perceptrón. Las otras pérdidas están diseñadas para la regresión, pero también pueden ser útiles en la clasificación; consulta <a class="reference internal" href="sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor" title="sklearn.linear_model.SGDRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">SGDRegressor</span></code></a> para una descripción.</p>
<p>Puedes encontrar más detalles sobre las fórmulas de pérdidas en la <a class="reference internal" href="../sgd.html#sgd-mathematical-formulation"><span class="std std-ref">Manual de usuario</span></a>.</p>
</dd>
<dt><strong>penalty</strong><span class="classifier">{“l2”, “l1”, “elasticnet”}, default=”l2”</span></dt><dd><p>La penalización (término de regularización) que se utilizará. Por defecto es “l2” que es el regularizador estándar para los modelos SVM lineales. l1” y “elasticnet” pueden aportar una dispersión al modelo (selección de características) que no se consigue con “l2”.</p>
</dd>
<dt><strong>alpha</strong><span class="classifier">float, default=0.0001</span></dt><dd><p>Constante que multiplica el término de regularización. Cuanto mayor sea el valor, más fuerte será la regularización. También se utiliza para calcular la tasa de aprendizaje cuando se establece en <code class="docutils literal notranslate"><span class="pre">learning_rate</span></code> se establece en “optimal”.</p>
</dd>
<dt><strong>l1_ratio</strong><span class="classifier">float, default=0.15</span></dt><dd><p>El parámetro de mezcla de la red elástica, con 0 &lt;= l1_ratio &lt;= 1. l1_ratio=0 corresponde a la penalización L2, l1_ratio=1 a la L1. Sólo se utiliza si <code class="docutils literal notranslate"><span class="pre">penalty</span></code> es “elasticnet”.</p>
</dd>
<dt><strong>fit_intercept</strong><span class="classifier">bool, default=True</span></dt><dd><p>Si el intercepto debe ser estimado o no. Si es False, se asume que los datos ya están centrados.</p>
</dd>
<dt><strong>max_iter</strong><span class="classifier">entero, default=1000</span></dt><dd><p>El número máximo de pasadas sobre los datos de entrenamiento (también conocido como épocas o epochs). Sólo afecta al comportamiento del método <code class="docutils literal notranslate"><span class="pre">fit</span></code>, y no al método <a class="reference internal" href="#sklearn.linear_model.SGDClassifier.partial_fit" title="sklearn.linear_model.SGDClassifier.partial_fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">partial_fit</span></code></a>.</p>
<div class="versionadded">
<p><span class="versionmodified added">Nuevo en la versión 0.19.</span></p>
</div>
</dd>
<dt><strong>tol</strong><span class="classifier">float, default=1e-3</span></dt><dd><p>El criterio de parada. Si no es None, el entrenamiento se detendrá cuando (loss &gt; best_loss - tol) para <code class="docutils literal notranslate"><span class="pre">n_iter_no_change</span></code> épocas consecutivas.</p>
<div class="versionadded">
<p><span class="versionmodified added">Nuevo en la versión 0.19.</span></p>
</div>
</dd>
<dt><strong>shuffle</strong><span class="classifier">bool, default=True</span></dt><dd><p>Si los datos de entrenamiento deben ser aleatorizados o no después de cada época.</p>
</dd>
<dt><strong>verbose</strong><span class="classifier">int, default=0</span></dt><dd><p>Nivel de verbosidad.</p>
</dd>
<dt><strong>epsilon</strong><span class="classifier">float, default=0.1</span></dt><dd><p>Epsilon en las funciones de pérdida insensibles a epsilon; sólo si <code class="docutils literal notranslate"><span class="pre">loss</span></code> es “huber”, “epsilon_insensitive”, o “squared_epsilon_insensitive”. En el caso de “huber”, determina el umbral a partir del cual es menos importante acertar la predicción. En el caso de insensible a épsilon, cualquier diferencia entre la predicción actual y la etiqueta correcta se ignora si es menor que este umbral.</p>
</dd>
<dt><strong>n_jobs</strong><span class="classifier">int, default=None</span></dt><dd><p>El número de CPUs a utilizar para realizar el cálculo OVA (Uno Contra Todos, One Versus All, para problemas multiclase). <code class="docutils literal notranslate"><span class="pre">None</span></code> significa 1 a menos que esté en un contexto <a class="reference external" href="https://joblib.readthedocs.io/en/latest/parallel.html#joblib.parallel_backend" title="(en joblib versión 1.1.0.dev0)"><code class="xref py py-obj docutils literal notranslate"><span class="pre">joblib.parallel_backend</span></code></a>. <code class="docutils literal notranslate"><span class="pre">-1</span></code> significa utilizar todos los procesadores. Ver <a class="reference internal" href="../../glossary.html#term-n_jobs"><span class="xref std std-term">Glosario</span></a> para más detalles.</p>
</dd>
<dt><strong>random_state</strong><span class="classifier">int, RandomState instance, default=None</span></dt><dd><p>Se utiliza para barajar los datos, cuando <code class="docutils literal notranslate"><span class="pre">shuffle</span></code> se establece en <code class="docutils literal notranslate"><span class="pre">True</span></code>. Pase un int para una salida reproducible a través de múltiples llamadas a la función. Consulta <a class="reference internal" href="../../glossary.html#term-random_state"><span class="xref std std-term">Glosario</span></a>.</p>
</dd>
<dt><strong>learning_rate</strong><span class="classifier">str, default=”optimal”</span></dt><dd><p>El programa de la tasa de aprendizaje:</p>
<ul>
<li><p>“constant”: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">eta0</span></code></p></li>
<li><p>“optimal”: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">1.0</span> <span class="pre">/</span> <span class="pre">(alpha</span> <span class="pre">*</span> <span class="pre">(t</span> <span class="pre">+</span> <span class="pre">t0))</span></code> donde t0 se elige mediante una heurística propuesta por Leon Bottou.</p></li>
<li><p>“invscaling”: <code class="docutils literal notranslate"><span class="pre">eta</span> <span class="pre">=</span> <span class="pre">eta0</span> <span class="pre">/</span> <span class="pre">pow(t,</span> <span class="pre">power_t)</span></code></p></li>
<li><p>“adaptive”: eta = eta0, siempre que el entrenamiento siga disminuyendo. Cada vez que n_iter_no_change consecutivo no consigue disminuir la pérdida asociada al entrenamiento en tol o no consigue aumentar la puntuación de validación en tol si early_stopping es True, la tasa de aprendizaje actual se divide por 5.</p>
<blockquote>
<div><div class="versionadded">
<p><span class="versionmodified added">Nuevo en la versión 0.20: </span>Added “adaptive” option</p>
</div>
</div></blockquote>
</li>
</ul>
</dd>
<dt><strong>eta0</strong><span class="classifier">double, default=0.0</span></dt><dd><p>La tasa de aprendizaje inicial para los programas “constant”, “invscaling” o “adaptive”. El valor por defecto es 0.0 ya que eta0 no es utilizado por el programa por defecto “optimal”.</p>
</dd>
<dt><strong>power_t</strong><span class="classifier">double, default=0.5</span></dt><dd><p>El exponente de la tasa de aprendizaje de escala inversa [por defecto 0.5].</p>
</dd>
<dt><strong>early_stopping</strong><span class="classifier">bool, default=False</span></dt><dd><p>Si se utiliza la parada temprana para terminar el entrenamiento cuando la puntuación de validación no está mejorando. Si se establece como True, se apartará automáticamente una fracción estratificada de los datos de entrenamiento como validación y terminará el entrenamiento cuando la puntuación de validación devuelta por el método <code class="docutils literal notranslate"><span class="pre">score</span></code> no mejore en al menos tol para n_iter_no_change épocas consecutivas.</p>
<div class="versionadded">
<p><span class="versionmodified added">Nuevo en la versión 0.20: </span>Opción “early_stopping” añadida</p>
</div>
</dd>
<dt><strong>validation_fraction</strong><span class="classifier">float, default=0.1</span></dt><dd><p>La proporción de los datos de entrenamiento que se reservan como conjunto de validación para la parada anticipada. Debe estar entre 0 y 1. Sólo se utiliza si <code class="docutils literal notranslate"><span class="pre">early_stopping</span></code> es True.</p>
<div class="versionadded">
<p><span class="versionmodified added">Nuevo en la versión 0.20: </span>Opción “validation_fraction” añadida</p>
</div>
</dd>
<dt><strong>n_iter_no_change</strong><span class="classifier">int, default=5</span></dt><dd><p>Número de iteraciones sin mejora que hay que esperar antes de la parada anticipada.</p>
<div class="versionadded">
<p><span class="versionmodified added">Nuevo en la versión 0.20: </span>Opción “n_iter_no_change” añadida</p>
</div>
</dd>
<dt><strong>class_weight</strong><span class="classifier">dict, {class_label: weight} or «balanced», default=None</span></dt><dd><p>Preajuste para el parámetro de ajuste class_weight.</p>
<p>Ponderación asociada a las clases. Si no se da, se supone que todas las clases tienen ponderación uno.</p>
<p>El modo «balanced» utiliza los valores de y para ajustar automáticamente las ponderaciones inversamente proporcionales a las frecuencias de clase en los datos de entrada como <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">/</span> <span class="pre">(n_classes</span> <span class="pre">*</span> <span class="pre">np.bincount(y))</span></code>.</p>
</dd>
<dt><strong>warm_start</strong><span class="classifier">bool, default=False</span></dt><dd><p>Cuando se establece a True, reutiliza la solución de la llamada anterior para ajustar como inicialización, de lo contrario, solamente borrará la solución anterior. Ver <a class="reference internal" href="../../glossary.html#term-warm_start"><span class="xref std std-term">Glosario</span></a>.</p>
<p>Llamar repetidamente a fit o partial_fit cuando warm_start es True puede dar lugar a una solución diferente que cuando se llama a fit una sola vez debido a la forma en que se revolver los datos. Si se utiliza una tasa de aprendizaje dinámico, la tasa de aprendizaje se adapta en función del número de muestras ya vistas. Llamar a <code class="docutils literal notranslate"><span class="pre">fit</span></code> reinicia este contador, mientras que <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> resultará en el aumento del contador existente.</p>
</dd>
<dt><strong>average</strong><span class="classifier">bool o int, default=False</span></dt><dd><p>Si se establece como True, calcula el promedio de los ponderados SGD en todas las actualizaciones y almacena el resultado en el atributo <code class="docutils literal notranslate"><span class="pre">coef_</span></code>. Si se establece como un int mayor que 1, el promedio comenzará una vez que el número total de muestras vistas alcance el <code class="docutils literal notranslate"><span class="pre">average</span></code>. Así, <code class="docutils literal notranslate"><span class="pre">average=10</span></code> se empieza a promediar después de ver 10 muestras.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Atributos</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>coef_</strong><span class="classifier">ndarray de forma (1, n_features) si n_classes == 2 de lo contrario             (n_classes, n_features)</span></dt><dd><p>Ponderaciones asignadas a las características.</p>
</dd>
<dt><strong>intercept_</strong><span class="classifier">ndarray de forma (1,) si n_classes == 2 de lo contrario (n_classes,)</span></dt><dd><p>Constantes en la función de decisión.</p>
</dd>
<dt><strong>n_iter_</strong><span class="classifier">int</span></dt><dd><p>El número real de iteraciones antes de alcanzar el criterio de parada. Para los ajustes multiclase, es el máximo sobre cada ajuste binario.</p>
</dd>
<dt><strong>loss_function_</strong><span class="classifier">concreto</span></dt><dd></dd>
<dt><strong>classes_</strong><span class="classifier">arreglo de forma (n_classes,)</span></dt><dd></dd>
<dt><strong>t_</strong><span class="classifier">int</span></dt><dd><p>Número de actualizaciones de ponderación realizadas durante el entrenamiento. Igual que <code class="docutils literal notranslate"><span class="pre">(n_iter_</span> <span class="pre">*</span> <span class="pre">n_samples)</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<div class="admonition seealso">
<p class="admonition-title">Ver también</p>
<dl class="simple">
<dt><a class="reference internal" href="sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sklearn.svm.LinearSVC</span></code></a></dt><dd><p>Clasificación lineal de vectores de soporte.</p>
</dd>
<dt><a class="reference internal" href="sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><code class="xref py py-obj docutils literal notranslate"><span class="pre">LogisticRegression</span></code></a></dt><dd><p>Regresión logística.</p>
</dd>
<dt><a class="reference internal" href="sklearn.linear_model.Perceptron.html#sklearn.linear_model.Perceptron" title="sklearn.linear_model.Perceptron"><code class="xref py py-obj docutils literal notranslate"><span class="pre">Perceptron</span></code></a></dt><dd><p>Hereda de SGDClassifier. <code class="docutils literal notranslate"><span class="pre">Perceptron()</span></code> es equivalente a <code class="docutils literal notranslate"><span class="pre">SGDClassifier(loss=&quot;perceptron&quot;,</span> <span class="pre">eta0=1,</span> <span class="pre">learning_rate=&quot;constant&quot;,</span> <span class="pre">penalty=None)</span></code>.</p>
</dd>
</dl>
</div>
<p class="rubric">Ejemplos</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">SGDClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Always scale the input. The most convenient way is to use a pipeline.</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span><span class="n">StandardScaler</span><span class="p">(),</span>
<span class="gp">... </span>                    <span class="n">SGDClassifier</span><span class="p">(</span><span class="n">max_iter</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
<span class="go">Pipeline(steps=[(&#39;standardscaler&#39;, StandardScaler()),</span>
<span class="go">                (&#39;sgdclassifier&#39;, SGDClassifier())])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<p class="rubric">Métodos</p>
<table class="longtable docutils align-default">
<colgroup>
<col style="width: 10%" />
<col style="width: 90%" />
</colgroup>
<tbody>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.decision_function" title="sklearn.linear_model.SGDClassifier.decision_function"><code class="xref py py-obj docutils literal notranslate"><span class="pre">decision_function</span></code></a></p></td>
<td><p>Predecir las puntuaciones de confianza de las muestras.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.densify" title="sklearn.linear_model.SGDClassifier.densify"><code class="xref py py-obj docutils literal notranslate"><span class="pre">densify</span></code></a></p></td>
<td><p>Convierte la matriz de coeficientes en formato de arreglo denso.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.fit" title="sklearn.linear_model.SGDClassifier.fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">fit</span></code></a></p></td>
<td><p>Ajustar el modelo lineal con el Descenso Gradiente Estocástico.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.get_params" title="sklearn.linear_model.SGDClassifier.get_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">get_params</span></code></a></p></td>
<td><p>Obtiene los parámetros para este estimador.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.partial_fit" title="sklearn.linear_model.SGDClassifier.partial_fit"><code class="xref py py-obj docutils literal notranslate"><span class="pre">partial_fit</span></code></a></p></td>
<td><p>Realiza una época de descenso de gradiente estocástico en las muestras dadas.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.predict" title="sklearn.linear_model.SGDClassifier.predict"><code class="xref py py-obj docutils literal notranslate"><span class="pre">predict</span></code></a></p></td>
<td><p>Predice las etiquetas de clase para las muestras en X.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.score" title="sklearn.linear_model.SGDClassifier.score"><code class="xref py py-obj docutils literal notranslate"><span class="pre">score</span></code></a></p></td>
<td><p>Devuelve la precisión media en los datos de prueba y las etiquetas dados.</p></td>
</tr>
<tr class="row-even"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.set_params" title="sklearn.linear_model.SGDClassifier.set_params"><code class="xref py py-obj docutils literal notranslate"><span class="pre">set_params</span></code></a></p></td>
<td><p>Establecer y validar los parámetros del estimador.</p></td>
</tr>
<tr class="row-odd"><td><p><a class="reference internal" href="#sklearn.linear_model.SGDClassifier.sparsify" title="sklearn.linear_model.SGDClassifier.sparsify"><code class="xref py py-obj docutils literal notranslate"><span class="pre">sparsify</span></code></a></p></td>
<td><p>Convierte la matriz de coeficientes en formato disperso.</p></td>
</tr>
</tbody>
</table>
<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.decision_function">
<span class="sig-name descname"><span class="pre">decision_function</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.decision_function" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Predecir las puntuaciones de confianza de las muestras.</p>
<p>La puntuación de confianza de una muestra es proporcional a la distancia con signo de esa muestra al hiperplano.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like o matriz dispersa, forma (n_samples, n_features)</span></dt><dd><p>Muestras.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt>arreglo, forma=(n_samples,) si n_classes == 2 de lo contrario (n_samples, n_classes)</dt><dd><p>Puntuaciones de confianza por combinación (muestra, clase). En el caso binario, la puntuación de confianza para self.classes_[1] donde &gt;0 significa que esta clase sería predicha.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.densify">
<span class="sig-name descname"><span class="pre">densify</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.densify" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Convierte la matriz de coeficientes en formato de arreglo denso.</p>
<p>Convierte el miembro <code class="docutils literal notranslate"><span class="pre">coef_</span></code> (de vuelta) en un numpy.ndarray. Este es el formato predeterminado de <code class="docutils literal notranslate"><span class="pre">coef_</span></code> y se requiere para el ajuste, por lo que llamar a este método sólo es necesario en los modelos que han sido previamente sparsified; de lo contrario, es un no-op.</p>
<dl class="field-list simple">
<dt class="field-odd">Devuelve</dt>
<dd class="field-odd"><dl class="simple">
<dt>self</dt><dd><p>Estimador ajustado.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.fit">
<span class="sig-name descname"><span class="pre">fit</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.fit" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Ajustar el modelo lineal con el Descenso Gradiente Estocástico.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, forma (n_samples, n_features)</span></dt><dd><p>Datos de entrenamiento.</p>
</dd>
<dt><strong>y</strong><span class="classifier">ndarray de forma (n_samples,)</span></dt><dd><p>Valores objetivo.</p>
</dd>
<dt><strong>coef_init</strong><span class="classifier">ndarray de forma (n_classes, n_features), default=None</span></dt><dd><p>Los coeficientes iniciales para iniciar la optimización en caliente.</p>
</dd>
<dt><strong>intercept_init</strong><span class="classifier">ndarray de forma (n_classes,), default=None</span></dt><dd><p>La intercepción inicial la optimización en caliente.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like, forma (n_samples,), default=None</span></dt><dd><p>Ponderaciones aplicadas a las muestras individuales. Si no se proporciona, se asumen ponderados uniformes. Estas ponderaciones se multiplicarán por el peso de la clase (pasado a través del constructor) si se especifica el peso de la clase.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt>self :</dt><dd><p>Devuelve una instancia de sí misma.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.get_params">
<span class="sig-name descname"><span class="pre">get_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.get_params" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Obtiene los parámetros para este estimador.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>deep</strong><span class="classifier">bool, default=True</span></dt><dd><p>Si es True, devolverá los parámetros para este estimador y los sub objetos contenidos que son estimadores.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>params</strong><span class="classifier">dict</span></dt><dd><p>Nombres de parámetros mapeados a sus valores.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.partial_fit">
<span class="sig-name descname"><span class="pre">partial_fit</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.partial_fit" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Realiza una época de descenso de gradiente estocástico en las muestras dadas.</p>
<p>Internamente, este método utiliza <code class="docutils literal notranslate"><span class="pre">max_iter</span> <span class="pre">=</span> <span class="pre">1</span></code>. Por lo tanto, no se garantiza que se alcance un mínimo de la función de coste después de llamarlo una vez. Cuestiones como la convergencia del objetivo y la parada anticipada deben ser manejadas por el usuario.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, forma (n_samples, n_features)</span></dt><dd><p>Subconjunto de los datos de entrenamiento.</p>
</dd>
<dt><strong>y</strong><span class="classifier">ndarray de forma (n_samples,)</span></dt><dd><p>Subconjunto de los valores objetivos.</p>
</dd>
<dt><strong>classes</strong><span class="classifier">ndarray de forma (n_classes,), default=None</span></dt><dd><p>Clases a través de todas las llamadas a partial_fit. Puede obtenerse mediante <code class="docutils literal notranslate"><span class="pre">np.unique(y_all)</span></code>, donde y_all es el vector objetivo de todo el conjunto de datos. Este argumento es necesario para la primera llamada a partial_fit y puede omitirse en las siguientes. Tenga en cuenta que no es necesario que y contenga todas las etiquetas de <code class="docutils literal notranslate"><span class="pre">classes</span></code>.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like, forma (n_samples,), default=None</span></dt><dd><p>Ponderaciones aplicadas a las muestras individuales. Si no se proporciona, se suponen ponderados uniformes.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt>self :</dt><dd><p>Devuelve una instancia de sí misma.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.predict">
<span class="sig-name descname"><span class="pre">predict</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.predict" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Predice las etiquetas de clase para las muestras en X.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like o matriz dispersa, forma (n_samples, n_features)</span></dt><dd><p>Muestras.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>C</strong><span class="classifier">arreglo, forma [n_samples]</span></dt><dd><p>Etiqueta de clase predicha por muestra.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.predict_log_proba">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">predict_log_proba</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.predict_log_proba" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Registro de estimaciones de probabilidad.</p>
<p>Este método sólo está disponible para la pérdida de registro y la pérdida de Huber modificada.</p>
<p>Cuando loss=»modified_huber», las estimaciones de probabilidad pueden ser ceros y unos enteros, por lo que no es posible tomar el logaritmo.</p>
<p>Ver <code class="docutils literal notranslate"><span class="pre">predict_proba</span></code> para más detalles.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix} de forma (n_samples, n_features)</span></dt><dd><p>Datos de entrada para predicción.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>T</strong><span class="classifier">array-like, forma (n_samples, n_classes)</span></dt><dd><p>Devuelve la probabilidad logarítmica de la muestra para cada clase en el modelo, donde las clases están ordenadas como lo están en <code class="docutils literal notranslate"><span class="pre">self.classes_</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py property">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.predict_proba">
<em class="property"><span class="pre">property</span> </em><span class="sig-name descname"><span class="pre">predict_proba</span></span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.predict_proba" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Estimaciones de probabilidad.</p>
<p>Este método sólo está disponible para la pérdida de registro y la pérdida de Huber modificada.</p>
<p>Las estimaciones de probabilidad multiclase se derivan de las estimaciones binarias (uno contra resto) mediante una simple normalización, como recomiendan Zadrozny y Elkan.</p>
<p>Las estimaciones de probabilidad binarias para loss=»modified_huber» vienen dadas por (clip(decision_function(X), -1, 1) + 1) / 2. Para otras funciones de pérdida es necesario realizar una calibración de probabilidad adecuada envolviendo el clasificador con <a class="reference internal" href="sklearn.calibration.CalibratedClassifierCV.html#sklearn.calibration.CalibratedClassifierCV" title="sklearn.calibration.CalibratedClassifierCV"><code class="xref py py-class docutils literal notranslate"><span class="pre">CalibratedClassifierCV</span></code></a> en su lugar.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">{array-like, sparse matrix}, forma (n_samples, n_features)</span></dt><dd><p>Datos de entrada para predicción.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt>ndarray de forma (n_samples, n_classes)</dt><dd><p>Devuelve la probabilidad de la muestra para cada clase en el modelo, donde las clases se ordenan como están en <code class="docutils literal notranslate"><span class="pre">self.classes_</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Referencias</p>
<p>Zadrozny and Elkan, «Transforming classifier scores into multiclass
probability estimates», SIGKDD’02,
<a class="reference external" href="http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf">http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf</a></p>
<p>The justification for the formula in the loss=»modified_huber»
case is in the appendix B in:
<a class="reference external" href="http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf">http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf</a></p>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.score">
<span class="sig-name descname"><span class="pre">score</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.score" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Devuelve la precisión media en los datos de prueba y las etiquetas dados.</p>
<p>En la clasificación multietiqueta, se trata de la precisión del subconjunto, que es una métrica rigurosa, ya que se requiere para cada muestra que cada conjunto de etiquetas sea predicho correctamente.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>X</strong><span class="classifier">array-like de forma (n_samples, n_features)</span></dt><dd><p>Muestras de prueba.</p>
</dd>
<dt><strong>y</strong><span class="classifier">array-like de forma (n_samples,) or (n_samples, n_outputs)</span></dt><dd><p>Etiquetas verdaderas para <code class="docutils literal notranslate"><span class="pre">X</span></code>.</p>
</dd>
<dt><strong>sample_weight</strong><span class="classifier">array-like de forma (n_samples,), default=None</span></dt><dd><p>Ponderados de muestras.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>score</strong><span class="classifier">float</span></dt><dd><p>Precisión media de <code class="docutils literal notranslate"><span class="pre">self.predict(X)</span></code> con respecto a <code class="docutils literal notranslate"><span class="pre">y</span></code>.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.set_params">
<span class="sig-name descname"><span class="pre">set_params</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.set_params" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Establecer y validar los parámetros del estimador.</p>
<dl class="field-list simple">
<dt class="field-odd">Parámetros</dt>
<dd class="field-odd"><dl class="simple">
<dt><strong>**kwargs</strong><span class="classifier">dict</span></dt><dd><p>Parámetros del estimador.</p>
</dd>
</dl>
</dd>
<dt class="field-even">Devuelve</dt>
<dd class="field-even"><dl class="simple">
<dt><strong>self</strong><span class="classifier">object</span></dt><dd><p>Instancia de estimador.</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

<dl class="py method">
<dt class="sig sig-object py" id="sklearn.linear_model.SGDClassifier.sparsify">
<span class="sig-name descname"><span class="pre">sparsify</span></span><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="headerlink" href="#sklearn.linear_model.SGDClassifier.sparsify" title="Enlazar permanentemente con esta definición">¶</a></dt>
<dd><p>Convierte la matriz de coeficientes en formato disperso.</p>
<p>Convierte el miembro <code class="docutils literal notranslate"><span class="pre">coef_</span></code> en una matriz scipy.sparse, que para los modelos L1-regularizados puede ser mucho más eficiente en cuanto a memoria y almacenamiento que la representación numpy.ndarray habitual.</p>
<p>El miembro <code class="docutils literal notranslate"><span class="pre">intercept_</span></code> no se convierte.</p>
<dl class="field-list simple">
<dt class="field-odd">Devuelve</dt>
<dd class="field-odd"><dl class="simple">
<dt>self</dt><dd><p>Estimador ajustado.</p>
</dd>
</dl>
</dd>
</dl>
<p class="rubric">Notas</p>
<p>Para los modelos no dispersos, es decir, cuando no hay muchos ceros en <code class="docutils literal notranslate"><span class="pre">coef_</span></code>, esto puede en realidad <em>aumentar</em> el uso de la memoria, así que utilice este método con cuidado. Una regla general es que el número de elementos cero, que puede ser calculado con <code class="docutils literal notranslate"><span class="pre">(coef_</span> <span class="pre">==</span> <span class="pre">0).sum()</span></code>, debe ser más del 50% para que esto proporcione beneficios significativos.</p>
<p>Después de llamar a este método, el ajuste posterior con el método partial_fit (si lo hay) no funcionará hasta que llames a densify.</p>
</dd></dl>

</dd></dl>

<section id="examples-using-sklearn-linear-model-sgdclassifier">
<h2>Ejemplos usando <code class="docutils literal notranslate"><span class="pre">sklearn.linear_model.SGDClassifier</span></code><a class="headerlink" href="#examples-using-sklearn-linear-model-sgdclassifier" title="Enlazar permanentemente con este título">¶</a></h2>
<div class="sphx-glr-thumbcontainer" tooltip="The dataset used in this example is the 20 newsgroups dataset which will be automatically downl..."><figure class="align-default" id="id1">
<img alt="Sample pipeline for text feature extraction and evaluation" src="../../_images/sphx_glr_grid_search_text_feature_extraction_thumb.png" />
<figcaption>
<p><span class="caption-text"><a class="reference internal" href="../../auto_examples/model_selection/grid_search_text_feature_extraction.html#sphx-glr-auto-examples-model-selection-grid-search-text-feature-extraction-py"><span class="std std-ref">Ejemplo de pipeline para la extracción y evaluación de características de texto</span></a></span><a class="headerlink" href="#id1" title="Enlace permanente a esta imagen">¶</a></p>
</figcaption>
</figure>
</div><div class="clearer"></div></section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../../_sources/modules/generated/sklearn.linear_model.SGDClassifier.rst.txt" rel="nofollow">Mostrar la fuente de esta página</a>
      </footer>
    </div>
  </div>
</div>
<script src="../../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>