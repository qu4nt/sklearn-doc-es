# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 12:56-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/generated/sklearn.linear_model.SGDClassifier.rst:2
msgid ":mod:`sklearn.linear_model`.SGDClassifier"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:2
msgid "Linear classifiers (SVM, logistic regression, etc.) with SGD training."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:4
msgid ""
"This estimator implements regularized linear models with stochastic "
"gradient descent (SGD) learning: the gradient of the loss is estimated "
"each sample at a time and the model is updated along the way with a "
"decreasing strength schedule (aka learning rate). SGD allows minibatch "
"(online/out-of-core) learning via the `partial_fit` method. For best "
"results using the default learning rate schedule, the data should have "
"zero mean and unit variance."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:12
msgid ""
"This implementation works with data represented as dense or sparse arrays"
" of floating point values for the features. The model it fits can be "
"controlled with the loss parameter; by default, it fits a linear support "
"vector machine (SVM)."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:17
msgid ""
"The regularizer is a penalty added to the loss function that shrinks "
"model parameters towards the zero vector using either the squared "
"euclidean norm L2 or the absolute norm L1 or a combination of both "
"(Elastic Net). If the parameter update crosses the 0.0 value because of "
"the regularizer, the update is truncated to 0.0 to allow for learning "
"sparse models and achieve online feature selection."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:24
msgid "Read more in the :ref:`User Guide <sgd>`."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params sklearn.base.ClassifierMixin.score
#: sklearn.linear_model.SGDClassifier.predict_log_proba
#: sklearn.linear_model.SGDClassifier.predict_proba
#: sklearn.linear_model._base.LinearClassifierMixin.decision_function
#: sklearn.linear_model._base.LinearClassifierMixin.predict
#: sklearn.linear_model._stochastic_gradient.BaseSGD.set_params
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit
#: sklearn.linear_model._stochastic_gradient.SGDClassifier
msgid "Parameters"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:46
msgid "**loss**"
msgstr ""

#: of
msgid "str, default='hinge'"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:29
msgid ""
"The loss function to be used. Defaults to 'hinge', which gives a linear "
"SVM."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:32
msgid ""
"The possible options are 'hinge', 'log', 'modified_huber', "
"'squared_hinge', 'perceptron', or a regression loss: 'squared_loss', "
"'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:36
msgid ""
"The 'log' loss gives logistic regression, a probabilistic classifier. "
"'modified_huber' is another smooth loss that brings tolerance to outliers"
" as well as probability estimates. 'squared_hinge' is like hinge but is "
"quadratically penalized. 'perceptron' is the linear loss used by the "
"perceptron algorithm. The other losses are designed for regression but "
"can be useful in classification as well; see "
":class:`~sklearn.linear_model.SGDRegressor` for a description."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:45
msgid ""
"More details about the losses formulas can be found in the :ref:`User "
"Guide <sgd_mathematical_formulation>`."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:52
msgid "**penalty**"
msgstr ""

#: of
msgid "{'l2', 'l1', 'elasticnet'}, default='l2'"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:49
msgid ""
"The penalty (aka regularization term) to be used. Defaults to 'l2' which "
"is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' "
"might bring sparsity to the model (feature selection) not achievable with"
" 'l2'."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:58
msgid "**alpha**"
msgstr ""

#: of
msgid "float, default=0.0001"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:55
msgid ""
"Constant that multiplies the regularization term. The higher the value, "
"the stronger the regularization. Also used to compute the learning rate "
"when set to `learning_rate` is set to 'optimal'."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:63
msgid "**l1_ratio**"
msgstr ""

#: of
msgid "float, default=0.15"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:61
msgid ""
"The Elastic Net mixing parameter, with 0 <= l1_ratio <= 1. l1_ratio=0 "
"corresponds to L2 penalty, l1_ratio=1 to L1. Only used if `penalty` is "
"'elasticnet'."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:67
msgid "**fit_intercept**"
msgstr ""

#: of
msgid "bool, default=True"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:66
msgid ""
"Whether the intercept should be estimated or not. If False, the data is "
"assumed to be already centered."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:74
msgid "**max_iter**"
msgstr ""

#: of
msgid "int, default=1000"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:70
msgid ""
"The maximum number of passes over the training data (aka epochs). It only"
" impacts the behavior in the ``fit`` method, and not the "
":meth:`partial_fit` method."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:81
msgid "**tol**"
msgstr ""

#: of
msgid "float, default=1e-3"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:77
msgid ""
"The stopping criterion. If it is not None, training will stop when (loss "
"> best_loss - tol) for ``n_iter_no_change`` consecutive epochs."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:84
msgid "**shuffle**"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:84
msgid "Whether or not the training data should be shuffled after each epoch."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:87
msgid "**verbose**"
msgstr ""

#: of
msgid "int, default=0"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:87
msgid "The verbosity level."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:95
msgid "**epsilon**"
msgstr ""

#: of
msgid "float, default=0.1"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:90
msgid ""
"Epsilon in the epsilon-insensitive loss functions; only if `loss` is "
"'huber', 'epsilon_insensitive', or 'squared_epsilon_insensitive'. For "
"'huber', determines the threshold at which it becomes less important to "
"get the prediction exactly right. For epsilon-insensitive, any "
"differences between the current prediction and the correct label are "
"ignored if they are less than this threshold."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:102
msgid "**n_jobs**"
msgstr ""

#: of
msgid "int, default=None"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:98
msgid ""
"The number of CPUs to use to do the OVA (One Versus All, for multi-class "
"problems) computation. ``None`` means 1 unless in a "
":obj:`joblib.parallel_backend` context. ``-1`` means using all "
"processors. See :term:`Glossary <n_jobs>` for more details."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:107
msgid "**random_state**"
msgstr ""

#: of
msgid "int, RandomState instance, default=None"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:105
msgid ""
"Used for shuffling the data, when ``shuffle`` is set to ``True``. Pass an"
" int for reproducible output across multiple function calls. See "
":term:`Glossary <random_state>`."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:122
msgid "**learning_rate**"
msgstr ""

#: of
msgid "str, default='optimal'"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:110
msgid "The learning rate schedule:"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:112
msgid "'constant': `eta = eta0`"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:113
msgid ""
"'optimal': `eta = 1.0 / (alpha * (t + t0))` where t0 is chosen by a "
"heuristic proposed by Leon Bottou."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:115
msgid "'invscaling': `eta = eta0 / pow(t, power_t)`"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:116
msgid ""
"'adaptive': eta = eta0, as long as the training keeps decreasing. Each "
"time n_iter_no_change consecutive epochs fail to decrease the training "
"loss by tol or fail to increase validation score by tol if early_stopping"
" is True, the current learning rate is divided by 5."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:121
msgid "Added 'adaptive' option"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:127
msgid "**eta0**"
msgstr ""

#: of
msgid "double, default=0.0"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:125
msgid ""
"The initial learning rate for the 'constant', 'invscaling' or 'adaptive' "
"schedules. The default value is 0.0 as eta0 is not used by the default "
"schedule 'optimal'."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:130
msgid "**power_t**"
msgstr ""

#: of
msgid "double, default=0.5"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:130
msgid "The exponent for inverse scaling learning rate [default 0.5]."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:140
msgid "**early_stopping**"
msgstr ""

#: of
msgid "bool, default=False"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:133
msgid ""
"Whether to use early stopping to terminate training when validation score"
" is not improving. If set to True, it will automatically set aside a "
"stratified fraction of training data as validation and terminate training"
" when validation score returned by the `score` method is not improving by"
" at least tol for n_iter_no_change consecutive epochs."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:139
msgid "Added 'early_stopping' option"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:148
msgid "**validation_fraction**"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:143
msgid ""
"The proportion of training data to set aside as validation set for early "
"stopping. Must be between 0 and 1. Only used if `early_stopping` is True."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:147
msgid "Added 'validation_fraction' option"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:154
msgid "**n_iter_no_change**"
msgstr ""

#: of
msgid "int, default=5"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:151
msgid "Number of iterations with no improvement to wait before early stopping."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:153
msgid "Added 'n_iter_no_change' option"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:164
msgid "**class_weight**"
msgstr ""

#: of
msgid "dict, {class_label: weight} or \"balanced\", default=None"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:157
msgid "Preset for the class_weight fit parameter."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:159
msgid ""
"Weights associated with classes. If not given, all classes are supposed "
"to have weight one."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:162
msgid ""
"The \"balanced\" mode uses the values of y to automatically adjust "
"weights inversely proportional to class frequencies in the input data as "
"``n_samples / (n_classes * np.bincount(y))``."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:177
msgid "**warm_start**"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:167
msgid ""
"When set to True, reuse the solution of the previous call to fit as "
"initialization, otherwise, just erase the previous solution. See "
":term:`the Glossary <warm_start>`."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:171
msgid ""
"Repeatedly calling fit or partial_fit when warm_start is True can result "
"in a different solution than when calling fit a single time because of "
"the way the data is shuffled. If a dynamic learning rate is used, the "
"learning rate is adapted depending on the number of samples already seen."
" Calling ``fit`` resets this counter, while ``partial_fit`` will result "
"in increasing the existing counter."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:187
msgid "**average**"
msgstr ""

#: of
msgid "bool or int, default=False"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:180
msgid ""
"When set to True, computes the averaged SGD weights accross all updates "
"and stores the result in the ``coef_`` attribute. If set to an int "
"greater than 1, averaging will begin once the total number of samples "
"seen reaches `average`. So ``average=10`` will begin averaging after "
"seeing 10 samples."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier
msgid "Attributes"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:192
msgid "**coef_**"
msgstr ""

#: of
msgid ""
"ndarray of shape (1, n_features) if n_classes == 2 else             "
"(n_classes, n_features)"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:192
msgid "Weights assigned to the features."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:195
msgid "**intercept_**"
msgstr ""

#: of
msgid "ndarray of shape (1,) if n_classes == 2 else (n_classes,)"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:195
msgid "Constants in decision function."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:199
msgid "**n_iter_**"
msgstr ""

#: of
msgid "int"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:198
msgid ""
"The actual number of iterations before reaching the stopping criterion. "
"For multiclass fits, it is the maximum over every binary fit."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:202
msgid "**loss_function_** : concrete ``LossFunction``"
msgstr ""

#: of
msgid "concrete"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:205
msgid "**classes_**"
msgstr ""

#: of
msgid "array of shape (n_classes,)"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:212
msgid "**t_**"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:208
msgid ""
"Number of weight updates performed during training. Same as ``(n_iter_ * "
"n_samples)``."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:217
msgid ":obj:`sklearn.svm.LinearSVC`"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:218
msgid "Linear support vector classification."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:219
msgid ":obj:`LogisticRegression`"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:220
msgid "Logistic regression."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:221
msgid ":obj:`Perceptron`"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:222
msgid ""
"Inherits from SGDClassifier. ``Perceptron()`` is equivalent to "
"``SGDClassifier(loss=\"perceptron\", eta0=1, learning_rate=\"constant\", "
"penalty=None)``."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:228
msgid "Examples"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.SGDClassifier:245
msgid "Methods"
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ""
":obj:`decision_function "
"<sklearn.linear_model.SGDClassifier.decision_function>`\\"
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.decision_function:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Predict confidence scores for samples."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`densify <sklearn.linear_model.SGDClassifier.densify>`\\"
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.densify:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Convert coefficient matrix to dense array format."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`fit <sklearn.linear_model.SGDClassifier.fit>`\\"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Fit linear model with Stochastic Gradient Descent."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`get_params <sklearn.linear_model.SGDClassifier.get_params>`\\"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Get parameters for this estimator."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`partial_fit <sklearn.linear_model.SGDClassifier.partial_fit>`\\"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Perform one epoch of stochastic gradient descent on given samples."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`predict <sklearn.linear_model.SGDClassifier.predict>`\\"
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.predict:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Predict class labels for samples in X."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`score <sklearn.linear_model.SGDClassifier.score>`\\"
msgstr ""

#: of sklearn.base.ClassifierMixin.score:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Return the mean accuracy on the given test data and labels."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`set_params <sklearn.linear_model.SGDClassifier.set_params>`\\"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGD.set_params:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Set and validate the parameters of estimator."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid ":obj:`sparsify <sklearn.linear_model.SGDClassifier.sparsify>`\\"
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.sparsify:2
#: sklearn.linear_model._stochastic_gradient.SGDClassifier:257:<autosummary>:1
msgid "Convert coefficient matrix to sparse format."
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.decision_function:4
msgid ""
"The confidence score for a sample is proportional to the signed distance "
"of that sample to the hyperplane."
msgstr ""

#: of sklearn.base.ClassifierMixin.score:11
#: sklearn.linear_model.SGDClassifier.predict_log_proba:14
#: sklearn.linear_model.SGDClassifier.predict_proba:19
#: sklearn.linear_model._base.LinearClassifierMixin.decision_function:10
#: sklearn.linear_model._base.LinearClassifierMixin.predict:8
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:8
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:12
msgid "**X**"
msgstr ""

#: of
msgid "array-like or sparse matrix, shape (n_samples, n_features)"
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.decision_function:10
#: sklearn.linear_model._base.LinearClassifierMixin.predict:8
msgid "Samples."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params sklearn.base.ClassifierMixin.score
#: sklearn.linear_model.SGDClassifier.predict_log_proba
#: sklearn.linear_model.SGDClassifier.predict_proba
#: sklearn.linear_model._base.LinearClassifierMixin.decision_function
#: sklearn.linear_model._base.LinearClassifierMixin.predict
#: sklearn.linear_model._base.SparseCoefMixin.densify
#: sklearn.linear_model._base.SparseCoefMixin.sparsify
#: sklearn.linear_model._stochastic_gradient.BaseSGD.set_params
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit
msgid "Returns"
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.decision_function:28
msgid "array, shape=(n_samples,) if n_classes == 2 else (n_samples, n_classes)"
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.decision_function:15
msgid ""
"Confidence scores per (sample, class) combination. In the binary case, "
"confidence score for self.classes_[1] where >0 means this class would be "
"predicted."
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.densify:4
msgid ""
"Converts the ``coef_`` member (back) to a numpy.ndarray. This is the "
"default format of ``coef_`` and is required for fitting, so calling this "
"method is only required on models that have previously been sparsified; "
"otherwise, it is a no-op."
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.densify:24
#: sklearn.linear_model._base.SparseCoefMixin.sparsify:21
msgid "self"
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.densify:13
#: sklearn.linear_model._base.SparseCoefMixin.sparsify:14
msgid "Fitted estimator."
msgstr ""

#: of
msgid "{array-like, sparse matrix}, shape (n_samples, n_features)"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:8
msgid "Training data."
msgstr ""

#: of sklearn.base.ClassifierMixin.score:14
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:11
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:15
msgid "**y**"
msgstr ""

#: of
msgid "ndarray of shape (n_samples,)"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:11
msgid "Target values."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:14
msgid "**coef_init**"
msgstr ""

#: of
msgid "ndarray of shape (n_classes, n_features), default=None"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:14
msgid "The initial coefficients to warm-start the optimization."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:17
msgid "**intercept_init**"
msgstr ""

#: of
msgid "ndarray of shape (n_classes,), default=None"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:17
msgid "The initial intercept to warm-start the optimization."
msgstr ""

#: of sklearn.base.ClassifierMixin.score:17
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:23
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:27
msgid "**sample_weight**"
msgstr ""

#: of
msgid "array-like, shape (n_samples,), default=None"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:20
msgid ""
"Weights applied to individual samples. If not provided, uniform weights "
"are assumed. These weights will be multiplied with class_weight (passed "
"through the constructor) if class_weight is specified."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:39
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:43
msgid "self :"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.fit:28
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:32
msgid "Returns an instance of self."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:9
msgid "**deep**"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:8
msgid ""
"If True, will return the parameters for this estimator and contained "
"subobjects that are estimators."
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:25
msgid "**params**"
msgstr ""

#: of
msgid "dict"
msgstr ""

#: of sklearn.base.BaseEstimator.get_params:14
msgid "Parameter names mapped to their values."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:4
msgid ""
"Internally, this method uses ``max_iter = 1``. Therefore, it is not "
"guaranteed that a minimum of the cost function is reached after calling "
"it once. Matters such as objective convergence and early stopping should "
"be handled by the user."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:12
msgid "Subset of the training data."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:15
msgid "Subset of the target values."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:23
msgid "**classes**"
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:18
msgid ""
"Classes across all calls to partial_fit. Can be obtained by via "
"`np.unique(y_all)`, where y_all is the target vector of the entire "
"dataset. This argument is required for the first call to partial_fit and "
"can be omitted in the subsequent calls. Note that y doesn't need to "
"contain all labels in `classes`."
msgstr ""

#: of
#: sklearn.linear_model._stochastic_gradient.BaseSGDClassifier.partial_fit:26
msgid ""
"Weights applied to individual samples. If not provided, uniform weights "
"are assumed."
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.predict:24
msgid "**C**"
msgstr ""

#: of
msgid "array, shape [n_samples]"
msgstr ""

#: of sklearn.linear_model._base.LinearClassifierMixin.predict:13
msgid "Predicted class label per sample."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:2
msgid "Log of probability estimates."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:4
#: sklearn.linear_model.SGDClassifier.predict_proba:4
msgid "This method is only available for log loss and modified Huber loss."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:6
msgid ""
"When loss=\"modified_huber\", probability estimates may be hard zeros and"
" ones, so taking the logarithm is not possible."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:9
msgid "See ``predict_proba`` for details."
msgstr ""

#: of
msgid "{array-like, sparse matrix} of shape (n_samples, n_features)"
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:14
#: sklearn.linear_model.SGDClassifier.predict_proba:19
msgid "Input data for prediction."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:32
msgid "**T**"
msgstr ""

#: of
msgid "array-like, shape (n_samples, n_classes)"
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_log_proba:19
msgid ""
"Returns the log-probability of the sample for each class in the model, "
"where classes are ordered as they are in `self.classes_`."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:2
msgid "Probability estimates."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:6
msgid ""
"Multiclass probability estimates are derived from binary (one-vs.-rest) "
"estimates by simple normalization, as recommended by Zadrozny and Elkan."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:10
msgid ""
"Binary probability estimates for loss=\"modified_huber\" are given by "
"(clip(decision_function(X), -1, 1) + 1) / 2. For other loss functions it "
"is necessary to perform proper probability calibration by wrapping the "
"classifier with :class:`~sklearn.calibration.CalibratedClassifierCV` "
"instead."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:33
msgid "ndarray of shape (n_samples, n_classes)"
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:24
msgid ""
"Returns the probability of the sample for each class in the model, where "
"classes are ordered as they are in `self.classes_`."
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:36
msgid "References"
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:37
msgid ""
"Zadrozny and Elkan, \"Transforming classifier scores into multiclass "
"probability estimates\", SIGKDD'02, "
"http://www.research.ibm.com/people/z/zadrozny/kdd2002-Transf.pdf"
msgstr ""

#: of sklearn.linear_model.SGDClassifier.predict_proba:41
msgid ""
"The justification for the formula in the loss=\"modified_huber\" case is "
"in the appendix B in: "
"http://jmlr.csail.mit.edu/papers/volume2/zhang02c/zhang02c.pdf"
msgstr ""

#: of sklearn.base.ClassifierMixin.score:4
msgid ""
"In multi-label classification, this is the subset accuracy which is a "
"harsh metric since you require for each sample that each label set be "
"correctly predicted."
msgstr ""

#: of
msgid "array-like of shape (n_samples, n_features)"
msgstr ""

#: of sklearn.base.ClassifierMixin.score:11
msgid "Test samples."
msgstr ""

#: of
msgid "array-like of shape (n_samples,) or (n_samples, n_outputs)"
msgstr ""

#: of sklearn.base.ClassifierMixin.score:14
msgid "True labels for `X`."
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.base.ClassifierMixin.score:17
msgid "Sample weights."
msgstr ""

#: of sklearn.base.ClassifierMixin.score:33
msgid "**score**"
msgstr ""

#: of
msgid "float"
msgstr ""

#: of sklearn.base.ClassifierMixin.score:22
msgid "Mean accuracy of ``self.predict(X)`` wrt. `y`."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGD.set_params:8
msgid "**\\*\\*kwargs**"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGD.set_params:8
msgid "Estimator parameters."
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGD.set_params:24
msgid "**self**"
msgstr ""

#: of
msgid "object"
msgstr ""

#: of sklearn.linear_model._stochastic_gradient.BaseSGD.set_params:13
msgid "Estimator instance."
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.sparsify:4
msgid ""
"Converts the ``coef_`` member to a scipy.sparse matrix, which for "
"L1-regularized models can be much more memory- and storage-efficient than"
" the usual numpy.ndarray representation."
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.sparsify:8
msgid "The ``intercept_`` member is not converted."
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.sparsify:24
msgid "Notes"
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.sparsify:25
#, python-format
msgid ""
"For non-sparse models, i.e. when there are not many zeros in ``coef_``, "
"this may actually *increase* memory usage, so use this method with care. "
"A rule of thumb is that the number of zero elements, which can be "
"computed with ``(coef_ == 0).sum()``, must be more than 50% for this to "
"provide significant benefits."
msgstr ""

#: of sklearn.linear_model._base.SparseCoefMixin.sparsify:31
msgid ""
"After calling this method, further fitting with the partial_fit method "
"(if any) will not work until you call densify."
msgstr ""

#: ../modules/generated/sklearn.linear_model.SGDClassifier.examples:4
msgid "Examples using ``sklearn.linear_model.SGDClassifier``"
msgstr ""

#: ../modules/generated/sklearn.linear_model.SGDClassifier.examples:15
#: ../modules/generated/sklearn.linear_model.SGDClassifier.examples:23
msgid ":ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`"
msgstr ""

#~ msgid ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_iris.py`"
#~ msgstr ""

#~ msgid ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_early_stopping.py`"
#~ msgstr ""

#~ msgid ":ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_approximation.py`"
#~ msgstr ""

#~ msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_randomized_search.py`"
#~ msgstr ""

#~ msgid ":ref:`sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_newsgroups.py`"
#~ msgstr ""

#~ msgid ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"
#~ msgstr ""

