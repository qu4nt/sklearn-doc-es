

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>1.6. Vecino más cercano &mdash; documentación de scikit-learn - 0.24.2</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/neighbors.html" />

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../index.html">
        <img
          class="sk-brand-img"
          src="../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../install.html">Instalación</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">Manual de Usuario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Ejemplos</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../getting_started.html">¿Cómo empezar?</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../whats_new/v0.24.html">Novedades</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../glossary.html">Glosario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../developers/index.html">Desarrollo</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../support.html">Soporte</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../related_projects.html">Paquetes relacionados</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../roadmap.html">Hoja de ruta</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../about.html">Sobre nosotros</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Más</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../getting_started.html">¿Cómo empezar?</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../whats_new/v0.24.html">Novedades</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../glossary.html">Glosario</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../developers/index.html">Desarrollo</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../support.html">Soporte</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../related_projects.html">Paquetes relacionados</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../roadmap.html">Hoja de ruta</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../about.html">Sobre nosotros</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Ir a" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Alternar menú</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../index.html">
            <img
              class="sk-brand-img"
              src="../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="sgd.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.5. Descenso de Gradiente Estocástico">Prev</a><a href="../supervised_learning.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1. Aprendizaje supervisado">Arriba</a>
            <a href="gaussian_process.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="1.7. Procesos Gaussianos">Sig.</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.24.2</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Otras versiones</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Por favor <a class="font-weight-bold" href="../about.html#citing-scikit-learn"><string>cítanos</string></a> si usas el software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">1.6. Vecino más cercano</a><ul>
<li><a class="reference internal" href="#unsupervised-nearest-neighbors">1.6.1. Vecinos más cercanos no supervisados</a><ul>
<li><a class="reference internal" href="#finding-the-nearest-neighbors">1.6.1.1. Encontrar a los vecinos más cercanos</a></li>
<li><a class="reference internal" href="#kdtree-and-balltree-classes">1.6.1.2. Clases de árbol KD y árbol de bolas</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nearest-neighbors-classification">1.6.2. Clasificación de los vecinos más cercanos</a></li>
<li><a class="reference internal" href="#nearest-neighbors-regression">1.6.3. Regresión de vecinos más cercanos</a></li>
<li><a class="reference internal" href="#nearest-neighbor-algorithms">1.6.4. Algoritmos del vecino más cercano</a><ul>
<li><a class="reference internal" href="#brute-force">1.6.4.1. Fuerza bruta</a></li>
<li><a class="reference internal" href="#k-d-tree">1.6.4.2. Árbol K-D</a></li>
<li><a class="reference internal" href="#ball-tree">1.6.4.3. Árbol de bolas</a></li>
<li><a class="reference internal" href="#choice-of-nearest-neighbors-algorithm">1.6.4.4. Selección del algoritmo de los vecinos más cercanos</a></li>
<li><a class="reference internal" href="#effect-of-leaf-size">1.6.4.5. Efecto de <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code></a></li>
</ul>
</li>
<li><a class="reference internal" href="#nearest-centroid-classifier">1.6.5. Clasificador de centroides más cercano</a><ul>
<li><a class="reference internal" href="#nearest-shrunken-centroid">1.6.5.1. Centroide de Shrunken más cercano</a></li>
</ul>
</li>
<li><a class="reference internal" href="#nearest-neighbors-transformer">1.6.6. Transformador de vecinos más cercanos</a></li>
<li><a class="reference internal" href="#neighborhood-components-analysis">1.6.7. Análisis de componentes de Neighborhood</a><ul>
<li><a class="reference internal" href="#id4">1.6.7.1. Clasificación</a></li>
<li><a class="reference internal" href="#dimensionality-reduction">1.6.7.2. Reducción de Dimensionalidad</a></li>
<li><a class="reference internal" href="#mathematical-formulation">1.6.7.3. Formulación matemática</a><ul>
<li><a class="reference internal" href="#mahalanobis-distance">1.6.7.3.1. Distancia de Mahalanobis</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation">1.6.7.4. Implementación</a></li>
<li><a class="reference internal" href="#complexity">1.6.7.5. Complejidad</a><ul>
<li><a class="reference internal" href="#training">1.6.7.5.1. Formación</a></li>
<li><a class="reference internal" href="#transform">1.6.7.5.2. Transformación</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="nearest-neighbors">
<span id="neighbors"></span><h1><span class="section-number">1.6. </span>Vecino más cercano<a class="headerlink" href="#nearest-neighbors" title="Enlazar permanentemente con este título">¶</a></h1>
<p><a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> proporciona funcionalidad para métodos de aprendizaje basados en vecinos no supervisados y supervisados.  Los vecinos más cercanos no supervisados son la base de muchos otros métodos de aprendizaje, especialmente el aprendizaje múltiple y la agrupación espectral.  El aprendizaje basado en vecinos supervisados tiene dos variantes: <a class="reference internal" href="#classification">classification</a> para datos con etiquetas discretas, y <a class="reference internal" href="#regression">regression</a> para datos con etiquetas continuas.</p>
<p>El principio en el que se basan los métodos del vecino más cercano es encontrar un número predefinido de muestras de entrenamiento más cercanas en distancia al nuevo punto, y predecir la etiqueta a partir de ellas.  El número de muestras puede ser una constante definida por el usuario (aprendizaje del k vecino más cercano), o variar en función de la densidad local de puntos (aprendizaje del vecino basado en el radio). La distancia puede ser, en general, cualquier medida métrica: la distancia euclidiana estándar es la opción más común. Los métodos basados en los vecinos se conocen como métodos de aprendizaje automático <em>no generalizadores</em>, ya que simplemente «recuerdan» todos sus datos de entrenamiento (posiblemente transformados en una estructura de indexación rápida como un <a class="reference internal" href="#ball-tree"><span class="std std-ref">Árbol de bolas</span></a> o un <a class="reference internal" href="#kd-tree"><span class="std std-ref">Árbol KD</span></a>).</p>
<p>A pesar de su simplicidad, los vecinos más cercanos han tenido éxito en un gran número de problemas de clasificación y regresión, incluyendo dígitos escritos a mano y escenas de imágenes de satélite. Al ser un método no paramétrico, suele tener éxito en situaciones de clasificación en las que el límite de decisión es muy irregular.</p>
<p>Las clases de <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a> pueden manejar matrices NumPy o matrices <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> como entrada.  En el caso de las matrices densas, se admite un gran número de métricas de distancia posibles.  Para matrices dispersas, se admiten métricas de Minkowski arbitrarias para las búsquedas.</p>
<p>Hay muchas rutinas de aprendizaje que se basan en los vecinos más cercanos.  Un ejemplo es <a class="reference internal" href="density.html#kernel-density"><span class="std std-ref">kernel density estimation</span></a>, discutido en la sección <a class="reference internal" href="density.html#density-estimation"><span class="std std-ref">density estimation</span></a>.</p>
<section id="unsupervised-nearest-neighbors">
<span id="unsupervised-neighbors"></span><h2><span class="section-number">1.6.1. </span>Vecinos más cercanos no supervisados<a class="headerlink" href="#unsupervised-nearest-neighbors" title="Enlazar permanentemente con este título">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a> implementa el aprendizaje no supervisado de los vecinos más cercanos. Actúa como una interfaz uniforme para tres algoritmos diferentes de vecinos más cercanos: <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a>, <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a>, y un algoritmo de fuerza bruta basado en las rutinas de <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a>. La elección del algoritmo de búsqueda de vecinos se controla a través de la palabra clave <code class="docutils literal notranslate"><span class="pre">'algorithm'</span></code>, que debe ser uno de <code class="docutils literal notranslate"><span class="pre">['auto',</span> <span class="pre">'ball_tree',</span> <span class="pre">'kd_tree',</span> <span class="pre">'brute']</span></code>.  Cuando se pasa el valor por defecto <code class="docutils literal notranslate"><span class="pre">'auto'</span></code>, el algoritmo intenta determinar la mejor aproximación a partir de los datos de entrenamiento.  Para una discusión de los puntos fuertes y débiles de cada opción, consulta <a class="reference internal" href="#nearest-neighbor-algorithms">Nearest Neighbor Algorithms</a>.</p>
<blockquote>
<div><div class="admonition warning">
<p class="admonition-title">Advertencia</p>
<p>En cuanto a los algoritmos de vecinos más cercanos, si dos vecinos <span class="math notranslate nohighlight">\(k+1\)</span> y <span class="math notranslate nohighlight">\(k\)</span> tienen distancias idénticas pero etiquetas diferentes, el resultado dependerá del orden de los datos de entrenamiento.</p>
</div>
</div></blockquote>
<section id="finding-the-nearest-neighbors">
<h3><span class="section-number">1.6.1.1. </span>Encontrar a los vecinos más cercanos<a class="headerlink" href="#finding-the-nearest-neighbors" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Para la sencilla tarea de encontrar los vecinos más cercanos entre dos conjuntos de datos, se pueden utilizar los algoritmos no supervisados de <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a>:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestNeighbors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span> <span class="o">=</span> <span class="n">NearestNeighbors</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">algorithm</span><span class="o">=</span><span class="s1">&#39;ball_tree&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span><span class="p">,</span> <span class="n">indices</span> <span class="o">=</span> <span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">indices</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [2, 1],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3],</span>
<span class="go">       [5, 4]]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">distances</span>
<span class="go">array([[0.        , 1.        ],</span>
<span class="go">       [0.        , 1.        ],</span>
<span class="go">       [0.        , 1.41421356],</span>
<span class="go">       [0.        , 1.        ],</span>
<span class="go">       [0.        , 1.        ],</span>
<span class="go">       [0.        , 1.41421356]])</span>
</pre></div>
</div>
<p>Como el conjunto de consulta se corresponde con el conjunto de entrenamiento, el vecino más cercano de cada punto es el propio punto, a una distancia de cero.</p>
<p>También es posible producir eficazmente un gráfico disperso que muestre las conexiones entre los puntos vecinos:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">nbrs</span><span class="o">.</span><span class="n">kneighbors_graph</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="go">array([[1., 1., 0., 0., 0., 0.],</span>
<span class="go">       [1., 1., 0., 0., 0., 0.],</span>
<span class="go">       [0., 1., 1., 0., 0., 0.],</span>
<span class="go">       [0., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 0., 0., 1., 1., 0.],</span>
<span class="go">       [0., 0., 0., 0., 1., 1.]])</span>
</pre></div>
</div>
<p>El conjunto de datos está estructurado de manera que los puntos cercanos en el orden de los índices están cerca en el espacio de los parámetros, lo que conduce a una matriz aproximadamente diagonal de bloques de los vecinos más cercanos.  Este gráfico disperso es útil en una variedad de circunstancias que hacen uso de las relaciones espaciales entre los puntos para el aprendizaje no supervisado: en particular, véase <a class="reference internal" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="xref py py-class docutils literal notranslate"><span class="pre">Isomap</span></code></a>, <a class="reference internal" href="generated/sklearn.manifold.LocallyLinearEmbedding.html#sklearn.manifold.LocallyLinearEmbedding" title="sklearn.manifold.LocallyLinearEmbedding"><code class="xref py py-class docutils literal notranslate"><span class="pre">LocallyLinearEmbedding</span></code></a>, y <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralClustering</span></code></a>.</p>
</section>
<section id="kdtree-and-balltree-classes">
<h3><span class="section-number">1.6.1.2. </span>Clases de árbol KD y árbol de bolas<a class="headerlink" href="#kdtree-and-balltree-classes" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Como alternativa, se pueden utilizar las clases <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> o <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> directamente para encontrar los vecinos más cercanos.  Esta es la funcionalidad que envuelve la clase <a class="reference internal" href="generated/sklearn.neighbors.NearestNeighbors.html#sklearn.neighbors.NearestNeighbors" title="sklearn.neighbors.NearestNeighbors"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestNeighbors</span></code></a> utilizada anteriormente.  El árbol de bolas y el árbol KD tienen la misma interfaz; aquí mostraremos un ejemplo de uso del árbol KD:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KDTree</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kdt</span> <span class="o">=</span> <span class="n">KDTree</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">leaf_size</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">metric</span><span class="o">=</span><span class="s1">&#39;euclidean&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">kdt</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">return_distance</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="go">array([[0, 1],</span>
<span class="go">       [1, 0],</span>
<span class="go">       [2, 1],</span>
<span class="go">       [3, 4],</span>
<span class="go">       [4, 3],</span>
<span class="go">       [5, 4]]...)</span>
</pre></div>
</div>
<p>Consulta la documentación de las clases <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> y <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> para obtener más información sobre las opciones disponibles para las búsquedas de vecinos más cercanos, incluyendo la especificación de estrategias de consulta, métricas de distancia, etc. Para una lista de métricas disponibles, consulta la documentación de la clase <a class="reference internal" href="generated/sklearn.neighbors.DistanceMetric.html#sklearn.neighbors.DistanceMetric" title="sklearn.neighbors.DistanceMetric"><code class="xref py py-class docutils literal notranslate"><span class="pre">DistanceMetric</span></code></a>.</p>
</section>
</section>
<section id="nearest-neighbors-classification">
<span id="classification"></span><h2><span class="section-number">1.6.2. </span>Clasificación de los vecinos más cercanos<a class="headerlink" href="#nearest-neighbors-classification" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La clasificación basada en vecinos es un tipo de <em>aprendizaje basado en instancias</em> o <em>aprendizaje no generalizador</em>: no intenta construir un modelo interno general, sino que simplemente almacena instancias de los datos de entrenamiento. La clasificación se calcula a partir de una simple mayoría de votos de los vecinos más cercanos de cada punto: a un punto de consulta se le asigna la clase de datos que tiene más representantes dentro de los vecinos más cercanos del punto.</p>
<p>scikit-learn implementa dos clasificadores diferentes de vecinos más cercanos: <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> implementa el aprendizaje basado en los <span class="math notranslate nohighlight">\(k\)</span> vecinos más cercanos de cada punto de consulta, donde <span class="math notranslate nohighlight">\(k\)</span> es un valor entero especificado por el usuario.  <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a> implementa el aprendizaje basado en el número de vecinos dentro de un radio fijo <span class="math notranslate nohighlight">\(r\)</span> de cada punto de entrenamiento, donde <span class="math notranslate nohighlight">\(r\)</span> es un valor de punto flotante especificado por el usuario.</p>
<p>La clasificación <span class="math notranslate nohighlight">\(k\)</span>-neighbors en <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> es la técnica más utilizada. La elección óptima del valor <span class="math notranslate nohighlight">\(k\)</span> depende en gran medida de los datos: en general, un <span class="math notranslate nohighlight">\(k\)</span> mayor suprime los efectos del ruido, pero hace que los límites de la clasificación sean menos nítidos.</p>
<p>En los casos en los que los datos no están muestreados uniformemente, la clasificación de vecinos basada en el radio en <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsClassifier.html#sklearn.neighbors.RadiusNeighborsClassifier" title="sklearn.neighbors.RadiusNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsClassifier</span></code></a> puede ser una mejor opción. El usuario especifica un radio fijo <span class="math notranslate nohighlight">\(r\)</span>, de forma que los puntos de los vecindarios más dispersos utilizan menos vecinos más cercanos para la clasificación.  Para los espacios de parámetros de alta dimensión, este método se vuelve menos eficaz debido a la llamada «maldición de la dimensión».</p>
<p>La clasificación básica de vecinos más cercanos utiliza ponderaciones uniformes: es decir, el valor asignado a un punto de consulta se calcula a partir de un voto mayoritario simple de los vecinos más cercanos.  En algunas circunstancias, es mejor ponderar los vecinos de forma que los más cercanos contribuyan más al ajuste.  Esto puede lograrse mediante la palabra clave <code class="docutils literal notranslate"><span class="pre">weights</span></code>.  El valor por defecto, <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'uniform'</span></code>, asigna pesos uniformes a cada vecino. El valor <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'distance</span></code> asigna pesos proporcionales a la inversa de la distancia al punto de consulta.  Alternativamente, se puede proporcionar una función definida por el usuario de la distancia para calcular los pesos.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_classification.html"><img alt="classification_1" src="../_images/sphx_glr_plot_classification_001.png" style="width: 400.0px; height: 300.0px;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_classification.html"><img alt="classification_2" src="../_images/sphx_glr_plot_classification_002.png" style="width: 400.0px; height: 300.0px;" /></a></strong></p><div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="std std-ref">Clasificación de los Vecinos más Cercanos</span></a>: un ejemplo de clasificación usando vecinos más cercanos.</p></li>
</ul>
</div>
</section>
<section id="nearest-neighbors-regression">
<span id="regression"></span><h2><span class="section-number">1.6.3. </span>Regresión de vecinos más cercanos<a class="headerlink" href="#nearest-neighbors-regression" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La regresión basada en los vecinos puede utilizarse en los casos en que las etiquetas de los datos son variables continuas y no discretas.  La etiqueta asignada a un punto de consulta se calcula a partir de la media de las etiquetas de sus vecinos más cercanos.</p>
<p>scikit-learn implementa dos regresores de vecinos diferentes: <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a> implementa el aprendizaje basado en los <span class="math notranslate nohighlight">\(k\)</span> vecinos más cercanos de cada punto de consulta, donde <span class="math notranslate nohighlight">\(k\)</span> es un valor entero especificado por el usuario.  <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsRegressor.html#sklearn.neighbors.RadiusNeighborsRegressor" title="sklearn.neighbors.RadiusNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsRegressor</span></code></a> implementa el aprendizaje basado en los vecinos dentro de un radio fijo <span class="math notranslate nohighlight">\(r\)</span> del punto de consulta, donde <span class="math notranslate nohighlight">\(r\)</span> es un valor de punto flotante especificado por el usuario.</p>
<p>La regresión básica de vecinos más cercanos utiliza pesos uniformes: es decir, cada punto de la vecindad local contribuye uniformemente a la clasificación de un punto de consulta.  En algunas circunstancias, puede ser ventajoso ponderar los puntos de manera que los puntos cercanos contribuyan más a la regresión que los puntos lejanos.  Esto puede lograrse mediante la palabra clave <code class="docutils literal notranslate"><span class="pre">weights</span></code>.  El valor por defecto, <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'uniform'</span></code>, asigna pesos iguales a todos los puntos.  El valor <code class="docutils literal notranslate"><span class="pre">weights</span> <span class="pre">=</span> <span class="pre">'distance'</span></code> asigna pesos proporcionales a la inversa de la distancia al punto de consulta. Alternativamente, se puede proporcionar una función definida por el usuario de la distancia, que se utilizará para calcular los pesos.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/neighbors/plot_regression.html"><img alt="../_images/sphx_glr_plot_regression_001.png" src="../_images/sphx_glr_plot_regression_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>El uso de vecinos más cercanos de salida múltiple para la regresión se demuestra en <a class="reference internal" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py"><span class="std std-ref">Finalización facial con estimadores de salida múltiple</span></a>. En este ejemplo, las entradas X son los píxeles de la mitad superior de las caras y las salidas Y son los píxeles de la mitad inferior de esas caras.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html"><img alt="../_images/sphx_glr_plot_multioutput_face_completion_001.png" src="../_images/sphx_glr_plot_multioutput_face_completion_001.png" style="width: 750.0px; height: 847.5px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_classification.html#sphx-glr-auto-examples-neighbors-plot-classification-py"><span class="std std-ref">Clasificación de los Vecinos más Cercanos</span></a>: un ejemplo de clasificación usando vecinos más cercanos.</p></li>
<li><p><a class="reference internal" href="../auto_examples/miscellaneous/plot_multioutput_face_completion.html#sphx-glr-auto-examples-miscellaneous-plot-multioutput-face-completion-py"><span class="std std-ref">Finalización facial con estimadores de salida múltiple</span></a>: un ejemplo de regresión de multi-salida usando vecinos más cercanos.</p></li>
</ul>
</div>
</section>
<section id="nearest-neighbor-algorithms">
<h2><span class="section-number">1.6.4. </span>Algoritmos del vecino más cercano<a class="headerlink" href="#nearest-neighbor-algorithms" title="Enlazar permanentemente con este título">¶</a></h2>
<section id="brute-force">
<span id="id1"></span><h3><span class="section-number">1.6.4.1. </span>Fuerza bruta<a class="headerlink" href="#brute-force" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El cálculo rápido de los vecinos más cercanos es un área activa de investigación en el aprendizaje automático. La implementación más simple de la búsqueda de vecinos implica el cálculo por fuerza bruta de las distancias entre todos los pares de puntos del conjunto de datos: para <span class="math notranslate nohighlight">\(N\)</span> muestras en <span class="math notranslate nohighlight">\(D\)</span> dimensiones, este enfoque se escala como <span class="math notranslate nohighlight">\(O[D N^2]\)</span>.  La búsqueda eficiente de vecinos por fuerza bruta puede ser muy competitiva para muestras de datos pequeñas. Sin embargo, a medida que el número de muestras <span class="math notranslate nohighlight">\(N\)</span> crece, el enfoque de fuerza bruta se vuelve rápidamente inviable.  En las clases dentro de <a class="reference internal" href="classes.html#module-sklearn.neighbors" title="sklearn.neighbors"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.neighbors</span></code></a>, las búsquedas de vecinos por fuerza bruta se especifican utilizando la palabra clave <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">=</span> <span class="pre">'brute'</span></code>, y se calculan utilizando las rutinas disponibles en <a class="reference internal" href="classes.html#module-sklearn.metrics.pairwise" title="sklearn.metrics.pairwise"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.metrics.pairwise</span></code></a>.</p>
</section>
<section id="k-d-tree">
<span id="kd-tree"></span><h3><span class="section-number">1.6.4.2. </span>Árbol K-D<a class="headerlink" href="#k-d-tree" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Para hacer frente a las ineficiencias computacionales del enfoque de fuerza bruta, se han inventado diversas estructuras de datos basadas en árboles.  En general, estas estructuras intentan reducir el número de cálculos de distancia necesarios codificando de forma eficiente la información de distancia agregada para la muestra. La idea básica es que si el punto <span class="math notranslate nohighlight">\(A\)</span> está muy distante del punto <span class="math notranslate nohighlight">\(B\)</span>, y el punto <span class="math notranslate nohighlight">\(B\)</span> está muy cerca del punto <span class="math notranslate nohighlight">\(C\)</span>, entonces sabemos que los puntos <span class="math notranslate nohighlight">\(A\)</span> y <span class="math notranslate nohighlight">\(C\)</span> están muy distantes, <em>sin tener que calcular explícitamente su distancia</em>. De este modo, el coste computacional de una búsqueda de vecinos más cercanos puede reducirse a <span class="math notranslate nohighlight">\(O[D N \log(N)]\)</span> o mejor. Esto supone una mejora significativa respecto a la fuerza bruta para grandes <span class="math notranslate nohighlight">\(N\)</span>.</p>
<p>Uno de los primeros enfoques para aprovechar esta información agregada fue la estructura de datos de <em>KD tree</em> o <em>árbol KD</em> (abreviatura de <em>árbol K-dimensional</em>), que generaliza los <em>árboles Quad</em> (<em>Quad-trees</em>) bidimensionales y los <em>árboles Oct</em>  (<em>Oct-trees</em>) tridimensionales a un número arbitrario de dimensiones.  El árbol KD es una estructura de árbol binario que particiona recursivamente el espacio de los parámetros a lo largo de los ejes de los datos, dividiéndolo en regiones ortotrópicas anidadas en las que se archivan los puntos de datos.  La construcción de un árbol KD es muy rápida: como la partición se realiza sólo a lo largo de los ejes de datos, no es necesario calcular distancias <span class="math notranslate nohighlight">\(D\)</span>-dimensionales. Una vez construido, el vecino más cercano de un punto de consulta puede determinarse con sólo cálculos de distancia <span class="math notranslate nohighlight">\(O[\log(N)]\)</span>. Aunque el enfoque del árbol KD es muy rápido para búsquedas de vecinos de baja dimensión (<span class="math notranslate nohighlight">\(D &lt; 20\)</span>), se vuelve ineficiente a medida que <span class="math notranslate nohighlight">\(D\)</span> aumenta: esta es una manifestación de la llamada «maldición de la dimensionalidad». En scikit-learn, las búsquedas de vecinos en el árbol KD se especifican utilizando la palabra clave <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">=</span> <span class="pre">'kd_tree'</span></code>, y se calculan utilizando la clase <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a>.</p>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://dl.acm.org/citation.cfm?doid=361002.361007">«Multidimensional binary search trees used for associative searching»</a>,
Bentley, J.L., Communications of the ACM (1975)</p></li>
</ul>
</div>
</section>
<section id="ball-tree">
<span id="id2"></span><h3><span class="section-number">1.6.4.3. </span>Árbol de bolas<a class="headerlink" href="#ball-tree" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Para hacer frente a las ineficiencias de los árboles KD en dimensiones superiores, se desarrolló la estructura de datos <em>árbol de bolas</em>.  Mientras que los árboles KD dividen los datos a lo largo de ejes cartesianos, los árboles de bolas dividen los datos en una serie de hiperesferas anidadas.  Esto hace que la construcción del árbol sea más costosa que la del árbol KD, pero da como resultado una estructura de datos que puede ser muy eficiente en datos altamente estructurados, incluso en dimensiones muy altas.</p>
<p>Un árbol de bolas divide recursivamente los datos en nodos definidos por un centroide <span class="math notranslate nohighlight">\(C\)</span> y un radio <span class="math notranslate nohighlight">\(r\)</span>, de forma que cada punto del nodo se encuentra dentro de la hiperesfera definida por <span class="math notranslate nohighlight">\(r\)</span> y <span class="math notranslate nohighlight">\(C\)</span>. El número de puntos candidatos para la búsqueda de vecinos se reduce mediante el uso de la <em>desigualdad triangular</em>:</p>
<div class="math notranslate nohighlight">
\[|x+y| \leq |x| + |y|\]</div>
<p>Con esta configuración, un único cálculo de la distancia entre un punto de prueba y el centroide es suficiente para determinar un límite inferior y superior de la distancia a todos los puntos dentro del nodo. Debido a la geometría esférica de los nodos del árbol de bolas, puede superar a un <em>árbol KD</em> en dimensiones altas, aunque el rendimiento real depende en gran medida de la estructura de los datos de entrenamiento. En scikit-learn, las búsquedas de vecinos basadas en el árbol de bolas se especifican utilizando la palabra clave <code class="docutils literal notranslate"><span class="pre">algoritmo</span> <span class="pre">=</span> <span class="pre">'ball_tree'</span></code>, y se calculan utilizando la clase <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a>. Alternativamente, el usuario puede trabajar con la clase <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> directamente.</p>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.91.8209">«Five balltree construction algorithms»</a>,
Omohundro, S.M., International Computer Science Institute
Technical Report (1989)</p></li>
</ul>
</div>
</section>
<section id="choice-of-nearest-neighbors-algorithm">
<h3><span class="section-number">1.6.4.4. </span>Selección del algoritmo de los vecinos más cercanos<a class="headerlink" href="#choice-of-nearest-neighbors-algorithm" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El algoritmo óptimo para un conjunto de datos determinado es una elección complicada y depende de varios factores:</p>
<ul>
<li><p>número de muestras <span class="math notranslate nohighlight">\(N\)</span> (es decir, <code class="docutils literal notranslate"><span class="pre">n_muestras</span></code>) y dimensionalidad <span class="math notranslate nohighlight">\(D\)</span> (es decir, <code class="docutils literal notranslate"><span class="pre">n_características</span></code>).</p>
<ul class="simple">
<li><p>El tiempo de consulta de <em>fuerza bruta</em> crece como <span class="math notranslate nohighlight">\(O[D N]\)</span></p></li>
<li><p><a href="#id1"><span class="problematic" id="id2">*</span></a>El tiempo de consulta del árbol de bolas crece aproximadamente como <span class="math notranslate nohighlight">\(O[D \log(N)]\)</span></p></li>
<li><p><a href="#id1"><span class="problematic" id="id2">*</span></a>El tiempo de consulta del árbol KD cambia con <span class="math notranslate nohighlight">\(D\)</span> de una manera que es difícil de caracterizar con precisión.  Para <span class="math notranslate nohighlight">\(D\)</span> pequeños (menos de 20 aproximadamente) el costo es aproximadamente <span class="math notranslate nohighlight">\(O[D\log(N)]\)</span>, y la consulta del árbol KD puede ser muy eficiente. Para <span class="math notranslate nohighlight">\(D\)</span> más grandes, el costo aumenta hasta casi <span class="math notranslate nohighlight">\(O[DN]\)</span>, y la sobrecarga debida a la estructura de árbol puede llevar a consultas más lentas que la fuerza bruta.</p></li>
</ul>
<p>Para conjuntos de datos pequeños (<span class="math notranslate nohighlight">\(N\)</span> menos de 30 aproximadamente), <span class="math notranslate nohighlight">\(log(N)\)</span> es comparable a <span class="math notranslate nohighlight">\(N\)</span>, y los algoritmos de fuerza bruta pueden ser más eficientes que un enfoque basado en árboles.  Tanto <a class="reference internal" href="generated/sklearn.neighbors.KDTree.html#sklearn.neighbors.KDTree" title="sklearn.neighbors.KDTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">KDTree</span></code></a> como <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> abordan esta cuestión proporcionando un parámetro de <em>tamaño de hoja</em>: éste controla el número de muestras a partir del cual una consulta pasa a ser de fuerza bruta.  Esto permite que ambos algoritmos se aproximen a la eficiencia de un cálculo de fuerza bruta para un <span class="math notranslate nohighlight">\(N\)</span> pequeño.</p>
</li>
<li><p>estructura de los datos: La <em>dimensionalidad intrínseca</em> de los datos y/o la <em>especificidad</em> de los datos. La dimensionalidad intrínseca se refiere a la dimensión <span class="math notranslate nohighlight">\(d \le D\)</span> de un colector en el que se encuentran los datos, que puede estar incrustado de forma lineal o no lineal en el espacio de parámetros. La dispersión se refiere al grado en que los datos llenan el espacio de parámetros (esto debe distinguirse del concepto utilizado en las matrices «dispersas».  La matriz de datos puede no tener entradas nulas, pero la <strong>estructura</strong> puede seguir siendo «dispersa» en este sentido).</p>
<ul class="simple">
<li><p>El tiempo de consulta de <em>fuerza bruta</em> no se ve afectado por la estructura de datos.</p></li>
<li><p>Los tiempos de consulta del <em>árbol de bolas</em> y del <em>árbol KD</em> pueden verse muy influidos por la estructura de los datos.  En general, los datos más dispersos con una menor dimensionalidad intrínseca conducen a tiempos de consulta más rápidos.  Dado que la representación interna del árbol KD está alineada con los ejes de los parámetros, en general no mostrará tantas mejoras como el árbol de bolas para datos estructurados de forma arbitraria.</p></li>
</ul>
<p>Los conjuntos de datos utilizados en el aprendizaje automático suelen estar muy estructurados y son muy adecuados para las consultas basadas en árboles.</p>
</li>
<li><p>número de vecinos <span class="math notranslate nohighlight">\(k\)</span> solicitados para un punto de consulta.</p>
<ul class="simple">
<li><p><a href="#id1"><span class="problematic" id="id2">*</span></a>El tiempo de consulta por fuerza bruta no se ve afectado por el valor de <span class="math notranslate nohighlight">\(k\)</span></p></li>
<li><p>El tiempo de consulta del <em>árbol de bolas</em> y del <em>árbol KD</em> será más lento a medida que <span class="math notranslate nohighlight">\(k\)</span> aumente.  Esto se debe a dos efectos: en primer lugar, un <span class="math notranslate nohighlight">\(k\)</span> mayor conduce a la necesidad de buscar una porción mayor del espacio de parámetros. En segundo lugar, el uso de <span class="math notranslate nohighlight">\(k &gt; 1\)</span> requiere la puesta en cola interna de los resultados a medida que se recorre el árbol.</p></li>
</ul>
<p>A medida que <span class="math notranslate nohighlight">\(k\)</span> se hace grande en comparación con <span class="math notranslate nohighlight">\(N\)</span>, la capacidad de podar ramas en una consulta basada en un árbol se reduce.  En esta situación, las consultas de fuerza bruta pueden ser más eficientes.</p>
</li>
<li><p>número de puntos de consulta.  Tanto el árbol de bolas como el árbol KD requieren una fase de construcción.  El coste de esta construcción resulta insignificante cuando se amortiza en muchas consultas.  Sin embargo, si sólo se va a realizar un pequeño número de consultas, la construcción puede suponer una fracción significativa del coste total.  Si se necesitan muy pocos puntos de consulta, la fuerza bruta es mejor que un método basado en árboles.</p></li>
</ul>
<p>Actualmente, <code class="docutils literal notranslate"><span class="pre">algorithm</span> <span class="pre">=</span> <span class="pre">'auto'</span></code> selecciona <code class="docutils literal notranslate"><span class="pre">'brute'</span></code> si se verifica alguna de las siguientes condiciones:</p>
<ul class="simple">
<li><p>los datos de entrada son dispersos</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">metric</span> <span class="pre">=</span> <span class="pre">'precomputed'</span></code></p></li>
<li><p><span class="math notranslate nohighlight">\(D &gt; 15\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(k &gt;= N/2\)</span></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">effective_metric_</span></code> no está en la lista <code class="docutils literal notranslate"><span class="pre">VALID_METRICS</span></code> para <code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> o <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code></p></li>
</ul>
<p>En caso contrario, selecciona el primero de <code class="docutils literal notranslate"><span class="pre">'kd_tree'</span></code> y <code class="docutils literal notranslate"><span class="pre">'ball_tree'</span></code> que tenga <code class="docutils literal notranslate"><span class="pre">effective_metric_</span></code> en su lista <code class="docutils literal notranslate"><span class="pre">VALID_METRICS</span></code>. Esta heurística se basa en las siguientes suposiciones:</p>
<ul class="simple">
<li><p>el número de puntos de consulta es al menos del mismo orden que el número de puntos de entrenamiento</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> está cerca de su valor por defecto de <code class="docutils literal notranslate"><span class="pre">30</span></code></p></li>
<li><p>cuando <span class="math notranslate nohighlight">\(D &gt; 15\)</span>, la dimensionalidad intrínseca de los datos es generalmente demasiado alta para los métodos basados en árboles</p></li>
</ul>
</section>
<section id="effect-of-leaf-size">
<h3><span class="section-number">1.6.4.5. </span>Efecto de <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code><a class="headerlink" href="#effect-of-leaf-size" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Como se ha mencionado anteriormente, para tamaños de muestra pequeños, una búsqueda de fuerza bruta puede ser más eficiente que una consulta basada en un árbol.  Este hecho se tiene en cuenta en el árbol de bolas y en el árbol KD cambiando internamente a búsquedas de fuerza bruta dentro de los nodos hoja.  El nivel de este cambio puede especificarse con el parámetro <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code>.  La elección de este parámetro tiene muchos efectos:</p>
<dl class="simple">
<dt><strong>tiempo de construcción</strong></dt><dd><p>Un <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> mayor acelera el tiempo de construcción del árbol, ya que hay que crear menos nodos</p>
</dd>
<dt><strong>tiempo de consulta</strong></dt><dd><p>Tanto un <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> grande como pequeño puede llevar a un coste de consulta subóptimo. Si <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> se aproxima a 1, la sobrecarga que supone recorrer los nodos puede ralentizar considerablemente los tiempos de consulta.  Si el tamaño de las hojas se aproxima al tamaño del conjunto de entrenamiento, las consultas se convierten en fuerza bruta. Un buen compromiso entre ambos es <code class="docutils literal notranslate"><span class="pre">leaf_size</span> <span class="pre">=</span> <span class="pre">30</span></code>, el valor por defecto del parámetro.</p>
</dd>
<dt><strong>memoria</strong></dt><dd><p>A medida que <code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> aumenta, la memoria requerida para almacenar una estructura de árbol disminuye. Esto es especialmente importante en el caso del árbol de bolas, que almacena un <span class="math notranslate nohighlight">\(D\)</span>-dimensional centroid para cada nodo. El espacio de almacenamiento requerido para <a class="reference internal" href="generated/sklearn.neighbors.BallTree.html#sklearn.neighbors.BallTree" title="sklearn.neighbors.BallTree"><code class="xref py py-class docutils literal notranslate"><span class="pre">BallTree</span></code></a> es aproximadamente <code class="docutils literal notranslate"><span class="pre">1</span> <span class="pre">/</span> <span class="pre">leaf_size</span></code> el tamaño del conjunto de entrenamiento.</p>
</dd>
</dl>
<p><code class="docutils literal notranslate"><span class="pre">leaf_size</span></code> no está referenciado para consultas de fuerza bruta.</p>
</section>
</section>
<section id="nearest-centroid-classifier">
<span id="id3"></span><h2><span class="section-number">1.6.5. </span>Clasificador de centroides más cercano<a class="headerlink" href="#nearest-centroid-classifier" title="Enlazar permanentemente con este título">¶</a></h2>
<p>El clasificador <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> es un algoritmo simple que representa cada clase por el centroide de sus miembros. En efecto, esto lo hace similar a la fase de actualización de etiquetas del algoritmo <a class="reference internal" href="generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans" title="sklearn.cluster.KMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">KMeans</span></code></a>. Tampoco hay que elegir parámetros, lo que lo convierte en un buen clasificador de referencia. Sin embargo, sufre en clases no convexas, así como cuando las clases tienen varianzas drásticamente diferentes, ya que se asume una varianza igual en todas las dimensiones. Consulta Linear Discriminant Analysis (<a class="reference internal" href="generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html#sklearn.discriminant_analysis.LinearDiscriminantAnalysis" title="sklearn.discriminant_analysis.LinearDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">LinearDiscriminantAnalysis</span></code></a>) y Quadratic Discriminant Analysis (<a class="reference internal" href="generated/sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis.html#sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis" title="sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">QuadraticDiscriminantAnalysis</span></code></a>) para métodos más complejos que no hacen esta suposición. El uso de la clase <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> por defecto es sencillo:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">NearestCentroid</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mi">3</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">2</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">NearestCentroid</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="go">NearestCentroid()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="o">-</span><span class="mf">0.8</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]]))</span>
<span class="go">[1]</span>
</pre></div>
</div>
<section id="nearest-shrunken-centroid">
<h3><span class="section-number">1.6.5.1. </span>Centroide de Shrunken más cercano<a class="headerlink" href="#nearest-shrunken-centroid" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El clasificador <a class="reference internal" href="generated/sklearn.neighbors.NearestCentroid.html#sklearn.neighbors.NearestCentroid" title="sklearn.neighbors.NearestCentroid"><code class="xref py py-class docutils literal notranslate"><span class="pre">NearestCentroid</span></code></a> tiene un parámetro <code class="docutils literal notranslate"><span class="pre">shrink_threshold</span></code>, que implementa el clasificador shrunken centroid más cercano. En efecto, el valor de cada característica para cada centroide se divide por la varianza dentro de la clase de esa característica. A continuación, los valores de las características se reducen en <code class="docutils literal notranslate"><span class="pre">shrink_threshold</span></code>. En particular, si el valor de una característica particular es cero, se pone a cero. En efecto, esto hace que la característica no afecte a la clasificación. Esto es útil, por ejemplo, para eliminar características ruidosas.</p>
<p>En el ejemplo siguiente, el uso de un reducido umbral de contracción aumenta la precisión del modelo de 0,81 a 0,82.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_nearest_centroid.html"><img alt="nearest_centroid_1" src="../_images/sphx_glr_plot_nearest_centroid_001.png" style="width: 320.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_nearest_centroid.html"><img alt="nearest_centroid_2" src="../_images/sphx_glr_plot_nearest_centroid_002.png" style="width: 320.0px; height: 240.0px;" /></a></strong></p><div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_nearest_centroid.html#sphx-glr-auto-examples-neighbors-plot-nearest-centroid-py"><span class="std std-ref">Clasificación del Centroide más Cercano</span></a>: un ejemplo de clasificación utilizando el centroide más cercano con diferentes umbrales de reducción.</p></li>
</ul>
</div>
</section>
</section>
<section id="nearest-neighbors-transformer">
<span id="neighbors-transformer"></span><h2><span class="section-number">1.6.6. </span>Transformador de vecinos más cercanos<a class="headerlink" href="#nearest-neighbors-transformer" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Muchos estimadores de scikit-learn se basan en los vecinos más cercanos: Varios clasificadores y regresores como <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> y <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsRegressor.html#sklearn.neighbors.KNeighborsRegressor" title="sklearn.neighbors.KNeighborsRegressor"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsRegressor</span></code></a>, pero también algunos métodos de agrupamiento como <a class="reference internal" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code class="xref py py-class docutils literal notranslate"><span class="pre">DBSCAN</span></code></a> y <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralClustering</span></code></a>, y algunos incrustados como <a class="reference internal" href="generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="sklearn.manifold.TSNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNE</span></code></a> y <a class="reference internal" href="generated/sklearn.manifold.Isomap.html#sklearn.manifold.Isomap" title="sklearn.manifold.Isomap"><code class="xref py py-class docutils literal notranslate"><span class="pre">Isomap</span></code></a>.</p>
<p>Todos estos estimadores pueden calcular internamente los vecinos más cercanos, pero la mayoría de ellos también aceptan el <a class="reference internal" href="../glossary.html#term-sparse-graph"><span class="xref std std-term">grafo disperso</span></a> de los vecinos más cercanos precalculados, como se da en <a class="reference internal" href="generated/sklearn.neighbors.kneighbors_graph.html#sklearn.neighbors.kneighbors_graph" title="sklearn.neighbors.kneighbors_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">kneighbors_graph</span></code></a> y <a class="reference internal" href="generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph" title="sklearn.neighbors.radius_neighbors_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">radius_neighbors_graph</span></code></a>. Con el modo <code class="docutils literal notranslate"><span class="pre">mode='connectivity'</span></code>, estas funciones devuelven un gráfico binario de adyacencia disperso como se requiere, por ejemplo, en <a class="reference internal" href="generated/sklearn.cluster.SpectralClustering.html#sklearn.cluster.SpectralClustering" title="sklearn.cluster.SpectralClustering"><code class="xref py py-class docutils literal notranslate"><span class="pre">SpectralClustering</span></code></a>. Mientras que con <code class="docutils literal notranslate"><span class="pre">mode='distance'</span></code>, devuelven un grafo disperso de distancia como se requiere, por ejemplo, en <a class="reference internal" href="generated/sklearn.cluster.DBSCAN.html#sklearn.cluster.DBSCAN" title="sklearn.cluster.DBSCAN"><code class="xref py py-class docutils literal notranslate"><span class="pre">DBSCAN</span></code></a>. Para incluir estas funciones en un pipeline de scikit-learn, también se pueden utilizar las clases correspondientes <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer" title="sklearn.neighbors.KNeighborsTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsTransformer</span></code></a> y <a class="reference internal" href="generated/sklearn.neighbors.RadiusNeighborsTransformer.html#sklearn.neighbors.RadiusNeighborsTransformer" title="sklearn.neighbors.RadiusNeighborsTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">RadiusNeighborsTransformer</span></code></a>. Los beneficios de esta API de gráficos dispersos son múltiples.</p>
<p>En primer lugar, el gráfico precalculado puede reutilizarse varias veces, por ejemplo, al variar un parámetro del estimador. Esto se puede hacer manualmente por el usuario, o utilizando las propiedades de almacenamiento en caché de la pipeline de scikit-learn:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">Isomap</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsTransformer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">make_pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">estimator</span> <span class="o">=</span> <span class="n">make_pipeline</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">KNeighborsTransformer</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;distance&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">Isomap</span><span class="p">(</span><span class="n">neighbors_algorithm</span><span class="o">=</span><span class="s1">&#39;precomputed&#39;</span><span class="p">),</span>
<span class="gp">... </span>    <span class="n">memory</span><span class="o">=</span><span class="s1">&#39;/path/to/cache&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>En segundo lugar, precalcular el gráfico puede dar un control más fino sobre la estimación de los vecinos más cercanos, por ejemplo, permitiendo el multiprocesamiento a través del parámetro <code class="docutils literal notranslate"><span class="pre">n_jobs</span></code>, que podría no estar disponible en todos los estimadores.</p>
<p>Finalmente, el precálculo puede ser realizado por estimadores personalizados para utilizar diferentes implementaciones, como los métodos de vecinos más cercanos aproximados, o la implementación con tipos de datos especiales. El <a class="reference internal" href="../glossary.html#term-sparse-graph"><span class="xref std std-term">grafo disperso</span></a> de vecinos precalculados debe tener el mismo formato que la salida de <a class="reference internal" href="generated/sklearn.neighbors.radius_neighbors_graph.html#sklearn.neighbors.radius_neighbors_graph" title="sklearn.neighbors.radius_neighbors_graph"><code class="xref py py-func docutils literal notranslate"><span class="pre">radius_neighbors_graph</span></code></a>:</p>
<ul class="simple">
<li><p>una matriz CSR (aunque se aceptarán COO, CSC o LIL).</p></li>
<li><p>sólo almacena explícitamente los vecindarios más cercanos de cada muestra con respecto a los datos de entrenamiento. Esto debería incluir los que se encuentran a una distancia de 0 de un punto de consulta, incluyendo la diagonal de la matriz cuando se calculan los vecindarios más cercanos entre los datos de entrenamiento y ellos mismos.</p></li>
<li><p>los <code class="docutils literal notranslate"><span class="pre">datos</span></code> de cada fila deben almacenar la distancia en orden creciente (opcional. Los datos no ordenados serán ordenados de forma estable, lo que añade una sobrecarga computacional).</p></li>
<li><p>todos los valores en los datos no deben ser negativos.</p></li>
<li><p>no debe haber <code class="docutils literal notranslate"><span class="pre">indices</span></code> duplicados en ninguna fila (consulta <a class="reference external" href="https://github.com/scipy/scipy/issues/5807">https://github.com/scipy/scipy/issues/5807</a>).</p></li>
<li><p>si el algoritmo al que se le pasa la matriz precalculada utiliza k vecinos más cercanos (en lugar de vecindad de radio), se deben almacenar al menos k vecinos en cada fila (o k+1, como se explica en la siguiente nota).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Cuando se consulta un número específico de vecinos (utilizando <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer" title="sklearn.neighbors.KNeighborsTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsTransformer</span></code></a>), la definición de <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code> es ambigua ya que puede incluir cada punto de entrenamiento como su propio vecino, o excluirlos. Ninguna de las dos opciones es perfecta, ya que incluirlos conduce a un número diferente de vecinos no propios durante el entrenamiento y la prueba, mientras que excluirlos conduce a una diferencia entre <code class="docutils literal notranslate"><span class="pre">fit(X).transform(X)</span></code> y <code class="docutils literal notranslate"><span class="pre">fit_transform(X)</span></code>, lo que va en contra de la API de scikit-learn. En <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer" title="sklearn.neighbors.KNeighborsTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsTransformer</span></code></a> utilizamos la definición que incluye cada punto de entrenamiento como su propio vecino en la cuenta de <code class="docutils literal notranslate"><span class="pre">n_neighbors</span></code>. Sin embargo, por razones de compatibilidad con otros estimadores que utilizan la otra definición, se calculará un vecino extra cuando <code class="docutils literal notranslate"><span class="pre">mode</span> <span class="pre">==</span> <span class="pre">'distance'</span></code>. Para maximizar la compatibilidad con todos los estimadores, una opción segura es incluir siempre un vecino extra en un estimador personalizado de vecinos más cercanos, ya que los vecinos innecesarios serán filtrados por los siguientes estimadores.</p>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/neighbors/approximate_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-approximate-nearest-neighbors-py"><span class="std std-ref">Vecinos más cercanos aproximados en TSNE</span></a>: un ejemplo de pipelining <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer" title="sklearn.neighbors.KNeighborsTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsTransformer</span></code></a> y <a class="reference internal" href="generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE" title="sklearn.manifold.TSNE"><code class="xref py py-class docutils literal notranslate"><span class="pre">TSNE</span></code></a>. También propone dos estimadores personalizados de vecinos más cercanos basados en paquetes externos.</p></li>
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_caching_nearest_neighbors.html#sphx-glr-auto-examples-neighbors-plot-caching-nearest-neighbors-py"><span class="std std-ref">Almacenamiento en caché de los vecinos más cercanos</span></a>: un ejemplo de pipelining <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsTransformer.html#sklearn.neighbors.KNeighborsTransformer" title="sklearn.neighbors.KNeighborsTransformer"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsTransformer</span></code></a> y <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> para permitir el almacenamiento en caché del grafo de vecinos durante una búsqueda en cuadrícula de hiperparámetros.</p></li>
</ul>
</div>
</section>
<section id="neighborhood-components-analysis">
<span id="nca"></span><h2><span class="section-number">1.6.7. </span>Análisis de componentes de Neighborhood<a class="headerlink" href="#neighborhood-components-analysis" title="Enlazar permanentemente con este título">¶</a></h2>
<p>Análisis de componentes de Neighborhood (NCA, <a class="reference internal" href="generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis" title="sklearn.neighbors.NeighborhoodComponentsAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeighborhoodComponentsAnalysis</span></code></a>) es un algoritmo de aprendizaje de la métrica de la distancia cuyo objetivo es mejorar la exactitud de la clasificación de los vecinos más cercanos en comparación con la distancia euclidiana estándar. El algoritmo maximiza directamente una variante estocástica de la puntuación de los vecinos más cercanos (KNN) en el conjunto de entrenamiento. También puede aprender una proyección lineal de baja dimensión de los datos que puede utilizarse para la visualización de datos y la clasificación rápida.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_nca_illustration.html"><img alt="nca_illustration_1" src="../_images/sphx_glr_plot_nca_illustration_001.png" style="width: 320.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_nca_illustration.html"><img alt="nca_illustration_2" src="../_images/sphx_glr_plot_nca_illustration_002.png" style="width: 320.0px; height: 240.0px;" /></a></strong></p><p>En la figura ilustrada arriba, consideramos algunos puntos de un conjunto de datos generados aleatoriamente. Nos centramos en la clasificación KNN estocástica del punto nº 3. El grosor de un enlace entre la muestra 3 y otro punto es proporcional a su distancia, y puede verse como el peso relativo (o probabilidad) que una regla de predicción estocástica del vecino más cercano asignaría a este punto. En el espacio original, la muestra 3 tiene muchos vecinos estocásticos de varias clases, por lo que la clase correcta no es muy probable. Sin embargo, en el espacio proyectado aprendido por NCA, los únicos vecinos estocásticos con peso no despreciable son de la misma clase que la muestra 3, lo que garantiza que esta última estará bien clasificada. Consulta la <a class="reference internal" href="#nca-mathematical-formulation"><span class="std std-ref">formulación matemática</span></a> para obtener más detalles.</p>
<section id="id4">
<h3><span class="section-number">1.6.7.1. </span>Clasificación<a class="headerlink" href="#id4" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Combinado con un clasificador de vecinos más cercanos (<a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a>), el NCA es atractivo para la clasificación porque puede manejar naturalmente problemas de multiclases sin ningún aumento en el tamaño del modelo, y no introduce parámetros adicionales que requieran un ajuste fino por parte del usuario.</p>
<p>Se ha demostrado que la clasificación NCA funciona bien en la práctica para conjuntos de datos de tamaño y dificultad variables. A diferencia de otros métodos relacionados, como el análisis discriminante lineal, el NCA no hace ninguna suposición sobre las distribuciones de las clases. La clasificación del vecino más cercano puede producir naturalmente límites de decisión muy irregulares.</p>
<p>Para utilizar este modelo para la clasificación, es necesario combinar una instancia <a class="reference internal" href="generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis" title="sklearn.neighbors.NeighborhoodComponentsAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeighborhoodComponentsAnalysis</span></code></a> que aprende la transformación óptima con una instancia <a class="reference internal" href="generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier" title="sklearn.neighbors.KNeighborsClassifier"><code class="xref py py-class docutils literal notranslate"><span class="pre">KNeighborsClassifier</span></code></a> que realiza la clasificación en el espacio proyectado. Este es un ejemplo que utiliza las dos clases:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="p">(</span><span class="n">NeighborhoodComponentsAnalysis</span><span class="p">,</span>
<span class="gp">... </span><span class="n">KNeighborsClassifier</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">load_iris</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">load_iris</span><span class="p">(</span><span class="n">return_X_y</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span>
<span class="gp">... </span><span class="n">stratify</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nca</span> <span class="o">=</span> <span class="n">NeighborhoodComponentsAnalysis</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">knn</span> <span class="o">=</span> <span class="n">KNeighborsClassifier</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nca_pipe</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;nca&#39;</span><span class="p">,</span> <span class="n">nca</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;knn&#39;</span><span class="p">,</span> <span class="n">knn</span><span class="p">)])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">nca_pipe</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="go">Pipeline(...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">nca_pipe</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="go">0.96190476...</span>
</pre></div>
</div>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_nca_classification.html"><img alt="nca_classification_1" src="../_images/sphx_glr_plot_nca_classification_001.png" style="width: 320.0px; height: 240.0px;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_nca_classification.html"><img alt="nca_classification_2" src="../_images/sphx_glr_plot_nca_classification_002.png" style="width: 320.0px; height: 240.0px;" /></a></strong></p><p>El gráfico muestra los límites de decisión para la clasificación de vecino más cercano y la clasificación de análisis de componentes de vecindad en el conjunto de datos iris, cuando se entrena y puntúa sólo con dos características, a efectos de visualización.</p>
</section>
<section id="dimensionality-reduction">
<span id="nca-dim-reduction"></span><h3><span class="section-number">1.6.7.2. </span>Reducción de Dimensionalidad<a class="headerlink" href="#dimensionality-reduction" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El NCA puede utilizarse para realizar una reducción de la dimensionalidad supervisada. Los datos de entrada se proyectan en un subespacio lineal formado por las direcciones que minimizan el objetivo del NCA. La dimensionalidad deseada puede establecerse mediante el parámetro <code class="docutils literal notranslate"><span class="pre">n_components</span></code>. Por ejemplo, la siguiente figura muestra una comparación de la reducción de la dimensionalidad con el Análisis de Componentes Principales (<a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>), el Análisis Discriminante Lineal (<code class="xref py py-class docutils literal notranslate"> <span class="pre">LinearDiscriminantAnalysis</span></code>) y Neighborhood Component Analysis (<a class="reference internal" href="generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis" title="sklearn.neighbors.NeighborhoodComponentsAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">NeighborhoodComponentsAnalysis</span></code></a>) en el conjunto de datos Digits, un conjunto de datos con tamaño <span class="math notranslate nohighlight">\(n_{samples} = 1797\)</span> y <span class="math notranslate nohighlight">\(n_{features} = 64\)</span>. El conjunto de datos se divide en un conjunto de entrenamiento y otro de prueba de igual tamaño, y luego se estandariza. Para la evaluación, se calcula la exactitud de la clasificación de 3 vecinos más cercanos en los puntos proyectados bidimensionales encontrados por cada método. Cada muestra de datos pertenece a una de las 10 clases.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/neighbors/plot_nca_dim_reduction.html"><img alt="nca_dim_reduction_1" src="../_images/sphx_glr_plot_nca_dim_reduction_001.png" style="width: 32%;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_nca_dim_reduction.html"><img alt="nca_dim_reduction_2" src="../_images/sphx_glr_plot_nca_dim_reduction_002.png" style="width: 32%;" /></a> <a class="reference external" href="../auto_examples/neighbors/plot_nca_dim_reduction.html"><img alt="nca_dim_reduction_3" src="../_images/sphx_glr_plot_nca_dim_reduction_003.png" style="width: 32%;" /></a></strong></p><div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_nca_classification.html#sphx-glr-auto-examples-neighbors-plot-nca-classification-py"><span class="std std-ref">Comparación de Vecinos más Cercanos con y sin Análisis de Componentes de Vecindario</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/neighbors/plot_nca_dim_reduction.html#sphx-glr-auto-examples-neighbors-plot-nca-dim-reduction-py"><span class="std std-ref">Reducción de dimensionalidad con Análisis de Componentes de Vecindario (Neighborhood Components Analysis)</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py"><span class="std std-ref">Aprendizaje múltiple sobre dígitos manuscritos: Incrustación local lineal, Isomap…</span></a></p></li>
</ul>
</div>
</section>
<section id="mathematical-formulation">
<span id="nca-mathematical-formulation"></span><h3><span class="section-number">1.6.7.3. </span>Formulación matemática<a class="headerlink" href="#mathematical-formulation" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El objetivo del NCA es aprender una matriz de transformación lineal óptima de tamaño <code class="docutils literal notranslate"><span class="pre">(n_components,</span> <span class="pre">n_features)</span></code>, que maximice la suma sobre todas las muestras <span class="math notranslate nohighlight">\(i\)</span> de la probabilidad <span class="math notranslate nohighlight">\(p_i\)</span> de que <span class="math notranslate nohighlight">\(i\)</span> se clasifique correctamente, es decir:</p>
<div class="math notranslate nohighlight">
\[\underset{L}{\arg\max} \sum\limits_{i=0}^{N - 1} p_{i}\]</div>
<p>con <span class="math notranslate nohighlight">\(N\)</span> = <code class="docutils literal notranslate"><span class="pre">n_samples</span></code> y <span class="math notranslate nohighlight">\(p_i\)</span> la probabilidad de que la muestra <span class="math notranslate nohighlight">\(i\)</span> se clasifique correctamente según una regla estocástica de vecinos más cercanos en el espacio incrustado aprendido:</p>
<div class="math notranslate nohighlight">
\[p_{i}=\sum\limits_{j \in C_i}{p_{i j}}\]</div>
<p>donde <span class="math notranslate nohighlight">\(C_i\)</span> es el conjunto de puntos en la misma clase que la muestra <span class="math notranslate nohighlight">\(i\)</span>, y <span class="math notranslate nohighlight">\(p_{i j}\)</span> es el máximo softmax sobre distancias euclidianas en el espacio incrustado:</p>
<div class="math notranslate nohighlight">
\[p_{i j} = \frac{\exp(-||L x_i - L x_j||^2)}{\sum\limits_{k \ne
          i} {\exp{-(||L x_i - L x_k||^2)}}} , \quad p_{i i} = 0\]</div>
<section id="mahalanobis-distance">
<h4><span class="section-number">1.6.7.3.1. </span>Distancia de Mahalanobis<a class="headerlink" href="#mahalanobis-distance" title="Enlazar permanentemente con este título">¶</a></h4>
<p>El NCA puede verse como el aprendizaje de una métrica de distancia de Mahalanobis (al cuadrado):</p>
<div class="math notranslate nohighlight">
\[|| L(x_i - x_j)||^2 = (x_i - x_j)^TM(x_i - x_j),\]</div>
<p>donde <span class="math notranslate nohighlight">\(M = L^T L\)</span> es una matriz simétrica semidefinida positiva de tamaño <code class="docutils literal notranslate"><span class="pre">(n_features,</span> <span class="pre">n_features)</span></code>.</p>
</section>
</section>
<section id="implementation">
<h3><span class="section-number">1.6.7.4. </span>Implementación<a class="headerlink" href="#implementation" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Esta implementación sigue lo explicado en el artículo original <a class="footnote-reference brackets" href="#id6" id="id5">1</a>. Para el método de optimización, actualmente utiliza el L-BFGS-B de scipy con un cálculo de gradiente completo en cada iteración, para evitar afinar la tasa de aprendizaje y proporcionar un aprendizaje estable.</p>
<p>Puedes ver los ejemplos a continuación y la cadena de caracteres de <a class="reference internal" href="generated/sklearn.neighbors.NeighborhoodComponentsAnalysis.html#sklearn.neighbors.NeighborhoodComponentsAnalysis.fit" title="sklearn.neighbors.NeighborhoodComponentsAnalysis.fit"><code class="xref py py-meth docutils literal notranslate"><span class="pre">NeighborhoodComponentsAnalysis.fit</span></code></a> para más información.</p>
</section>
<section id="complexity">
<h3><span class="section-number">1.6.7.5. </span>Complejidad<a class="headerlink" href="#complexity" title="Enlazar permanentemente con este título">¶</a></h3>
<section id="training">
<h4><span class="section-number">1.6.7.5.1. </span>Formación<a class="headerlink" href="#training" title="Enlazar permanentemente con este título">¶</a></h4>
<p>El NCA almacena una matriz de distancias entre pares, ocupando <code class="docutils literal notranslate"><span class="pre">n_samples</span> <span class="pre">**</span> <span class="pre">2</span></code> de memoria. La complejidad del tiempo depende del número de iteraciones realizadas por el algoritmo de optimización. Sin embargo, se puede establecer el número máximo de iteraciones con el argumento <code class="docutils literal notranslate"><span class="pre">max_iter</span></code>. Para cada iteración, la complejidad temporal es <code class="docutils literal notranslate"><span class="pre">O(n_components</span> <span class="pre">x</span> <span class="pre">n_samples</span> <span class="pre">x</span> <span class="pre">min(n_samples,</span> <span class="pre">n_features))</span></code>.</p>
</section>
<section id="transform">
<h4><span class="section-number">1.6.7.5.2. </span>Transformación<a class="headerlink" href="#transform" title="Enlazar permanentemente con este título">¶</a></h4>
<p>Aquí la operación <code class="docutils literal notranslate"><span class="pre">transform</span></code> devuelve <span class="math notranslate nohighlight">\(LX^T\)</span>, por lo tanto su complejidad de tiempo es igual a <code class="docutils literal notranslate"><span class="pre">n_components</span> <span class="pre">*</span> <span class="pre">n_features</span> <span class="pre">*</span> <span class="pre">n_samples_test</span></code>. No hay complejidad de espacio añadida en la operación.</p>
<div class="topic">
<p class="topic-title">Referencias:</p>
<dl class="footnote brackets">
<dt class="label" id="id6"><span class="brackets"><a class="fn-backref" href="#id5">1</a></span></dt>
<dd><p><a class="reference external" href="http://www.cs.nyu.edu/~roweis/papers/ncanips.pdf">«Neighbourhood Components Analysis»</a>, J. Goldberger, S. Roweis, G. Hinton, R. Salakhutdinov, Advances in Neural Information Processing Systems, Vol. 17, mayo 2005, pp. 513-520.</p>
</dd>
</dl>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Neighbourhood_components_analysis">Entrada de Wikipedia en Neighborhood Components Analysis</a></p>
</div>
</section>
</section>
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../_sources/modules/neighbors.rst.txt" rel="nofollow">Mostrar la fuente de esta página</a>
      </footer>
    </div>
  </div>
</div>
<script src="../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>