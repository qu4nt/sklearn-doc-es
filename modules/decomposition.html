

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  
  <title>2.5. Descomposición de señales en componentes (problemas de factorización de matrices) &mdash; documentación de scikit-learn - 0.24.2</title>
  
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/decomposition.html" />

  
  <link rel="shortcut icon" href="../_static/favicon.ico"/>
  

  <link rel="stylesheet" href="../_static/css/vendor/bootstrap.min.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-rendered-html.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
<script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
<script src="../_static/jquery.js"></script> 
</head>
<body>
<nav id="navbar" class="sk-docs-navbar navbar navbar-expand-md navbar-light bg-light py-0">
  <div class="container-fluid sk-docs-container px-0">
      <a class="navbar-brand py-0" href="../index.html">
        <img
          class="sk-brand-img"
          src="../_static/scikit-learn-logo-small.png"
          alt="logo"/>
      </a>
    <button
      id="sk-navbar-toggler"
      class="navbar-toggler"
      type="button"
      data-toggle="collapse"
      data-target="#navbarSupportedContent"
      aria-controls="navbarSupportedContent"
      aria-expanded="false"
      aria-label="Toggle navigation"
    >
      <span class="navbar-toggler-icon"></span>
    </button>

    <div class="sk-navbar-collapse collapse navbar-collapse" id="navbarSupportedContent">
      <ul class="navbar-nav mr-auto">
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../install.html">Instalación</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../user_guide.html">Manual de Usuario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="classes.html">API</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link" href="../auto_examples/index.html">Ejemplos</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../getting_started.html">¿Cómo empezar?</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../tutorial/index.html">Tutorial</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../whats_new/v0.24.html">Novedades</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../glossary.html">Glosario</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../developers/index.html">Desarrollo</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../faq.html">FAQ</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../support.html">Soporte</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../related_projects.html">Paquetes relacionados</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../roadmap.html">Hoja de ruta</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="../about.html">Sobre nosotros</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
        </li>
        <li class="nav-item">
          <a class="sk-nav-link nav-link nav-more-item-mobile-items" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
        </li>
        <li class="nav-item dropdown nav-more-item-dropdown">
          <a class="sk-nav-link nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">Más</a>
          <div class="dropdown-menu" aria-labelledby="navbarDropdown">
              <a class="sk-nav-dropdown-item dropdown-item" href="../getting_started.html">¿Cómo empezar?</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../tutorial/index.html">Tutorial</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../whats_new/v0.24.html">Novedades</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../glossary.html">Glosario</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../developers/index.html">Desarrollo</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../faq.html">FAQ</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../support.html">Soporte</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../related_projects.html">Paquetes relacionados</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../roadmap.html">Hoja de ruta</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="../about.html">Sobre nosotros</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://github.com/scikit-learn/scikit-learn">GitHub</a>
              <a class="sk-nav-dropdown-item dropdown-item" href="https://scikit-learn.org/dev/versions.html">Otras versiones y descargas</a>
          </div>
        </li>
      </ul>
      <div id="searchbox" role="search">
          <div class="searchformwrapper">
          <form class="search" action="../search.html" method="get">
            <input class="sk-search-text-input" type="text" name="q" aria-labelledby="searchlabel" />
            <input class="sk-search-text-btn" type="submit" value="Ir a" />
          </form>
          </div>
      </div>
    </div>
  </div>
</nav>
<div class="d-flex" id="sk-doc-wrapper">
    <input type="checkbox" name="sk-toggle-checkbox" id="sk-toggle-checkbox">
    <label id="sk-sidemenu-toggle" class="sk-btn-toggle-toc btn sk-btn-primary" for="sk-toggle-checkbox">Alternar menú</label>
    <div id="sk-sidebar-wrapper" class="border-right">
      <div class="sk-sidebar-toc-wrapper">
        <div class="sk-sidebar-toc-logo">
          <a href="../index.html">
            <img
              class="sk-brand-img"
              src="../_static/scikit-learn-logo-small.png"
              alt="logo"/>
          </a>
        </div>
        <div class="btn-group w-100 mb-2" role="group" aria-label="rellinks">
            <a href="biclustering.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2.4. Biclustering">Prev</a><a href="../unsupervised_learning.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2. Aprendizaje no supervisado">Arriba</a>
            <a href="covariance.html" role="button" class="btn sk-btn-rellink py-1" sk-rellink-tooltip="2.6. Estimación de covarianza">Sig.</a>
        </div>
        <div class="alert alert-danger p-1 mb-2" role="alert">
          <p class="text-center mb-0">
          <strong>scikit-learn 0.24.2</strong><br/>
          <a href="http://scikit-learn.org/dev/versions.html">Otras versiones</a>
          </p>
        </div>
        <div class="alert alert-warning p-1 mb-2" role="alert">
          <p class="text-center mb-0">
            Por favor <a class="font-weight-bold" href="../about.html#citing-scikit-learn"><string>cítanos</string></a> si usas el software.
          </p>
        </div>
            <div class="sk-sidebar-toc">
              <ul>
<li><a class="reference internal" href="#">2.5. Descomposición de señales en componentes (problemas de factorización de matrices)</a><ul>
<li><a class="reference internal" href="#principal-component-analysis-pca">2.5.1. Análisis de componentes principales (PCA)</a><ul>
<li><a class="reference internal" href="#exact-pca-and-probabilistic-interpretation">2.5.1.1. PCA exacto e interpretación probabilista</a></li>
<li><a class="reference internal" href="#incremental-pca">2.5.1.2. PCA Incremental</a></li>
<li><a class="reference internal" href="#pca-using-randomized-svd">2.5.1.3. PCA usando SVD aleatorio</a></li>
<li><a class="reference internal" href="#kernel-pca">2.5.1.4. Kernel PCA</a></li>
<li><a class="reference internal" href="#sparse-principal-components-analysis-sparsepca-and-minibatchsparsepca">2.5.1.5. Análisis de componentes principales dispersos (SparsePCA y MiniBatchSparsePCA)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#truncated-singular-value-decomposition-and-latent-semantic-analysis">2.5.2. Descomposición de valor singular truncado y análisis semántico latente</a></li>
<li><a class="reference internal" href="#dictionary-learning">2.5.3. Diccionario de aprrendizaje</a><ul>
<li><a class="reference internal" href="#sparse-coding-with-a-precomputed-dictionary">2.5.3.1. Codificación dispersa con un diccionario precalculado</a></li>
<li><a class="reference internal" href="#generic-dictionary-learning">2.5.3.2. Diccionario genérico de aprendizaje</a></li>
<li><a class="reference internal" href="#mini-batch-dictionary-learning">2.5.3.3. Diccionario de aprendizaje por mini lotes</a></li>
</ul>
</li>
<li><a class="reference internal" href="#factor-analysis">2.5.4. Análisis de factores</a></li>
<li><a class="reference internal" href="#independent-component-analysis-ica">2.5.5. Análisis de componentes independientes (ICA)</a></li>
<li><a class="reference internal" href="#non-negative-matrix-factorization-nmf-or-nnmf">2.5.6. Factorización matricial no negativa (NMF o NNMF)</a><ul>
<li><a class="reference internal" href="#nmf-with-the-frobenius-norm">2.5.6.1. NMF con la norma de Frobenius</a></li>
<li><a class="reference internal" href="#nmf-with-a-beta-divergence">2.5.6.2. NMF con una divergencia beta</a></li>
</ul>
</li>
<li><a class="reference internal" href="#latent-dirichlet-allocation-lda">2.5.7. Asignación Latente de Dirichlet (LDA)</a></li>
</ul>
</li>
</ul>

            </div>
      </div>
    </div>
    <div id="sk-page-content-wrapper">
      <div class="sk-page-content container-fluid body px-md-3" role="main">
        
  <section id="decomposing-signals-in-components-matrix-factorization-problems">
<span id="decompositions"></span><h1><span class="section-number">2.5. </span>Descomposición de señales en componentes (problemas de factorización de matrices)<a class="headerlink" href="#decomposing-signals-in-components-matrix-factorization-problems" title="Enlazar permanentemente con este título">¶</a></h1>
<section id="principal-component-analysis-pca">
<span id="pca"></span><h2><span class="section-number">2.5.1. </span>Análisis de componentes principales (PCA)<a class="headerlink" href="#principal-component-analysis-pca" title="Enlazar permanentemente con este título">¶</a></h2>
<section id="exact-pca-and-probabilistic-interpretation">
<h3><span class="section-number">2.5.1.1. </span>PCA exacto e interpretación probabilista<a class="headerlink" href="#exact-pca-and-probabilistic-interpretation" title="Enlazar permanentemente con este título">¶</a></h3>
<p>PCA se utiliza para descomponer un conjunto de datos multivariantes en un conjunto de componentes ortogonales sucesivos que explican una cantidad máxima de la varianza. En scikit-learn, <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> es implementado como un objeto <em>transformador</em> que aprende componentes :math: <code class="docutils literal notranslate"><span class="pre">n</span></code> en su método <code class="docutils literal notranslate"><span class="pre">fit</span></code>, y se puede utilizar en nuevos datos para proyectarlos en estos componentes.</p>
<p>PCA centra pero no escala los datos de entrada para cada característica antes de aplicar la SVD. El parámetro opcional <code class="docutils literal notranslate"><span class="pre">whiten=True</span></code> permite proyectar los datos en el espacio singular mientras se escala cada componente a la varianza unitaria. Esto suele ser útil si los modelos subsiguientes hacen suposiciones firmes sobre la isotropía de la señal: este es el caso, por ejemplo, de las Máquinas de Vectores de Apoyo con el núcleo RBF y el algoritmo de agrupación K-Medias.</p>
<p>A continuación se muestra un ejemplo del conjunto de datos del iris, que se compone de 4 características, proyectadas en las 2 dimensiones que explican la mayor parte de la varianza:</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_lda.html"><img alt="../_images/sphx_glr_plot_pca_vs_lda_001.png" src="../_images/sphx_glr_plot_pca_vs_lda_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>El objeto <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> también proporciona una interpretación probabilística del PCA que puede dar una verosimilitud de los datos basada en la cantidad de varianza que explica. Como tal, implementa un método <a class="reference internal" href="../glossary.html#term-score"><span class="xref std std-term">score</span></a> que puede utilizarse en la validación cruzada:</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="../_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png" src="../_images/sphx_glr_plot_pca_vs_fa_model_selection_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_lda.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-lda-py"><span class="std std-ref">Comparación de la proyección arreglo 2D de LDA y PCA del conjunto de datos de Iris</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Selección de modelos con el ACP probabilístico y el análisis factorial (AF)</span></a></p></li>
</ul>
</div>
</section>
<section id="incremental-pca">
<span id="incrementalpca"></span><h3><span class="section-number">2.5.1.2. </span>PCA Incremental<a class="headerlink" href="#incremental-pca" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El objeto <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> es muy útil, pero tiene ciertas limitaciones para conjuntos de datos grandes. La mayor limitación es que <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> sólo admite el procesamiento por lotes, lo que significa que todos los datos a procesar deben caber en la memoria principal. El objeto <a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> utiliza una forma diferente de procesamiento y permite realizar cálculos parciales que coinciden casi exactamente con los resultados de <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> mientras se procesan los datos de en mini lotes. <a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> permite implementar el análisis de componentes principales fuera del núcleo:</p>
<blockquote>
<div><ul class="simple">
<li><p>Utilizando su método <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> en fragmentos de datos obtenidos secuencialmente desde el disco duro local o de una base de datos en la red.</p></li>
<li><p>Llamando a su método de ajuste en una matriz dispersa o un archivo mapeado en memoria usando <code class="docutils literal notranslate"><span class="pre">numpy.memmap</span></code>.</p></li>
</ul>
</div></blockquote>
<p><a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> sólo almacena las estimaciones de las varianzas de los componentes y del ruido, con el fin de actualizar la <code class="docutils literal notranslate"><span class="pre">explained_variance_ratio_</span></code> de forma incremental. Por ello, el uso de la memoria depende del número de muestras por lote, y no del número de muestras a procesar en el conjunto de datos.</p>
<p>Como en el <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>, el <a class="reference internal" href="generated/sklearn.decomposition.IncrementalPCA.html#sklearn.decomposition.IncrementalPCA" title="sklearn.decomposition.IncrementalPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">IncrementalPCA</span></code></a> centra pero no escala los datos de entrada para cada característica antes de aplicar la SVD.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="../_images/sphx_glr_plot_incremental_pca_001.png" src="../_images/sphx_glr_plot_incremental_pca_001.png" style="width: 600.0px; height: 600.0px;" /></a>
</figure>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_incremental_pca.html"><img alt="../_images/sphx_glr_plot_incremental_pca_002.png" src="../_images/sphx_glr_plot_incremental_pca_002.png" style="width: 600.0px; height: 600.0px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_incremental_pca.html#sphx-glr-auto-examples-decomposition-plot-incremental-pca-py"><span class="std std-ref">PCA Incremental</span></a></p></li>
</ul>
</div>
</section>
<section id="pca-using-randomized-svd">
<span id="randomizedpca"></span><h3><span class="section-number">2.5.1.3. </span>PCA usando SVD aleatorio<a class="headerlink" href="#pca-using-randomized-svd" title="Enlazar permanentemente con este título">¶</a></h3>
<p>A menudo es interesante proyectar los datos a un espacio de menor dimensión que conserve la mayor parte de la varianza, dejando de lado el vector singular de los componentes asociados a los valores singulares más bajos.</p>
<p>Por ejemplo, si trabajamos con imágenes de 64x64 píxeles de escala de grises para el reconocimiento facial, la dimensionalidad de los datos es de 4096 y se hace lento entrenar una máquina de vectores de soporte RBF en datos tan amplios. Además, sabemos que la dimensionalidad intrínseca de los datos es mucho menor que 4096, ya que todas las imágenes de rostros humanos se parecen en cierto modo. Las muestras se sitúan en una matriz de dimensión mucho menor (por ejemplo, alrededor de 200). El algoritmo PCA puede utilizarse para transformar linealmente los datos y, al mismo tiempo, reducir la dimensionalidad y conservar la mayor parte de la varianza explicada.</p>
<p>La clase <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> utilizada con el parámetro opcional <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> es muy útil en ese caso: como vamos a descartar la mayoría de los vectores singulares, es mucho más eficiente limitar el cálculo a una estimación aproximada de los vectores singulares que mantendremos para realizar realmente la transformación.</p>
<p>Por ejemplo, a continuación se presentan 16 retratos de muestra (centrados en 0,0) del conjunto de datos Olivetti. En el lado derecho están los primeros 16 vectores singulares transformados en retratos. Dado que sólo necesitamos los 16 primeros vectores singulares de un conjunto de datos con tamaño <span class="math notranslate nohighlight">\(n_{samples} = 400\)</span> y <span class="math notranslate nohighlight">\(n_{features} = 64 \times 64 = 4096\)</span>, el tiempo de cálculo es inferior a 1s:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="orig_img" src="../_images/sphx_glr_plot_faces_decomposition_001.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img" src="../_images/sphx_glr_plot_faces_decomposition_002.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>Si observamos <span class="math notranslate nohighlight">\(n_{\max} = \max(n_{\mathrm{samples}}, n_{\mathrm{features}})\)</span> y <span class="math notranslate nohighlight">\(n_{\min} = \min(n_{\mathrm{samples}}, n_{\mathrm{features}})\)</span>, la complejidad temporal del <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> aleatorizado es <span class="math notranslate nohighlight">\(O(n_{\max}^2 \cdot n_{\mathrm{components}})\)</span> en lugar de <span class="math notranslate nohighlight">\(O(n_{\max}^2 \cdot n_{\min})\)</span> para el método exacto implementado en <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>.</p>
<p>La huella de memoria del <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> aleatorio también es proporcional a <span class="math notranslate nohighlight">\(2 \cdot n_{\max} en lugar de \cdot n_{\mathrm{components}} en lugar de :math:\)</span> para el método exacto.</p>
<p>Nota: la implementación de <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code> en <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> con <code class="docutils literal notranslate"><span class="pre">svd_solver='randomized'</span></code> no es la transformada inversa exacta de <code class="docutils literal notranslate"><span class="pre">transform</span></code> incluso cuando <code class="docutils literal notranslate"><span class="pre">whiten=False</span></code> (por defecto).</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/applications/plot_face_recognition.html#sphx-glr-auto-examples-applications-plot-face-recognition-py"><span class="std std-ref">Ejemplo de reconocimiento de rostros mediante eigenfaces y SVM</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Descomposición de conjuntos de datos de caras</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/0909.4061">«Finding structure with randomness: Stochastic algorithms for
constructing approximate matrix decompositions»</a>
Halko, et al., 2009</p></li>
</ul>
</div>
</section>
<section id="kernel-pca">
<span id="id1"></span><h3><span class="section-number">2.5.1.4. </span>Kernel PCA<a class="headerlink" href="#kernel-pca" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelPCA</span></code></a> es una extensión de PCA que obtiene una reducción de la dimensionalidad no lineal mediante el uso de kernels (ver <a class="reference internal" href="metrics.html#metrics"><span class="std std-ref">Métricas por pares, afinidades y núcleos</span></a>). Tiene muchas aplicaciones, incluyendo la eliminación de ruido, la compresión y la predicción estructurada (estimación de la dependencia del núcleo). <a class="reference internal" href="generated/sklearn.decomposition.KernelPCA.html#sklearn.decomposition.KernelPCA" title="sklearn.decomposition.KernelPCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">KernelPCA</span></code></a> soporta tanto la <code class="docutils literal notranslate"><span class="pre">transform</span></code> como la <code class="docutils literal notranslate"><span class="pre">inverse_transform</span></code>.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_kernel_pca.html"><img alt="../_images/sphx_glr_plot_kernel_pca_001.png" src="../_images/sphx_glr_plot_kernel_pca_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_kernel_pca.html#sphx-glr-auto-examples-decomposition-plot-kernel-pca-py"><span class="std std-ref">Núcleo PCA</span></a></p></li>
</ul>
</div>
</section>
<section id="sparse-principal-components-analysis-sparsepca-and-minibatchsparsepca">
<span id="sparsepca"></span><h3><span class="section-number">2.5.1.5. </span>Análisis de componentes principales dispersos (SparsePCA y MiniBatchSparsePCA)<a class="headerlink" href="#sparse-principal-components-analysis-sparsepca-and-minibatchsparsepca" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparsePCA</span></code></a> es una variante del PCA, cuyo objetivo es extraer el conjunto de componentes dispersos que mejor reconstruyen los datos.</p>
<p>El PCA disperso en mini lotes (<a class="reference internal" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchSparsePCA</span></code></a>) es una variante de <a class="reference internal" href="generated/sklearn.decomposition.SparsePCA.html#sklearn.decomposition.SparsePCA" title="sklearn.decomposition.SparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparsePCA</span></code></a> que es más rápida pero menos precisa. El aumento de la velocidad se consigue iterando sobre pequeñas porciones del conjunto de características, para un número determinado de iteraciones.</p>
<p>El análisis de componentes principales (<a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>) tiene el inconveniente de que los componentes extraídos por este método tienen expresiones exclusivamente densas, es decir, tienen coeficientes distintos de cero cuando se expresan como combinaciones lineales de las variables originales. Esto puede dificultar su interpretación. En muchos casos, los componentes reales subyacentes pueden pensarse más naturalmente como vectores dispersos; por ejemplo, en el reconocimiento de rostros, los componentes podrían asignarse naturalmente a partes de rostros.</p>
<p>Los componentes principales dispersos producen una representación más sencilla e interpretable, destacando claramente cuáles son las características originales que contribuyen a las diferencias entre las muestras.</p>
<p>El siguiente ejemplo ilustra 16 componentes extraídos mediante PCA disperso del conjunto de datos de rostros Olivetti.  Se puede observar cómo el término de regularización induce muchos ceros. Además, la estructura natural de los datos hace que los coeficientes no nulos sean verticalmente adyacentes.El modelo no impone esto matemáticamente: cada componente es un vector <span class="math notranslate nohighlight">\(h \in \mathbf{R}^{4096}\)</span>, y no hay ninguna noción de adyacencia vertical excepto durante la visualización amigable para el ser humano como imágenes de 64x64 píxeles. El hecho de que los componentes mostrados a continuación aparezcan locales es el efecto de la estructura inherente de los datos, que hace que tales patrones locales minimicen el error de reconstrucción.  Existen normas que inducen la dispersión que tienen en cuenta la adyacencia y diferentes tipos de estructura; véase <a class="reference internal" href="#jen09" id="id2"><span>[Jen09]</span></a> para una revisión de tales métodos. Para más detalles sobre el uso de Sparse PCA, véase la sección Ejemplos, más abajo.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img" src="../_images/sphx_glr_plot_faces_decomposition_002.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="spca_img" src="../_images/sphx_glr_plot_faces_decomposition_005.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>Tenga en cuenta que hay muchas formulaciones diferentes para el problema de Sparse PCA. La implementada aquí se basa en <a class="reference internal" href="#mrl09" id="id3"><span>[Mrl09]</span></a> . El problema de optimización que se resuelve es un problema PCA (diccionario de aprendizaje) con una penalidad <span class="math notranslate nohighlight">\(\ell_1\)</span> sobre los componentes:</p>
<div class="math notranslate nohighlight">
\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\, } &amp; \frac{1}{2}
             |||X-UV||_2^2+\alpha||V||_1 \\
             \text{subject to } &amp; ||U_k||_2 = 1 \text{ for all }
             0 \leq k &lt; n_{components}\end{split}\]</div>
<p>La norma <span class="math notranslate nohighlight">\(\ell_1\)</span>, que induce la dispersión, también impide el aprendizaje de componentes a partir del ruido cuando se dispone de pocas muestras. El grado de penalidad (y, por tanto, la dispersión) puede ajustarse mediante el hiperparámetro <code class="docutils literal notranslate"><span class="pre">alpha</span></code>. Los valores pequeños conducen a una factorización ligeramente regularizada, mientras que los valores más grandes reducen muchos coeficientes a cero.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Aunque se trata de un algoritmo online, la clase <a class="reference internal" href="generated/sklearn.decomposition.MiniBatchSparsePCA.html#sklearn.decomposition.MiniBatchSparsePCA" title="sklearn.decomposition.MiniBatchSparsePCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchSparsePCA</span></code></a> no implementa <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code> porque el algoritmo está en línea en la dirección de las características, no en la dirección de las muestras.</p>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Descomposición de conjuntos de datos de caras</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<dl class="citation">
<dt class="label" id="mrl09"><span class="brackets"><a class="fn-backref" href="#id3">Mrl09</a></span></dt>
<dd><p><a class="reference external" href="https://www.di.ens.fr/sierra/pdfs/icml09.pdf">«Online Dictionary Learning for Sparse Coding»</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</p>
</dd>
<dt class="label" id="jen09"><span class="brackets"><a class="fn-backref" href="#id2">Jen09</a></span></dt>
<dd><p><a class="reference external" href="https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf">«Structured Sparse Principal Component Analysis»</a>
R. Jenatton, G. Obozinski, F. Bach, 2009</p>
</dd>
</dl>
</div>
</section>
</section>
<section id="truncated-singular-value-decomposition-and-latent-semantic-analysis">
<span id="lsa"></span><h2><span class="section-number">2.5.2. </span>Descomposición de valor singular truncado y análisis semántico latente<a class="headerlink" href="#truncated-singular-value-decomposition-and-latent-semantic-analysis" title="Enlazar permanentemente con este título">¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> implementa una variante de la descomposición del valor singular (SVD) que sólo calcula los valores singulares más grandes de <span class="math notranslate nohighlight">\(k\)</span>, donde <span class="math notranslate nohighlight">\(k\)</span> es un parámetro especificado por el usuario.</p>
<p>Cuando se aplica la SVD truncada a las matrices término-documento (como las devueltas por <a class="reference internal" href="generated/sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">CountVectorizer</span></code></a> o <a class="reference internal" href="generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><code class="xref py py-class docutils literal notranslate"><span class="pre">TfidfVectorizer</span></code></a>), esta transformación se conoce como <a class="reference external" href="https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">análisis semántico latente</a> (LSA), porque transforma dichas matrices a un espacio «semántico» de baja dimensionalidad. En particular, el LSA es conocido por combatir los efectos de la sinonimia y la polisemia (ambos significan, a grandes rasgos, que hay múltiples significados por palabra), que hacen que las matrices término-documento sean demasiado escasas y muestren una pobre similitud bajo medidas como la similitud del coseno.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>El LSA también se conoce como indexación semántica latente, LSI, aunque estrictamente se refiere a su uso en índices persistentes con fines de recuperación de información.</p>
</div>
<p>Matemáticamente, la SVD truncada aplicada a las muestras de entrenamiento <span class="math notranslate nohighlight">\(X\)</span> produce una aproximación de bajo rango <span class="math notranslate nohighlight">\(X\)</span>:</p>
<div class="math notranslate nohighlight">
\[X \approx X_k = U_k \Sigma_k V_k^\top\]</div>
<p>Después de esta operación, <span class="math notranslate nohighlight">\(U_k \Sigma_k^\top\)</span> es el conjunto de entrenamiento transformado con características <span class="math notranslate nohighlight">\(k\)</span> (llamadas <code class="docutils literal notranslate"><span class="pre">n_components</span></code> en la API).</p>
<p>Para transformar también un conjunto de pruebas <span class="math notranslate nohighlight">\(X\)</span>, lo multiplicamos por <span class="math notranslate nohighlight">\(V_k\)</span>:</p>
<div class="math notranslate nohighlight">
\[X' = X V_k\]</div>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>La mayoría de los tratamientos de LSA en el procesamiento de lenguaje natural (PLN) y la literatura de recuperación de información (RI) intercambian los ejes de la matriz <span class="math notranslate nohighlight">\(X\)</span> para que tenga forma <code class="docutils literal notranslate"><span class="pre">n_features</span></code> × <code class="docutils literal notranslate"><span class="pre">n_samples</span></code>. Presentamos la LSA de una manera diferente que se ajusta mejor con la API de la scikit-learn, pero los valores singulares encontrados son los mismos.</p>
</div>
<p><a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> es muy similar a <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>, pero difiere en que la matriz <span class="math notranslate nohighlight">\(X\)</span> no necesita estar centrada. Cuando las medias a nivel de columna (por característica) de <span class="math notranslate nohighlight">\(X\)</span> se restan de los valores de las características, la SVD truncada en la matriz resultante es equivalente a la PCA. En términos prácticos, esto significa que el transformador <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> acepta matrices <code class="docutils literal notranslate"><span class="pre">scipy.sparse</span></code> sin necesidad de densificarlas, ya que la densificación puede llenar la memoria incluso para colecciones de documentos de tamaño medio.</p>
<p>Aunque el transformador <a class="reference internal" href="generated/sklearn.decomposition.TruncatedSVD.html#sklearn.decomposition.TruncatedSVD" title="sklearn.decomposition.TruncatedSVD"><code class="xref py py-class docutils literal notranslate"><span class="pre">TruncatedSVD</span></code></a> funciona con cualquier matriz de características, se recomienda su uso con matrices tf-idf en lugar de con recuentos de frecuencias en bruto en un entorno de procesamiento de LSA/documentos. En particular, el escalado sublineal y la frecuencia inversa del documento deberían estar activados (<code class="docutils literal notranslate"><span class="pre">sublinear_tf=True,</span> <span class="pre">use_idf=True</span></code>) para acercar los valores de las características a una Distribución Gaussiana, compensando las suposiciones erróneas de LSA sobre los datos textuales.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"><span class="std std-ref">Análisis de conglomerados en documentos de texto utilizando k-medias(k-means)</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p>Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008),
<em>Introduction to Information Retrieval</em>, Cambridge University Press,
chapter 18: <a class="reference external" href="https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf">Matrix decompositions &amp; latent semantic indexing</a></p></li>
</ul>
</div>
</section>
<section id="dictionary-learning">
<span id="dictionarylearning"></span><h2><span class="section-number">2.5.3. </span>Diccionario de aprrendizaje<a class="headerlink" href="#dictionary-learning" title="Enlazar permanentemente con este título">¶</a></h2>
<section id="sparse-coding-with-a-precomputed-dictionary">
<span id="sparsecoder"></span><h3><span class="section-number">2.5.3.1. </span>Codificación dispersa con un diccionario precalculado<a class="headerlink" href="#sparse-coding-with-a-precomputed-dictionary" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El objeto <a class="reference internal" href="generated/sklearn.decomposition.SparseCoder.html#sklearn.decomposition.SparseCoder" title="sklearn.decomposition.SparseCoder"><code class="xref py py-class docutils literal notranslate"><span class="pre">SparseCoder</span></code></a> es un estimador que puede utilizarse para transformar las señales en una combinación lineal dispersa de átomos a partir de un diccionario fijo precalculado, como una base wavelet discreta. Por lo tanto, este objeto no implementa un método de <code class="docutils literal notranslate"><span class="pre">fit</span></code>. La transformación equivale a un problema de codificación dispersa: encontrar una representación de los datos como una combinación lineal del menor número posible de átomos del diccionario. Todas las variaciones del diccionario de aprendizaje implementan los siguientes métodos de transformación, controlables a través del parámetro de inicialización <code class="docutils literal notranslate"><span class="pre">transform_method</span></code>:</p>
<ul class="simple">
<li><p>Búsqueda de coincidencias ortogonales (<a class="reference internal" href="linear_model.html#omp"><span class="std std-ref">Búsqueda de coincidencias ortogonales (OMP)</span></a>)</p></li>
<li><p>Regresión de ángulo mínimo (<a class="reference internal" href="linear_model.html#least-angle-regression"><span class="std std-ref">Regresión de ángulo mínimo</span></a>)</p></li>
<li><p>Lasso calculado por regresión de ángulo mínimo</p></li>
<li><p>Lasso usando coordenadas de descenso (<a class="reference internal" href="linear_model.html#lasso"><span class="std std-ref">Lasso</span></a>)</p></li>
<li><p>Fijar umbrales</p></li>
</ul>
<p>La fijación de umbrales es muy rápida pero no produce reconstrucciones precisas. Se ha demostrado su utilidad en la literatura para tareas de clasificación. Para las tareas de reconstrucción de imágenes, la búsqueda de coincidencias ortogonales produce la reconstrucción más precisa y no sesgada.</p>
<p>Los objetos del diccionario de aprendizaje ofrecen, a través del parámetro <code class="docutils literal notranslate"><span class="pre">split_code</span></code>, la posibilidad de separar los valores positivos y negativos en los resultados de la codificación dispersa. Esto es útil cuando el diccionario de aprendizaje se usa para extraer características que se utilizarán para el aprendizaje supervisado, porque permite que el algoritmo de aprendizaje asigne pesos diferentes a las cargas negativas de un átomo en particular, a partir de la carga positiva correspondiente.</p>
<p>El código dividido para una sola muestra tiene una longitud de <code class="docutils literal notranslate"><span class="pre">2</span> <span class="pre">*</span> <span class="pre">n_components</span></code> y se construye utilizando la siguiente regla: Primero, se calcula el código regular de longitud <code class="docutils literal notranslate"><span class="pre">n_components</span></code>. A continuación, las primeras entradas de <code class="docutils literal notranslate"><span class="pre">n_components</span></code> del <code class="docutils literal notranslate"><span class="pre">split_code</span></code> se rellenan con la parte positiva del vector de código regular. La segunda mitad del código dividido se rellena con la parte negativa del vector de códigos, sólo que con signo positivo. Por lo tanto, el split_code es no negativo.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_sparse_coding.html#sphx-glr-auto-examples-decomposition-plot-sparse-coding-py"><span class="std std-ref">Codificación dispersa con un diccionario precalculado</span></a></p></li>
</ul>
</div>
</section>
<section id="generic-dictionary-learning">
<h3><span class="section-number">2.5.3.2. </span>Diccionario genérico de aprendizaje<a class="headerlink" href="#generic-dictionary-learning" title="Enlazar permanentemente con este título">¶</a></h3>
<p>El diccionario de aprendizaje (<a class="reference internal" href="generated/sklearn.decomposition.DictionaryLearning.html#sklearn.decomposition.DictionaryLearning" title="sklearn.decomposition.DictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">DictionaryLearning</span></code></a>) es un problema de factorización de matrices que consiste en encontrar un diccionario (normalmente sobrecompleto) que funciona bien en la codificación dispersa de los datos ajustados.</p>
<p>Se sugiere que la representación de los datos como combinaciones dispersas de átomos de un diccionario sobrecompleto es la forma en que funciona la córtex visual primario de los mamíferos. En consecuencia, el diccionario de aprendizaje aplicado a parches de imágenes ha demostrado dar buenos resultados en tareas de procesamiento de imágenes como la finalización, el repintado y la eliminación de ruido, así como en tareas de reconocimiento supervisado.</p>
<p>El diccionario de aprendizaje es un problema de optimización que se resuelve actualizando alternativamente el código disperso, como solución a múltiples problemas de Lasso, considerando el diccionario fijo, y luego actualizando el diccionario para que se ajuste mejor al código disperso.</p>
<div class="math notranslate nohighlight">
\[\begin{split}(U^*, V^*) = \underset{U, V}{\operatorname{arg\,min\,}} &amp; \frac{1}{2}
             ||X-UV||_2^2+\alpha||U||_1 \\
             \text{subject to } &amp; ||V_k||_2 = 1 \text{ for all }
             0 \leq k &lt; n_{\mathrm{atoms}}\end{split}\]</div>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img2" src="../_images/sphx_glr_plot_faces_decomposition_002.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="dict_img2" src="../_images/sphx_glr_plot_faces_decomposition_006.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>Después de utilizar este procedimiento para ajustar el diccionario, la transformación es simplemente un paso de codificación dispersa que comparte la misma implementación con todos los objetos del diccionario de aprendizaje(véase <a class="reference internal" href="#sparsecoder"><span class="std std-ref">Codificación dispersa con un diccionario precalculado</span></a>).</p>
<p>También es posible restringir el diccionario y/o el código para que sea positivo para que coincida con las restricciones que puedan estar presentes en los datos. A continuación se muestran las caras con diferentes restricciones de positividad aplicadas. El rojo indica valores negativos, el azul indica valores positivos y el blanco representa ceros.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos1" src="../_images/sphx_glr_plot_faces_decomposition_011.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos2" src="../_images/sphx_glr_plot_faces_decomposition_012.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos3" src="../_images/sphx_glr_plot_faces_decomposition_013.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="dict_img_pos4" src="../_images/sphx_glr_plot_faces_decomposition_014.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>La siguiente imagen muestra el aspecto de un diccionario obtenido a partir de fragmentos de imagen de 4x4 píxeles extraídos de una zona de la imagen de la cara de un mapache.</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_image_denoising.html"><img alt="../_images/sphx_glr_plot_image_denoising_001.png" src="../_images/sphx_glr_plot_image_denoising_001.png" style="width: 210.0px; height: 200.0px;" /></a>
</figure>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_image_denoising.html#sphx-glr-auto-examples-decomposition-plot-image-denoising-py"><span class="std std-ref">Desnaturalización de imágenes mediante el aprendizaje de diccionarios</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.di.ens.fr/sierra/pdfs/icml09.pdf">«Online dictionary learning for sparse coding»</a>
J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009</p></li>
</ul>
</div>
</section>
<section id="mini-batch-dictionary-learning">
<span id="minibatchdictionarylearning"></span><h3><span class="section-number">2.5.3.3. </span>Diccionario de aprendizaje por mini lotes<a class="headerlink" href="#mini-batch-dictionary-learning" title="Enlazar permanentemente con este título">¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchDictionaryLearning</span></code></a> implementa una versión más rápida, pero menos precisa, del algoritmo de diccionario de aprendizaje que es más adecuado para conjuntos de datos grandes.</p>
<p>Por defecto, <a class="reference internal" href="generated/sklearn.decomposition.MiniBatchDictionaryLearning.html#sklearn.decomposition.MiniBatchDictionaryLearning" title="sklearn.decomposition.MiniBatchDictionaryLearning"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchDictionaryLearning</span></code></a> divide los datos en mini lotes y los optimiza de forma directa, recorriendo los mini lotes durante el número de iteraciones especificado. Sin embargo, por el momento no implementa una condición de parada.</p>
<p>El estimador también implementa <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>, que actualiza el diccionario iterando sólo una vez sobre un mini lote. Esto puede utilizarse para el aprendizaje en línea cuando los datos no están disponibles desde el principio, o para cuando los datos no caben en la memoria.</p>
<a class="reference external image-reference" href="../auto_examples/cluster/plot_dict_face_patches.html"><img alt="../_images/sphx_glr_plot_dict_face_patches_001.png" class="align-right" src="../_images/sphx_glr_plot_dict_face_patches_001.png" style="width: 210.0px; height: 200.0px;" /></a>
<div class="topic">
<p class="topic-title"><strong>Análisis de conglomerados para el aprendizaje de diccionario</strong></p>
<p>Ten en cuenta que cuando se utiliza el aprendizaje de diccionario para extraer una representación (por ejemplo, para la codificación dispersa) el análisis de conglomerados o agrupamiento puede ser un buen sustituto para aprender el diccionario. Por ejemplo, el estimador <a class="reference internal" href="generated/sklearn.cluster.MiniBatchKMeans.html#sklearn.cluster.MiniBatchKMeans" title="sklearn.cluster.MiniBatchKMeans"><code class="xref py py-class docutils literal notranslate"><span class="pre">MiniBatchKMeans</span></code></a> es computacionalmente eficiente e implementa el aprendizaje en línea con un método <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>.</p>
<blockquote>
<div><p>Ejemplo: <a class="reference internal" href="../auto_examples/cluster/plot_dict_face_patches.html#sphx-glr-auto-examples-cluster-plot-dict-face-patches-py"><span class="std std-ref">Aprendizaje en línea de un diccionario de partes de caras</span></a></p>
</div></blockquote>
</div>
</section>
</section>
<section id="factor-analysis">
<span id="fa"></span><h2><span class="section-number">2.5.4. </span>Análisis de factores<a class="headerlink" href="#factor-analysis" title="Enlazar permanentemente con este título">¶</a></h2>
<p>En el aprendizaje no supervisado sólo tenemos un conjunto de datos <span class="math notranslate nohighlight">\(X = \{x_1, x_2, \dots, x_n \}\)</span>. ¿Cómo se puede describir matemáticamente este conjunto de datos? Un modelo muy sencillo de <code class="docutils literal notranslate"><span class="pre">variable</span> <span class="pre">latente</span> <span class="pre">continua</span></code> para <span class="math notranslate nohighlight">\(X\)</span> es</p>
<div class="math notranslate nohighlight">
\[x_i = W h_i + \mu + \epsilon\]</div>
<p>El vector <span class="math notranslate nohighlight">\(h_i\)</span> se llama «latente» porque no se observa. <span class="math notranslate nohighlight">\(\epsilon\)</span> se considera un término de ruido distribuido según una Gaussiana con media 0 y covarianza <span class="math notranslate nohighlight">\(\Psi\)</span> (es decir, <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \Psi)\)</span>), <span class="math notranslate nohighlight">\(\mu\)</span> es algún vector de desplazamiento arbitrario. Este modelo se llama «generativo», ya que describe cómo se genera <span class="math notranslate nohighlight">\(x_i\)</span> a partir de <span class="math notranslate nohighlight">\(h_i\)</span>. Si utilizamos todas las <span class="math notranslate nohighlight">\(x_i\)</span> como columnas para formar una matriz <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> y todas las <span class="math notranslate nohighlight">\(h_i\)</span> como columnas de una matriz <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> entonces podemos escribir (con <span class="math notranslate nohighlight">\(\mathbf{M}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{E}\)</span> adecuadamente definidos):</p>
<div class="math notranslate nohighlight">
\[\mathbf{X} = W \mathbf{H} + \mathbf{M} + \mathbf{E}\]</div>
<p>En otras palabras, hemos <em>descompuesto</em> la matriz <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>.</p>
<p>Si se da <span class="math notranslate nohighlight">\(h_i\)</span>, la ecuación anterior implica automáticamente la siguiente interpretación probabilística:</p>
<div class="math notranslate nohighlight">
\[p(x_i|h_i) = \mathcal{N}(Wh_i + \mu, \Psi)\]</div>
<p>Para un modelo probabilístico completo necesitamos también una distribución a priori para la variable latente <span class="math notranslate nohighlight">\(h\)</span>. La suposición más directa (basada en las buenas propiedades de la distribución Gaussiana) es <span class="math notranslate nohighlight">\(h \sim \mathcal{N}(0, \mathbf{I})\)</span>.  Esto da como resultado una Gaussiana como distribución marginal de <span class="math notranslate nohighlight">\(x\)</span>:</p>
<div class="math notranslate nohighlight">
\[p(x) = \mathcal{N}(\mu, WW^T + \Psi)\]</div>
<p>Ahora, sin ninguna otra suposición, la idea de tener una variable latente <span class="math notranslate nohighlight">\(h\)</span> sería superflua, ya que <span class="math notranslate nohighlight">\(x\)</span> puede modelarse completamente con una media y una covarianza. Por ello, necesitamos imponer alguna estructura más específica a uno de estos dos parámetros. Una simple suposición adicional se refiere a la estructura de la covarianza del error <span class="math notranslate nohighlight">\(\Psi\)</span>:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\Psi = \sigma^2 \mathbf{I}\)</span>: Esta suposición conduce al modelo probabilístico de <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>.</p></li>
<li><p><span class="math notranslate nohighlight">\(\Psi = \mathrm{diag}(\psi_1, \psi_2, \dots, \psi_n)\)</span>: This model is called
<a class="reference internal" href="generated/sklearn.decomposition.FactorAnalysis.html#sklearn.decomposition.FactorAnalysis" title="sklearn.decomposition.FactorAnalysis"><code class="xref py py-class docutils literal notranslate"><span class="pre">FactorAnalysis</span></code></a>, a classical statistical model. The matrix W is
sometimes called the «factor loading matrix».</p></li>
</ul>
<p>Ambos modelos estiman esencialmente una Gaussiana con una matriz de covarianza de bajo rango. Como ambos modelos son probabilísticos, pueden integrarse en modelos más complejos como, por ejemplo, la Mezcla de Analizadores de Factores. Se obtienen modelos muy diferentes (por ejemplo, <a class="reference internal" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">FastICA</span></code></a>) si se asumen no Gaussianos a priori en las variables latentes.</p>
<p>El análisis factorial <em>puede</em> arrojar componentes similares (las columnas de su matriz de carga) al <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>. Sin embargo, no se puede hacer ninguna afirmación general sobre estos componentes (por ejemplo, sobre si son ortogonales):</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img3" src="../_images/sphx_glr_plot_faces_decomposition_002.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="fa_img3" src="../_images/sphx_glr_plot_faces_decomposition_009.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>La principal ventaja del Análisis Factorial sobre el <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> es que puede modelar la varianza en cada dirección del espacio de entrada de forma independiente (ruido heteroscedástico):</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="../_images/sphx_glr_plot_faces_decomposition_008.png" src="../_images/sphx_glr_plot_faces_decomposition_008.png" style="width: 150.0px; height: 168.75px;" /></a>
</figure>
<p>Esto permite una mejor selección del modelo que el PCA probabilístico en presencia de ruido heteroscedástico:</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html"><img alt="../_images/sphx_glr_plot_pca_vs_fa_model_selection_002.png" src="../_images/sphx_glr_plot_pca_vs_fa_model_selection_002.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>El análisis factorial suele ir seguido de una rotación de los factores (con el parámetro <code class="docutils literal notranslate"><span class="pre">rotation</span></code>), normalmente para mejorar la interpretabilidad. Por ejemplo, la rotación Varimax maximiza la suma de las varianzas de las cargas al cuadrado, es decir, tiende a producir factores más dispersos, en los que sólo influyen unas pocas características cada uno (la «estructura simple»). Ver por ejemplo, el primer caso a continuación.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_varimax_fa.html#sphx-glr-auto-examples-decomposition-plot-varimax-fa-py"><span class="std std-ref">Análisis factorial (con rotación) para visualizar patrones</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_pca_vs_fa_model_selection.html#sphx-glr-auto-examples-decomposition-plot-pca-vs-fa-model-selection-py"><span class="std std-ref">Selección de modelos con el ACP probabilístico y el análisis factorial (AF)</span></a></p></li>
</ul>
</div>
</section>
<section id="independent-component-analysis-ica">
<span id="ica"></span><h2><span class="section-number">2.5.5. </span>Análisis de componentes independientes (ICA)<a class="headerlink" href="#independent-component-analysis-ica" title="Enlazar permanentemente con este título">¶</a></h2>
<p>El análisis de componentes independientes separa una señal multivariante en subcomponentes aditivos que son independientes al máximo. Se implementa en scikit-learn utilizando el algoritmo <a class="reference internal" href="generated/sklearn.decomposition.FastICA.html#sklearn.decomposition.FastICA" title="sklearn.decomposition.FastICA"><code class="xref py py-class docutils literal notranslate"><span class="pre">Fast</span> <span class="pre">ICA</span></code></a>. Normalmente, el ICA no se utiliza para reducir la dimensionalidad, sino para separar las señales superpuestas. Dado que el modelo ICA no incluye un término de ruido, para que el modelo sea correcto se debe aplicar un whitening. Esto puede hacerse internamente usando el argumento whiten o manualmente usando una de las variantes de PCA.</p>
<p>Se utiliza tradicionalmente para separar señales mixtas (un problema conocido como <em>separación ciega de fuentes</em>), como en el ejemplo siguiente:</p>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html"><img alt="../_images/sphx_glr_plot_ica_blind_source_separation_001.png" src="../_images/sphx_glr_plot_ica_blind_source_separation_001.png" style="width: 384.0px; height: 288.0px;" /></a>
</figure>
<p>El ICA también puede utilizarse como otra descomposición no lineal que encuentra componentes con cierta dispersión:</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img4" src="../_images/sphx_glr_plot_faces_decomposition_002.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="ica_img4" src="../_images/sphx_glr_plot_faces_decomposition_004.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_ica_blind_source_separation.html#sphx-glr-auto-examples-decomposition-plot-ica-blind-source-separation-py"><span class="std std-ref">Separación ciega de fuentes mediante FastICA</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_ica_vs_pca.html#sphx-glr-auto-examples-decomposition-plot-ica-vs-pca-py"><span class="std std-ref">FastICA en nubes de puntos 2D</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Descomposición de conjuntos de datos de caras</span></a></p></li>
</ul>
</div>
</section>
<section id="non-negative-matrix-factorization-nmf-or-nnmf">
<span id="nmf"></span><h2><span class="section-number">2.5.6. </span>Factorización matricial no negativa (NMF o NNMF)<a class="headerlink" href="#non-negative-matrix-factorization-nmf-or-nnmf" title="Enlazar permanentemente con este título">¶</a></h2>
<section id="nmf-with-the-frobenius-norm">
<h3><span class="section-number">2.5.6.1. </span>NMF con la norma de Frobenius<a class="headerlink" href="#nmf-with-the-frobenius-norm" title="Enlazar permanentemente con este título">¶</a></h3>
<p>La <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> <a class="footnote-reference brackets" href="#id11" id="id5">1</a> es un enfoque alternativo a la descomposición que asume que los datos y los componentes son no negativos. La <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> puede utilizarse en lugar del <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a> o sus variantes, en los casos en que la matriz de datos no contenga valores negativos. Encuentra una descomposición de las muestras <span class="math notranslate nohighlight">\(X\)</span> en dos matrices <span class="math notranslate nohighlight">\(W\)</span> y <span class="math notranslate nohighlight">\(H\)</span> de elementos no negativos, optimizando la distancia <span class="math notranslate nohighlight">\(d\)</span> entre <span class="math notranslate nohighlight">\(X\)</span> y el producto matricial <span class="math notranslate nohighlight">\(WH\)</span>. La función de distancia más utilizada es la norma de Frobenius al cuadrado, que es una extensión obvia de la norma euclidiana a las matrices:</p>
<div class="math notranslate nohighlight">
\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{\mathrm{Fro}}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]</div>
<p>A diferencia del <a class="reference internal" href="generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA" title="sklearn.decomposition.PCA"><code class="xref py py-class docutils literal notranslate"><span class="pre">PCA</span></code></a>, la representación de un vector se obtiene de forma aditiva, superponiendo los componentes, sin restar. Estos modelos aditivos son eficaces para representar imágenes y textos.</p>
<p>Se ha observado en [Hoyer, 2004] <a class="footnote-reference brackets" href="#id12" id="id6">2</a> que, cuando se restringe cuidadosamente, la <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> puede producir una representación basada en partes del conjunto de datos, dando lugar a modelos interpretables. El siguiente ejemplo muestra 16 componentes dispersos encontrados por <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> a partir de las imágenes del conjunto de datos de rostros Olivetti, en comparación con las caras propias del PCA.</p>
<p class="centered">
<strong><a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="pca_img5" src="../_images/sphx_glr_plot_faces_decomposition_002.png" style="width: 360.0px; height: 270.59999999999997px;" /></a> <a class="reference external" href="../auto_examples/decomposition/plot_faces_decomposition.html"><img alt="nmf_img5" src="../_images/sphx_glr_plot_faces_decomposition_003.png" style="width: 360.0px; height: 270.59999999999997px;" /></a></strong></p><p>El atributo <code class="xref py py-attr docutils literal notranslate"><span class="pre">init</span></code> determina el método de inicialización aplicado, que tiene un gran impacto en el rendimiento del método. La <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> implementa el método Descomposición del Valor Singular Doble No Negativo. El NNDSVD <a class="footnote-reference brackets" href="#id13" id="id7">4</a> se basa en dos procesos SVD, uno aproximando la matriz de datos, el otro aproximando secciones positivas de los factores parciales SVD resultantes, utilizando una propiedad algebraica de las matrices de rango unitario. El algoritmo básico NNDSVD se adapta mejor a la factorización dispersa. Sus variantes NNDSVDa (en la que todos los ceros se ajustan a la media de todos los elementos de los datos), y NNDSVDar (en la que los ceros se fijan a perturbaciones aleatorias menores que la media de los datos dividida por 100) se recomiendan en el caso denso.</p>
<p>Tenga en cuenta que el solucionador de actualización multiplicativa (“mu”) no puede actualizar los ceros presentes en la inicialización, por lo que conduce a resultados más pobres cuando se utiliza conjuntamente con el algoritmo básico NNDSVD que introduce muchos ceros; en este caso, se debe preferir NNDSVDa o NNDSVDar.</p>
<p>La <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> también puede inicializarse con matrices aleatorias no negativas correctamente escaladas estableciendo <code class="xref py py-attr docutils literal notranslate"><span class="pre">init=&quot;random&quot;</span></code>. También se puede pasar una semilla entera o un <code class="docutils literal notranslate"><span class="pre">RandomState</span></code> a <code class="xref py py-attr docutils literal notranslate"><span class="pre">random_state</span></code> para controlar la reproducibilidad.</p>
<p>En <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a>, se pueden añadir los valores a priori L1 y L2 a la función de pérdida para regularizar el modelo. El valor a priori L2 utiliza la norma de Frobenius, mientras que el valor a priori L1 utiliza una norma L1 elemental. Como en <code class="xref py py-class docutils literal notranslate"><span class="pre">ElasticNet</span></code>, controlamos la combinación de L1 y L2 con el parámetro <code class="xref py py-attr docutils literal notranslate"><span class="pre">l1_ratio</span></code> (<span class="math notranslate nohighlight">\(\rho\)</span>), y la intensidad de la regularización con el parámetro <code class="xref py py-attr docutils literal notranslate"><span class="pre">alpha</span></code> (<span class="math notranslate nohighlight">\(\alpha\)</span>). Entonces los términos a priori son:</p>
<div class="math notranslate nohighlight">
\[\alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\]</div>
<p>y la función objetivo regularizada es:</p>
<div class="math notranslate nohighlight">
\[d_{\mathrm{Fro}}(X, WH)
+ \alpha \rho ||W||_1 + \alpha \rho ||H||_1
+ \frac{\alpha(1-\rho)}{2} ||W||_{\mathrm{Fro}} ^ 2
+ \frac{\alpha(1-\rho)}{2} ||H||_{\mathrm{Fro}} ^ 2\]</div>
<p>La <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> regulariza tanto W como H por defecto. El parámetro <code class="xref py py-attr docutils literal notranslate"><span class="pre">regularization</span></code> permite un control más fino, con el que se puede regularizar sólo W, sólo H, o ambos.</p>
</section>
<section id="nmf-with-a-beta-divergence">
<h3><span class="section-number">2.5.6.2. </span>NMF con una divergencia beta<a class="headerlink" href="#nmf-with-a-beta-divergence" title="Enlazar permanentemente con este título">¶</a></h3>
<p>Como se ha descrito anteriormente, la función de distancia más utilizada es la norma de Frobenius al cuadrado, que es una extensión obvia de la norma Euclidiana a las matrices:</p>
<div class="math notranslate nohighlight">
\[d_{\mathrm{Fro}}(X, Y) = \frac{1}{2} ||X - Y||_{Fro}^2 = \frac{1}{2} \sum_{i,j} (X_{ij} - {Y}_{ij})^2\]</div>
<p>Se pueden utilizar otras funciones de distancia en el NMF como, por ejemplo, la divergencia (generalizada) de Kullback-Leibler (KL), también denominada divergencia I:</p>
<div class="math notranslate nohighlight">
\[d_{KL}(X, Y) = \sum_{i,j} (X_{ij} \log(\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\]</div>
<p>O la divergencia Itakura-Saito (IS):</p>
<div class="math notranslate nohighlight">
\[d_{IS}(X, Y) = \sum_{i,j} (\frac{X_{ij}}{Y_{ij}} - \log(\frac{X_{ij}}{Y_{ij}}) - 1)\]</div>
<p>Estas tres distancias son casos especiales de la familia de las divergencias beta, con <span class="math notranslate nohighlight">\(\beta = 2, 1, 0\)</span> respectivamente <a class="footnote-reference brackets" href="#id15" id="id8">6</a>. Las divergencias beta se definen por:</p>
<div class="math notranslate nohighlight">
\[d_{\beta}(X, Y) = \sum_{i,j} \frac{1}{\beta(\beta - 1)}(X_{ij}^\beta + (\beta-1)Y_{ij}^\beta - \beta X_{ij} Y_{ij}^{\beta - 1})\]</div>
<figure class="align-center">
<a class="reference external image-reference" href="../auto_examples/decomposition/plot_beta_divergence.html"><img alt="../_images/sphx_glr_plot_beta_divergence_001.png" src="../_images/sphx_glr_plot_beta_divergence_001.png" style="width: 480.0px; height: 360.0px;" /></a>
</figure>
<p>Ten en cuenta que esta definición no es válida si <span class="math notranslate nohighlight">\(\beta \in (0; 1)\)</span>, pero puede extenderse continuamente a las definiciones de <span class="math notranslate nohighlight">\(d_{KL}\)</span> y <span class="math notranslate nohighlight">\(d_{IS}\)</span> respectivamente.</p>
<p>La <a class="reference internal" href="generated/sklearn.decomposition.NMF.html#sklearn.decomposition.NMF" title="sklearn.decomposition.NMF"><code class="xref py py-class docutils literal notranslate"><span class="pre">NMF</span></code></a> implementa dos solucionadores, utilizando el Descenso de Coordenadas (“cd”) <a class="footnote-reference brackets" href="#id14" id="id9">5</a>, y la Actualización Multiplicativa (“mu”) <a class="footnote-reference brackets" href="#id15" id="id10">6</a>. El solucionador “mu” puede optimizar todas las divergencias beta, incluyendo por supuesto la norma de Frobenius (<span class="math notranslate nohighlight">\(\beta=2\)</span>), la divergencia (generalizada) de Kullback-Leibler (<span class="math notranslate nohighlight">\(\beta=1\)</span>) y la divergencia de Itakura-Saito (<span class="math notranslate nohighlight">\(\beta=0\)</span>). Observa que para <span class="math notranslate nohighlight">\(\beta \in (1; 2)\)</span>, el solucionador “mu” es significativamente más rápido que para otros valores de <span class="math notranslate nohighlight">\(beta\)</span>. Observa también que con un valor negativo (o 0, es decir, “itakura-saito”) de <span class="math notranslate nohighlight">\(beta\)</span>, la matriz de entrada no puede contener valores cero.</p>
<p>El solucionador “cd” sólo puede optimizar la norma de Frobenius. Debido a la no-convexidad subyacente de la NMF, los diferentes solucionadores pueden converger en diferentes mínimos, incluso cuando se optimiza la misma función de distancia.</p>
<p>La NMF se utiliza mejor con el método <code class="docutils literal notranslate"><span class="pre">fit_transform</span></code>, que devuelve la matriz W. La matriz H se almacena en el modelo ajustado en el atributo <code class="docutils literal notranslate"><span class="pre">components_</span></code>; el método <code class="docutils literal notranslate"><span class="pre">transform</span></code> descompondrá una nueva matriz X_new basada en estos componentes almacenados:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mf">1.2</span><span class="p">],</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">],</span> <span class="p">[</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.decomposition</span> <span class="kn">import</span> <span class="n">NMF</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">NMF</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">init</span><span class="o">=</span><span class="s1">&#39;random&#39;</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">H</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">components_</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_new</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mf">6.1</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.2</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">4</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W_new</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X_new</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py"><span class="std std-ref">Descomposición de conjuntos de datos de caras</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Extracción de temas con Factorización Matricial No Negativa y Asignación de Dirichlet Latente</span></a></p></li>
<li><p><a class="reference internal" href="../auto_examples/decomposition/plot_beta_divergence.html#sphx-glr-auto-examples-decomposition-plot-beta-divergence-py"><span class="std std-ref">Funciones de pérdida de divergencia beta</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<dl class="footnote brackets">
<dt class="label" id="id11"><span class="brackets"><a class="fn-backref" href="#id5">1</a></span></dt>
<dd><p><a class="reference external" href="http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf">«Learning the parts of objects by non-negative matrix factorization»</a>
D. Lee, S. Seung, 1999</p>
</dd>
<dt class="label" id="id12"><span class="brackets"><a class="fn-backref" href="#id6">2</a></span></dt>
<dd><p><a class="reference external" href="http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">«Non-negative Matrix Factorization with Sparseness Constraints»</a>
P. Hoyer, 2004</p>
</dd>
<dt class="label" id="id13"><span class="brackets"><a class="fn-backref" href="#id7">4</a></span></dt>
<dd><p><a class="reference external" href="http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf">«SVD based initialization: A head start for nonnegative
matrix factorization»</a>
C. Boutsidis, E. Gallopoulos, 2008</p>
</dd>
<dt class="label" id="id14"><span class="brackets"><a class="fn-backref" href="#id9">5</a></span></dt>
<dd><p><a class="reference external" href="http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf">«Fast local algorithms for large scale nonnegative matrix and tensor
factorizations.»</a>
A. Cichocki, A. Phan, 2009</p>
</dd>
<dt class="label" id="id15"><span class="brackets">6</span><span class="fn-backref">(<a href="#id8">1</a>,<a href="#id10">2</a>)</span></dt>
<dd><p><a class="reference external" href="https://arxiv.org/pdf/1010.1763.pdf">«Algorithms for nonnegative matrix factorization with the beta-divergence»</a>
C. Fevotte, J. Idier, 2011</p>
</dd>
</dl>
</div>
</section>
</section>
<section id="latent-dirichlet-allocation-lda">
<span id="latentdirichletallocation"></span><h2><span class="section-number">2.5.7. </span>Asignación Latente de Dirichlet (LDA)<a class="headerlink" href="#latent-dirichlet-allocation-lda" title="Enlazar permanentemente con este título">¶</a></h2>
<p>La Asignación Latente de Dirichlet es un modelo probabilístico generativo para colecciones de conjuntos de datos discretos, como los corpus de texto. También es un modelo temático que se utiliza para descubrir temas abstractos a partir de una colección de documentos.</p>
<p>El modelo gráfico de LDA es un modelo generativo de tres niveles:</p>
<img alt="../_images/lda_model_graph.png" class="align-center" src="../_images/lda_model_graph.png" />
<p>Nota sobre las notaciones presentadas en el modelo gráfico anterior, que se puede encontrar en Hoffman et al. (2013):</p>
<blockquote>
<div><ul class="simple">
<li><p>El corpus es una colección de documentos <span class="math notranslate nohighlight">\(D\)</span>.</p></li>
<li><p>Un documento es una secuencia de palabras <span class="math notranslate nohighlight">\(N\)</span>.</p></li>
<li><p>Hay temas <span class="math notranslate nohighlight">\(K\)</span> en el corpus.</p></li>
<li><p>Los recuadros representan un muestreo repetido.</p></li>
</ul>
</div></blockquote>
<p>En el modelo gráfico, cada nodo es una variable aleatoria y tiene un papel en el proceso generativo. Un nodo sombreado indica una variable observada y un nodo no sombreado indica una variable oculta (latente). En este caso, las palabras del corpus son los únicos datos que observamos. Las variables latentes determinan la mezcla aleatoria de temas en el corpus y la distribución de palabras en los documentos. El objetivo del LDA es utilizar las palabras observadas para inferir la estructura temática oculta.</p>
<p>Cuando se modelan corpus de texto, el modelo asume el siguiente proceso generativo para un corpus con <span class="math notranslate nohighlight">\(D\)</span> documentos y <span class="math notranslate nohighlight">\(K\)</span> temas, con <span class="math notranslate nohighlight">\(K\)</span> correspondiente a <code class="xref py py-attr docutils literal notranslate"><span class="pre">n_components</span></code> en el API:</p>
<blockquote>
<div><ol class="arabic simple">
<li><p>Para cada tema <span class="math notranslate nohighlight">\(k \in K\)</span>, dibujar <span class="math notranslate nohighlight">\(\beta_k \sim \mathrm{Dirichlet}(\eta)\)</span>. Esto proporciona una distribución sobre las palabras, es decir, la probabilidad de que una palabra aparezca en el tema <span class="math notranslate nohighlight">\(k\)</span>. <span class="math notranslate nohighlight">\(\eta\)</span> corresponde a <code class="xref py py-attr docutils literal notranslate"><span class="pre">topic_word_prior</span></code>.</p></li>
<li><p>Para cada documento <span class="math notranslate nohighlight">\(d \in D\)</span>, dibujar las proporciones del tema <span class="math notranslate nohighlight">\(\theta_d \sim \mathrm{Dirichlet}(\alpha)\)</span>. <span class="math notranslate nohighlight">\(\alpha\)</span> corresponde a <code class="xref py py-attr docutils literal notranslate"><span class="pre">doc_topic_prior</span></code>.</p></li>
<li><p>Para cada palabra <span class="math notranslate nohighlight">\(i\)</span> en el documento <span class="math notranslate nohighlight">\(d\)</span>:</p></li>
</ol>
<blockquote>
<div><ol class="loweralpha simple">
<li><p>Dibuja la asignación del tema <span class="math notranslate nohighlight">\(z_{di} \sim \mathrm{Multinomial} (\theta_d)\)</span></p></li>
<li><p>Dibuja la palabra observada <span class="math notranslate nohighlight">\(w_{ij} \sim \mathrm{Multinomial} (\beta_{z_{di}})\)</span></p></li>
</ol>
</div></blockquote>
</div></blockquote>
<p>Para la estimación de los parámetros, la distribución posterior es:</p>
<div class="math notranslate nohighlight">
\[p(z, \theta, \beta |w, \alpha, \eta) =
  \frac{p(z, \theta, \beta|\alpha, \eta)}{p(w|\alpha, \eta)}\]</div>
<p>Como la posterior es inabordable, el método bayesiano variacional utiliza una distribución más simple <span class="math notranslate nohighlight">\(q(z,\theta,\beta | \lambda, \phi, \gamma)\)</span> para aproximarla, y esos parámetros variacionales <span class="math notranslate nohighlight">\(lambda\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span>, <span class="math notranslate nohighlight">\(gamma\)</span> se optimizan para maximizar el Límite Inferior de Evidencia (ELBO):</p>
<div class="math notranslate nohighlight">
\[\log\: P(w | \alpha, \eta) \geq L(w,\phi,\gamma,\lambda) \overset{\triangle}{=}
  E_{q}[\log\:p(w,z,\theta,\beta|\alpha,\eta)] - E_{q}[\log\:q(z, \theta, \beta)]\]</div>
<p>Maximizar el ELBO es equivalente a minimizar la divergencia de Kullback-Leibler (KL) entre <span class="math notranslate nohighlight">\(q(z,\theta,\beta)\)</span> y la verdadera posterior <span class="math notranslate nohighlight">\(p(z, \theta, \beta |w, \alpha, \eta)\)</span>.</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> implementa el algoritmo Bayes variacional en línea y admite métodos de actualización en línea y por lotes. Mientras que el método por lotes actualiza las variables variacionales después de cada paso completo por los datos, el método en línea actualiza las variables variacionales a partir de puntos de datos de mini lotes.</p>
<div class="admonition note">
<p class="admonition-title">Nota</p>
<p>Aunque se garantiza que el método en línea converge a un punto óptimo local, la calidad del punto óptimo y la velocidad de convergencia pueden depender del tamaño del mini lote y de los atributos relacionados con la configuración de la tasa de aprendizaje.</p>
</div>
<p>Cuando se aplica <a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> a una matriz «documento-término», la matriz se descompone en una matriz «tema-término» y una matriz «documento-tema». Mientras que la matriz «tema-término» se almacena como <code class="xref py py-attr docutils literal notranslate"><span class="pre">components_</span></code> en el modelo, la matriz «documento-tema» puede calcularse a partir del método <code class="docutils literal notranslate"><span class="pre">transform</span></code>.</p>
<p><a class="reference internal" href="generated/sklearn.decomposition.LatentDirichletAllocation.html#sklearn.decomposition.LatentDirichletAllocation" title="sklearn.decomposition.LatentDirichletAllocation"><code class="xref py py-class docutils literal notranslate"><span class="pre">LatentDirichletAllocation</span></code></a> también implementa el método <code class="docutils literal notranslate"><span class="pre">partial_fit</span></code>. Se utiliza cuando los datos pueden obtenerse de forma secuencial.</p>
<div class="topic">
<p class="topic-title">Ejemplos:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../auto_examples/applications/plot_topics_extraction_with_nmf_lda.html#sphx-glr-auto-examples-applications-plot-topics-extraction-with-nmf-lda-py"><span class="std std-ref">Extracción de temas con Factorización Matricial No Negativa y Asignación de Dirichlet Latente</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title">Referencias:</p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf">«Latent Dirichlet Allocation»</a>
D. Blei, A. Ng, M. Jordan, 2003</p></li>
<li><p><a class="reference external" href="https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf">«Online Learning for Latent Dirichlet Allocation”</a>
M. Hoffman, D. Blei, F. Bach, 2010</p></li>
<li><p><a class="reference external" href="http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf">«Stochastic Variational Inference»</a>
M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013</p></li>
<li><p><a class="reference external" href="https://link.springer.com/article/10.1007%2FBF02289233">«The varimax criterion for analytic rotation in factor analysis»</a>
H. F. Kaiser, 1958</p></li>
</ul>
</div>
<p>Revisa también <a class="reference internal" href="neighbors.html#nca-dim-reduction"><span class="std std-ref">Reducción de Dimensionalidad</span></a> para la reducción de la dimensionalidad con el Análisis de Componentes de Vecindad.</p>
</section>
</section>


      </div>
    <div class="container">
      <footer class="sk-content-footer">
            &copy; 2007 - 2020, scikit-learn developers (BSD License).
          <a href="../_sources/modules/decomposition.rst.txt" rel="nofollow">Mostrar la fuente de esta página</a>
      </footer>
    </div>
  </div>
</div>
<script src="../_static/js/vendor/bootstrap.min.js"></script>

<script>
    window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
    ga('create', 'UA-22606712-2', 'auto');
    ga('set', 'anonymizeIp', true);
    ga('send', 'pageview');
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>


<script>
$(document).ready(function() {
    /* Add a [>>>] button on the top-right corner of code samples to hide
     * the >>> and ... prompts and the output and thus make the code
     * copyable. */
    var div = $('.highlight-python .highlight,' +
                '.highlight-python3 .highlight,' +
                '.highlight-pycon .highlight,' +
		'.highlight-default .highlight')
    var pre = div.find('pre');

    // get the styles from the current theme
    pre.parent().parent().css('position', 'relative');
    var hide_text = 'Hide prompts and outputs';
    var show_text = 'Show prompts and outputs';

    // create and add the button to all the code blocks that contain >>>
    div.each(function(index) {
        var jthis = $(this);
        if (jthis.find('.gp').length > 0) {
            var button = $('<span class="copybutton">&gt;&gt;&gt;</span>');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
            jthis.prepend(button);
        }
        // tracebacks (.gt) contain bare text elements that need to be
        // wrapped in a span to work with .nextUntil() (see later)
        jthis.find('pre:has(.gt)').contents().filter(function() {
            return ((this.nodeType == 3) && (this.data.trim().length > 0));
        }).wrap('<span>');
    });

    // define the behavior of the button when it's clicked
    $('.copybutton').click(function(e){
        e.preventDefault();
        var button = $(this);
        if (button.data('hidden') === 'false') {
            // hide the code output
            button.parent().find('.go, .gp, .gt').hide();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'hidden');
            button.css('text-decoration', 'line-through');
            button.attr('title', show_text);
            button.data('hidden', 'true');
        } else {
            // show the code output
            button.parent().find('.go, .gp, .gt').show();
            button.next('pre').find('.gt').nextUntil('.gp, .go').css('visibility', 'visible');
            button.css('text-decoration', 'none');
            button.attr('title', hide_text);
            button.data('hidden', 'false');
        }
    });

	/*** Add permalink buttons next to glossary terms ***/
	$('dl.glossary > dt[id]').append(function() {
		return ('<a class="headerlink" href="#' +
			    this.getAttribute('id') +
			    '" title="Permalink to this term">¶</a>');
	});
  /*** Hide navbar when scrolling down ***/
  // Returns true when headerlink target matches hash in url
  (function() {
    hashTargetOnTop = function() {
        var hash = window.location.hash;
        if ( hash.length < 2 ) { return false; }

        var target = document.getElementById( hash.slice(1) );
        if ( target === null ) { return false; }

        var top = target.getBoundingClientRect().top;
        return (top < 2) && (top > -2);
    };

    // Hide navbar on load if hash target is on top
    var navBar = document.getElementById("navbar");
    var navBarToggler = document.getElementById("sk-navbar-toggler");
    var navBarHeightHidden = "-" + navBar.getBoundingClientRect().height + "px";
    var $window = $(window);

    hideNavBar = function() {
        navBar.style.top = navBarHeightHidden;
    };

    showNavBar = function() {
        navBar.style.top = "0";
    }

    if (hashTargetOnTop()) {
        hideNavBar()
    }

    var prevScrollpos = window.pageYOffset;
    hideOnScroll = function(lastScrollTop) {
        if (($window.width() < 768) && (navBarToggler.getAttribute("aria-expanded") === 'true')) {
            return;
        }
        if (lastScrollTop > 2 && (prevScrollpos <= lastScrollTop) || hashTargetOnTop()){
            hideNavBar()
        } else {
            showNavBar()
        }
        prevScrollpos = lastScrollTop;
    };

    /*** high performance scroll event listener***/
    var raf = window.requestAnimationFrame ||
        window.webkitRequestAnimationFrame ||
        window.mozRequestAnimationFrame ||
        window.msRequestAnimationFrame ||
        window.oRequestAnimationFrame;
    var lastScrollTop = $window.scrollTop();

    if (raf) {
        loop();
    }

    function loop() {
        var scrollTop = $window.scrollTop();
        if (lastScrollTop === scrollTop) {
            raf(loop);
            return;
        } else {
            lastScrollTop = scrollTop;
            hideOnScroll(lastScrollTop);
            raf(loop);
        }
    }
  })();
});

</script>
    
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>
    
</body>
</html>