# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../tutorial/text_analytics/working_with_text_data.rst:5
msgid "Working With Text Data"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:7
msgid ""
"The goal of this guide is to explore some of the main ``scikit-learn`` "
"tools on a single practical task: analyzing a collection of text "
"documents (newsgroups posts) on twenty different topics."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:11
msgid "In this section we will see how to:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:13
msgid "load the file contents and the categories"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:15
msgid "extract feature vectors suitable for machine learning"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:17
msgid "train a linear model to perform categorization"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:19
msgid ""
"use a grid search strategy to find a good configuration of both the "
"feature extraction components and the classifier"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:24
msgid "Tutorial setup"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:26
msgid ""
"To get started with this tutorial, you must first install *scikit-learn* "
"and all of its required dependencies."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:29
msgid ""
"Please refer to the :ref:`installation instructions <installation-"
"instructions>` page for more information and for system-specific "
"instructions."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:32
msgid "The source of this tutorial can be found within your scikit-learn folder::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:36
msgid ""
"The source can also be found `on Github <https://github.com/scikit-learn"
"/scikit-learn/tree/main/doc/tutorial/text_analytics>`_."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:39
msgid "The tutorial folder should contain the following sub-folders:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:41
msgid "``*.rst files`` - the source of the tutorial document written with sphinx"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:43
msgid "``data`` - folder to put the datasets used during the tutorial"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:45
msgid "``skeletons`` - sample incomplete scripts for the exercises"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:47
msgid "``solutions`` - solutions of the exercises"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:50
msgid ""
"You can already copy the skeletons into a new folder somewhere on your "
"hard-drive named ``sklearn_tut_workspace`` where you will edit your own "
"files for the exercises while keeping the original skeletons intact:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:60
msgid ""
"Machine learning algorithms need data. Go to each ``$TUTORIAL_HOME/data``"
" sub-folder and run the ``fetch_data.py`` script from there (after having"
" read them first)."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:64
msgid "For instance:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:74
msgid "Loading the 20 newsgroups dataset"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:76
msgid ""
"The dataset is called \"Twenty Newsgroups\". Here is the official "
"description, quoted from the `website "
"<http://people.csail.mit.edu/jrennie/20Newsgroups/>`_:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:80
msgid ""
"The 20 Newsgroups data set is a collection of approximately 20,000 "
"newsgroup documents, partitioned (nearly) evenly across 20 different "
"newsgroups. To the best of our knowledge, it was originally collected by "
"Ken Lang, probably for his paper \"Newsweeder: Learning to filter "
"netnews,\" though he does not explicitly mention this collection. The 20 "
"newsgroups collection has become a popular data set for experiments in "
"text applications of machine learning techniques, such as text "
"classification and text clustering."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:89
msgid ""
"In the following we will use the built-in dataset loader for 20 "
"newsgroups from scikit-learn. Alternatively, it is possible to download "
"the dataset manually from the website and use the "
":func:`sklearn.datasets.load_files` function by pointing it to the "
"``20news-bydate-train`` sub-folder of the uncompressed archive folder."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:95
msgid ""
"In order to get faster execution times for this first example we will "
"work on a partial dataset with only 4 categories out of the 20 available "
"in the dataset::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:102
msgid "We can now load the list of files matching those categories as follows::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:108
msgid ""
"The returned dataset is a ``scikit-learn`` \"bunch\": a simple holder "
"object with fields that can be both accessed as python ``dict`` keys or "
"``object`` attributes for convenience, for instance the ``target_names`` "
"holds the list of the requested category names::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:116
msgid ""
"The files themselves are loaded in memory in the ``data`` attribute. For "
"reference the filenames are also available::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:124
msgid "Let's print the first lines of the first loaded file::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:134
msgid ""
"Supervised learning algorithms will require a category label for each "
"document in the training set. In this case the category is the name of "
"the newsgroup which also happens to be the name of the folder holding the"
" individual documents."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:139
msgid ""
"For speed and space efficiency reasons ``scikit-learn`` loads the target "
"attribute as an array of integers that corresponds to the index of the "
"category name in the ``target_names`` list. The category integer id of "
"each sample is stored in the ``target`` attribute::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:147
msgid "It is possible to get back the category names as follows::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:163
msgid ""
"You might have noticed that the samples were shuffled randomly when we "
"called ``fetch_20newsgroups(..., shuffle=True, random_state=42)``: this "
"is useful if you wish to select only a subset of samples to quickly train"
" a model and get a first idea of the results before re-training on the "
"complete dataset later."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:170
msgid "Extracting features from text files"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:172
msgid ""
"In order to perform machine learning on text documents, we first need to "
"turn the text content into numerical feature vectors."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:179
msgid "Bags of words"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:181
msgid "The most intuitive way to do so is to use a bags of words representation:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:183
msgid ""
"Assign a fixed integer id to each word occurring in any document of the "
"training set (for instance by building a dictionary from words to integer"
" indices)."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:187
msgid ""
"For each document ``#i``, count the number of occurrences of each word "
"``w`` and store it in ``X[i, j]`` as the value of feature ``#j`` where "
"``j`` is the index of word ``w`` in the dictionary."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:191
msgid ""
"The bags of words representation implies that ``n_features`` is the "
"number of distinct words in the corpus: this number is typically larger "
"than 100,000."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:195
msgid ""
"If ``n_samples == 10000``, storing ``X`` as a NumPy array of type float32"
" would require 10000 x 100000 x 4 bytes = **4GB in RAM** which is barely "
"manageable on today's computers."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:199
msgid ""
"Fortunately, **most values in X will be zeros** since for a given "
"document less than a few thousand distinct words will be used. For this "
"reason we say that bags of words are typically **high-dimensional sparse "
"datasets**. We can save a lot of memory by only storing the non-zero "
"parts of the feature vectors in memory."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:205
msgid ""
"``scipy.sparse`` matrices are data structures that do exactly this, and "
"``scikit-learn`` has built-in support for these structures."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:210
msgid "Tokenizing text with ``scikit-learn``"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:212
msgid ""
"Text preprocessing, tokenizing and filtering of stopwords are all "
"included in :class:`CountVectorizer`, which builds a dictionary of "
"features and transforms documents to feature vectors::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:222
msgid ""
":class:`CountVectorizer` supports counts of N-grams of words or "
"consecutive characters. Once fitted, the vectorizer has built a "
"dictionary of feature indices::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:229
msgid ""
"The index value of a word in the vocabulary is linked to its frequency in"
" the whole training corpus."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:243
msgid "From occurrences to frequencies"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:245
msgid ""
"Occurrence count is a good start but there is an issue: longer documents "
"will have higher average count values than shorter documents, even though"
" they might talk about the same topics."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:249
msgid ""
"To avoid these potential discrepancies it suffices to divide the number "
"of occurrences of each word in a document by the total number of words in"
" the document: these new features are called ``tf`` for Term Frequencies."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:254
msgid ""
"Another refinement on top of tf is to downscale weights for words that "
"occur in many documents in the corpus and are therefore less informative "
"than those that occur only in a smaller portion of the corpus."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:259
msgid ""
"This downscaling is called `tf–idf`_ for \"Term Frequency times Inverse "
"Document Frequency\"."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:265
msgid ""
"Both **tf** and **tf–idf** can be computed as follows using "
":class:`TfidfTransformer`::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:274
msgid ""
"In the above example-code, we firstly use the ``fit(..)`` method to fit "
"our estimator to the data and secondly the ``transform(..)`` method to "
"transform our count-matrix to a tf-idf representation. These two steps "
"can be combined to achieve the same end result faster by skipping "
"redundant processing. This is done through using the "
"``fit_transform(..)`` method as shown below, and as mentioned in the note"
" in the previous section::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:289
msgid "Training a classifier"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:291
msgid ""
"Now that we have our features, we can train a classifier to try to "
"predict the category of a post. Let's start with a :ref:`naïve Bayes "
"<naive_bayes>` classifier, which provides a nice baseline for this task. "
"``scikit-learn`` includes several variants of this classifier; the one "
"most suitable for word counts is the multinomial variant::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:301
msgid ""
"To try to predict the outcome on a new document we need to extract the "
"features using almost the same feature extracting chain as before. The "
"difference is that we call ``transform`` instead of ``fit_transform`` on "
"the transformers, since they have already been fit to the training set::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:320
msgid "Building a pipeline"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:322
msgid ""
"In order to make the vectorizer => transformer => classifier easier to "
"work with, ``scikit-learn`` provides a "
":class:`~sklearn.pipeline.Pipeline` class that behaves like a compound "
"classifier::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:334
msgid ""
"The names ``vect``, ``tfidf`` and ``clf`` (classifier) are arbitrary. We "
"will use them to perform grid search for suitable hyperparameters below. "
"We can now train the model with a single command::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:343
msgid "Evaluation of the performance on the test set"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:345
msgid "Evaluating the predictive accuracy of the model is equally easy::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:355
msgid ""
"We achieved 83.5% accuracy. Let's see if we can do better with a linear "
":ref:`support vector machine (SVM) <svm>`, which is widely regarded as "
"one of the best text classification algorithms (although it's also a bit "
"slower than naïve Bayes). We can change the learner by simply plugging a "
"different classifier object into our pipeline::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:377
msgid ""
"We achieved 91.3% accuracy using the SVM. ``scikit-learn`` provides "
"further utilities for more detailed performance analysis of the results::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:401
msgid ""
"As expected the confusion matrix shows that posts from the newsgroups on "
"atheism and Christianity are more often confused for one another than "
"with computer graphics."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:424
msgid "Parameter tuning using grid search"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:426
msgid ""
"We've already encountered some parameters such as ``use_idf`` in the "
"``TfidfTransformer``. Classifiers tend to have many parameters as well; "
"e.g., ``MultinomialNB`` includes a smoothing parameter ``alpha`` and "
"``SGDClassifier`` has a penalty parameter ``alpha`` and configurable loss"
" and penalty terms in the objective function (see the module "
"documentation, or use the Python ``help`` function to get a description "
"of these)."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:433
msgid ""
"Instead of tweaking the parameters of the various components of the "
"chain, it is possible to run an exhaustive search of the best parameters "
"on a grid of possible values. We try out all classifiers on either words "
"or bigrams, with or without idf, and with a penalty parameter of either "
"0.01 or 0.001 for the linear SVM::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:447
msgid ""
"Obviously, such an exhaustive search can be expensive. If we have "
"multiple CPU cores at our disposal, we can tell the grid searcher to try "
"these eight parameter combinations in parallel with the ``n_jobs`` "
"parameter. If we give this parameter a value of ``-1``, grid search will "
"detect how many cores are installed and use them all::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:455
msgid ""
"The grid search instance behaves like a normal ``scikit-learn`` model. "
"Let's perform the search on a smaller subset of the training data to "
"speed up the computation::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:461
msgid ""
"The result of calling ``fit`` on a ``GridSearchCV`` object is a "
"classifier that we can use to ``predict``::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:467
msgid ""
"The object's ``best_score_`` and ``best_params_`` attributes store the "
"best mean score and the parameters setting corresponding to that score::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:479
msgid ""
"A more detailed summary of the search is available at "
"``gs_clf.cv_results_``."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:481
msgid ""
"The ``cv_results_`` parameter can be easily imported into pandas as a "
"``DataFrame`` for further inspection."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:492
msgid "Exercises"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:494
msgid ""
"To do the exercises, copy the content of the 'skeletons' folder as a new "
"folder named 'workspace':"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:502
msgid ""
"You can then edit the content of the workspace without fear of losing the"
" original exercise instructions."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:505
msgid "Then fire an ipython shell and run the work-in-progress script with::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:509
#, python-format
msgid ""
"If an exception is triggered, use ``%debug`` to fire-up a post mortem "
"ipdb session."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:512
msgid "Refine the implementation and iterate until the exercise is solved."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:514
msgid ""
"**For each exercise, the skeleton file provides all the necessary import "
"statements, boilerplate code to load the data and sample code to evaluate"
" the predictive accuracy of the model.**"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:520
msgid "Exercise 1: Language identification"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:522
msgid ""
"Write a text classification pipeline using a custom preprocessor and "
"``CharNGramAnalyzer`` using data from Wikipedia articles as training set."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:525
msgid "Evaluate the performance on some held out test set."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:527
#: ../tutorial/text_analytics/working_with_text_data.rst:542
msgid "ipython command line::"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:533
msgid "Exercise 2: Sentiment Analysis on movie reviews"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:535
msgid ""
"Write a text classification pipeline to classify movie reviews as either "
"positive or negative."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:538
msgid "Find a good set of parameters using grid search."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:540
msgid "Evaluate the performance on a held out test set."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:548
msgid "Exercise 3: CLI text classification utility"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:550
msgid ""
"Using the results of the previous exercises and the ``cPickle`` module of"
" the standard library, write a command line utility that detects the "
"language of some text provided on ``stdin`` and estimate the polarity "
"(positive or negative) if the text is written in English."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:556
msgid ""
"Bonus point if the utility is able to give a confidence level for its "
"predictions."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:561
msgid "Where to from here"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:563
msgid ""
"Here are a few suggestions to help further your scikit-learn intuition "
"upon the completion of this tutorial:"
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:567
msgid ""
"Try playing around with the ``analyzer`` and ``token normalisation`` "
"under :class:`CountVectorizer`."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:570
msgid ""
"If you don't have labels, try using :ref:`Clustering "
"<sphx_glr_auto_examples_text_plot_document_clustering.py>` on your "
"problem."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:574
msgid ""
"If you have multiple labels per document, e.g categories, have a look at "
"the :ref:`Multiclass and multilabel section <multiclass>`."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:577
msgid ""
"Try using :ref:`Truncated SVD <LSA>` for `latent semantic analysis "
"<https://en.wikipedia.org/wiki/Latent_semantic_analysis>`_."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:580
msgid ""
"Have a look at using :ref:`Out-of-core Classification "
"<sphx_glr_auto_examples_applications_plot_out_of_core_classification.py>`"
" to learn from data that would not fit into the computer main memory."
msgstr ""

#: ../tutorial/text_analytics/working_with_text_data.rst:585
msgid ""
"Have a look at the :ref:`Hashing Vectorizer <hashing_vectorizer>` as a "
"memory efficient alternative to :class:`CountVectorizer`."
msgstr ""

