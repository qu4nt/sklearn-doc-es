msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-05 14:11\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/tutorial/statistical_inference/model_selection.po\n"
"X-Crowdin-File-ID: 4058\n"
"Language: es_ES\n"

#: ../tutorial/statistical_inference/model_selection.rst:5
msgid "Model selection: choosing estimators and their parameters"
msgstr "Selección de modelos: elección de estimadores y sus parámetros"

#: ../tutorial/statistical_inference/model_selection.rst:8
msgid "Score, and cross-validated scores"
msgstr "Puntuación, y puntuaciones validadas de forma cruzada"

#: ../tutorial/statistical_inference/model_selection.rst:10
msgid "As we have seen, every estimator exposes a ``score`` method that can judge the quality of the fit (or the prediction) on new data. **Bigger is better**."
msgstr "Como hemos visto, cada estimador expone un método de ``puntuación`` que puede juzgar la calidad del ajuste (o de la predicción) sobre nuevos datos. **Cuanto más grande, mejor**."

#: ../tutorial/statistical_inference/model_selection.rst:22
msgid "To get a better measure of prediction accuracy (which we can use as a proxy for goodness of fit of the model), we can successively split the data in *folds* that we use for training and testing::"
msgstr "Para obtener una mejor medida de la precisión de la predicción (que podemos utilizar como indicador de la bondad del ajuste del modelo), podemos dividir sucesivamente los datos en *pliegues* que utilizaremos para el entrenamiento y la prueba::"

#: ../tutorial/statistical_inference/model_selection.rst:44
msgid "This is called a :class:`KFold` cross-validation."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:49
msgid "Cross-validation generators"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:51
msgid "Scikit-learn has a collection of classes which can be used to generate lists of train/test indices for popular cross-validation strategies."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:54
msgid "They expose a ``split`` method which accepts the input dataset to be split and yields the train/test set indices for each iteration of the chosen cross-validation strategy."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:58
msgid "This example shows an example usage of the ``split`` method."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:71
msgid "The cross-validation can then be performed easily::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:77
msgid "The cross-validation score can be directly calculated using the :func:`cross_val_score` helper. Given an estimator, the cross-validation object and the input dataset, the :func:`cross_val_score` splits the data repeatedly into a training and a testing set, trains the estimator using the training set and computes the scores based on the testing set for each iteration of cross-validation."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:83
msgid "By default the estimator's ``score`` method is used to compute the individual scores."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:85
msgid "Refer the :ref:`metrics module <metrics>` to learn more on the available scoring methods."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:91
msgid "`n_jobs=-1` means that the computation will be dispatched on all the CPUs of the computer."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:94
msgid "Alternatively, the ``scoring`` argument can be provided to specify an alternative scoring method."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:101
msgid "**Cross-validation generators**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:108
msgid ":class:`KFold` **(n_splits, shuffle, random_state)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:110
msgid ":class:`StratifiedKFold` **(n_splits, shuffle, random_state)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:112
msgid ":class:`GroupKFold` **(n_splits)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:117
msgid "Splits it into K folds, trains on K-1 and then tests on the left-out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:119
msgid "Same as K-Fold but preserves the class distribution within each fold."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:121
#: ../tutorial/statistical_inference/model_selection.rst:140
msgid "Ensures that the same group is not in both testing and training sets."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:128
msgid ":class:`ShuffleSplit` **(n_splits, test_size, train_size, random_state)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:130
msgid ":class:`StratifiedShuffleSplit`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:132
msgid ":class:`GroupShuffleSplit`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:136
msgid "Generates train/test indices based on random permutation."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:138
msgid "Same as shuffle split but preserves the class distribution within each iteration."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:147
msgid ":class:`LeaveOneGroupOut` **()**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:149
msgid ":class:`LeavePGroupsOut`  **(n_groups)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:151
msgid ":class:`LeaveOneOut` **()**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:157
msgid "Takes a group array to group observations."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:159
msgid "Leave P groups out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:161
msgid "Leave one observation out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:169
msgid ":class:`LeavePOut` **(p)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:171
msgid ":class:`PredefinedSplit`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:175
msgid "Leave P observations out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:177
msgid "Generates train/test indices based on predefined splits."
msgstr ""

msgid "**Exercise**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:184
msgid "On the digits dataset, plot the cross-validation score of a :class:`SVC` estimator with an linear kernel as a function of parameter ``C`` (use a logarithmic grid of points, from 1 to 10)."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:196
msgid "**Solution:** :ref:`sphx_glr_auto_examples_exercises_plot_cv_digits.py`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:199
msgid "Grid-search and cross-validated estimators"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:202
msgid "Grid-search"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:206
msgid "scikit-learn provides an object that, given data, computes the score during the fit of an estimator on a parameter grid and chooses the parameters to maximize the cross-validation score. This object takes an estimator during the construction and exposes an estimator API::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:227
msgid "By default, the :class:`GridSearchCV` uses a 5-fold cross-validation. However, if it detects that a classifier is passed, rather than a regressor, it uses a stratified 5-fold."
msgstr ""

msgid "Nested cross-validation"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:238
msgid "Two cross-validation loops are performed in parallel: one by the :class:`GridSearchCV` estimator to set ``gamma`` and the other one by ``cross_val_score`` to measure the prediction performance of the estimator. The resulting scores are unbiased estimates of the prediction score on new data."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:246
msgid "You cannot nest objects with parallel computing (``n_jobs`` different than 1)."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:252
msgid "Cross-validated estimators"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:254
msgid "Cross-validation to set a parameter can be done more efficiently on an algorithm-by-algorithm basis. This is why, for certain estimators, scikit-learn exposes :ref:`cross_validation` estimators that set their parameter automatically by cross-validation::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:268
msgid "These estimators are called similarly to their counterparts, with 'CV' appended to their name."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:273
msgid "On the diabetes dataset, find the optimal regularization parameter alpha."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:276
msgid "**Bonus**: How much can you trust the selection of alpha?"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:281
msgid "**Solution:** :ref:`sphx_glr_auto_examples_exercises_plot_cv_diabetes.py`"
msgstr ""

