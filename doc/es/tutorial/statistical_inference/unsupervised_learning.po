msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 00:15\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/tutorial/statistical_inference/unsupervised_learning.po\n"
"X-Crowdin-File-ID: 3960\n"
"Language: es_ES\n"

#: ../tutorial/statistical_inference/unsupervised_learning.rst:3
msgid "Unsupervised learning: seeking representations of the data"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:6
msgid "Clustering: grouping observations together"
msgstr ""

msgid "The problem solved in clustering"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:10
msgid "Given the iris dataset, if we knew that there were 3 types of iris, but did not have access to a taxonomist to label them: we could try a **clustering task**: split the observations into well-separated group called *clusters*."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:21
msgid "K-means clustering"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:23
msgid "Note that there exist a lot of different clustering criteria and associated algorithms. The simplest clustering algorithm is :ref:`k_means`."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:46
msgid "There is absolutely no guarantee of recovering a ground truth. First, choosing the right number of clusters is hard. Second, the algorithm is sensitive to initialization, and can fall into local minima, although scikit-learn employs several tricks to mitigate this issue."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:57
msgid "**Bad initialization**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:63
msgid "**8 clusters**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:69
msgid "**Ground truth**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:71
msgid "**Don't over-interpret clustering results**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:75
msgid "Clustering in general and KMeans, in particular, can be seen as a way of choosing a small number of exemplars to compress the information. The problem is sometimes known as `vector quantization <https://en.wikipedia.org/wiki/Vector_quantization>`_. For instance, this can be used to posterize an image::"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:100
msgid "**Raw image**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:105
msgid "**K-means quantization**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:110
msgid "**Equal bins**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:116
msgid "**Image histogram**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:119
msgid "Hierarchical agglomerative clustering: Ward"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:121
msgid "A :ref:`hierarchical_clustering` method is a type of cluster analysis that aims to build a hierarchy of clusters. In general, the various approaches of this technique are either:"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:125
msgid "**Agglomerative** - bottom-up approaches: each observation starts in its own cluster, and clusters are iteratively merged in such a way to minimize a *linkage* criterion. This approach is particularly interesting when the clusters of interest are made of only a few observations. When the number of clusters is large, it is much more computationally efficient than k-means."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:132
msgid "**Divisive** - top-down approaches: all observations start in one cluster, which is iteratively split as one moves down the hierarchy. For estimating large numbers of clusters, this approach is both slow (due to all observations starting as one cluster, which it splits recursively) and statistically ill-posed."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:139
msgid "Connectivity-constrained clustering"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:141
msgid "With agglomerative clustering, it is possible to specify which samples can be clustered together by giving a connectivity graph. Graphs in scikit-learn are represented by their adjacency matrix. Often, a sparse matrix is used. This can be useful, for instance, to retrieve connected regions (sometimes also referred to as connected components) when clustering an image."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:163
msgid "We need a vectorized version of the image. `'rescaled_coins'` is a down-scaled version of the coins image to speed up the process::"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:169
msgid "Define the graph structure of the data. Pixels connected to their neighbors::"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:181
msgid "Feature agglomeration"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:183
msgid "We have seen that sparsity could be used to mitigate the curse of dimensionality, *i.e* an insufficient amount of observations compared to the number of features. Another approach is to merge together similar features: **feature agglomeration**. This approach can be implemented by clustering in the feature direction, in other words clustering the transposed data."
msgstr ""

msgid "``transform`` and ``inverse_transform`` methods"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:213
msgid "Some estimators expose a ``transform`` method, for instance to reduce the dimensionality of the dataset."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:217
msgid "Decompositions: from a signal to components and loadings"
msgstr ""

msgid "**Components and loadings**"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:221
msgid "If X is our multivariate data, then the problem that we are trying to solve is to rewrite it on a different observational basis: we want to learn loadings L and a set of components C such that *X = L C*. Different criteria exist to choose the components"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:227
msgid "Principal component analysis: PCA"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:229
msgid ":ref:`PCA` selects the successive components that explain the maximum variance in the signal."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:242
msgid "|pca_3d_axis| |pca_3d_aligned|"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:244
msgid "The point cloud spanned by the observations above is very flat in one direction: one of the three univariate features can almost be exactly computed using the other two. PCA finds the directions in which the data is not *flat*"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:249
msgid "When used to *transform* data, PCA can reduce the dimensionality of the data by projecting on a principal subspace."
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:278
msgid "Independent Component Analysis: ICA"
msgstr ""

#: ../tutorial/statistical_inference/unsupervised_learning.rst:280
msgid ":ref:`ICA` selects components so that the distribution of their loadings carries a maximum amount of independent information. It is able to recover **non-Gaussian** independent signals:"
msgstr ""

