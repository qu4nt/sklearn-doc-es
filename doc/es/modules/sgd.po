msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-27 16:33\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/sgd.po\n"
"X-Crowdin-File-ID: 4814\n"
"Language: es_ES\n"

#: ../modules/sgd.rst:5
msgid "Stochastic Gradient Descent"
msgstr "Descenso de Gradiente Estocástico"

#: ../modules/sgd.rst:9
msgid "**Stochastic Gradient Descent (SGD)** is a simple yet very efficient approach to fitting linear classifiers and regressors under convex loss functions such as (linear) `Support Vector Machines <https://en.wikipedia.org/wiki/Support_vector_machine>`_ and `Logistic Regression <https://en.wikipedia.org/wiki/Logistic_regression>`_. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning."
msgstr "**Descenso de Gradiente Estocástico (Stochastic Gradient Descent, SGD)** es un enfoque simple pero muy eficiente para ajustar clasificadores y regresores lineales bajo funciones de pérdida convexas, como las `Máquinas de Vectores de Soporte <https://en.wikipedia.org/wiki/Support_vector_machine>`_ y `Regresión Logística <https://en.wikipedia.org/wiki/Logistic_regression>`_ (lineales). Aunque el SGD lleva mucho tiempo en la comunidad del aprendizaje automático, ha recibido una atención considerable recientemente en el contexto del aprendizaje a gran escala."

#: ../modules/sgd.rst:18
msgid "SGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing.  Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features."
msgstr "El SGD se ha aplicado con éxito a problemas de aprendizaje automático a gran escala y dispersos, encontrados a menudo en la clasificación de textos y el procesamiento del lenguaje natural. Dado que los datos son dispersos, los clasificadores de este módulo pueden escalar fácilmente a problemas con más de 10^5 ejemplos de entrenamiento y más de 10^5 características."

#: ../modules/sgd.rst:24
msgid "Strictly speaking, SGD is merely an optimization technique and does not correspond to a specific family of machine learning models. It is only a *way* to train a model. Often, an instance of :class:`SGDClassifier` or :class:`SGDRegressor` will have an equivalent estimator in the scikit-learn API, potentially using a different optimization technique. For example, using `SGDClassifier(loss='log')` results in logistic regression, i.e. a model equivalent to :class:`~sklearn.linear_model.LogisticRegression` which is fitted via SGD instead of being fitted by one of the other solvers in :class:`~sklearn.linear_model.LogisticRegression`. Similarly, `SGDRegressor(loss='squared_loss', penalty='l2')` and :class:`~sklearn.linear_model.Ridge` solve the same optimization problem, via different means."
msgstr "En sentido estricto, el SGD no es más que una técnica de optimización y no corresponde a una familia específica de modelos de aprendizaje automático. Es sólo una *forma* de entrenar un modelo. A menudo, una instancia de :class:`SGDClassifier` o :class:`SGDRegressor` tendrá un estimador equivalente en la API de scikit-learn, utilizando potencialmente una técnica de optimización diferente. Por ejemplo, el uso de `SGDClassifier(loss='log')` da como resultado una regresión logística, es decir, un modelo equivalente a :class:`~sklearn.linear_model.LogisticRegression` que se ajusta a través de SGD en lugar de ser ajustado por uno de los otros solucionadores en :class:`~sklearn.linear_model.LogisticRegression`. Del mismo modo, `SGDRegressor(loss='squared_loss', penalty='l2')` y :class:`~sklearn.linear_model.Ridge` resuelven el mismo problema de optimización, a través de diferentes medios."

#: ../modules/sgd.rst:37
msgid "The advantages of Stochastic Gradient Descent are:"
msgstr "Las ventajas del Descenso de Gradiente Estocástico son:"

#: ../modules/sgd.rst:39
msgid "Efficiency."
msgstr "Eficiencia."

#: ../modules/sgd.rst:41
msgid "Ease of implementation (lots of opportunities for code tuning)."
msgstr "Facilidad de implementación (muchas oportunidades para el ajuste del código)."

#: ../modules/sgd.rst:43
msgid "The disadvantages of Stochastic Gradient Descent include:"
msgstr "Las desventajas del Descenso de Gradiente Estocástico incluyen:"

#: ../modules/sgd.rst:45
msgid "SGD requires a number of hyperparameters such as the regularization parameter and the number of iterations."
msgstr "El SGD requiere una serie de hiperparámetros como el parámetro de regularización y el número de iteraciones."

#: ../modules/sgd.rst:48
msgid "SGD is sensitive to feature scaling."
msgstr "El SGD es sensible al escalamiento de características."

#: ../modules/sgd.rst:52
msgid "Make sure you permute (shuffle) your training data before fitting the model or use ``shuffle=True`` to shuffle after each iteration (used by default). Also, ideally, features should be standardized using e.g. `make_pipeline(StandardScaler(), SGDClassifier())` (see :ref:`Pipelines <combining_estimators>`)."
msgstr "Asegúrate de permutar (barajar) tus datos de entrenamiento antes de ajustar el modelo o utiliza ``shuffle=True`` para barajar después de cada iteración (utilizado por defecto). También, idealmente, las características deberían ser estandarizadas usando, por ejemplo, `make_pipeline(StandardScaler(), SGDClassifier())` (ver :ref:`Pipelines <combining_estimators>`)."

#: ../modules/sgd.rst:59
msgid "Classification"
msgstr "Clasificación"

#: ../modules/sgd.rst:62
msgid "The class :class:`SGDClassifier` implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties for classification. Below is the decision boundary of a :class:`SGDClassifier` trained with the hinge loss, equivalent to a linear SVM."
msgstr "La clase :class:`SGDClassifier` implementa una rutina de aprendizaje simple de descenso de gradiente estocástico que soporta diferentes funciones de pérdida y penalizaciones para la clasificación. A continuación se muestra la frontera de decisión de un :class:`SGDClassifier` entrenado con la pérdida de bisagra, equivalente a una SVM lineal."

#: ../modules/sgd.rst:72
msgid "As other classifiers, SGD has to be fitted with two arrays: an array `X` of shape (n_samples, n_features) holding the training samples, and an array y of shape (n_samples,) holding the target values (class labels) for the training samples::"
msgstr "Como otros clasificadores, el SGD tiene que ajustarse con dos arreglos: un arreglo `X` de forma (n_samples, n_features) que contiene las muestras de entrenamiento, y un arreglo `y` de forma (n_samples,) que contiene los valores objetivo (etiquetas de clase) para las muestras de entrenamiento::"

#: ../modules/sgd.rst:85
msgid "After being fitted, the model can then be used to predict new values::"
msgstr "Una vez ajustado, el modelo puede utilizarse para predecir nuevos valores::"

#: ../modules/sgd.rst:90
msgid "SGD fits a linear model to the training data. The ``coef_`` attribute holds the model parameters::"
msgstr "SGD ajusta un modelo lineal a los datos de entrenamiento. El atributo ``coef_`` contiene los parámetros del modelo::"

#: ../modules/sgd.rst:96
msgid "The ``intercept_`` attribute holds the intercept (aka offset or bias)::"
msgstr "El atributo ``intercept_`` contiene el intercepto (también conocido como desplazamiento o sesgo)::"

#: ../modules/sgd.rst:101
msgid "Whether or not the model should use an intercept, i.e. a biased hyperplane, is controlled by the parameter ``fit_intercept``."
msgstr "El parámetro ``fit_intercept`` controla si el modelo debe utilizar o no un intercepto, es decir, un hiperplano sesgado."

#: ../modules/sgd.rst:104
msgid "The signed distance to the hyperplane (computed as the dot product between the coefficients and the input sample, plus the intercept) is given by :meth:`SGDClassifier.decision_function`::"
msgstr "La distancia con signo al hiperplano (calculada como el producto punto entre los coeficientes y la muestra de entrada, más el intercepto) viene dada por :meth:`SGDClassifier.decision_function`::"

#: ../modules/sgd.rst:111
msgid "The concrete loss function can be set via the ``loss`` parameter. :class:`SGDClassifier` supports the following loss functions:"
msgstr "La función de pérdida concreta puede establecerse mediante el parámetro ``loss``. :class:`SGDClassifier` admite las siguientes funciones de pérdida:"

#: ../modules/sgd.rst:114
msgid "``loss=\"hinge\"``: (soft-margin) linear Support Vector Machine,"
msgstr "``loss=\"hinge\"``: (margen blando) Máquina de Vectores de Soporte lineal,"

#: ../modules/sgd.rst:115
msgid "``loss=\"modified_huber\"``: smoothed hinge loss,"
msgstr "``loss=\"modified_huber\"``: pérdida de bisagra suavizada,"

#: ../modules/sgd.rst:116
msgid "``loss=\"log\"``: logistic regression,"
msgstr "``loss=\"log\"``: regresión logística,"

#: ../modules/sgd.rst:117
msgid "and all regression losses below. In this case the target is encoded as -1 or 1, and the problem is treated as a regression problem. The predicted class then correspond to the sign of the predicted target."
msgstr "y todas las pérdidas de regresión más abajo. En este caso, el objetivo se codifica como -1 o 1, y el problema se trata como un problema de regresión. La clase predicha corresponde entonces al signo del objetivo predicho."

#: ../modules/sgd.rst:121
msgid "Please refer to the :ref:`mathematical section below <sgd_mathematical_formulation>` for formulas. The first two loss functions are lazy, they only update the model parameters if an example violates the margin constraint, which makes training very efficient and may result in sparser models (i.e. with more zero coefficents), even when L2 penalty is used."
msgstr "Por favor, consulta :ref:`la sección matemática más abajo <sgd_mathematical_formulation>` para ver las fórmulas. Las dos primeras funciones de pérdida son perezosas, sólo actualizan los parámetros del modelo si un ejemplo viola la restricción de margen, lo que hace que el entrenamiento sea muy eficiente y puede dar lugar a modelos más dispersos (es decir, con más coeficientes cero), incluso cuando se utiliza la penalización L2."

#: ../modules/sgd.rst:128
msgid "Using ``loss=\"log\"`` or ``loss=\"modified_huber\"`` enables the ``predict_proba`` method, which gives a vector of probability estimates :math:`P(y|x)` per sample :math:`x`::"
msgstr "El uso de ``loss=\"log\"` o ``loss=\"modified_huber\"`` habilita el método ``predict_proba``, que da un vector de estimaciones de probabilidad :math:`P(y|x)` por muestra :math:`x`::"

#: ../modules/sgd.rst:136
msgid "The concrete penalty can be set via the ``penalty`` parameter. SGD supports the following penalties:"
msgstr "La penalización concreta puede establecerse mediante el parámetro ``penalty``. SGD admite las siguientes penalizaciones:"

#: ../modules/sgd.rst:139
msgid "``penalty=\"l2\"``: L2 norm penalty on ``coef_``."
msgstr "``penalty=\"l2\"``: Penalización de la norma L2 en ``coef_``."

#: ../modules/sgd.rst:140
msgid "``penalty=\"l1\"``: L1 norm penalty on ``coef_``."
msgstr "``penalty=\"l1\"``: Penalización de la norma L1 en ``coef_``."

#: ../modules/sgd.rst:141
msgid "``penalty=\"elasticnet\"``: Convex combination of L2 and L1; ``(1 - l1_ratio) * L2 + l1_ratio * L1``."
msgstr "``penalty=\"elasticnet\"``: Combinación convexa de L2 y L1; ``(1 - l1_ratio) * L2 + l1_ratio * L1``."

#: ../modules/sgd.rst:144
msgid "The default setting is ``penalty=\"l2\"``. The L1 penalty leads to sparse solutions, driving most coefficients to zero. The Elastic Net [#5]_ solves some deficiencies of the L1 penalty in the presence of highly correlated attributes. The parameter ``l1_ratio`` controls the convex combination of L1 and L2 penalty."
msgstr "La configuración por defecto es ``penalty=\"l2\"``. La penalización L1 conduce a soluciones dispersas, llevando la mayoría de los coeficientes a cero. La Red Elástica [#5]_ resuelve algunas deficiencias de la penalización L1 en presencia de atributos altamente correlacionados. El parámetro ``l1_ratio`` controla la combinación convexa de la penalización L1 y L2."

#: ../modules/sgd.rst:150
msgid ":class:`SGDClassifier` supports multi-class classification by combining multiple binary classifiers in a \"one versus all\" (OVA) scheme. For each of the :math:`K` classes, a binary classifier is learned that discriminates between that and all other :math:`K-1` classes. At testing time, we compute the confidence score (i.e. the signed distances to the hyperplane) for each classifier and choose the class with the highest confidence. The Figure below illustrates the OVA approach on the iris dataset.  The dashed lines represent the three OVA classifiers; the background colors show the decision surface induced by the three classifiers."
msgstr ":class:`SGDClassifier` soporta la clasificación multiclase combinando múltiples clasificadores binarios en un esquema \"uno contra todos\" (OVA). Para cada una de las clases :math:`K`, se aprende un clasificador binario que discrimina entre esa y todas las demás clases :math:`K-1`. En el momento de la prueba, calculamos la puntuación de confianza (es decir, las distancias con signo al hiperplano) de cada clasificador y se elige la clase con mayor confianza. La Figura siguiente ilustra el enfoque OVA en el conjunto de datos del iris. Las líneas discontinuas representan los tres clasificadores OVA; los colores del fondo muestran la superficie de decisión inducida por los tres clasificadores."

#: ../modules/sgd.rst:165
msgid "In the case of multi-class classification ``coef_`` is a two-dimensional array of shape (n_classes, n_features) and ``intercept_`` is a one-dimensional array of shape (n_classes,). The i-th row of ``coef_`` holds the weight vector of the OVA classifier for the i-th class; classes are indexed in ascending order (see attribute ``classes_``). Note that, in principle, since they allow to create a probability model, ``loss=\"log\"`` and ``loss=\"modified_huber\"`` are more suitable for one-vs-all classification."
msgstr "En el caso de la clasificación multiclase, ``coef_`` es un arreglo bidimensional de forma (n_classes, n_features) e ``intercept_`` es un arreglo unidimensional de forma (n_classes,). La fila i-ésima de ``coef_`` contiene el vector de ponderaciones del clasificador OVA para la clase i-ésima; las clases se indexan en orden ascendente (ver el atributo ``classes_``). Ten en cuenta que, en principio, dado que permiten crear un modelo de probabilidad, ``loss=\"log\"`` y ``loss=\"modified_huber\"`` son más adecuados para la clasificación uno contra todos."

#: ../modules/sgd.rst:174
msgid ":class:`SGDClassifier` supports both weighted classes and weighted instances via the fit parameters ``class_weight`` and ``sample_weight``. See the examples below and the docstring of :meth:`SGDClassifier.fit` for further information."
msgstr ":class:`SGDClassifier` soporta tanto clases ponderadas como instancias ponderadas a través de los parámetros de ajuste ``class_weight`` y ``sample_weight``. Para más información, consulta los ejemplos siguientes y la cadena de documentación de :meth:`SGDClassifier.fit`."

#: ../modules/sgd.rst:179
msgid ":class:`SGDClassifier` supports averaged SGD (ASGD) [#4]_. Averaging can be enabled by setting `average=True`. ASGD performs the same updates as the regular SGD (see :ref:`sgd_mathematical_formulation`), but instead of using the last value of the coefficients as the `coef_` attribute (i.e. the values of the last update), `coef_` is set instead to the **average** value of the coefficients across all updates. The same is done for the `intercept_` attribute. When using ASGD the learning rate can be larger and even constant, leading on some datasets to a speed up in training time."
msgstr ":class:`SGDClassifier` soporta SGD promediado (averaged SGD, ASGD) [#4]_. El promediado puede activarse estableciendo `average=True`. ASGD realiza las mismas actualizaciones que el SGD regular (ver :ref:`sgd_mathematical_formulation`), pero en lugar de utilizar el último valor de los coeficientes como atributo `coef_` (es decir, los valores de la última actualización), `coef_` se establece en su lugar en el valor **promedio** de los coeficientes en todas las actualizaciones. Lo mismo se hace con el atributo `intercept_`. Cuando se utiliza el ASGD, la tasa de aprendizaje puede ser mayor e incluso constante, lo que lleva, en algunos conjuntos de datos, a una aceleración del tiempo de entrenamiento."

#: ../modules/sgd.rst:188
msgid "For classification with a logistic loss, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in :class:`LogisticRegression`."
msgstr "Para la clasificación con pérdida logística, otra variante de SGD con una estrategia de promediación está disponible con el algoritmo Gradiente Medio Estocástico (Stochastic Average Gradient , SAG), disponible como un solucionador en :class:`LogisticRegression`."

#: ../modules/sgd.rst:194
msgid ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_separating_hyperplane.py`,"
msgstr ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_separating_hyperplane.py`,"

#: ../modules/sgd.rst:195
msgid ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_iris.py`"
msgstr ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_iris.py`"

#: ../modules/sgd.rst:196
msgid ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_weighted_samples.py`"
msgstr ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_weighted_samples.py`"

#: ../modules/sgd.rst:197
msgid ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_comparison.py`"
msgstr ":ref:`sphx_glr_auto_examples_linear_model_plot_sgd_comparison.py`"

#: ../modules/sgd.rst:198
msgid ":ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py` (See the Note in the example)"
msgstr ":ref:`sphx_glr_auto_examples_svm_plot_separating_hyperplane_unbalanced.py` (Ver la Nota en el ejemplo)"

#: ../modules/sgd.rst:202
msgid "Regression"
msgstr "Regresión"

#: ../modules/sgd.rst:204
msgid "The class :class:`SGDRegressor` implements a plain stochastic gradient descent learning routine which supports different loss functions and penalties to fit linear regression models. :class:`SGDRegressor` is well suited for regression problems with a large number of training samples (> 10.000), for other problems we recommend :class:`Ridge`, :class:`Lasso`, or :class:`ElasticNet`."
msgstr "La clase :class:`SGDRegressor` implementa una rutina de aprendizaje de descenso de gradiente estocástico simple que soporta diferentes funciones de pérdida y penalizaciones para ajustar modelos de regresión lineal. :class:`SGDRegressor` es muy adecuada para problemas de regresión con un gran número de muestras de entrenamiento (> 10.000), para otros problemas recomendamos :class:`Ridge`, :class:`Lasso`, o :class:`ElasticNet`."

#: ../modules/sgd.rst:211
msgid "The concrete loss function can be set via the ``loss`` parameter. :class:`SGDRegressor` supports the following loss functions:"
msgstr "La función de pérdida concreta puede establecerse mediante el parámetro ``loss``. :class:`SGDRegressor` admite las siguientes funciones de pérdida:"

#: ../modules/sgd.rst:214
msgid "``loss=\"squared_loss\"``: Ordinary least squares,"
msgstr "``loss=\"squared_loss\"``: Mínimos cuadrados ordinarios,"

#: ../modules/sgd.rst:215
msgid "``loss=\"huber\"``: Huber loss for robust regression,"
msgstr "``loss=\"huber\"``: Pérdida de Huber para una regresión robusta,"

#: ../modules/sgd.rst:216
msgid "``loss=\"epsilon_insensitive\"``: linear Support Vector Regression."
msgstr "``loss=\"epsilon_insensitive\"``: Regresión de Vectores de Soporte lineal."

#: ../modules/sgd.rst:218
msgid "Please refer to the :ref:`mathematical section below <sgd_mathematical_formulation>` for formulas. The Huber and epsilon-insensitive loss functions can be used for robust regression. The width of the insensitive region has to be specified via the parameter ``epsilon``. This parameter depends on the scale of the target variables."
msgstr "Por favor, consulta las fórmulas en :ref:`la sección matemática más abajo <sgd_mathematical_formulation>`. Las funciones de pérdida Huber e insensibles a épsilon pueden utilizarse para una regresión robusta. La anchura de la región insensible debe especificarse mediante el parámetro ``epsilon``. Este parámetro depende de la escala de las variables objetivo."

#: ../modules/sgd.rst:225
msgid "The `penalty` parameter determines the regularization to be used (see description above in the classification section)."
msgstr "El parámetro `penalty` determina la regularización que se utilizará (ver la descripción anterior en la sección de clasificación)."

#: ../modules/sgd.rst:228
msgid ":class:`SGDRegressor` also supports averaged SGD [#4]_ (here again, see description above in the classification section)."
msgstr ":class:`SGDRegressor` también admite el SGD promediado [#4]_ (de nuevo, ver la descripción anterior en la sección de clasificación)."

#: ../modules/sgd.rst:231
msgid "For regression with a squared loss and a l2 penalty, another variant of SGD with an averaging strategy is available with Stochastic Average Gradient (SAG) algorithm, available as a solver in :class:`Ridge`."
msgstr "Para la regresión con una pérdida cuadrada y una penalización l2, está disponible otra variante de SGD con una estrategia de promediación con el algoritmo Gradiente Medio Estocástico (Stochastic Average Gradient, SAG), disponible como un solucionador en :class:`Ridge`."

#: ../modules/sgd.rst:237
msgid "Stochastic Gradient Descent for sparse data"
msgstr "Descenso de Gradiente Estocástico para datos dispersos"

#: ../modules/sgd.rst:239
msgid "The sparse implementation produces slightly different results from the dense implementation, due to a shrunk learning rate for the intercept. See :ref:`implementation_details`."
msgstr "La implementación dispersa produce resultados ligeramente diferentes de la implementación densa, debido a una tasa de aprendizaje reducida para el intercepto. Ver :ref:`implementation_details`."

#: ../modules/sgd.rst:243
msgid "There is built-in support for sparse data given in any matrix in a format supported by `scipy.sparse <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_. For maximum efficiency, however, use the CSR matrix format as defined in `scipy.sparse.csr_matrix <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_."
msgstr "Hay soporte integrado para datos dispersos dados en cualquier matriz en un formato soportado por `scipy.sparse <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_. Sin embargo, para obtener la máxima eficiencia, utiliza el formato de matriz CSR definido en `scipy.sparse.csr_matrix <https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html>`_."

#: ../modules/sgd.rst:252
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"

#: ../modules/sgd.rst:255
msgid "Complexity"
msgstr "Complejidad"

#: ../modules/sgd.rst:257
msgid "The major advantage of SGD is its efficiency, which is basically linear in the number of training examples. If X is a matrix of size (n, p) training has a cost of :math:`O(k n \\bar p)`, where k is the number of iterations (epochs) and :math:`\\bar p` is the average number of non-zero attributes per sample."
msgstr "La mayor ventaja de SGD es su eficiencia, que es básicamente lineal en el número de ejemplos de entrenamiento. Si X es una matriz de tamaño (n, p) el entrenamiento tiene un costo de :math:`O(k n \\bar p)`, donde k es el número de iteraciones (épocas) y :math:`bar p` es el número promedio de atributos distintos de cero por muestra."

#: ../modules/sgd.rst:263
msgid "Recent theoretical results, however, show that the runtime to get some desired optimization accuracy does not increase as the training set size increases."
msgstr "Sin embargo, los resultados teóricos recientes muestran que el tiempo de ejecución para obtener cierta precisión de optimización deseada no aumenta a medida que aumenta el tamaño del conjunto de entrenamiento."

#: ../modules/sgd.rst:267
msgid "Stopping criterion"
msgstr "Criterio de parada"

#: ../modules/sgd.rst:269
msgid "The classes :class:`SGDClassifier` and :class:`SGDRegressor` provide two criteria to stop the algorithm when a given level of convergence is reached:"
msgstr "Las clases :class:`SGDClassifier` y :class:`SGDRegressor` proporcionan dos criterios para detener el algoritmo cuando se alcanza un determinado nivel de convergencia:"

#: ../modules/sgd.rst:272
msgid "With ``early_stopping=True``, the input data is split into a training set and a validation set. The model is then fitted on the training set, and the stopping criterion is based on the prediction score (using the `score` method) computed on the validation set. The size of the validation set can be changed with the parameter ``validation_fraction``."
msgstr "Con ``early_stopping=True``, los datos de entrada se dividen en un conjunto de entrenamiento y un conjunto de validación. El modelo se ajusta al conjunto de entrenamiento y el criterio de parada se basa en la puntuación de la predicción (utilizando el método `score`) calculada en el conjunto de validación. El tamaño del conjunto de validación puede modificarse con el parámetro ``validation_fraction``."

#: ../modules/sgd.rst:277
msgid "With ``early_stopping=False``, the model is fitted on the entire input data and the stopping criterion is based on the objective function computed on the training data."
msgstr "Con ``early_stopping=False``, el modelo se ajusta a todos los datos de entrada y el criterio de parada se basa en la función objetivo calculada en los datos de entrenamiento."

#: ../modules/sgd.rst:281
msgid "In both cases, the criterion is evaluated once by epoch, and the algorithm stops when the criterion does not improve ``n_iter_no_change`` times in a row. The improvement is evaluated with absolute tolerance ``tol``, and the algorithm stops in any case after a maximum number of iteration ``max_iter``."
msgstr "En ambos casos, el criterio se evalúa una vez por época, y el algoritmo se detiene cuando el criterio no mejora ``n_iter_no_change`` veces seguidas. La mejora se evalúa con la tolerancia absoluta ``tol``, y el algoritmo se detiene en cualquier caso después de un número máximo de iteraciones ``max_iter``."

#: ../modules/sgd.rst:288
msgid "Tips on Practical Use"
msgstr "Consejos de Uso Práctico"

#: ../modules/sgd.rst:290
msgid "Stochastic Gradient Descent is sensitive to feature scaling, so it is highly recommended to scale your data. For example, scale each attribute on the input vector X to [0,1] or [-1,+1], or standardize it to have mean 0 and variance 1. Note that the *same* scaling must be applied to the test vector to obtain meaningful results. This can be easily done using :class:`StandardScaler`::"
msgstr "El Descenso de Gradiente Estocástico es sensible al escalamiento de las características, por lo que es muy recomendable escalar tus datos. Por ejemplo, escala cada atributo del vector de entrada X a [0,1] o [-1,+1], o estandarízalo para que tenga media 0 y varianza 1. Ten en cuenta que el *mismo* escalamiento debe aplicarse al vector de prueba para obtener resultados significativos. Esto puede hacerse fácilmente utilizando :class:`StandardScaler`::"

#: ../modules/sgd.rst:309
msgid "If your attributes have an intrinsic scale (e.g. word frequencies or indicator features) scaling is not needed."
msgstr "Si tus atributos tienen una escala intrínseca (por ejemplo, las frecuencias de las palabras o las características de los indicadores) el escalamiento no es necesario."

#: ../modules/sgd.rst:312
msgid "Finding a reasonable regularization term :math:`\\alpha` is best done using automatic hyper-parameter search, e.g. :class:`~sklearn.model_selection.GridSearchCV` or :class:`~sklearn.model_selection.RandomizedSearchCV`, usually in the range ``10.0**-np.arange(1,7)``."
msgstr "La mejor manera de encontrar un término de regularización razonable :math:`\\alpha` es utilizando la búsqueda automática de hiperparámetros, por ejemplo, :class:`~sklearn.model_selection.GridSearchCV` o :class:`~sklearn.model_selection.RandomizedSearchCV`, normalmente en el rango ``10.0**-np.arange(1,7)``."

#: ../modules/sgd.rst:318
msgid "Empirically, we found that SGD converges after observing approximately 10^6 training samples. Thus, a reasonable first guess for the number of iterations is ``max_iter = np.ceil(10**6 / n)``, where ``n`` is the size of the training set."
msgstr "Empíricamente, encontramos que el SGD converge después de observar aproximadamente 10^6 muestras de entrenamiento. Por tanto, una primera estimación razonable del número de iteraciones es ``max_iter = np.ceil(10**6 / n)``, donde ``n`` es el tamaño del conjunto de entrenamiento."

#: ../modules/sgd.rst:323
msgid "If you apply SGD to features extracted using PCA we found that it is often wise to scale the feature values by some constant `c` such that the average L2 norm of the training data equals one."
msgstr "Si se aplica el SGD a las características extraídas mediante el PCA, descubrimos que a menudo es prudente escalar los valores de las características mediante una constante \"c\" tal que la norma L2 promedio de los datos de entrenamiento sea igual a uno."

#: ../modules/sgd.rst:327
msgid "We found that Averaged SGD works best with a larger number of features and a higher eta0"
msgstr "Descubrimos que la SGD Promediada funciona mejor con un mayor número de características y una eta0 más alta"

#: ../modules/sgd.rst:332
msgid "`\"Efficient BackProp\" <http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>`_ Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998."
msgstr "`\"Efficient BackProp\" <http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf>`_ Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: Tricks of the Trade 1998."

#: ../modules/sgd.rst:339
msgid "Mathematical formulation"
msgstr "Formulación matemática"

#: ../modules/sgd.rst:341
msgid "We describe here the mathematical details of the SGD procedure. A good overview with convergence rates can be found in [#6]_."
msgstr "Describimos aquí los detalles matemáticos del procedimiento SGD. Un buen resumen con las tasas de convergencia se puede encontrar en [#6]_."

#: ../modules/sgd.rst:344
msgid "Given a set of training examples :math:`(x_1, y_1), \\ldots, (x_n, y_n)` where :math:`x_i \\in \\mathbf{R}^m` and :math:`y_i \\in \\mathcal{R}` (:math:`y_i \\in {-1, 1}` for classification), our goal is to learn a linear scoring function :math:`f(x) = w^T x + b` with model parameters :math:`w \\in \\mathbf{R}^m` and intercept :math:`b \\in \\mathbf{R}`. In order to make predictions for binary classification, we simply look at the sign of :math:`f(x)`. To find the model parameters, we minimize the regularized training error given by"
msgstr "Dado un conjunto de ejemplos de entrenamiento :math:`(x_1, y_1), \\ldots, (x_n, y_n)` donde :math:`x_i \\in \\mathbf{R}^m` y :math:`y_i \\in \\mathcal{R}` (:math: `y_i \\in {-1, 1}` para la clasificación), nuestro objetivo es aprender una función de puntuación lineal :math:`f(x) = w^T x + b` con parámetros de modelo :math:`w \\in \\mathbf{R}^m` e intercepto :math:`b \\in \\mathbf{R}`. Para hacer predicciones para la clasificación binaria, simplemente revisamos el signo de :math:`f(x)`. Para encontrar los parámetros del modelo, minimizamos el error de entrenamiento regularizado dado por"

#: ../modules/sgd.rst:352
msgid "E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\alpha R(w)"
msgstr "E(w,b) = \\frac{1}{n}\\sum_{i=1}^{n} L(y_i, f(x_i)) + \\alpha R(w)"

#: ../modules/sgd.rst:356
msgid "where :math:`L` is a loss function that measures model (mis)fit and :math:`R` is a regularization term (aka penalty) that penalizes model complexity; :math:`\\alpha > 0` is a non-negative hyperparameter that controls the regularization stength."
msgstr "donde :math:`L` es una función de pérdida que mide el (des)ajuste del modelo y :math:`R` es un término de regularización (también conocido como penalización) que penaliza la complejidad del modelo; :math:`\\alpha > 0` es un hiperparámetro no negativo que controla la fuerza de la regularización."

#: ../modules/sgd.rst:361
msgid "Different choices for :math:`L` entail different classifiers or regressors:"
msgstr "Diferentes elecciones de :math:`L` implican diferentes clasificadores o regresores:"

#: ../modules/sgd.rst:363
msgid "Hinge (soft-margin): equivalent to Support Vector Classification. :math:`L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))`."
msgstr "Bisagra (margen blando): equivalente a la Clasificación por Vectores de Soporte. :math:`L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))`."

#: ../modules/sgd.rst:365
msgid "Perceptron: :math:`L(y_i, f(x_i)) = \\max(0, - y_i f(x_i))`."
msgstr "Perceptrón: :math:`L(y_i, f(x_i)) = \\max(0, - y_i f(x_i))`."

#: ../modules/sgd.rst:367
msgid "Modified Huber: :math:`L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))^2` if :math:`y_i f(x_i) > 1`, and :math:`L(y_i, f(x_i)) = -4 y_i f(x_i)` otherwise."
msgstr "Huber modificado: :math:`L(y_i, f(x_i)) = \\max(0, 1 - y_i f(x_i))^2` si :math:`y_i f(x_i) > 1`, y :math:`L(y_i, f(x_i)) = -4 y_i f(x_i)` en caso contrario."

#: ../modules/sgd.rst:370
msgid "Log: equivalent to Logistic Regression. :math:`L(y_i, f(x_i)) = \\log(1 + \\exp (-y_i f(x_i)))`."
msgstr "Log: equivalente a la Regresión Logística. :math:`L(y_i, f(x_i)) = \\log(1 + \\exp (-y_i f(x_i)))`."

#: ../modules/sgd.rst:372
msgid "Least-Squares: Linear regression (Ridge or Lasso depending on :math:`R`). :math:`L(y_i, f(x_i)) = \\frac{1}{2}(y_i - f(x_i))^2`."
msgstr "Mínimos cuadrados: Regresión lineal (Ridge o Lasso dependiendo de :math:`R`). :math:`L(y_i, f(x_i)) = \\frac{1}{2}(y_i - f(x_i))^2`."

#: ../modules/sgd.rst:375
msgid "Huber: less sensitive to outliers than least-squares. It is equivalent to least squares when :math:`|y_i - f(x_i)| \\leq \\varepsilon`, and :math:`L(y_i, f(x_i)) = \\varepsilon |y_i - f(x_i)| - \\frac{1}{2} \\varepsilon^2` otherwise."
msgstr "Huber: menos sensible a los valores atípicos que los mínimos cuadrados. Es equivalente a los mínimos cuadrados cuando :math:`|y_i - f(x_i)|leq \\varepsilon`, y :math:`L(y_i, f(x_i)) = \\varepsilon |y_i - f(x_i)| - \\frac{1}{2} \\varepsilon^2` de lo contrario."

#: ../modules/sgd.rst:379
msgid "Epsilon-Insensitive: (soft-margin) equivalent to Support Vector Regression. :math:`L(y_i, f(x_i)) = \\max(0, |y_i - f(x_i)| - \\varepsilon)`."
msgstr "Insensible a épsilon: (margen blando) equivalente a la Regresión de Vectores de Soporte. :math:`L(y_i, f(x_i)) = \\max(0, |y_i - f(x_i)| - \\varepsilon)`."

#: ../modules/sgd.rst:382
msgid "All of the above loss functions can be regarded as an upper bound on the misclassification error (Zero-one loss) as shown in the Figure below."
msgstr "Todas las funciones de pérdida anteriores pueden considerarse como un límite superior del error de clasificación incorrecta (pérdida de cero a uno), como se muestra en la siguiente Figura."

#: ../modules/sgd.rst:390
msgid "Popular choices for the regularization term :math:`R` (the `penalty` parameter) include:"
msgstr "Las opciones populares para el término de regularización :math:`R` (el parámetro `penalty`) incluyen:"

#: ../modules/sgd.rst:393
msgid "L2 norm: :math:`R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2`,"
msgstr "Norma L2: :math:`R(w) := \\frac{1}{2} \\sum_{j=1}^{m} w_j^2 = ||w||_2^2`,"

#: ../modules/sgd.rst:394
msgid "L1 norm: :math:`R(w) := \\sum_{j=1}^{m} |w_j|`, which leads to sparse solutions."
msgstr "Norma L1: :math:`R(w) := \\sum_{j=1}^{m} |w_j|`, lo que conduce a soluciones dispersas."

#: ../modules/sgd.rst:396
msgid "Elastic Net: :math:`R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 + (1-\\rho) \\sum_{j=1}^{m} |w_j|`, a convex combination of L2 and L1, where :math:`\\rho` is given by ``1 - l1_ratio``."
msgstr "Red elástica: :math:`R(w) := \\frac{\\rho}{2} \\sum_{j=1}^{n} w_j^2 + (1-\\rho) \\sum_{j=1}^{m} |w_j|`, una combinación convexa de L2 y L1, donde :math:`\\rho` viene dado por ``1 - l1_ratio``."

#: ../modules/sgd.rst:400
msgid "The Figure below shows the contours of the different regularization terms in a 2-dimensional parameter space (:math:`m=2`) when :math:`R(w) = 1`."
msgstr "La Figura siguiente muestra los contornos de los diferentes términos de regularización en un espacio de parámetros bidimensional (:math:`m=2`) cuando :math:`R(w) = 1`."

#: ../modules/sgd.rst:409
msgid "SGD"
msgstr "SGD"

#: ../modules/sgd.rst:411
msgid "Stochastic gradient descent is an optimization method for unconstrained optimization problems. In contrast to (batch) gradient descent, SGD approximates the true gradient of :math:`E(w,b)` by considering a single training example at a time."
msgstr "El descenso de gradiente estocástico es un método de optimización para problemas de optimización sin restricciones. A diferencia del descenso de gradiente (por lotes), el SGD se aproxima al verdadero gradiente de :math:`E(w,b)` considerando un solo ejemplo de entrenamiento a la vez."

#: ../modules/sgd.rst:416
msgid "The class :class:`SGDClassifier` implements a first-order SGD learning routine.  The algorithm iterates over the training examples and for each example updates the model parameters according to the update rule given by"
msgstr "La clase :class:`SGDClassifier` implementa una rutina de aprendizaje SGD de primer orden. El algoritmo itera sobre los ejemplos de entrenamiento y para cada ejemplo actualiza los parámetros del modelo según la regla de actualización dada por"

#: ../modules/sgd.rst:420
msgid "w \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w}\n"
"+ \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]"
msgstr "w \\leftarrow w - \\eta \\left[\\alpha \\frac{\\partial R(w)}{\\partial w}\n"
"+ \\frac{\\partial L(w^T x_i + b, y_i)}{\\partial w}\\right]"

#: ../modules/sgd.rst:425
msgid "where :math:`\\eta` is the learning rate which controls the step-size in the parameter space.  The intercept :math:`b` is updated similarly but without regularization (and with additional decay for sparse matrices, as detailed in :ref:`implementation_details`)."
msgstr "donde :math:`\\eta` es la tasa de aprendizaje que controla el tamaño del paso en el espacio de parámetros. El intercepto :math:`b` se actualiza de forma similar pero sin regularización (y con un decaimiento adicional para matrices dispersas, como se detalla en :ref:`implementation_details`)."

#: ../modules/sgd.rst:430
msgid "The learning rate :math:`\\eta` can be either constant or gradually decaying. For classification, the default learning rate schedule (``learning_rate='optimal'``) is given by"
msgstr "La tasa de aprendizaje :math:`\\eta` puede ser constante o gradualmente decreciente. Para la clasificación, el programa de tasa de aprendizaje por defecto (``learning_rate='optimal'``) viene dado por"

#: ../modules/sgd.rst:434
msgid "\\eta^{(t)} = \\frac {1}{\\alpha  (t_0 + t)}"
msgstr "\\eta^{(t)} = \\frac {1}{\\alpha  (t_0 + t)}"

#: ../modules/sgd.rst:438
msgid "where :math:`t` is the time step (there are a total of `n_samples * n_iter` time steps), :math:`t_0` is determined based on a heuristic proposed by Léon Bottou such that the expected initial updates are comparable with the expected size of the weights (this assuming that the norm of the training samples is approx. 1). The exact definition can be found in ``_init_t`` in :class:`BaseSGD`."
msgstr "donde :math:`t` es el paso de tiempo (hay un total de `n_samples * n_iter` pasos de tiempo), :math:`t_0` se determina en base a una heurística propuesta por Léon Bottou de forma que las actualizaciones iniciales esperadas son comparables con el tamaño esperado de las ponderaciones (esto suponiendo que la norma de las muestras de entrenamiento es aproximadamente 1). La definición exacta se puede encontrar en ``_init_t`` en :class:`BaseSGD`."

#: ../modules/sgd.rst:445
msgid "For regression the default learning rate schedule is inverse scaling (``learning_rate='invscaling'``), given by"
msgstr "Para la regresión, el programa de la tasa de aprendizaje por defecto es el escalamiento inverso (``learning_rate='invscaling'``), dado por"

#: ../modules/sgd.rst:448
msgid "\\eta^{(t)} = \\frac{eta_0}{t^{power\\_t}}"
msgstr "\\eta^{(t)} = \\frac{eta_0}{t^{power\\_t}}"

#: ../modules/sgd.rst:452
msgid "where :math:`eta_0` and :math:`power\\_t` are hyperparameters chosen by the user via ``eta0`` and ``power_t``, resp."
msgstr "donde :math:`eta_0` y :math:`power\\_t` son hiperparámetros elegidos por el usuario mediante ``eta0`` y ``power_t``, respectivamente."

#: ../modules/sgd.rst:455
msgid "For a constant learning rate use ``learning_rate='constant'`` and use ``eta0`` to specify the learning rate."
msgstr "Para una tasa de aprendizaje constante, utiliza ``learning_rate='constant'`` y utiliza ``eta0`` para especificar la tasa de aprendizaje."

#: ../modules/sgd.rst:458
msgid "For an adaptively decreasing learning rate, use ``learning_rate='adaptive'`` and use ``eta0`` to specify the starting learning rate. When the stopping criterion is reached, the learning rate is divided by 5, and the algorithm does not stop. The algorithm stops when the learning rate goes below 1e-6."
msgstr "Para una tasa de aprendizaje adaptativamente decreciente, utiliza ``learning_rate='adaptive'`` y utiliza ``eta0`` para especificar la tasa de aprendizaje inicial. Cuando se alcanza el criterio de parada, la tasa de aprendizaje es dividida entre 5, y el algoritmo no se detiene. El algoritmo se detiene cuando la tasa de aprendizaje es inferior a 1e-6."

#: ../modules/sgd.rst:463
msgid "The model parameters can be accessed through the ``coef_`` and ``intercept_`` attributes: ``coef_`` holds the weights :math:`w` and ``intercept_`` holds :math:`b`."
msgstr "Se puede acceder a los parámetros del modelo a través de los atributos ``coef_`` y ``intercept_``: ``coef_`` contiene las ponderaciones :math:`w` e ``intercept_`` contiene :math:`b`."

#: ../modules/sgd.rst:467
msgid "When using Averaged SGD (with the `average` parameter), `coef_` is set to the average weight across all updates: `coef_` :math:`= \\frac{1}{T} \\sum_{t=0}^{T-1} w^{(t)}`, where :math:`T` is the total number of updates, found in the `t_` attribute."
msgstr "Cuando se utiliza el SGD Promediado (con el parámetro `average`), `coef_` se establece en la ponderación promedio de todas las actualizaciones: `coef_` :math:`= \\frac{1}{T} \\sum_{t=0}^{T-1} w^{(t)}`, donde :math:`T` es el número total de actualizaciones, encontradas en el atributo `t_`."

#: ../modules/sgd.rst:475
msgid "Implementation details"
msgstr "Detalles de implementación"

#: ../modules/sgd.rst:477
msgid "The implementation of SGD is influenced by the `Stochastic Gradient SVM` of [#1]_. Similar to SvmSGD, the weight vector is represented as the product of a scalar and a vector which allows an efficient weight update in the case of L2 regularization. In the case of sparse input `X`, the intercept is updated with a smaller learning rate (multiplied by 0.01) to account for the fact that it is updated more frequently. Training examples are picked up sequentially and the learning rate is lowered after each observed example. We adopted the learning rate schedule from [#2]_. For multi-class classification, a \"one versus all\" approach is used. We use the truncated gradient algorithm proposed in [#3]_ for L1 regularization (and the Elastic Net). The code is written in Cython."
msgstr "La implementación de SGD está influenciada por el `Stochastic Gradient SVM` de [#1]_. De forma similar a SvmSGD, el vector de ponderaciones se representa como el producto de un escalar y un vector que permite una actualización eficiente de las ponderaciones en el caso de la regularización L2. En el caso de una entrada dispersa `X`, el intercepto se actualiza con una tasa de aprendizaje menor (multiplicada por 0,01) para tener en cuenta el hecho de que se actualiza con mayor frecuencia. Los ejemplos de entrenamiento se recogen secuencialmente y la tasa de aprendizaje se reduce después de cada ejemplo observado. Adoptamos el programa de tasa de aprendizaje de [#2]_. Para la clasificación multiclase, se utiliza un enfoque de \"uno contra todos\". Utilizamos el algoritmo de gradiente truncado propuesto en [#3]_ para la regularización L1 (y la Red Elástica). El código está escrito en Cython."

#: ../modules/sgd.rst:494
msgid "`\"Stochastic Gradient Descent\" <https://leon.bottou.org/projects/sgd>`_ L. Bottou - Website, 2010."
msgstr "`\"Stochastic Gradient Descent\" <https://leon.bottou.org/projects/sgd>`_ L. Bottou - Website, 2010."

#: ../modules/sgd.rst:497
msgid "`\"Pegasos: Primal estimated sub-gradient solver for svm\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513>`_ S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07."
msgstr "`\"Pegasos: Primal estimated sub-gradient solver for svm\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.74.8513>`_ S. Shalev-Shwartz, Y. Singer, N. Srebro - In Proceedings of ICML '07."

#: ../modules/sgd.rst:501
msgid "`\"Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty\" <https://www.aclweb.org/anthology/P/P09/P09-1054.pdf>`_ Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL '09."
msgstr "`\"Stochastic gradient descent training for l1-regularized log-linear models with cumulative penalty\" <https://www.aclweb.org/anthology/P/P09/P09-1054.pdf>`_ Y. Tsuruoka, J. Tsujii, S. Ananiadou - In Proceedings of the AFNLP/ACL '09."

#: ../modules/sgd.rst:507
msgid "`\"Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent\" <https://arxiv.org/pdf/1107.2490v2.pdf>`_ Xu, Wei"
msgstr "`\"Towards Optimal One Pass Large Scale Learning with Averaged Stochastic Gradient Descent\" <https://arxiv.org/pdf/1107.2490v2.pdf>`_ Xu, Wei"

#: ../modules/sgd.rst:512
msgid "`\"Regularization and variable selection via the elastic net\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696>`_ H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320."
msgstr "`\"Regularization and variable selection via the elastic net\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.124.4696>`_ H. Zou, T. Hastie - Journal of the Royal Statistical Society Series B, 67 (2), 301-320."

#: ../modules/sgd.rst:517
msgid "`\"Solving large scale linear prediction problems using stochastic gradient descent algorithms\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377>`_ T. Zhang - In Proceedings of ICML '04."
msgstr "`\"Solving large scale linear prediction problems using stochastic gradient descent algorithms\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.58.7377>`_ T. Zhang - In Proceedings of ICML '04."

