msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-01 20:24\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/manifold.po\n"
"X-Crowdin-File-ID: 5932\n"
"Language: es_ES\n"

#: ../modules/manifold.rst:8
msgid "Manifold learning"
msgstr "Aprendizaje múltiple"

#: ../modules/manifold.rst
msgid "Look for the bare necessities"
msgstr "Busca lo más vital, no más"

#: ../modules/manifold.rst
msgid "The simple bare necessities"
msgstr "Lo que es necesidad no más"

#: ../modules/manifold.rst
msgid "Forget about your worries and your strife"
msgstr "Y olvídate de la preocupación"

#: ../modules/manifold.rst
msgid "I mean the bare necessities"
msgstr "Tan sólo, lo muy esencial"

#: ../modules/manifold.rst
msgid "Old Mother Nature's recipes"
msgstr "Para vivir sin batallar"

#: ../modules/manifold.rst
msgid "That bring the bare necessities of life"
msgstr "Y la naturaleza te lo da"

#: ../modules/manifold.rst
msgid "-- Baloo's song [The Jungle Book]"
msgstr "-- Canción de Baloo [El Libro de la Selva]"

#: ../modules/manifold.rst:28
msgid "Manifold learning is an approach to non-linear dimensionality reduction. Algorithms for this task are based on the idea that the dimensionality of many data sets is only artificially high."
msgstr "El aprendizaje múltiple es un acercamiento a la reducción de dimensionalidad no lineal. Los algoritmos para esta tarea se basan en la idea de que la dimensionalidad de muchos conjuntos de datos es sólo alta artificialmente."

#: ../modules/manifold.rst:34
msgid "Introduction"
msgstr "Introducción"

#: ../modules/manifold.rst:36
msgid "High-dimensional datasets can be very difficult to visualize.  While data in two or three dimensions can be plotted to show the inherent structure of the data, equivalent high-dimensional plots are much less intuitive.  To aid visualization of the structure of a dataset, the dimension must be reduced in some way."
msgstr "Los conjuntos de datos de alta dimensión pueden ser muy difíciles de visualizar. Mientras que los datos en dos o tres dimensiones se pueden graficar para mostrar la estructura inherente de los datos, los graficos equivalentes de alta dimensión son mucho menos intuitivos. Para ayudar a la visualización de la estructura de un conjunto de datos, la dimensión debe ser reducida de alguna forma."

#: ../modules/manifold.rst:42
msgid "The simplest way to accomplish this dimensionality reduction is by taking a random projection of the data.  Though this allows some degree of visualization of the data structure, the randomness of the choice leaves much to be desired.  In a random projection, it is likely that the more interesting structure within the data will be lost."
msgstr "La forma mas sencilla de lograr esta reducción de dimensionalidad es tomando una proyección aleatoria de los datos. Aunque esto permite cierto grado de visualización de la estructura de datos, la aleatoridad de la elección deja mucho que ser deseado. En una proyección aleatoria, es probable que las estructuras mas interesantes dentro de los datos se pierdan."

#: ../modules/manifold.rst:59
msgid "digits_img projected_img"
msgstr "digits_img projected_img"

#: ../modules/manifold.rst:60
msgid "To address this concern, a number of supervised and unsupervised linear dimensionality reduction frameworks have been designed, such as Principal Component Analysis (PCA), Independent Component Analysis, Linear Discriminant Analysis, and others.  These algorithms define specific rubrics to choose an \"interesting\" linear projection of the data. These methods can be powerful, but often miss important non-linear structure in the data."
msgstr "Para abordar esta inquietud, un número de frameworks de reducción de dimensionalidad lineal supervisada y no supervisada han sido diseñados, como el Análisis Principal de Componentes (APC, o PCA en ingles), Análisis Independiente de Componentes, Análisis Discriminante Lineal, y otros. Estos algoritmos definen rubricas especificas para escoger una proyección lineal \"interesante\" de los datos. Estos métodos pueden ser poderosos, pero frecuentemente pierden estructuras importantes no lineales en los datos."

#: ../modules/manifold.rst:78
msgid "PCA_img LDA_img"
msgstr "PCA_img LDA_img"

#: ../modules/manifold.rst:79
msgid "Manifold Learning can be thought of as an attempt to generalize linear frameworks like PCA to be sensitive to non-linear structure in data. Though supervised variants exist, the typical manifold learning problem is unsupervised: it learns the high-dimensional structure of the data from the data itself, without the use of predetermined classifications."
msgstr "El aprendizaje múltiple puede ser considerado como un intento de generalizar frameworks lineales como el APC para ser sensibles a estructuras no lineales en los datos. Aunque existen variantes supervisadas, el problema típico de aprendizaje múltiple es sin supervisión, aprende la estructura de alta-dimensionalidad de los datos dentro de los propios datos, sin el uso de clasificaciones predeterminadas."

#: ../modules/manifold.rst:88
msgid "See :ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py` for an example of dimensionality reduction on handwritten digits."
msgstr "Vea :ref:`sphx_glr_auto_examples_manifold_plot_lle_digits.py` para un ejemplo de reducción de dimensionalidad en dígitos escritos a mano."

#: ../modules/manifold.rst:91
msgid "See :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py` for an example of dimensionality reduction on a toy \"S-curve\" dataset."
msgstr "Vea :ref:`sphx_glr_auto_examples_manifold_plot_compare_methods.py` para un ejemplo de reducción de dimensionalidad en un conjunto de datos \"S-curve\" de juguete."

#: ../modules/manifold.rst:94
msgid "The manifold learning implementations available in scikit-learn are summarized below"
msgstr "Las implementaciones del aprendizaje múltiple disponibles en scikit-learn se resumen a continuación"

#: ../modules/manifold.rst:100
msgid "Isomap"
msgstr "Isomap"

#: ../modules/manifold.rst:102
msgid "One of the earliest approaches to manifold learning is the Isomap algorithm, short for Isometric Mapping.  Isomap can be viewed as an extension of Multi-dimensional Scaling (MDS) or Kernel PCA. Isomap seeks a lower-dimensional embedding which maintains geodesic distances between all points.  Isomap can be performed with the object :class:`Isomap`."
msgstr "Uno de los acercamientos más antiguos al aprendizaje múltiple es el algoritmo Isomap, abreviatura de Mapeo Isometrico. Isomap puede ser visto como una extensión del Escalado Multi-dimensional (EMD, o MDS en inglés) o APC Kernel. Isomap busca un embedding de dimensionalidad mas baja que mantiene distancias geodésicas entre todos los puntos. Se puede utilizar Isomap con el objeto :class:`Isomap`."

#: ../modules/manifold.rst:115 ../modules/manifold.rst:172
#: ../modules/manifold.rst:226 ../modules/manifold.rst:276
#: ../modules/manifold.rst:320 ../modules/manifold.rst:368
msgid "Complexity"
msgstr "Complejidad"

#: ../modules/manifold.rst:116
msgid "The Isomap algorithm comprises three stages:"
msgstr "El algoritmo Isomap comprende tres etapas:"

#: ../modules/manifold.rst:118
msgid "**Nearest neighbor search.**  Isomap uses :class:`~sklearn.neighbors.BallTree` for efficient neighbor search. The cost is approximately :math:`O[D \\log(k) N \\log(N)]`, for :math:`k` nearest neighbors of :math:`N` points in :math:`D` dimensions."
msgstr "**Búsqueda de vecino más cercana.** Isomap utiliza :class:`~sklearn.neighbors.BallTree` para una búsqueda eficiente de vecinos. El costo es de aproximadamente :math:`O[D \\log(k) N \\log(N)]`, para :math:`k` vecinos más cercanos de :math:`N` en :math:`D` dimensiones."

#: ../modules/manifold.rst:123
msgid "**Shortest-path graph search.**  The most efficient known algorithms for this are *Dijkstra's Algorithm*, which is approximately :math:`O[N^2(k + \\log(N))]`, or the *Floyd-Warshall algorithm*, which is :math:`O[N^3]`.  The algorithm can be selected by the user with the ``path_method`` keyword of ``Isomap``.  If unspecified, the code attempts to choose the best algorithm for the input data."
msgstr "**Búsqueda en el gráfico del camino más corto.** Los algoritmos conocidos más eficientes para esto son el *Algoritmo de Dijkstra*, el cual es aproximadamente :math:`O[N^2(k + \\log(N))]`, o el *Algoritmo Floyd-Warshall*, que es :math:`O[N^3]`. El algoritmo puede ser seleccionado por el usuario con la palabra clave ``path_method`` de ``Isomap``. Si se deja sin especificar, el código intenta escoger el mejor algoritmo para los dato de entrada."

#: ../modules/manifold.rst:130
msgid "**Partial eigenvalue decomposition.**  The embedding is encoded in the eigenvectors corresponding to the :math:`d` largest eigenvalues of the :math:`N \\times N` isomap kernel.  For a dense solver, the cost is approximately :math:`O[d N^2]`.  This cost can often be improved using the ``ARPACK`` solver.  The eigensolver can be specified by the user with the ``eigen_solver`` keyword of ``Isomap``.  If unspecified, the code attempts to choose the best algorithm for the input data."
msgstr "**Descomposición parcial del autovalor.** El embedding está codificado en los autovectores correspondiendo a los autovalores más altos :math:`d` del kernel isomap :math:`N \\times N`. Para un solucionador denso, el costo es aproximadamente :math:`O[d N^2]`. Este costo puede frecuentemente ser mejorado utilizando el solucionador ``ARPACK``. El autosolucionador puede ser especificado por el usuario con la palabra clave ``eigen_solver`` de ``Isomap``. Si se deja sin especificar, el código intenta escoger el mejor algoritmo para los datos de entrada."

#: ../modules/manifold.rst:138
msgid "The overall complexity of Isomap is :math:`O[D \\log(k) N \\log(N)] + O[N^2(k + \\log(N))] + O[d N^2]`."
msgstr "La complejidad general de Isomap es :math:`O[D \\\\log(k) N \\\\log(N)] + O[N^2(k + \\\\log(N))] + O[d N^2]`."

#: ../modules/manifold.rst:141 ../modules/manifold.rst:188
#: ../modules/manifold.rst:244 ../modules/manifold.rst:292
#: ../modules/manifold.rst:337 ../modules/manifold.rst:383
msgid ":math:`N` : number of training data points"
msgstr ":math:`N` : número de puntos de datos de entrenamiento"

#: ../modules/manifold.rst:142 ../modules/manifold.rst:189
#: ../modules/manifold.rst:245 ../modules/manifold.rst:293
#: ../modules/manifold.rst:338 ../modules/manifold.rst:384
msgid ":math:`D` : input dimension"
msgstr ":math:`D`: dimensión de entrada"

#: ../modules/manifold.rst:143 ../modules/manifold.rst:190
#: ../modules/manifold.rst:246 ../modules/manifold.rst:294
#: ../modules/manifold.rst:339 ../modules/manifold.rst:385
msgid ":math:`k` : number of nearest neighbors"
msgstr ":math:`k` : número de vecinos más cercanos"

#: ../modules/manifold.rst:144 ../modules/manifold.rst:191
#: ../modules/manifold.rst:247 ../modules/manifold.rst:295
#: ../modules/manifold.rst:340 ../modules/manifold.rst:386
msgid ":math:`d` : output dimension"
msgstr ":math:`d`: dimensión de salida"

#: ../modules/manifold.rst:148
msgid "`\"A global geometric framework for nonlinear dimensionality reduction\" <http://science.sciencemag.org/content/290/5500/2319.full>`_ Tenenbaum, J.B.; De Silva, V.; & Langford, J.C.  Science 290 (5500)"
msgstr "`\"A global geometric framework for nonlinear dimensionality reduction\" <http://science.sciencemag.org/content/290/5500/2319.full>`_ Tenenbaum, J.B.; De Silva, V.; & Langford, J.C.  Science 290 (5500)"

#: ../modules/manifold.rst:155
msgid "Locally Linear Embedding"
msgstr "Embedding localmente lineal"

#: ../modules/manifold.rst:157
msgid "Locally linear embedding (LLE) seeks a lower-dimensional projection of the data which preserves distances within local neighborhoods.  It can be thought of as a series of local Principal Component Analyses which are globally compared to find the best non-linear embedding."
msgstr "El embedding localmente lineal (ELL, o LLE en inglés) busca una proyección de menor dimensionalidad de los datos que preserva distancias dentro de los vecindarios locales. Puede ser pensado como una serie de Análisis Principales de Componentes, los cuales son comparados globalmente para encontrar la mejor incrustación no lineal."

#: ../modules/manifold.rst:162
msgid "Locally linear embedding can be performed with function :func:`locally_linear_embedding` or its object-oriented counterpart :class:`LocallyLinearEmbedding`."
msgstr "El embedding localmente lineal puede ser realizado con la función :func:`locally_linear_embedding` o su contraparte orientada a objetos :class:`LocallyLinearEmbedding`."

#: ../modules/manifold.rst:174
msgid "The standard LLE algorithm comprises three stages:"
msgstr "El algoritmo ELL estándar comprende tres etapas:"

#: ../modules/manifold.rst:176
msgid "**Nearest Neighbors Search**.  See discussion under Isomap above."
msgstr "**Búsqueda de vecinos más cercanos**. Ver la discusión bajo Isomap más arriba."

#: ../modules/manifold.rst:178
msgid "**Weight Matrix Construction**. :math:`O[D N k^3]`. The construction of the LLE weight matrix involves the solution of a :math:`k \\times k` linear equation for each of the :math:`N` local neighborhoods"
msgstr "**Construcción de la matriz de ponderados**. :math:`O[D N k^3]`. La construcción de la matriz de ponderados ELL involucra la solución de una ecuación lineal :math:`k \\times k` para cada uno de los :math:`N` vecindarios locales"

#: ../modules/manifold.rst:183
msgid "**Partial Eigenvalue Decomposition**. See discussion under Isomap above."
msgstr "**Decomposición de Autovalores Parcial**. Ver la discusión bajo Isomap más arriba."

#: ../modules/manifold.rst:185
msgid "The overall complexity of standard LLE is :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`."
msgstr "La complejidad general del ELL estándar es :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`."

#: ../modules/manifold.rst:195
msgid "`\"Nonlinear dimensionality reduction by locally linear embedding\" <http://www.sciencemag.org/content/290/5500/2323.full>`_ Roweis, S. & Saul, L.  Science 290:2323 (2000)"
msgstr "`\"Nonlinear dimensionality reduction by locally linear embedding\" <http://www.sciencemag.org/content/290/5500/2323.full>`_ Roweis, S. & Saul, L.  Science 290:2323 (2000)"

#: ../modules/manifold.rst:201
msgid "Modified Locally Linear Embedding"
msgstr "Embedding modificado localmente lineal"

#: ../modules/manifold.rst:203
msgid "One well-known issue with LLE is the regularization problem.  When the number of neighbors is greater than the number of input dimensions, the matrix defining each local neighborhood is rank-deficient.  To address this, standard LLE applies an arbitrary regularization parameter :math:`r`, which is chosen relative to the trace of the local weight matrix.  Though it can be shown formally that as :math:`r \\to 0`, the solution converges to the desired embedding, there is no guarantee that the optimal solution will be found for :math:`r > 0`.  This problem manifests itself in embeddings which distort the underlying geometry of the manifold."
msgstr "Un problema bastante bien conocida con ELL es el problema de regularización. Cuando el número de vecinos es mayor que el número de dimensiones de entrada, la matriz definiendo cada vecindario local es deficiente de rango. Para abordar esto, el ELL estándar aplica un parámetro de regularización arbitraria :math:`r`, que se escoge en función del trazo de la matriz de ponderados local. Aunque puede comprobarse formalmente que cuando :math:`r \\to 0`, la solución converge al embedding buscado, no hay garantía de que la solución óptima se encontrara para  :math:`r > 0`. Este problema se manifiesta en embeddings que distorsionan la geometría subyacente del colector."

#: ../modules/manifold.rst:213
msgid "One method to address the regularization problem is to use multiple weight vectors in each neighborhood.  This is the essence of *modified locally linear embedding* (MLLE).  MLLE can be  performed with function :func:`locally_linear_embedding` or its object-oriented counterpart :class:`LocallyLinearEmbedding`, with the keyword ``method = 'modified'``. It requires ``n_neighbors > n_components``."
msgstr "Un método para abordar el problema de regularización es usar multiples ponderados de vectores en cada vecindario. Esta es la esencia del *embedding modíficado localmente lineal* (EMLL). El EMLL puede ser utilizado con la función :func:`locally_linear_embedding`, y también su contraparte orientada a objetos :class:`LocallyLinearEmbedding`, con la palabra clave ``method = 'modified'``. Requiere que ``n_neighbors > n_components``."

#: ../modules/manifold.rst:228
msgid "The MLLE algorithm comprises three stages:"
msgstr "El algoritmo EMLL comprende tres etapas:"

#: ../modules/manifold.rst:230 ../modules/manifold.rst:280
#: ../modules/manifold.rst:372
msgid "**Nearest Neighbors Search**.  Same as standard LLE"
msgstr "**Búsqueda de vecinos más cercanos**. Igual que el ELL estándar"

#: ../modules/manifold.rst:232
msgid "**Weight Matrix Construction**. Approximately :math:`O[D N k^3] + O[N (k-D) k^2]`.  The first term is exactly equivalent to that of standard LLE.  The second term has to do with constructing the weight matrix from multiple weights.  In practice, the added cost of constructing the MLLE weight matrix is relatively small compared to the cost of stages 1 and 3."
msgstr "**Construcción de matríz de ponderados**. Aproximadamente :math:`O[D N k^3] + O[N (k-D) k^2]`. El primer termino es exactamente equivalente a aquél del ELL estándar. El segundo termino tiene que ver con la construcción de la matriz de ponderados desde multiples ponderados. En la practica, el costo añadido de construir la matriz de ponderados EMLL es relativamente pequeña comparada al costo de las etapas 1 y 3."

#: ../modules/manifold.rst:239 ../modules/manifold.rst:287
#: ../modules/manifold.rst:378
msgid "**Partial Eigenvalue Decomposition**. Same as standard LLE"
msgstr "**Decomposición de Autovalores Parcial**. Igual que el ELL estándar"

#: ../modules/manifold.rst:241
msgid "The overall complexity of MLLE is :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]`."
msgstr "La complejidad general del EMLL es :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N (k-D) k^2] + O[d N^2]`."

#: ../modules/manifold.rst:251
msgid "`\"MLLE: Modified Locally Linear Embedding Using Multiple Weights\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382>`_ Zhang, Z. & Wang, J."
msgstr "`\"MLLE: Modified Locally Linear Embedding Using Multiple Weights\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.70.382>`_ Zhang, Z. & Wang, J."

#: ../modules/manifold.rst:257
msgid "Hessian Eigenmapping"
msgstr "Eigenmapeo Hessiano"

#: ../modules/manifold.rst:259
msgid "Hessian Eigenmapping (also known as Hessian-based LLE: HLLE) is another method of solving the regularization problem of LLE.  It revolves around a hessian-based quadratic form at each neighborhood which is used to recover the locally linear structure.  Though other implementations note its poor scaling with data size, ``sklearn`` implements some algorithmic improvements which make its cost comparable to that of other LLE variants for small output dimension.  HLLE can be  performed with function :func:`locally_linear_embedding` or its object-oriented counterpart :class:`LocallyLinearEmbedding`, with the keyword ``method = 'hessian'``. It requires ``n_neighbors > n_components * (n_components + 3) / 2``."
msgstr "El Eigenmapeo Hessiano (también conocido como ELL basado en Hessians: ELLH) es otro método para resolver el problema de regularización del ELL. Revuelve alrededor de una forma cuadrática basada en hessianos en cada vecindad que es usada para recuperar la estructura localmente lineal. Aunque otras implementaciones señalan su mal escalamiento con el tamaño de los datos, ``sklearn`` implementa algunas mejoras algorítmicas que hacen que su costo sea comparable a aquel de otras variantes ELL para dimensiones de salida pequeñas. Se puede realizar el ELLH con la función :func:`locally_linear_embedding` o su contraparte orientada a objetos :class:`LocallyLinearEmbedding`, con la palabra clave ``method = 'hessian'``. Requiere que ``n_neighbors > n_components * (n_components + 3) / 2``."

#: ../modules/manifold.rst:278
msgid "The HLLE algorithm comprises three stages:"
msgstr "El algoritmo ELLH comprende tres etapas:"

#: ../modules/manifold.rst:282
msgid "**Weight Matrix Construction**. Approximately :math:`O[D N k^3] + O[N d^6]`.  The first term reflects a similar cost to that of standard LLE.  The second term comes from a QR decomposition of the local hessian estimator."
msgstr "**Construcción de la Matriz de Ponderado**.  Aproximadamente :math:`O[D N k^3] + O[N d^6]`. El primer término refleja un costo similar a aquel del ELL estándar. El segundo término viene de una descomposición QR del estimador local de hessianos."

#: ../modules/manifold.rst:289
msgid "The overall complexity of standard HLLE is :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]`."
msgstr "La complejidad general del ELLH estándar es :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[N d^6] + O[d N^2]`."

#: ../modules/manifold.rst:299
msgid "`\"Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data\" <http://www.pnas.org/content/100/10/5591>`_ Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)"
msgstr "`\"Hessian Eigenmaps: Locally linear embedding techniques for high-dimensional data\" <http://www.pnas.org/content/100/10/5591>`_ Donoho, D. & Grimes, C. Proc Natl Acad Sci USA. 100:5591 (2003)"

#: ../modules/manifold.rst:306
msgid "Spectral Embedding"
msgstr "Embedding Espectral"

#: ../modules/manifold.rst:308
msgid "Spectral Embedding is an approach to calculating a non-linear embedding. Scikit-learn implements Laplacian Eigenmaps, which finds a low dimensional representation of the data using a spectral decomposition of the graph Laplacian. The graph generated can be considered as a discrete approximation of the low dimensional manifold in the high dimensional space. Minimization of a cost function based on the graph ensures that points close to each other on the manifold are mapped close to each other in the low dimensional space, preserving local distances. Spectral embedding can be  performed with the function :func:`spectral_embedding` or its object-oriented counterpart :class:`SpectralEmbedding`."
msgstr "El Embedding Espectral es un acercamiento al calculo de un embedding no lineal. Scikit-learn implementa Eigenmaps Laplacianos, los cuales encuentran una representación de baja dimensión de los datos usando una descomposición espectral del Laplaciano del gráfico. El gráfico generado puede considerarse una aproximación discreta del colector de baja dimensión en el espacio de alta dimensión. La minimización de una función de costo basada en el gráfico asegura que los puntos cercanos en el colector son mapeados cerca uno de los otros en el espacio de baja dimensión, preservando distancias locales. El Embedding Espectral puede realizarse con la función :func:`spectral_embedding` o con su contraparte orientada a objetos :class:`SpectralEmbedding`."

#: ../modules/manifold.rst:322
msgid "The Spectral Embedding (Laplacian Eigenmaps) algorithm comprises three stages:"
msgstr "El algoritmo de Embedding Espectral (Eigenmaps Laplacianos) comprende tres etapas:"

#: ../modules/manifold.rst:324
msgid "**Weighted Graph Construction**. Transform the raw input data into graph representation using affinity (adjacency) matrix representation."
msgstr "**Construcción de gráfico ponderado**. Transforma los datos de entrada en bruto en una representación gráfica usando una representación de matriz por afinidad (adyacencia)."

#: ../modules/manifold.rst:327
msgid "**Graph Laplacian Construction**. unnormalized Graph Laplacian is constructed as :math:`L = D - A` for and normalized one as :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`."
msgstr "**Construcción del Grafo Laplaciano**. El grafo Laplaciano sin normalizar se construye como :math:`L = D - A` y el normalizado se construye como :math:`L = D^{-\\frac{1}{2}} (D - A) D^{-\\frac{1}{2}}`."

#: ../modules/manifold.rst:331
msgid "**Partial Eigenvalue Decomposition**. Eigenvalue decomposition is done on graph Laplacian"
msgstr "**Decomposición parcial de Autovalor**. La descomposición del autovalor se realiza en el grafo Laplaciano"

#: ../modules/manifold.rst:334
msgid "The overall complexity of spectral embedding is :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`."
msgstr "La complejidad general del embedding espectral es :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[d N^2]`."

#: ../modules/manifold.rst:344
msgid "`\"Laplacian Eigenmaps for Dimensionality Reduction and Data Representation\" <https://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf>`_ M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396"
msgstr "`\"Laplacian Eigenmaps for Dimensionality Reduction and Data Representation\" <https://web.cse.ohio-state.edu/~mbelkin/papers/LEM_NC_03.pdf>`_ M. Belkin, P. Niyogi, Neural Computation, June 2003; 15 (6):1373-1396"

#: ../modules/manifold.rst:351
msgid "Local Tangent Space Alignment"
msgstr "Alineación del Espacio Tangente Local"

#: ../modules/manifold.rst:353
msgid "Though not technically a variant of LLE, Local tangent space alignment (LTSA) is algorithmically similar enough to LLE that it can be put in this category. Rather than focusing on preserving neighborhood distances as in LLE, LTSA seeks to characterize the local geometry at each neighborhood via its tangent space, and performs a global optimization to align these local tangent spaces to learn the embedding.  LTSA can be performed with function :func:`locally_linear_embedding` or its object-oriented counterpart :class:`LocallyLinearEmbedding`, with the keyword ``method = 'ltsa'``."
msgstr "Aunque técnicamente no sea una variante del ELL, el alineamiento del espacio tangente local (AETL, o LTSA en inglés) es lo suficientemente similar algoritmicamente al ELL que se puede poner en esta categoría. En lugar de enfocarse en preservar distancias de vecindad como en el ELL, el AETL busca caracterizar la geometría local en cada vecindad mediante su espacio tangente, y realiza una optimización global para alinear estos espacios locales tangentes y así  aprender el embedding. El AETL puede realizarse con la función :func:`locally_linear_embedding` o su contraparte orientada a objetos :class:`LocallyLinearEmbedding`, con la palabra clave ``method = 'ltsa'``."

#: ../modules/manifold.rst:370
msgid "The LTSA algorithm comprises three stages:"
msgstr "El algoritmo AETL comprende tres etapas:"

#: ../modules/manifold.rst:374
msgid "**Weight Matrix Construction**. Approximately :math:`O[D N k^3] + O[k^2 d]`.  The first term reflects a similar cost to that of standard LLE."
msgstr "**Construcción de la matriz de ponderado**. Aproximadamente :math:`O[D N k^3] + O[k^2 d]`. El primer termino refleja un costo similar al del ELL estándar."

#: ../modules/manifold.rst:380
msgid "The overall complexity of standard LTSA is :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]`."
msgstr "La complejidad general del AETL estándar es :math:`O[D \\log(k) N \\log(N)] + O[D N k^3] + O[k^2 d] + O[d N^2]`."

#: ../modules/manifold.rst:390
msgid "`\"Principal manifolds and nonlinear dimensionality reduction via tangent space alignment\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.3693>`_ Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)"
msgstr "`\"Principal manifolds and nonlinear dimensionality reduction via tangent space alignment\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.3693>`_ Zhang, Z. & Zha, H. Journal of Shanghai Univ. 8:406 (2004)"

#: ../modules/manifold.rst:398
msgid "Multi-dimensional Scaling (MDS)"
msgstr "Escalamiento Multidimensional (EMD)"

#: ../modules/manifold.rst:400
msgid "`Multidimensional scaling <https://en.wikipedia.org/wiki/Multidimensional_scaling>`_ (:class:`MDS`) seeks a low-dimensional representation of the data in which the distances respect well the distances in the original high-dimensional space."
msgstr "`Escalamiento multidimensional <https://es.wikipedia.org/wiki/Escalamiento_multidimensional>`_(:class:`MDS`) busca una representación de dimensión baja de los datos en los cuales las distancias respetan bien las distancias en el espacio altodimensional original."

#: ../modules/manifold.rst:405
msgid "In general, :class:`MDS` is a technique used for analyzing similarity or dissimilarity data. It attempts to model similarity or dissimilarity data as distances in a geometric spaces. The data can be ratings of similarity between objects, interaction frequencies of molecules, or trade indices between countries."
msgstr "En general, :class:`MDS` es una técnica utilizada para analizar datos de similitud o disimilitud. Intenta modelar la similitud o disimilitud de los datos como distancias en espacios geométricos. Los datos pueden ser calificaciones de similitud entre objetos, frecuencias de interacción de moléculas, o índices de comercio entre países."

#: ../modules/manifold.rst:411
msgid "There exists two types of MDS algorithm: metric and non metric. In the scikit-learn, the class :class:`MDS` implements both. In Metric MDS, the input similarity matrix arises from a metric (and thus respects the triangular inequality), the distances between output two points are then set to be as close as possible to the similarity or dissimilarity data. In the non-metric version, the algorithms will try to preserve the order of the distances, and hence seek for a monotonic relationship between the distances in the embedded space and the similarities/dissimilarities."
msgstr "Existen dos tipos del algoritmo EMD: métrico y no métrico. En scikit-learn, la clase :class:`MDS` implementa ambos. En el EMD métrico, la matriz de similitud de entrada viene de una métrica (y por lo tanto respeta la desigualdad triangular), las distancias entre dos puntos de salida son entonces establecidas para ser tan cercanas como es posible a los datos de similitud o disimilitud. En la versión no métrica, los algoritmos intentaran preservar el orden de las distancias, y entonces buscaran una relación monotonica entre las distancias en el espacio con embedding y las similitudes/disimilitudes."

#: ../modules/manifold.rst:426
msgid "Let :math:`S` be the similarity matrix, and :math:`X` the coordinates of the :math:`n` input points. Disparities :math:`\\hat{d}_{ij}` are transformation of the similarities chosen in some optimal ways. The objective, called the stress, is then defined by :math:`\\sum_{i < j} d_{ij}(X) - \\hat{d}_{ij}(X)`"
msgstr "Sea :math:`S` la matriz de similitud, y :math:`X` las coordenadas de los :math:`n` puntos de entrada. La disparidades :math:`\\hat{d}_{ij}` son transformaciones de las similitudes escogidas en ciertas maneras óptimas. El objetivo, llamado el estrés, entonces es definido por :math:`\\sum_{i < j} d_{ij}(X) - \\hat{d}_{ij}(X)`"

#: ../modules/manifold.rst:433
msgid "Metric MDS"
msgstr "EMD Métrico"

#: ../modules/manifold.rst:435
msgid "The simplest metric :class:`MDS` model, called *absolute MDS*, disparities are defined by :math:`\\hat{d}_{ij} = S_{ij}`. With absolute MDS, the value :math:`S_{ij}` should then correspond exactly to the distance between point :math:`i` and :math:`j` in the embedding point."
msgstr "En el modelo :class:`MDS` métrico mas simple, llamado *EMD absoluto*, las disparidades son definidas por :math:`\\hat{d}_{ij} = S_{ij}`. Con el EMD absoluto, el valor :math:`S_{ij}` debería entonces corresponder exactamente a la distancia entre el punto :math:`i` y :math:`j` en el punto de embedding."

#: ../modules/manifold.rst:440
msgid "Most commonly, disparities are set to :math:`\\hat{d}_{ij} = b S_{ij}`."
msgstr "Mas comúnmente, las disparidades están configuradas como :math:`\\hat{d}_{ij} = b S_{ij}`."

#: ../modules/manifold.rst:443
msgid "Nonmetric MDS"
msgstr "EMD No métrico"

#: ../modules/manifold.rst:445
msgid "Non metric :class:`MDS` focuses on the ordination of the data. If :math:`S_{ij} < S_{jk}`, then the embedding should enforce :math:`d_{ij} < d_{jk}`. A simple algorithm to enforce that is to use a monotonic regression of :math:`d_{ij}` on :math:`S_{ij}`, yielding disparities :math:`\\hat{d}_{ij}` in the same order as :math:`S_{ij}`."
msgstr "El :class:`MDS` no métrico se enfoca en la ordenación de los datos. Si :math:`S_{ij} < S_{jk}`, entonces el embedding debería imponer :math:`d_{ij} < d_{jk}`. Un algoritmo simple para imponerlo es el uso de una regresión monotónica de :math:`d_{ij}` en :math:`S_{ij}`, dando las disparidades :math:`\\hat{d}_{ij}` en el mismo orden que :math:`S_{ij}`."

#: ../modules/manifold.rst:451
msgid "A trivial solution to this problem is to set all the points on the origin. In order to avoid that, the disparities :math:`\\hat{d}_{ij}` are normalized."
msgstr "Una solución trivial a este problema es fijar todos los puntos en el origen. Para evitar eso, las disparidades :math:`\\hat{d}_{ij}` son normalizadas."

#: ../modules/manifold.rst:463
msgid "`\"Modern Multidimensional Scaling - Theory and Applications\" <https://www.springer.com/fr/book/9780387251509>`_ Borg, I.; Groenen P. Springer Series in Statistics (1997)"
msgstr "`\"Modern Multidimensional Scaling - Theory and Applications\" <https://www.springer.com/fr/book/9780387251509>`_ Borg, I.; Groenen P. Springer Series in Statistics (1997)"

#: ../modules/manifold.rst:467
#, python-format
msgid "`\"Nonmetric multidimensional scaling: a numerical method\" <https://link.springer.com/article/10.1007%2FBF02289694>`_ Kruskal, J. Psychometrika, 29 (1964)"
msgstr "`\"Nonmetric multidimensional scaling: a numerical method\" <https://link.springer.com/article/10.1007%2FBF02289694>`_ Kruskal, J. Psychometrika, 29 (1964)"

#: ../modules/manifold.rst:471
#, python-format
msgid "`\"Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis\" <https://link.springer.com/article/10.1007%2FBF02289565>`_ Kruskal, J. Psychometrika, 29, (1964)"
msgstr "`\"Multidimensional scaling by optimizing goodness of fit to a nonmetric hypothesis\" <https://link.springer.com/article/10.1007%2FBF02289565>`_ Kruskal, J. Psychometrika, 29, (1964)"

#: ../modules/manifold.rst:478
msgid "t-distributed Stochastic Neighbor Embedding (t-SNE)"
msgstr "Embedding Estocástico de Vecinos distribuido en t (t-EEV, o t-SNE en inglés)"

#: ../modules/manifold.rst:480
msgid "t-SNE (:class:`TSNE`) converts affinities of data points to probabilities. The affinities in the original space are represented by Gaussian joint probabilities and the affinities in the embedded space are represented by Student's t-distributions. This allows t-SNE to be particularly sensitive to local structure and has a few other advantages over existing techniques:"
msgstr "t-EEV (:class:`TSNE`) convierte las afinidades de puntos de datos a probabilidades. Las afinidades en el espacio original están representadas por probabilidades conjuntas Gaussianas y las afinidades en el espacio con embedding son representadas por las distribuciones en t del estudiante. Esto permite al t-EEV ser particularmente sensible a la estructura local y tiene algunas otras ventajas sobre técnicas existentes:"

#: ../modules/manifold.rst:486
msgid "Revealing the structure at many scales on a single map"
msgstr "Revelar la estructura en muchas escalas en un solo mapa"

#: ../modules/manifold.rst:487
msgid "Revealing data that lie in multiple, different, manifolds or clusters"
msgstr "Revelar datos que yacen en múltiples, distintos colectores o clústers"

#: ../modules/manifold.rst:488
msgid "Reducing the tendency to crowd points together at the center"
msgstr "Reducir la tendencia a la acumular puntos juntos en el centro"

#: ../modules/manifold.rst:490
msgid "While Isomap, LLE and variants are best suited to unfold a single continuous low dimensional manifold, t-SNE will focus on the local structure of the data and will tend to extract clustered local groups of samples as highlighted on the S-curve example. This ability to group samples based on the local structure might be beneficial to visually disentangle a dataset that comprises several manifolds at once as is the case in the digits dataset."
msgstr "Mientras que Isomap, ELL y sus variantes son más adecuadas para desplegar un solo colector continuo de dimensionalidad baja, el t-EEV se enfocara en la estructura local de los datos y va a tender a extraer grupos locales de muestras conglomerados como se resalta en el ejemplo de la curva en S. Esta habilidad de agrupas muestras basadas en la estructura local quizás sea beneficiosa para desenredar un conjunto de datos visualmente que compromete múltiples colectores al mismo tiempo como es el caso en el conjunto de datos digits."

#: ../modules/manifold.rst:497
msgid "The Kullback-Leibler (KL) divergence of the joint probabilities in the original space and the embedded space will be minimized by gradient descent. Note that the KL divergence is not convex, i.e. multiple restarts with different initializations will end up in local minima of the KL divergence. Hence, it is sometimes useful to try different seeds and select the embedding with the lowest KL divergence."
msgstr "La divergencia Kullback-Leibler (KL) de las probabilidades conjuntas en el espacio original y en el espacio con embedding sera minimizada por descenso por gradiente. Note que la divergencia KL no es convexa, es decir, multiples reinicios con inicializaciones distintas van a terminar en los mínimos locales de la divergencia KL. Por lo tanto, a veces es útil utilizar diferentes semillas y seleccionar el embedding con la menor divergencia KL."

#: ../modules/manifold.rst:504
msgid "The disadvantages to using t-SNE are roughly:"
msgstr "Las desventajas de usar t-EEV son aproximadamente:"

#: ../modules/manifold.rst:506
msgid "t-SNE is computationally expensive, and can take several hours on million-sample datasets where PCA will finish in seconds or minutes"
msgstr "el t-EEV es computacionalmente costoso, y puede tomar varias horas en conjuntos de datos con millones de muestras, donde el APC terminaría en segundos o minutos"

#: ../modules/manifold.rst:508
msgid "The Barnes-Hut t-SNE method is limited to two or three dimensional embeddings."
msgstr "El método t-EEV Barnes-Hut está limitado a embeddings de dos o tres dimensiones."

#: ../modules/manifold.rst:509
msgid "The algorithm is stochastic and multiple restarts with different seeds can yield different embeddings. However, it is perfectly legitimate to pick the embedding with the least error."
msgstr "El algoritmo es estocástico y varios reinicios con diferentes semillas pueden producir embeddings distintos. Sin embargo, es perfectamente legítimo elegir el embedding con el menor error."

#: ../modules/manifold.rst:512
msgid "Global structure is not explicitly preserved. This problem is mitigated by initializing points with PCA (using `init='pca'`)."
msgstr "La estructura global no se preserva explícitamente, Este problema se puede mitigar inicializando puntos con APC (utilizando `init='pca'`)."

#: ../modules/manifold.rst:522
msgid "Optimizing t-SNE"
msgstr "Optimizando t-EEV"

#: ../modules/manifold.rst:523
msgid "The main purpose of t-SNE is visualization of high-dimensional data. Hence, it works best when the data will be embedded on two or three dimensions."
msgstr "El principal objetivo del t-EEV es la visualización de datos de alta dimensión. Por lo tanto, funciona mejor cuando los datos serán integrados en dos o tres dimensiones."

#: ../modules/manifold.rst:526
msgid "Optimizing the KL divergence can be a little bit tricky sometimes. There are five parameters that control the optimization of t-SNE and therefore possibly the quality of the resulting embedding:"
msgstr "Optimizar la divergencia KL puede ser un poco complicado a veces. Hay 5 parámetros que controlan la optimización del t-EEV y entonces posiblemente la calidad del embedding resultante:"

#: ../modules/manifold.rst:530
msgid "perplexity"
msgstr "perplejidad"

#: ../modules/manifold.rst:531
msgid "early exaggeration factor"
msgstr "factor de exageración temprana"

#: ../modules/manifold.rst:532
msgid "learning rate"
msgstr "tasa de aprendizaje"

#: ../modules/manifold.rst:533
msgid "maximum number of iterations"
msgstr "número máximo de iteraciones"

#: ../modules/manifold.rst:534
msgid "angle (not used in the exact method)"
msgstr "ángulo (no usado en el método exacto)"

#: ../modules/manifold.rst:536
msgid "The perplexity is defined as :math:`k=2^{(S)}` where :math:`S` is the Shannon entropy of the conditional probability distribution. The perplexity of a :math:`k`-sided die is :math:`k`, so that :math:`k` is effectively the number of nearest neighbors t-SNE considers when generating the conditional probabilities. Larger perplexities lead to more nearest neighbors and less sensitive to small structure. Conversely a lower perplexity considers a smaller number of neighbors, and thus ignores more global information in favour of the local neighborhood. As dataset sizes get larger more points will be required to get a reasonable sample of the local neighborhood, and hence larger perplexities may be required. Similarly noisier datasets will require larger perplexity values to encompass enough local neighbors to see beyond the background noise."
msgstr "La perplejidad esta definida como :math:`k=2^{(S)}` donde :math:`S` es la entropía de Shannon de la distribución de probabilidad condicional. La perplejidad de un dado de :math:`k` lados es :math:`k`, por lo que :math:`k` es efectivamente el número de vecinos mas cercanos que considera t-EEV cuando genera las probabilidades condicionales. Las perplejidades mas grandes llevan a mas vecinos cercanos y menor sensibilidad a estructuras pequeñas. Por el contrario, una perplejidad menor considera un número pequeño de vecinos, y por lo tanto ignora mas información global a favor de la vecindad local. Mientras los conjuntos de datos se vuelven mas grandes, mas puntos serán requeridos para obtener una muestra razonable de la vecindad local, y por lo tanto quizás sean requeridas perplejidades mas grandes. Similarmente, conjuntos de datos con mas ruido van a requerir valores de perplejidad mas grandes para abarcar suficientes vecinos locales para ver mas allá del ruido de fondo."

#: ../modules/manifold.rst:549
msgid "The maximum number of iterations is usually high enough and does not need any tuning. The optimization consists of two phases: the early exaggeration phase and the final optimization. During early exaggeration the joint probabilities in the original space will be artificially increased by multiplication with a given factor. Larger factors result in larger gaps between natural clusters in the data. If the factor is too high, the KL divergence could increase during this phase. Usually it does not have to be tuned. A critical parameter is the learning rate. If it is too low gradient descent will get stuck in a bad local minimum. If it is too high the KL divergence will increase during optimization. More tips can be found in Laurens van der Maaten's FAQ (see references). The last parameter, angle, is a tradeoff between performance and accuracy. Larger angles imply that we can approximate larger regions by a single point, leading to better speed but less accurate results."
msgstr "El número maximo de iteraciones suele ser lo suficientemente alto y no requiere ningún ajuste. La optimización consiste en dos fases: la fase de exageración temprana y la última optimización. Durante la exageración temprana las probabilidades de conjunto en el espacio original serán incrementadas artificialmente mediante la multiplicación con un factor dado. Los factores mas grandes resultan en brechas mas grandes entre clusters naturales en los datos. Si el factor es demasiado grande, la divergencia KL podría incrementar en esta fase. Normalmente no requiere ser ajustado. Un parámetro crítico es la tasa de aprendizaje. Si es demasiado alta, la divergencia KL incrementara durante la optimización. Mas tips pueden ser encontrados en el FAQ de Laurens van der Maaten (ver referencias). El último parámetro, el ángulo, es un intercambio entre precisión y rendimiento. Ángulos mas grandes implican que podemos aproximar regiones mas grandes por un punto único, llevando a una mejor velocidad pero resultados menos precisos."

#: ../modules/manifold.rst:564
msgid "`\"How to Use t-SNE Effectively\" <https://distill.pub/2016/misread-tsne/>`_ provides a good discussion of the effects of the various parameters, as well as interactive plots to explore the effects of different parameters."
msgstr "`\"Como usar t-EEV efectivamente\" <https://distill.pub/2016/misread-tsne/>`_ proporciona una buena discusión de los efectos de los diferentes parámetros, así como gráficos interactivos para explorar los efectos de diferentes parámetros."

#: ../modules/manifold.rst:569
msgid "Barnes-Hut t-SNE"
msgstr "t-EEV Barnes-Hut"

#: ../modules/manifold.rst:571
msgid "The Barnes-Hut t-SNE that has been implemented here is usually much slower than other manifold learning algorithms. The optimization is quite difficult and the computation of the gradient is :math:`O[d N log(N)]`, where :math:`d` is the number of output dimensions and :math:`N` is the number of samples. The Barnes-Hut method improves on the exact method where t-SNE complexity is :math:`O[d N^2]`, but has several other notable differences:"
msgstr "El t-EEV Barnes-Hut que se ha implementado aquí es normalmente mucho más lento que otros algoritmos de aprendizaje de colectores. La optimización es bastante difícil y la computación del gradiente es :math:`O[d N log(N)]`, donde :math:`d` es el número de dimensiones de salida y :math:`N` es el número de muestras. El método Barnes-Hut mejora en el método exacto donde la complejidad t-EEV es :math:`O[d N^2]`, pero tiene otras diferencias notables:"

#: ../modules/manifold.rst:578
msgid "The Barnes-Hut implementation only works when the target dimensionality is 3 or less. The 2D case is typical when building visualizations."
msgstr "La implementación Barnes-Hut sólo funciona cuando la dimensionalidad objetivo es 3 o menos. El caso 2D es típico cuando se construyen visualizaciones."

#: ../modules/manifold.rst:580
msgid "Barnes-Hut only works with dense input data. Sparse data matrices can only be embedded with the exact method or can be approximated by a dense low rank projection for instance using :class:`~sklearn.decomposition.TruncatedSVD`"
msgstr "Barnes-Hut solo funciona con datos de entrada densos. A las matrices de datos dispersas solo se les puede realizar embedding con el método exacto o pueden ser aproximadas por una proyección de bajo rango densa, por ejemplo usando :class:`~sklearn.decomposition.TruncatedSVD`"

#: ../modules/manifold.rst:583
msgid "Barnes-Hut is an approximation of the exact method. The approximation is parameterized with the angle parameter, therefore the angle parameter is unused when method=\"exact\""
msgstr "Barnes-Hut es una aproximación del método exacto. La aproximación se parametriza con el parámetro del ángulo, por lo tanto el parámetro angle no se utiliza cuando method=\"exact\""

#: ../modules/manifold.rst:586
msgid "Barnes-Hut is significantly more scalable. Barnes-Hut can be used to embed hundred of thousands of data points while the exact method can handle thousands of samples before becoming computationally intractable"
msgstr "Barnes-Hut es significativamente más escalable. Barnes-Hut puede ser usado para realizar embedding en cientos de miles de puntos de datos, mientras que el método exacto solo puede manejar miles de muestras antes de volverse inmanejable computacionalmente"

#: ../modules/manifold.rst:590
msgid "For visualization purpose (which is the main use case of t-SNE), using the Barnes-Hut method is strongly recommended. The exact t-SNE method is useful for checking the theoretically properties of the embedding possibly in higher dimensional space but limit to small datasets due to computational constraints."
msgstr "Para fines de visualización (el cual es el principal uso del t-EEV) se recomienda encarecidamente utilizar el método Barnes-Hut. El método exacto t-EEV es útil para comprobar las propiedades teóricas del embedding, posiblemente en espacio de alta dimensionalidad pero limitado a conjuntos de datos pequeños debido a restricciones computacionales."

#: ../modules/manifold.rst:595
msgid "Also note that the digits labels roughly match the natural grouping found by t-SNE while the linear 2D projection of the PCA model yields a representation where label regions largely overlap. This is a strong clue that this data can be well separated by non linear methods that focus on the local structure (e.g. an SVM with a Gaussian RBF kernel). However, failing to visualize well separated homogeneously labeled groups with t-SNE in 2D does not necessarily imply that the data cannot be correctly classified by a supervised model. It might be the case that 2 dimensions are not low enough to accurately represents the internal structure of the data."
msgstr "También note que las etiquetas de los digitos mas o menos corresponden al agrupamiento natural encontrado por t-EEV mientras que la proyección 2D lineal del modelo APC da una representación donde gran parte de las regiones de etiqueta se solapan. Esta es una pista importante de que estos datos pueden ser bien separados por métodos no lineales que se enfocan en la estructural local (por ejemplo, un SVM con un kernel RBF Gaussiana). Sin embargo, no visualizar grupos bien separados etiquetados de forma homogénea con t-EEV en 2D no implica necesariamente que los datos no pueden ser clasificados correctamente por un modelo supervisado. Quizás sea el caso que 2 dimensiones no son lo suficientemente bajas como para representar de manera precisa la estructura interna de los datos."

#: ../modules/manifold.rst:608
msgid "`\"Visualizing High-Dimensional Data Using t-SNE\" <http://jmlr.org/papers/v9/vandermaaten08a.html>`_ van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)"
msgstr "`\"Visualizing High-Dimensional Data Using t-SNE\" <http://jmlr.org/papers/v9/vandermaaten08a.html>`_ van der Maaten, L.J.P.; Hinton, G. Journal of Machine Learning Research (2008)"

#: ../modules/manifold.rst:613
msgid "`\"t-Distributed Stochastic Neighbor Embedding\" <https://lvdmaaten.github.io/tsne/>`_ van der Maaten, L.J.P."
msgstr "`\"t-Distributed Stochastic Neighbor Embedding\" <https://lvdmaaten.github.io/tsne/>`_ van der Maaten, L.J.P."

#: ../modules/manifold.rst:617
msgid "`\"Accelerating t-SNE using Tree-Based Algorithms.\" <https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf>`_ L.J.P. van der Maaten.  Journal of Machine Learning Research 15(Oct):3221-3245, 2014."
msgstr "`\"Accelerating t-SNE using Tree-Based Algorithms.\" <https://lvdmaaten.github.io/publications/papers/JMLR_2014.pdf>`_ L.J.P. van der Maaten.  Journal of Machine Learning Research 15(Oct):3221-3245, 2014."

#: ../modules/manifold.rst:622
msgid "Tips on practical use"
msgstr "Consejos sobre el uso práctico"

#: ../modules/manifold.rst:624
msgid "Make sure the same scale is used over all features. Because manifold learning methods are based on a nearest-neighbor search, the algorithm may perform poorly otherwise.  See :ref:`StandardScaler <preprocessing_scaler>` for convenient ways of scaling heterogeneous data."
msgstr "Asegúrese que la misma escala se usa en toda las características, Debido a que los métodos de aprendizaje de colector están basados en una búsqueda de vecino más cercano. el algoritmo podría tener un rendimiento deficiente si no es así. Vea :ref:`StandardScaler <preprocessing_scaler>` para formas convenientes de escalar datos heterogéneos."

#: ../modules/manifold.rst:629
msgid "The reconstruction error computed by each routine can be used to choose the optimal output dimension.  For a :math:`d`-dimensional manifold embedded in a :math:`D`-dimensional parameter space, the reconstruction error will decrease as ``n_components`` is increased until ``n_components == d``."
msgstr "El error de reconstrucción calculado por cada rutina se puede utilizar para escoger la dimensión de salida óptima. Para un colector :math:`d`-dimensional incrustado en un espacio de parámetro :math:`D`-dimensional, el error de reconstrucción disminuirá a medida que ``n_componentes`` se incremente hasta que ``n_components == d``."

#: ../modules/manifold.rst:634
msgid "Note that noisy data can \"short-circuit\" the manifold, in essence acting as a bridge between parts of the manifold that would otherwise be well-separated.  Manifold learning on noisy and/or incomplete data is an active area of research."
msgstr "Tenga en cuenta que los datos ruidosos pueden hacerle un \"cortocircuito\" al colector, en esencia actuando como un puente entre partes del colector que de otro modo estarían bien separadas. El aprendizaje del colector en datos incompletos o ruidosos es una área activa de investigación."

#: ../modules/manifold.rst:639
msgid "Certain input configurations can lead to singular weight matrices, for example when more than two points in the dataset are identical, or when the data is split into disjointed groups.  In this case, ``solver='arpack'`` will fail to find the null space.  The easiest way to address this is to use ``solver='dense'`` which will work on a singular matrix, though it may be very slow depending on the number of input points.  Alternatively, one can attempt to understand the source of the singularity: if it is due to disjoint sets, increasing ``n_neighbors`` may help.  If it is due to identical points in the dataset, removing these points may help."
msgstr "Ciertas configuraciones de entrada pueden llevar a matrices de ponderado singulares, por ejemplo cuando mas de dos puntos en el conjunto de datos son idénticos, o cuando los datos son divididos en grupos disociados. En este caso, ``solver='arpack'`` no logrará encontrar el espacio nulo. La forma mas sencilla de abordar esto es usar ``solver='dense'``, lo cual funcionara en una matriz singular, aunque quizás sea muy lento dependiendo del número de puntos de entrada. Alternativamente, uno puede intentar entender la fuente de la singularidad: si es debido a conjuntos disjuntos, incrementar ``n_neighbors`` quizás ayude. Si es debido a puntos idénticos en el conjunto de datos, quitar estos puntos quizás ayude."

#: ../modules/manifold.rst:651
msgid ":ref:`random_trees_embedding` can also be useful to derive non-linear representations of feature space, also it does not perform dimensionality reduction."
msgstr ":ref:`random_trees_embedding` también puede ser útil para derivar representaciones no lineales del espacio de característica, y no realiza una reducción de dimensionalidad."

