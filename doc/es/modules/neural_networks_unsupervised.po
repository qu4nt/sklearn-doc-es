msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-03 03:29\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/neural_networks_unsupervised.po\n"
"X-Crowdin-File-ID: 4826\n"
"Language: es_ES\n"

#: ../modules/neural_networks_unsupervised.rst:5
msgid "Neural network models (unsupervised)"
msgstr "Modelos de red neural (no supervisados)"

#: ../modules/neural_networks_unsupervised.rst:13
msgid "Restricted Boltzmann machines"
msgstr "Máquinas Boltzmann restringidas"

#: ../modules/neural_networks_unsupervised.rst:15
msgid "Restricted Boltzmann machines (RBM) are unsupervised nonlinear feature learners based on a probabilistic model. The features extracted by an RBM or a hierarchy of RBMs often give good results when fed into a linear classifier such as a linear SVM or a perceptron."
msgstr "Las máquinas de Boltzmann restringidas (RBM) son aprendices de características no lineales no supervisadas basadas en un modelo probabilístico. Las características extraídas por una RBM o una jerarquía de RBM suelen dar buenos resultados cuando se introducen en un clasificador lineal como una SVM lineal o un perceptrón."

#: ../modules/neural_networks_unsupervised.rst:20
msgid "The model makes assumptions regarding the distribution of inputs. At the moment, scikit-learn only provides :class:`BernoulliRBM`, which assumes the inputs are either binary values or values between 0 and 1, each encoding the probability that the specific feature would be turned on."
msgstr "El modelo hace suposiciones sobre la distribución de las entradas. Por el momento, scikit-learn sólo proporciona :class:`BernoulliRBM`, que asume que las entradas son valores binarios o valores entre 0 y 1, cada uno codificando la probabilidad de que la característica específica se active."

#: ../modules/neural_networks_unsupervised.rst:25
msgid "The RBM tries to maximize the likelihood of the data using a particular graphical model. The parameter learning algorithm used (:ref:`Stochastic Maximum Likelihood <sml>`) prevents the representations from straying far from the input data, which makes them capture interesting regularities, but makes the model less useful for small datasets, and usually not useful for density estimation."
msgstr "El RBM trata de maximizar la probabilidad de los datos utilizando un modelo gráfico determinado. El algoritmo de aprendizaje de parámetros utilizado (:ref:`Máxima Verosimilitud Estocástica <sml>`) evita que las representaciones se alejen de los datos de entrada, lo que hace que capturen regularidades interesantes, pero hace que el modelo sea menos útil para conjuntos de datos pequeños, y normalmente no es útil para la estimación de la densidad."

#: ../modules/neural_networks_unsupervised.rst:32
msgid "The method gained popularity for initializing deep neural networks with the weights of independent RBMs. This method is known as unsupervised pre-training."
msgstr "El método ganó popularidad para inicializar redes neuronales profundas con los pesos de RBMs independientes. Este método se conoce como preentrenamiento no supervisado."

#: ../modules/neural_networks_unsupervised.rst:42
msgid ":ref:`sphx_glr_auto_examples_neural_networks_plot_rbm_logistic_classification.py`"
msgstr ":ref:`sphx_glr_auto_examples_neural_networks_plot_rbm_logistic_classification.py`"

#: ../modules/neural_networks_unsupervised.rst:46
msgid "Graphical model and parametrization"
msgstr "Modelo gráfico y parametrización"

#: ../modules/neural_networks_unsupervised.rst:48
msgid "The graphical model of an RBM is a fully-connected bipartite graph."
msgstr "El modelo gráfico de un RBM es un grafo bipartito completamente conectado."

#: ../modules/neural_networks_unsupervised.rst:53
msgid "The nodes are random variables whose states depend on the state of the other nodes they are connected to. The model is therefore parameterized by the weights of the connections, as well as one intercept (bias) term for each visible and hidden unit, omitted from the image for simplicity."
msgstr "Los nodos son variables aleatorias cuyos estados dependen del estado de los otros nodos a los que están conectados. Por lo tanto, el modelo está parametrizado por las ponderaciones de las conexiones, así como un término de intercepción (sesgo) para cada unidad visible y oculta, omitido en la imagen por simplicidad."

#: ../modules/neural_networks_unsupervised.rst:58
msgid "The energy function measures the quality of a joint assignment:"
msgstr "La función energética mide la calidad de una asignación conjunta:"

#: ../modules/neural_networks_unsupervised.rst:60
msgid "E(\\mathbf{v}, \\mathbf{h}) = -\\sum_i \\sum_j w_{ij}v_ih_j - \\sum_i b_iv_i\n"
"  - \\sum_j c_jh_j"
msgstr "E(\\mathbf{v}, \\mathbf{h}) = -\\sum_i \\sum_j w_{ij}v_ih_j - \\sum_i b_iv_i\n"
"  - \\sum_j c_jh_j"

#: ../modules/neural_networks_unsupervised.rst:65
msgid "In the formula above, :math:`\\mathbf{b}` and :math:`\\mathbf{c}` are the intercept vectors for the visible and hidden layers, respectively. The joint probability of the model is defined in terms of the energy:"
msgstr "En la fórmula anterior, :math:`\\mathbf{b}` y :math:`\\mathbf{c}` son los vectores de intercepción para las capas visible y oculta, respectivamente. La probabilidad conjunta del modelo se define en términos de energía:"

#: ../modules/neural_networks_unsupervised.rst:69
msgid "P(\\mathbf{v}, \\mathbf{h}) = \\frac{e^{-E(\\mathbf{v}, \\mathbf{h})}}{Z}"
msgstr "P(\\mathbf{v}, \\mathbf{h}) = \\frac{e^{-E(\\mathbf{v}, \\mathbf{h})}}{Z}"

#: ../modules/neural_networks_unsupervised.rst:74
msgid "The word *restricted* refers to the bipartite structure of the model, which prohibits direct interaction between hidden units, or between visible units. This means that the following conditional independencies are assumed:"
msgstr "La palabra *restringida* se refiere a la estructura bipartita del modelo, que prohíbe la interacción directa entre unidades ocultas, o entre unidades visibles. Esto significa que se asumen las siguientes independencias condicionales:"

#: ../modules/neural_networks_unsupervised.rst:78
msgid "h_i \\bot h_j | \\mathbf{v} \\\\\n"
"v_i \\bot v_j | \\mathbf{h}"
msgstr "h_i \\bot h_j | \\mathbf{v} \\\\\n"
"v_i \\bot v_j | \\mathbf{h}"

#: ../modules/neural_networks_unsupervised.rst:83
msgid "The bipartite structure allows for the use of efficient block Gibbs sampling for inference."
msgstr "La estructura bipartita permite el uso de un eficiente muestreo de Gibbs en bloque para la inferencia."

#: ../modules/neural_networks_unsupervised.rst:87
msgid "Bernoulli Restricted Boltzmann machines"
msgstr "Máquinas de Boltzmann restringidas de Bernoulli"

#: ../modules/neural_networks_unsupervised.rst:89
msgid "In the :class:`BernoulliRBM`, all units are binary stochastic units. This means that the input data should either be binary, or real-valued between 0 and 1 signifying the probability that the visible unit would turn on or off. This is a good model for character recognition, where the interest is on which pixels are active and which aren't. For images of natural scenes it no longer fits because of background, depth and the tendency of neighbouring pixels to take the same values."
msgstr "En la :clase:`BernoulliRBM`, todas las unidades son unidades estocásticas binarias. Esto significa que los datos de entrada deben ser binarios, o de valor real entre 0 y 1, lo que significa la probabilidad de que la unidad visible se encienda o se apague. Este es un buen modelo para el reconocimiento de caracteres, donde el interés está en qué píxeles están activos y cuáles no. Para las imágenes de escenas naturales ya no se ajusta debido al fondo, la profundidad y la tendencia de los píxeles vecinos a tomar los mismos valores."

#: ../modules/neural_networks_unsupervised.rst:97
msgid "The conditional probability distribution of each unit is given by the logistic sigmoid activation function of the input it receives:"
msgstr "La distribución de probabilidad condicional de cada unidad viene dada por la función de activación sigmoidal logística de la entrada que recibe:"

#: ../modules/neural_networks_unsupervised.rst:100
msgid "P(v_i=1|\\mathbf{h}) = \\sigma(\\sum_j w_{ij}h_j + b_i) \\\\\n"
"P(h_i=1|\\mathbf{v}) = \\sigma(\\sum_i w_{ij}v_i + c_j)"
msgstr "P(v_i=1|\\mathbf{h}) = \\sigma(\\sum_j w_{ij}h_j + b_i) \\\\\n"
"P(h_i=1|\\mathbf{v}) = \\sigma(\\sum_i w_{ij}v_i + c_j)"

#: ../modules/neural_networks_unsupervised.rst:105
msgid "where :math:`\\sigma` is the logistic sigmoid function:"
msgstr "donde :math:`\\sigma` es la función sigmoidal logística:"

#: ../modules/neural_networks_unsupervised.rst:107
msgid "\\sigma(x) = \\frac{1}{1 + e^{-x}}"
msgstr "\\sigma(x) = \\frac{1}{1 + e^{-x}}"

#: ../modules/neural_networks_unsupervised.rst:114
msgid "Stochastic Maximum Likelihood learning"
msgstr "Aprendizaje por máxima verosimilitud estocástica"

#: ../modules/neural_networks_unsupervised.rst:116
msgid "The training algorithm implemented in :class:`BernoulliRBM` is known as Stochastic Maximum Likelihood (SML) or Persistent Contrastive Divergence (PCD). Optimizing maximum likelihood directly is infeasible because of the form of the data likelihood:"
msgstr "El algoritmo de entrenamiento implementado en :class:`BernoulliRBM` se conoce como Máxima Verosimilitud Estocástica (SML) o Divergencia Contrastiva Persistente (PCD). Optimizar la máxima verosimilitud directamente es inviable debido a la forma de la verosimilitud de los datos:"

#: ../modules/neural_networks_unsupervised.rst:121
msgid "\\log P(v) = \\log \\sum_h e^{-E(v, h)} - \\log \\sum_{x, y} e^{-E(x, y)}"
msgstr "\\log P(v) = \\log \\sum_h e^{-E(v, h)} - \\log \\sum_{x, y} e^{-E(x, y)}"

#: ../modules/neural_networks_unsupervised.rst:125
msgid "For simplicity the equation above is written for a single training example. The gradient with respect to the weights is formed of two terms corresponding to the ones above. They are usually known as the positive gradient and the negative gradient, because of their respective signs.  In this implementation, the gradients are estimated over mini-batches of samples."
msgstr "Para simplificar, la ecuación anterior está escrita para un solo ejemplo de entrenamiento. El gradiente con respecto a los pesos está formado por dos términos correspondientes a los anteriores. Suelen conocerse como gradiente positivo y gradiente negativo, por sus respectivos signos.  En esta implementación, los gradientes se estiman sobre mini-lotes de muestras."

#: ../modules/neural_networks_unsupervised.rst:131
msgid "In maximizing the log-likelihood, the positive gradient makes the model prefer hidden states that are compatible with the observed training data. Because of the bipartite structure of RBMs, it can be computed efficiently. The negative gradient, however, is intractable. Its goal is to lower the energy of joint states that the model prefers, therefore making it stay true to the data. It can be approximated by Markov chain Monte Carlo using block Gibbs sampling by iteratively sampling each of :math:`v` and :math:`h` given the other, until the chain mixes. Samples generated in this way are sometimes referred as fantasy particles. This is inefficient and it is difficult to determine whether the Markov chain mixes."
msgstr "Al maximizar la log-verosimilitud, el gradiente positivo hace que el modelo prefiera los estados ocultos que son compatibles con los datos de entrenamiento observados. Debido a la estructura bipartita de los RBM, puede calcularse de forma eficiente. El gradiente negativo, sin embargo, es inmanejable. Su objetivo es reducir la energía de los estados conjuntos que el modelo prefiere, haciendo que se mantenga fiel a los datos. Puede aproximarse mediante el Monte Carlo de cadenas de Markov utilizando el muestreo de Gibbs en bloque, muestreando iterativamente cada uno de los :math:`v` y :math:`h` dado el otro, hasta que la cadena se mezcle. Las muestras generadas de este modo se denominan a veces partículas de fantasía. Esto es ineficiente y es difícil determinar si la cadena de Markov se mezcla."

#: ../modules/neural_networks_unsupervised.rst:142
msgid "The Contrastive Divergence method suggests to stop the chain after a small number of iterations, :math:`k`, usually even 1. This method is fast and has low variance, but the samples are far from the model distribution."
msgstr "El método de Divergencia Contrastiva sugiere detener la cadena después de un pequeño número de iteraciones, :math:`k`, por lo general hasta 1. Este método es rápido y tiene una baja varianza, pero las muestras están lejos de la distribución del modelo."

#: ../modules/neural_networks_unsupervised.rst:146
msgid "Persistent Contrastive Divergence addresses this. Instead of starting a new chain each time the gradient is needed, and performing only one Gibbs sampling step, in PCD we keep a number of chains (fantasy particles) that are updated :math:`k` Gibbs steps after each weight update. This allows the particles to explore the space more thoroughly."
msgstr "La Divergencia Contrastiva Persistente aborda esta cuestión. En lugar de iniciar una nueva cadena cada vez que se necesita el gradiente, y realizar sólo un paso de muestreo de Gibbs, en la PCD mantenemos un número de cadenas (partículas de fantasía) que se actualizan :math:`k` pasos de Gibbs después de cada actualización de ponderaciones. Esto permite que las partículas exploren el espacio más a fondo."

#: ../modules/neural_networks_unsupervised.rst:154
msgid "`\"A fast learning algorithm for deep belief nets\" <https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf>`_ G. Hinton, S. Osindero, Y.-W. Teh, 2006"
msgstr "`\"A fast learning algorithm for deep belief nets\" <https://www.cs.toronto.edu/~hinton/absps/fastnc.pdf>`_ G. Hinton, S. Osindero, Y.-W. Teh, 2006"

#: ../modules/neural_networks_unsupervised.rst:158
msgid "`\"Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient\" <https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf>`_ T. Tieleman, 2008"
msgstr "`\"Training Restricted Boltzmann Machines using Approximations to the Likelihood Gradient\" <https://www.cs.toronto.edu/~tijmen/pcd/pcd.pdf>`_ T. Tieleman, 2008"

