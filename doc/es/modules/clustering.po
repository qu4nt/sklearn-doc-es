msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-26 17:04\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/clustering.po\n"
"X-Crowdin-File-ID: 4850\n"
"Language: es_ES\n"

#: ../modules/clustering.rst:5
msgid "Clustering"
msgstr "Análisis de conglomerados (Agrupamiento)"

#: ../modules/clustering.rst:7
msgid "`Clustering <https://en.wikipedia.org/wiki/Cluster_analysis>`__ of unlabeled data can be performed with the module :mod:`sklearn.cluster`."
msgstr "`El análisis de conglomerados <https://en.wikipedia.org/wiki/Cluster_analysis>`__ de datos no etiquetados puede realizarse con el módulo :mod:`sklearn.cluster`."

#: ../modules/clustering.rst:10
msgid "Each clustering algorithm comes in two variants: a class, that implements the ``fit`` method to learn the clusters on train data, and a function, that, given train data, returns an array of integer labels corresponding to the different clusters. For the class, the labels over the training data can be found in the ``labels_`` attribute."
msgstr "Cada algoritmo de análisis de conglomerados viene en dos variantes: una clase, que implementa el método ``fit`` para aprender los conglomerados en los datos de entrenamiento, y una función, que, dados los datos de entrenamiento, devuelve un arreglo de etiquetas de enteros correspondientes a los diferentes conglomerados. Para la clase, las etiquetas sobre los datos de entrenamiento se pueden encontrar en el atributo ``labels_``."

msgid "Input data"
msgstr "Datos de entrada"

#: ../modules/clustering.rst:20
msgid "One important thing to note is that the algorithms implemented in this module can take different kinds of matrix as input. All the methods accept standard data matrices of shape ``(n_samples, n_features)``. These can be obtained from the classes in the :mod:`sklearn.feature_extraction` module. For :class:`AffinityPropagation`, :class:`SpectralClustering` and :class:`DBSCAN` one can also input similarity matrices of shape ``(n_samples, n_samples)``. These can be obtained from the functions in the :mod:`sklearn.metrics.pairwise` module."
msgstr "Una cosa importante a tener en cuenta es que los algoritmos implementados en este módulo pueden tomar diferentes tipos de matriz como entrada. Todos los métodos aceptan matrices de datos estándar con la forma ``(n_samples, n_features)``. Estos pueden obtenerse de las clases en el módulo :mod:`sklearn. módulo eature_extraction`. Para :class:`AffinityPropagation`, :class:`SpectralClustering` y :class:`DBSCAN` también se pueden introducir matrices de similitud de la forma ``(n_samples, n_muestras)``. Estos se pueden obtener de las funciones en el módulo :mod:`sklearn.metrics.pairwise`."

#: ../modules/clustering.rst:30
msgid "Overview of clustering methods"
msgstr "Resumen de los métodos de análisis de conglomerados"

#: ../modules/clustering.rst:37
msgid "A comparison of the clustering algorithms in scikit-learn"
msgstr "Una comparación de los algoritmos de análisis de conglomerados en scikit-learn"

#: ../modules/clustering.rst:44
msgid "Method name"
msgstr "Nombre del método"

#: ../modules/clustering.rst:45
msgid "Parameters"
msgstr "Parámetros"

#: ../modules/clustering.rst:46
msgid "Scalability"
msgstr "Escalabilidad"

#: ../modules/clustering.rst:47
msgid "Usecase"
msgstr "Casos de uso"

#: ../modules/clustering.rst:48
msgid "Geometry (metric used)"
msgstr "Geometría (métrica utilizada)"

#: ../modules/clustering.rst:50
msgid ":ref:`K-Means <k_means>`"
msgstr ":ref:`K-Medias <k_means>`"

#: ../modules/clustering.rst:51 ../modules/clustering.rst:70
msgid "number of clusters"
msgstr "número de conglomerados"

#: ../modules/clustering.rst:52
msgid "Very large ``n_samples``, medium ``n_clusters`` with :ref:`MiniBatch code <mini_batch_kmeans>`"
msgstr "``n_samples`` muy grande, ``n_clusters`` mediano con :ref:`Código del mini lote <mini_batch_kmeans>`"

#: ../modules/clustering.rst:54
msgid "General-purpose, even cluster size, flat geometry, not too many clusters"
msgstr "Uso general, tamaño uniforme de los conglomerados, geometría plana, no demasiados conglomerados"

#: ../modules/clustering.rst:55 ../modules/clustering.rst:67
#: ../modules/clustering.rst:79 ../modules/clustering.rst:98
msgid "Distances between points"
msgstr "Distancias entre puntos"

#: ../modules/clustering.rst:57
msgid ":ref:`Affinity propagation <affinity_propagation>`"
msgstr ":ref:`Propagación por afinidad <affinity_propagation>`"

#: ../modules/clustering.rst:58
msgid "damping, sample preference"
msgstr "amortiguación, preferencia de muestra"

#: ../modules/clustering.rst:59
msgid "Not scalable with n_samples"
msgstr "No escalable con `n_samples`"

#: ../modules/clustering.rst:60 ../modules/clustering.rst:66
msgid "Many clusters, uneven cluster size, non-flat geometry"
msgstr "Muchos conglomerados, tamaño desigual de los conglomerados, geometría no plana"

#: ../modules/clustering.rst:61 ../modules/clustering.rst:73
msgid "Graph distance (e.g. nearest-neighbor graph)"
msgstr "Distancia gráfica (por ejemplo, el gráfico vecino más cercano)"

#: ../modules/clustering.rst:63
msgid ":ref:`Mean-shift <mean_shift>`"
msgstr ":ref:`Cambio medio <mean_shift>`"

#: ../modules/clustering.rst:64
msgid "bandwidth"
msgstr "bandwidth"

#: ../modules/clustering.rst:65
msgid "Not scalable with ``n_samples``"
msgstr "No escalable con ``n_samples``"

#: ../modules/clustering.rst:69
msgid ":ref:`Spectral clustering <spectral_clustering>`"
msgstr ":ref:`Análisis espectral de conglomerados <spectral_clustering>`"

#: ../modules/clustering.rst:71
msgid "Medium ``n_samples``, small ``n_clusters``"
msgstr "``n_samples`` mediano, ``n_clusters`` pequeño"

#: ../modules/clustering.rst:72
msgid "Few clusters, even cluster size, non-flat geometry"
msgstr "Pocos conglomerados, tamaño uniforme de los conglomerados, geometría no plana"

#: ../modules/clustering.rst:75
msgid ":ref:`Ward hierarchical clustering <hierarchical_clustering>`"
msgstr ":ref:`Análisis de conglomerados jerárquicos de Ward <hierarchical_clustering>`"

#: ../modules/clustering.rst:76
msgid "number of clusters or distance threshold"
msgstr "número de conglomerados o umbral de distancia"

#: ../modules/clustering.rst:77 ../modules/clustering.rst:83
msgid "Large ``n_samples`` and ``n_clusters``"
msgstr "``n_samples`` y ``n_clusters`` grandes"

#: ../modules/clustering.rst:78
msgid "Many clusters, possibly connectivity constraints"
msgstr "Muchos conglomerados, posiblemente con limitaciones de conectividad"

#: ../modules/clustering.rst:81
msgid ":ref:`Agglomerative clustering <hierarchical_clustering>`"
msgstr ":ref:`Análisis de conglomerados aglomerativos <hierarchical_clustering>`"

#: ../modules/clustering.rst:82
msgid "number of clusters or distance threshold, linkage type, distance"
msgstr "número de conglomerados o umbral de distancia, tipo de vinculación, distancia"

#: ../modules/clustering.rst:84
msgid "Many clusters, possibly connectivity constraints, non Euclidean distances"
msgstr "Muchos conglomerados, posibles restricciones de conectividad, distancias no euclidianas"

#: ../modules/clustering.rst:86
msgid "Any pairwise distance"
msgstr "Cualquier distancia entre pares"

#: ../modules/clustering.rst:88
msgid ":ref:`DBSCAN <dbscan>`"
msgstr ":ref:`DBSCAN <dbscan>`"

#: ../modules/clustering.rst:89
msgid "neighborhood size"
msgstr "tamaño del vecindario"

#: ../modules/clustering.rst:90
msgid "Very large ``n_samples``, medium ``n_clusters``"
msgstr "``n_samples`` muy grande, ``n_clusters`` mediano"

#: ../modules/clustering.rst:91
msgid "Non-flat geometry, uneven cluster sizes"
msgstr "Geometría no plana, tamaños desiguales de los conglomerados"

#: ../modules/clustering.rst:92
msgid "Distances between nearest points"
msgstr "Distancias entre los puntos más cercanos"

#: ../modules/clustering.rst:94
msgid ":ref:`OPTICS <optics>`"
msgstr ":ref:`OPTICS <optics>`"

#: ../modules/clustering.rst:95
msgid "minimum cluster membership"
msgstr "número mínimo de miembros del conglomerado"

#: ../modules/clustering.rst:96
msgid "Very large ``n_samples``, large ``n_clusters``"
msgstr "``n_samples`` muy grande, ``n_clusters`` grande"

#: ../modules/clustering.rst:97
msgid "Non-flat geometry, uneven cluster sizes, variable cluster density"
msgstr "Geometría no plana, tamaños desiguales de los conglomerados, densidad variable de los conglomerados"

#: ../modules/clustering.rst:100
msgid ":ref:`Gaussian mixtures <mixture>`"
msgstr ":ref:`Mezclas Gaussianas <mixture>`"

#: ../modules/clustering.rst:101
msgid "many"
msgstr "muchos"

#: ../modules/clustering.rst:102
msgid "Not scalable"
msgstr "No escalable"

#: ../modules/clustering.rst:103
msgid "Flat geometry, good for density estimation"
msgstr "Geometría plana, buena para la estimación de la densidad"

#: ../modules/clustering.rst:104
msgid "Mahalanobis distances to  centers"
msgstr "Distancias de Mahalanobis para los centros"

#: ../modules/clustering.rst:106
msgid ":ref:`Birch`"
msgstr ":ref:`Birch`"

#: ../modules/clustering.rst:107
msgid "branching factor, threshold, optional global clusterer."
msgstr "factor de ramificación, umbral, agrupador global opcional."

#: ../modules/clustering.rst:108
msgid "Large ``n_clusters`` and ``n_samples``"
msgstr "``n_clusters`` y ``n_samples`` grandes"

#: ../modules/clustering.rst:109
msgid "Large dataset, outlier removal, data reduction."
msgstr "Conjunto de datos grande, eliminación de valores atípicos, reducción de datos."

#: ../modules/clustering.rst:110
msgid "Euclidean distance between points"
msgstr "Distancia euclidiana entre puntos"

#: ../modules/clustering.rst:112
msgid "Non-flat geometry clustering is useful when the clusters have a specific shape, i.e. a non-flat manifold, and the standard euclidean distance is not the right metric. This case arises in the two top rows of the figure above."
msgstr "El análisis de conglomerados de geometría no plana es útil cuando los conglomerados tienen una forma específica, es decir, una variedad no plana, y la distancia euclidiana estándar no es la métrica adecuada. Este caso se da en las dos filas superiores de la figura anterior."

#: ../modules/clustering.rst:117
msgid "Gaussian mixture models, useful for clustering, are described in :ref:`another chapter of the documentation <mixture>` dedicated to mixture models. KMeans can be seen as a special case of Gaussian mixture model with equal covariance per component."
msgstr "Los modelos de mezclas Gaussianas, útiles para el análisis de conglomerados, se describen en :ref:`otro capítulo de la documentación <mixture>` dedicado a modelos de mezclas. KMedias puede ser visto como un caso especial del modelo de mezcla Gaussiana con igual covarianza por componente."

#: ../modules/clustering.rst:125
msgid "K-means"
msgstr "K-medias"

#: ../modules/clustering.rst:127
msgid "The :class:`KMeans` algorithm clusters data by trying to separate samples in n groups of equal variance, minimizing a criterion known as the *inertia* or within-cluster sum-of-squares (see below). This algorithm requires the number of clusters to be specified. It scales well to large number of samples and has been used across a large range of application areas in many different fields."
msgstr "El algoritmo :class:`KMeans` agrupa los datos tratando de separar muestras en n grupos de igual varianza, minimizando un criterio conocido como la *inercia* o la suma de cuadrados dentro del conglomerado (ver abajo). Este algoritmo requiere que se especifique el número de conglomerados. Se ajusta bien a un gran número de muestras y se ha utilizado en una una gran variedad de áreas de aplicación en muchos campos diferentes."

#: ../modules/clustering.rst:133
msgid "The k-means algorithm divides a set of :math:`N` samples :math:`X` into :math:`K` disjoint clusters :math:`C`, each described by the mean :math:`\\mu_j` of the samples in the cluster. The means are commonly called the cluster \"centroids\"; note that they are not, in general, points from :math:`X`, although they live in the same space."
msgstr "El algoritmo k-medias divide un conjunto :math:`N` de :math:`X` muestras en :math:`K` conglomerados disjuntos :math:`C`, cada uno descrito por la media :math:`mu_j` de las muestras en el conglomerado. Los medias se denominan comúnmente los \"centroides\" del conglomerado; nótese que no son, en general, puntos de :math:`X`, aunque viven en el mismo espacio."

#: ../modules/clustering.rst:139
msgid "The K-means algorithm aims to choose centroids that minimise the **inertia**, or **within-cluster sum-of-squares criterion**:"
msgstr "El algoritmo K-medias tiene como objetivo elegir los centroides que minimicen la **inercia**, o el **criterio de la suma de cuadrados dentro del conglomerado**:"

#: ../modules/clustering.rst:142
msgid "\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\n\n"
msgstr "\\sum_{i=0}^{n}\\min_{\\mu_j \\in C}(||x_i - \\mu_j||^2)\n\n"

#: ../modules/clustering.rst:144
msgid "Inertia can be recognized as a measure of how internally coherent clusters are. It suffers from various drawbacks:"
msgstr "La inercia puede reconocerse como una medida de la coherencia interna de los conglomerados. Sufre varios inconvenientes:"

#: ../modules/clustering.rst:147
msgid "Inertia makes the assumption that clusters are convex and isotropic, which is not always the case. It responds poorly to elongated clusters, or manifolds with irregular shapes."
msgstr "La inercia supone que los conglomerados son convexos e isotrópicos, lo que no siempre es el caso. Responde mal a los conglomerados alargados o a las variedades con formas irregulares."

#: ../modules/clustering.rst:151
msgid "Inertia is not a normalized metric: we just know that lower values are better and zero is optimal. But in very high-dimensional spaces, Euclidean distances tend to become inflated (this is an instance of the so-called \"curse of dimensionality\"). Running a dimensionality reduction algorithm such as :ref:`PCA` prior to k-means clustering can alleviate this problem and speed up the computations."
msgstr "La inercia no es una métrica normalizada: sólo sabemos que los valores más bajos son mejores y cero es óptimo. Pero en espacios de muy alta dimensión, las distancias Euclidianas tienden a inflarse (este es un ejemplo de la llamada \"maldición de la dimensión\"). Ejecutar un algoritmo de reducción de dimensionalidad como :ref:`PCA` antes de un análisis de conglomerados mediante k-medias puede minimizar este problema y acelerar los cálculos."

#: ../modules/clustering.rst:164
msgid "K-means is often referred to as Lloyd's algorithm. In basic terms, the algorithm has three steps. The first step chooses the initial centroids, with the most basic method being to choose :math:`k` samples from the dataset :math:`X`. After initialization, K-means consists of looping between the two other steps. The first step assigns each sample to its nearest centroid. The second step creates new centroids by taking the mean value of all of the samples assigned to each previous centroid. The difference between the old and the new centroids are computed and the algorithm repeats these last two steps until this value is less than a threshold. In other words, it repeats until the centroids do not move significantly."
msgstr "K-medias se conoce a menudo como el algoritmo de Lloyd. En términos básicos, el algoritmo tiene tres pasos. El primer paso elige los centroides iniciales, siendo el método más básico elegir :math:`k` muestras del conjunto de datos :math:`X`. Después de la inicialización, K-medias consiste en un bucle entre los otros dos pasos. El primer paso asigna cada muestra a su centroide más cercano. El segundo paso crea nuevos centroides tomando el valor medio de todas las muestras asignadas a cada centroide anterior. Se calcula la diferencia entre los centroides antiguos y los nuevos y el algoritmo repite estos dos últimos pasos hasta que este valor sea menor que un umbral. En otras palabras, se repite hasta que los centroides no se muevan de forma significativa."

#: ../modules/clustering.rst:180
msgid "K-means is equivalent to the expectation-maximization algorithm with a small, all-equal, diagonal covariance matrix."
msgstr "K-medias es equivalente al algoritmo de maximización de la esperanza con una matriz de covarianzas pequeña, toda igual y diagonal."

#: ../modules/clustering.rst:183
msgid "The algorithm can also be understood through the concept of `Voronoi diagrams <https://en.wikipedia.org/wiki/Voronoi_diagram>`_. First the Voronoi diagram of the points is calculated using the current centroids. Each segment in the Voronoi diagram becomes a separate cluster. Secondly, the centroids are updated to the mean of each segment. The algorithm then repeats this until a stopping criterion is fulfilled. Usually, the algorithm stops when the relative decrease in the objective function between iterations is less than the given tolerance value. This is not the case in this implementation: iteration stops when centroids move less than the tolerance."
msgstr "El algoritmo también puede entenderse a través del concepto de Voronoi diagrams <https://en.wikipedia.org/wiki/Voronoi_diagram>`_. En primer lugar, se calcula el diagrama de Voronoi de los puntos utilizando los centroides actuales. Cada segmento del diagrama de Voronoi se convierte en un conglomerado separado. En segundo lugar, los centroides se actualizan a la media de cada segmento. El algoritmo repite esto hasta que se cumpla un criterio de parada. Normalmente, el algoritmo se detiene cuando la disminución relativa de la función objetiva entre iteraciones es menor que el valor de tolerancia dado. Este no es el caso en esta implementación: la iteración se detiene cuando los centroides se mueven por debajo de la tolerancia."

#: ../modules/clustering.rst:193
msgid "Given enough time, K-means will always converge, however this may be to a local minimum. This is highly dependent on the initialization of the centroids. As a result, the computation is often done several times, with different initializations of the centroids. One method to help address this issue is the k-means++ initialization scheme, which has been implemented in scikit-learn (use the ``init='k-means++'`` parameter). This initializes the centroids to be (generally) distant from each other, leading to provably better results than random initialization, as shown in the reference."
msgstr "Con suficiente tiempo, K-medias siempre convergerá sin embargo, esto puede ser a un mínimo local. Esto depende en gran medida de la inicialización de los centroides. Como resultado, el cálculo se realiza a menudo varias veces, con diferentes inicializaciones de los centroides. Un método que ayuda a resolver este problema es el esquema de inicialización de k-medias++, que se ha implementado en scikit-learn (utilice el parámetro ``init='k-means++'``). Esto inicializa los centroides para que sean (generalmente) distantes entre sí, conduciendo a resultados probadamente mejores que la inicialización aleatoria, como se muestra en la referencia."

#: ../modules/clustering.rst:202
msgid "K-means++ can also be called independently to select seeds for other clustering algorithms, see :func:`sklearn.cluster.kmeans_plusplus` for details and example usage."
msgstr "K-medias++ también puede ser llamado independientemente para seleccionar semillas para otros algoritmos de análisis de conglomerados ver :func:`sklearn.cluster.kmeans_plus` para detalles y ejemplos de uso."

#: ../modules/clustering.rst:206
msgid "The algorithm supports sample weights, which can be given by a parameter ``sample_weight``. This allows to assign more weight to some samples when computing cluster centers and values of inertia. For example, assigning a weight of 2 to a sample is equivalent to adding a duplicate of that sample to the dataset :math:`X`."
msgstr "El algoritmo admite pesos de las muestras, que pueden ser dados por un parámetro ``sample_weight``. Esto permite asignar más peso a algunas muestras cuando se calculan los centros de los conglomerados y los valores de inercia. Por ejemplo, asignar un peso de 2 a una muestra equivale a añadir un duplicado de esa muestra al conjunto de datos :math:`X`."

#: ../modules/clustering.rst:212
msgid "K-means can be used for vector quantization. This is achieved using the transform method of a trained model of :class:`KMeans`."
msgstr "K-medias puede utilizarse para la cuantificación de vectores. Esto se consigue utilizando el método de transformación de un modelo entrenado de :class:`KMeans`."

#: ../modules/clustering.rst:216
msgid "Low-level parallelism"
msgstr "Paralelismo de bajo nivel"

#: ../modules/clustering.rst:218
msgid ":class:`KMeans` benefits from OpenMP based parallelism through Cython. Small chunks of data (256 samples) are processed in parallel, which in addition yields a low memory footprint. For more details on how to control the number of threads, please refer to our :ref:`parallelism` notes."
msgstr ":class:`KMeans` se beneficia del paralelismo basado en OpenMP a través de Cython. Pequeñas porciones de datos (256 muestras) se procesan en paralelo, lo que, además, supone un bajo consumo de memoria. Para más detalles sobre cómo controlar el número de hilos, por favor consulta nuestras notas de :ref:`parallelism`."

#: ../modules/clustering.rst:225
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`: Demonstrating when k-means performs intuitively and when it does not"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_kmeans_assumptions.py`: Demostración de cuándo k-medias funciona intuitivamente y cuándo no"

#: ../modules/clustering.rst:227
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`: Clustering handwritten digits"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_kmeans_digits.py`: Análisis de conglomerados de dígitos manuscritos"

#: ../modules/clustering.rst:231
msgid "`\"k-means++: The advantages of careful seeding\" <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_ Arthur, David, and Sergei Vassilvitskii, *Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms*, Society for Industrial and Applied Mathematics (2007)"
msgstr "`\"k-means++: The advantages of careful seeding\" <http://ilpubs.stanford.edu:8090/778/1/2006-13.pdf>`_ Arthur, David, and Sergei Vassilvitskii, *Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms*, Society for Industrial and Applied Mathematics (2007)"

#: ../modules/clustering.rst:240
msgid "Mini Batch K-Means"
msgstr "K-medias de mini lotes"

#: ../modules/clustering.rst:242
msgid "The :class:`MiniBatchKMeans` is a variant of the :class:`KMeans` algorithm which uses mini-batches to reduce the computation time, while still attempting to optimise the same objective function. Mini-batches are subsets of the input data, randomly sampled in each training iteration. These mini-batches drastically reduce the amount of computation required to converge to a local solution. In contrast to other algorithms that reduce the convergence time of k-means, mini-batch k-means produces results that are generally only slightly worse than the standard algorithm."
msgstr "El :class:`MiniBatchKMeans` es una variante del algoritmo :class:`KMeans` que usa mini lotes para reducir el tiempo de cálculo, mientras se intenta optimizar la misma función objetivo. Los mini lotes son subconjuntos de los datos de entrada, muestreados aleatoriamente en cada iteración de entrenamiento. Estos mini lotes reducen drásticamente la cantidad de cálculo necesaria para converger a una solución local. En contraste con otros algoritmos que reducen el tiempo de convergencia de k-medias, k-medias en mini lotes produce resultados que generalmente son ligeramente peores que el algoritmo estándar."

#: ../modules/clustering.rst:251
msgid "The algorithm iterates between two major steps, similar to vanilla k-means. In the first step, :math:`b` samples are drawn randomly from the dataset, to form a mini-batch. These are then assigned to the nearest centroid. In the second step, the centroids are updated. In contrast to k-means, this is done on a per-sample basis. For each sample in the mini-batch, the assigned centroid is updated by taking the streaming average of the sample and all previous samples assigned to that centroid. This has the effect of decreasing the rate of change for a centroid over time. These steps are performed until convergence or a predetermined number of iterations is reached."
msgstr "El algoritmo itera entre dos pasos principales, similares a las k-medias vanilla. En el primer paso, las :math:`b` muestras se extraen aleatoriamente del conjunto de datos, para formar un mini lote. Estos se asignan al centroide más cercano. En el segundo paso, se actualizan los centroides. En contraste con las k-medias, esto se hace sobre una base por muestra. Para cada muestra en el mini lote, el centroide asignado se actualiza tomando la media de transmisión de la muestra y todas las muestras anteriores asignadas a ese centroide. Esto tiene el efecto de disminuir la tasa de cambio de un centroide en el tiempo. Estos pasos se realizan hasta que se alcanza la convergencia o un número predeterminado de iteraciones."

#: ../modules/clustering.rst:261
msgid ":class:`MiniBatchKMeans` converges faster than :class:`KMeans`, but the quality of the results is reduced. In practice this difference in quality can be quite small, as shown in the example and cited reference."
msgstr ":class:`MiniBatchKMeans` converge más rápido que :class:`KMeans`, pero la calidad de los resultados se reduce. En la práctica, esta diferencia de calidad puede ser bastante pequeña, como se muestra en el ejemplo y referencia citada."

#: ../modules/clustering.rst:273
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`: Comparison of KMeans and MiniBatchKMeans"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_mini_batch_kmeans.py`: Comparación de las KMedias y K-Medias de mini lotes"

#: ../modules/clustering.rst:276
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: Document clustering using sparse MiniBatchKMeans"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`: Análisis de conglomerados de documentos utilizando K-medias de mini lotes dispersos"

#: ../modules/clustering.rst:279
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`"

#: ../modules/clustering.rst:284
msgid "`\"Web Scale K-Means clustering\" <https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_ D. Sculley, *Proceedings of the 19th international conference on World wide web* (2010)"
msgstr "`\"Web Scale K-Means clustering\" <https://www.eecs.tufts.edu/~dsculley/papers/fastkmeans.pdf>`_ D. Sculley, *Proceedings of the 19th international conference on World wide web* (2010)"

#: ../modules/clustering.rst:292
msgid "Affinity Propagation"
msgstr "Propagación de la afinidad"

#: ../modules/clustering.rst:294
msgid ":class:`AffinityPropagation` creates clusters by sending messages between pairs of samples until convergence. A dataset is then described using a small number of exemplars, which are identified as those most representative of other samples. The messages sent between pairs represent the suitability for one sample to be the exemplar of the other, which is updated in response to the values from other pairs. This updating happens iteratively until convergence, at which point the final exemplars are chosen, and hence the final clustering is given."
msgstr ":class:`AffinityPropagation` crea conglomerados enviando mensajes entre pares de muestras hasta la convergencia. A continuación, se describe un conjunto de datos utilizando un pequeño número de ejemplares, que se identifican como los más representativos de otras muestras. Los mensajes enviados entre pares representan la idoneidad de una muestra para ser el ejemplar de la otra, que se actualiza en respuesta a los valores de otros pares. Esta actualización ocurre iterativamente hasta la convergencia, momento en el que se eligen los ejemplos finales, y por lo tanto se da el análisis de conglomerados final."

#: ../modules/clustering.rst:309
msgid "Affinity Propagation can be interesting as it chooses the number of clusters based on the data provided. For this purpose, the two important parameters are the *preference*, which controls how many exemplars are used, and the *damping factor* which damps the responsibility and availability messages to avoid numerical oscillations when updating these messages."
msgstr "La Propagación de la Afinidad puede ser interesante ya que elige el número de conglomerados basado en los datos proporcionados. Para este propósito, los dos parámetros importantes son la *preferencia*, que controla cuántos ejemplares se utilizan, y el *factor de amortiguación* que reduce la responsabilidad y la disponibilidad de los mensajes para evitar oscilaciones numéricas al actualizar estos mensajes."

#: ../modules/clustering.rst:316
msgid "The main drawback of Affinity Propagation is its complexity. The algorithm has a time complexity of the order :math:`O(N^2 T)`, where :math:`N` is the number of samples and :math:`T` is the number of iterations until convergence. Further, the memory complexity is of the order :math:`O(N^2)` if a dense similarity matrix is used, but reducible if a sparse similarity matrix is used. This makes Affinity Propagation most appropriate for small to medium sized datasets."
msgstr "El principal inconveniente de la Propagación de la Afinidad es su complejidad. El algoritmo tiene una complejidad temporal del orden :math:`O(N^2 T)`, donde :math:`N` es el número de muestras y :math:`T` es el número de iteraciones hasta la convergencia. Además, la complejidad de la memoria es del orden :math:`O(N^2)` si se utiliza una matriz de similitud densa, pero reducible si se utiliza una matriz de similitud dispersa. Esto hace que la Propagación de la Afinidad sea la más apropiada para los conjuntos de datos de tamaño pequeño o mediano."

#: ../modules/clustering.rst:326
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`: Affinity Propagation on a synthetic 2D datasets with 3 classes."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_affinity_propagation.py`: Propagación de la Afinidad en un conjunto de datos sintéticos 2D con 3 clases."

#: ../modules/clustering.rst:329
msgid ":ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` Affinity Propagation on Financial time series to find groups of companies"
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_stock_market.py` Propagación de afinidad en series temporales financieras para encontrar grupos de empresas"

#: ../modules/clustering.rst:333
msgid "**Algorithm description:** The messages sent between points belong to one of two categories. The first is the responsibility :math:`r(i, k)`, which is the accumulated evidence that sample :math:`k` should be the exemplar for sample :math:`i`. The second is the availability :math:`a(i, k)` which is the accumulated evidence that sample :math:`i` should choose sample :math:`k` to be its exemplar, and considers the values for all other samples that :math:`k` should be an exemplar. In this way, exemplars are chosen by samples if they are (1) similar enough to many samples and (2) chosen by many samples to be representative of themselves."
msgstr "**Descripción del algoritmo:** Los mensajes enviados entre puntos pertenecen a una de las dos categorías. La primera es la responsabilidad :math:`r(i, k)`, que es la evidencia acumulada de que la muestra :math:`k` debe ser el ejemplar para la muestra :math:`i`. La segunda es la disponibilidad :math:`a(i, k)` que es la evidencia acumulada de que la muestra :math:`i` debe elegir la muestra :math:`k` para ser su ejemplar, y considera los valores para todas las demás muestras que :math:`k` debe ser un ejemplar. De este modo, los ejemplares son elegidos por las muestras si son (1) lo suficientemente similares a muchas muestras y (2) elegidos por muchas muestras para ser representativos de sí mismos."

#: ../modules/clustering.rst:346
msgid "More formally, the responsibility of a sample :math:`k` to be the exemplar of sample :math:`i` is given by:"
msgstr "Más formalmente, la responsabilidad de una muestra :math:`k` de ser el ejemplar de la muestra :math:`i` viene dada por:"

#: ../modules/clustering.rst:349
msgid "r(i, k) \\leftarrow s(i, k) - max [ a(i, k') + s(i, k') \\forall k' \\neq k ]"
msgstr "r(i, k) \\leftarrow s(i, k) - max [ a(i, k') + s(i, k') \\forall k' \\neq k ]"

#: ../modules/clustering.rst:353
msgid "Where :math:`s(i, k)` is the similarity between samples :math:`i` and :math:`k`. The availability of sample :math:`k` to be the exemplar of sample :math:`i` is given by:"
msgstr "Donde :math:`s(i, k)` es la similitud entre las muestras :math:`i` y :math:`k`. La disponibilidad de la muestra :math:`k` para ser el ejemplar de la muestra :math:`i` viene dada por:"

#: ../modules/clustering.rst:357
msgid "a(i, k) \\leftarrow min [0, r(k, k) + \\sum_{i'~s.t.~i' \\notin \\{i, k\\}}{r(i', k)}]"
msgstr "a(i, k) \\leftarrow min [0, r(k, k) + \\sum_{i'~s.t.~i' \\notin \\{i, k\\}}{r(i', k)}]"

#: ../modules/clustering.rst:361
msgid "To begin with, all values for :math:`r` and :math:`a` are set to zero, and the calculation of each iterates until convergence. As discussed above, in order to avoid numerical oscillations when updating the messages, the damping factor :math:`\\lambda` is introduced to iteration process:"
msgstr "Para empezar, todos los valores de :math:`r` y :math:`a` se establecen en cero, y el cálculo de cada uno itera hasta la convergencia. Como se ha comentado anteriormente, para evitar las oscilaciones numéricas al actualizar los mensajes, se introduce el factor de amortiguación :math:`\\lambda` en el proceso de iteración:"

#: ../modules/clustering.rst:366
msgid "r_{t+1}(i, k) = \\lambda\\cdot r_{t}(i, k) + (1-\\lambda)\\cdot r_{t+1}(i, k)\n\n"
msgstr "r_{t+1}(i, k) = \\lambda\\cdot r_{t}(i, k) + (1-\\lambda)\\cdot r_{t+1}(i, k)\n\n"

#: ../modules/clustering.rst:367
msgid "a_{t+1}(i, k) = \\lambda\\cdot a_{t}(i, k) + (1-\\lambda)\\cdot a_{t+1}(i, k)\n\n"
msgstr "a_{t+1}(i, k) = \\lambda\\cdot a_{t}(i, k) + (1-\\lambda)\\cdot a_{t+1}(i, k)\n\n"

#: ../modules/clustering.rst:369
msgid "where :math:`t` indicates the iteration times."
msgstr "donde :math:`t` indica los tiempos de iteración."

#: ../modules/clustering.rst:374
msgid "Mean Shift"
msgstr "Media desplazada"

#: ../modules/clustering.rst:375
msgid ":class:`MeanShift` clustering aims to discover *blobs* in a smooth density of samples. It is a centroid based algorithm, which works by updating candidates for centroids to be the mean of the points within a given region. These candidates are then filtered in a post-processing stage to eliminate near-duplicates to form the final set of centroids."
msgstr "El análisis de conglomerados por :class:`MeanShift` tiene como objetivo descubrir *manchas* en una densidad suave de muestras. Se trata de un algoritmo basado en el centroide, que funciona actualizando los candidatos a centroides para que sean la media de los puntos dentro de una región determinada. Estos candidatos se filtran en una etapa de post-procesamiento para eliminar los duplicados cercanos y formar el conjunto final de centroides."

#: ../modules/clustering.rst:381
msgid "Given a candidate centroid :math:`x_i` for iteration :math:`t`, the candidate is updated according to the following equation:"
msgstr "Dado un centroide candidato :math:`x_i` para la iteración :math:`t`, el candidato se actualiza según la siguiente ecuación:"

#: ../modules/clustering.rst:384
msgid "x_i^{t+1} = m(x_i^t)"
msgstr "x_i^{t+1} = m(x_i^t)"

#: ../modules/clustering.rst:388
msgid "Where :math:`N(x_i)` is the neighborhood of samples within a given distance around :math:`x_i` and :math:`m` is the *mean shift* vector that is computed for each centroid that points towards a region of the maximum increase in the density of points. This is computed using the following equation, effectively updating a centroid to be the mean of the samples within its neighborhood:"
msgstr "Donde :math:`N(x_i)` es el vecindario de muestras dentro de una distancia determinada alrededor de :math:`x_i` y :math:`m` es el vector de *medias desplazadas* que se calcula para cada centroide que apunta hacia una región de máximo aumento de la densidad de puntos. Esto se calcula utilizando la siguiente ecuación, actualizando efectivamente un centroide para que sea la media de las muestras dentro de su vecindario:"

#: ../modules/clustering.rst:394
msgid "m(x_i) = \\frac{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)x_j}{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)}"
msgstr "m(x_i) = \\frac{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)x_j}{\\sum_{x_j \\in N(x_i)}K(x_j - x_i)}"

#: ../modules/clustering.rst:398
msgid "The algorithm automatically sets the number of clusters, instead of relying on a parameter ``bandwidth``, which dictates the size of the region to search through. This parameter can be set manually, but can be estimated using the provided ``estimate_bandwidth`` function, which is called if the bandwidth is not set."
msgstr "El algoritmo establece automáticamente el número de conglomerados, en lugar de depender de un parámetro ``bandwidth``, que dicta el tamaño de la región a buscar. Este parámetro se puede definir manualmente, pero se puede estimar usando la función proporcionada ``estimate_bandwidth``, que se llama si el bandwidth no está establecido."

#: ../modules/clustering.rst:403
msgid "The algorithm is not highly scalable, as it requires multiple nearest neighbor searches during the execution of the algorithm. The algorithm is guaranteed to converge, however the algorithm will stop iterating when the change in centroids is small."
msgstr "El algoritmo no es altamente escalable, ya que requiere múltiples búsquedas del vecino más cercano durante la ejecución del algoritmo. El algoritmo está garantizado para converger, sin embargo, el algoritmo dejará de iterar cuando el cambio en los centroides sea pequeño."

#: ../modules/clustering.rst:408
msgid "Labelling a new sample is performed by finding the nearest centroid for a given sample."
msgstr "El etiquetado de una nueva muestra se realiza buscando el centroide más cercano para una muestra determinada."

#: ../modules/clustering.rst:420
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`: Mean Shift clustering on a synthetic 2D datasets with 3 classes."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_mean_shift.py`: El análisis de conglomerados por media desplazada en un conjunto de datos 2D sintéticos con 3 clases."

#: ../modules/clustering.rst:425
msgid "`\"Mean shift: A robust approach toward feature space analysis.\" <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&rep=rep1&type=pdf>`_ D. Comaniciu and P. Meer, *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2002)"
msgstr "`\"Mean shift: A robust approach toward feature space analysis.\" <http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.76.8968&rep=rep1&type=pdf>`_ D. Comaniciu and P. Meer, *IEEE Transactions on Pattern Analysis and Machine Intelligence* (2002)"

#: ../modules/clustering.rst:433
msgid "Spectral clustering"
msgstr "Análisis espectral de conglomerados"

#: ../modules/clustering.rst:435
msgid ":class:`SpectralClustering` performs a low-dimension embedding of the affinity matrix between samples, followed by clustering, e.g., by KMeans, of the components of the eigenvectors in the low dimensional space. It is especially computationally efficient if the affinity matrix is sparse and the `amg` solver is used for the eigenvalue problem (Note, the `amg` solver requires that the `pyamg <https://github.com/pyamg/pyamg>`_ module is installed.)"
msgstr ":class:`SpectralClustering` realiza una incrustación de baja dimensión de la matriz de afinidad entre las muestras, seguida del agrupamiento, por ejemplo, mediante KMedias, de los componentes de los autovectores en el espacio de baja dimensión. Es especialmente eficiente desde el punto de vista computacional si la matriz de afinidad es dispersa y se utiliza el solucionador `amg` para el problema de autovalores (Nota, el solucionador `amg` requiere que el módulo `pyamg <https://github.com/pyamg/pyamg>`_ esté instalado.)"

#: ../modules/clustering.rst:442
msgid "The present version of SpectralClustering requires the number of clusters to be specified in advance. It works well for a small number of clusters, but is not advised for many clusters."
msgstr "La versión actual de SpectralClustering requiere que se especifique de antemano el número de conglomerados. Funciona bien para un número pequeño de conglomerados, pero no se aconseja para muchos conglomerados."

#: ../modules/clustering.rst:446
msgid "For two clusters, SpectralClustering solves a convex relaxation of the `normalised cuts <https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf>`_ problem on the similarity graph: cutting the graph in two so that the weight of the edges cut is small compared to the weights of the edges inside each cluster. This criteria is especially interesting when working on images, where graph vertices are pixels, and weights of the edges of the similarity graph are computed using a function of a gradient of the image."
msgstr "Para dos conglomerados, SpectralClustering resuelve una relajación convexa del problema de los `cortes normalizados <https://people.eecs.berkeley.edu/~malik/papers/SM-ncut.pdf>`_ en el grafo de similitud: cortar el grafo en dos para que el peso de las aristas cortadas sea pequeño comparado con los pesos de las aristas dentro de cada conglomerado. Este criterio es especialmente interesante cuando se trabaja con imágenes, en las que los vértices del grafo son píxeles, y los pesos de las aristas del grafo de similitud se calculan utilizando una función del gradiente de la imagen."

#: ../modules/clustering.rst:464
msgid "noisy_img segmented_img"
msgstr "noisy_img segmented_img"

#: ../modules/clustering.rst:465
msgid "Transforming distance to well-behaved similarities"
msgstr "Transformación de la distancia en similitudes bien avenidas"

#: ../modules/clustering.rst:467
msgid "Note that if the values of your similarity matrix are not well distributed, e.g. with negative values or with a distance matrix rather than a similarity, the spectral problem will be singular and the problem not solvable. In which case it is advised to apply a transformation to the entries of the matrix. For instance, in the case of a signed distance matrix, is common to apply a heat kernel::"
msgstr "Tenga en cuenta que si los valores de su matriz de similitud no están bien distribuidos, por ejemplo, con valores negativos o con una matriz de distancias en lugar de similitud, el problema espectral será singular y el problema no se podrá resolver. En ese caso, se aconseja aplicar una transformación a las entradas de la matriz. Por ejemplo, en el caso de una matriz de distancias es común aplicar un kernel de calor::"

#: ../modules/clustering.rst:476
msgid "See the examples for such an application."
msgstr "Vea los ejemplos para una aplicación de este tipo."

#: ../modules/clustering.rst:480
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: Segmenting objects from a noisy background using spectral clustering."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`: Segmentación de objetos de un contexto ruidoso utilizando análisis espectral de conglomerados."

#: ../modules/clustering.rst:483
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: Spectral clustering to split the image of coins in regions."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_coin_segmentation.py`: Análisis Espectral de Conglomerados para dividir la imagen de las monedas en regiones."

#: ../modules/clustering.rst:495
msgid "Different label assignment strategies"
msgstr "Diferentes estrategias de asignación de etiquetas"

#: ../modules/clustering.rst:497
#, python-format
msgid "Different label assignment strategies can be used, corresponding to the ``assign_labels`` parameter of :class:`SpectralClustering`. ``\"kmeans\"`` strategy can match finer details, but can be unstable. In particular, unless you control the ``random_state``, it may not be reproducible from run-to-run, as it depends on random initialization. The alternative ``\"discretize\"`` strategy is 100% reproducible, but tends to create parcels of fairly even and geometrical shape."
msgstr "Se pueden utilizar diferentes estrategias de asignación de etiquetas, correspondientes al parámetro ``assign_labels`` de :class:`SpectralClustering`. La estrategia ``\"kmeans\"`` puede coincidir con detalles más precisos, pero puede ser inestable. En particular, a menos que controle el ``random_state``, puede no ser reproducible de una ejecución a otra, ya que depende de una inicialización aleatoria. La estrategia alternativa ``\"discretize\"`` es 100% reproducible, pero tiende a crear parcelas de forma bastante uniforme y geométrica."

#: ../modules/clustering.rst:506
msgid "``assign_labels=\"kmeans\"``"
msgstr "``assign_labels=\"kmeans\"``"

#: ../modules/clustering.rst:506
msgid "``assign_labels=\"discretize\"``"
msgstr "``assign_labels=\"discretize\"``"

#: ../modules/clustering.rst:508
msgid "|coin_kmeans|"
msgstr "|coin_kmeans|"

#: ../modules/clustering.rst:508
msgid "|coin_discretize|"
msgstr "|coin_discretize|"

#: ../modules/clustering.rst:512
msgid "Spectral Clustering Graphs"
msgstr "Grafos de Análisis Espectral de conglomerados"

#: ../modules/clustering.rst:514
msgid "Spectral Clustering can also be used to partition graphs via their spectral embeddings.  In this case, the affinity matrix is the adjacency matrix of the graph, and SpectralClustering is initialized with `affinity='precomputed'`::"
msgstr "El Análisis Espectral de Conglomerados también puede utilizarse para dividir los grafos a través de sus incrustaciones espectrales.  En este caso, la matriz de afinidad es la matriz de adyacencia del grafo, y SpectralClustering se inicializa con `affinity='precomputed'`::"

#: ../modules/clustering.rst:525
msgid "`\"A Tutorial on Spectral Clustering\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323>`_ Ulrike von Luxburg, 2007"
msgstr "`\"A Tutorial on Spectral Clustering\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.165.9323>`_ Ulrike von Luxburg, 2007"

#: ../modules/clustering.rst:529
msgid "`\"Normalized cuts and image segmentation\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324>`_ Jianbo Shi, Jitendra Malik, 2000"
msgstr "`\"Normalized cuts and image segmentation\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.160.2324>`_ Jianbo Shi, Jitendra Malik, 2000"

#: ../modules/clustering.rst:533
msgid "`\"A Random Walks View of Spectral Segmentation\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501>`_ Marina Meila, Jianbo Shi, 2001"
msgstr "`\"A Random Walks View of Spectral Segmentation\" <http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.1501>`_ Marina Meila, Jianbo Shi, 2001"

#: ../modules/clustering.rst:537
msgid "`\"On Spectral Clustering: Analysis and an algorithm\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100>`_ Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001"
msgstr "`\"On Spectral Clustering: Analysis and an algorithm\" <http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.19.8100>`_ Andrew Y. Ng, Michael I. Jordan, Yair Weiss, 2001"

#: ../modules/clustering.rst:541
msgid "`\"Preconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge\" <https://arxiv.org/abs/1708.07481>`_ David Zhuzhunashvili, Andrew Knyazev"
msgstr "`\"Preconditioned Spectral Clustering for Stochastic Block Partition Streaming Graph Challenge\" <https://arxiv.org/abs/1708.07481>`_ David Zhuzhunashvili, Andrew Knyazev"

#: ../modules/clustering.rst:549
msgid "Hierarchical clustering"
msgstr "Análisis de conglomerados jerárquicos"

#: ../modules/clustering.rst:551
msgid "Hierarchical clustering is a general family of clustering algorithms that build nested clusters by merging or splitting them successively. This hierarchy of clusters is represented as a tree (or dendrogram). The root of the tree is the unique cluster that gathers all the samples, the leaves being the clusters with only one sample. See the `Wikipedia page <https://en.wikipedia.org/wiki/Hierarchical_clustering>`_ for more details."
msgstr "El análisis de conglomerados jerárquicos es una familia general de los algoritmos de agrupamiento que construyen conglomerados anidados fusionándolos o dividiéndolos sucesivamente. Esta jerarquía de conglomerados se representa como un árbol (o dendrograma). La raíz del árbol es el conglomerado único que reúne todas las muestras, siendo las hojas los conglomerados con una sola muestra. Véase la página de la `Wikipedia <https://en.wikipedia.org/wiki/Hierarchical_clustering>`_ para más detalles."

#: ../modules/clustering.rst:558
msgid "The :class:`AgglomerativeClustering` object performs a hierarchical clustering using a bottom up approach: each observation starts in its own cluster, and clusters are successively merged together. The linkage criteria determines the metric used for the merge strategy:"
msgstr "El objeto :class:`AgglomerativeClustering` realiza un análisis de conglomerados jerárquicos utilizando un enfoque ascendente: cada observación comienza en su propio conglomerado y los conglomerados se fusionan sucesivamente. El criterio de enlace determina la métrica utilizada para la estrategia de fusión:"

#: ../modules/clustering.rst:563
msgid "**Ward** minimizes the sum of squared differences within all clusters. It is a variance-minimizing approach and in this sense is similar to the k-means objective function but tackled with an agglomerative hierarchical approach."
msgstr "**Ward** minimiza la suma de las diferencias al cuadrado dentro de todos los conglomerados. Se trata de un enfoque de minimización de la varianza y, en este sentido, es similar a la función objetivo de k-medias pero abordada con un enfoque jerárquico aglomerativo."

#: ../modules/clustering.rst:567
msgid "**Maximum** or **complete linkage** minimizes the maximum distance between observations of pairs of clusters."
msgstr "**Máximo** o **enlazamiento completo** minimiza la distancia máxima entre las observaciones de pares de conglomerados."

#: ../modules/clustering.rst:569
msgid "**Average linkage** minimizes the average of the distances between all observations of pairs of clusters."
msgstr "**Enlazamiento promedio** minimiza el promedio de las distancias entre todas las observaciones de pares de conglomerados."

#: ../modules/clustering.rst:571
msgid "**Single linkage** minimizes the distance between the closest observations of pairs of clusters."
msgstr "El **enlazamiento simple** minimiza la distancia entre las observaciones más cercanas de los pares de conglomerados."

#: ../modules/clustering.rst:574
msgid ":class:`AgglomerativeClustering` can also scale to large number of samples when it is used jointly with a connectivity matrix, but is computationally expensive when no connectivity constraints are added between samples: it considers at each step all the possible merges."
msgstr ":class:`AglomerativeClustering` también puede escalar a un gran número de muestras cuando se utiliza conjuntamente con una matriz de conectividad, pero es computacionalmente costoso cuando no se añaden restricciones de conectividad entre las muestras: considera en cada paso todas las fusiones posibles."

msgid ":class:`FeatureAgglomeration`"
msgstr ":class:`FeatureAgglomeration`"

#: ../modules/clustering.rst:581
msgid "The :class:`FeatureAgglomeration` uses agglomerative clustering to group together features that look very similar, thus decreasing the number of features. It is a dimensionality reduction tool, see :ref:`data_reduction`."
msgstr ":class:`FeatureAgglomeration` utiliza el agrupamiento aglomerativo para agrupar características muy similares, disminuyendo así el número de características. Es una herramienta de reducción de la dimensionalidad, véase :ref:`data_reduction`."

#: ../modules/clustering.rst:587
msgid "Different linkage type: Ward, complete, average, and single linkage"
msgstr "Diferentes tipos de enlazamientos: Ward, completo, promedio y enlazamiento simple"

#: ../modules/clustering.rst:589
msgid ":class:`AgglomerativeClustering` supports Ward, single, average, and complete linkage strategies."
msgstr ":class:`AgglomerativeClustering` admite las estrategias de enlazamientos Ward, simple, promedio, y completos."

#: ../modules/clustering.rst:596
msgid "Agglomerative cluster has a \"rich get richer\" behavior that leads to uneven cluster sizes. In this regard, single linkage is the worst strategy, and Ward gives the most regular sizes. However, the affinity (or distance used in clustering) cannot be varied with Ward, thus for non Euclidean metrics, average linkage is a good alternative. Single linkage, while not robust to noisy data, can be computed very efficiently and can therefore be useful to provide hierarchical clustering of larger datasets. Single linkage can also perform well on non-globular data."
msgstr "El conglomerado aglomerativo tiene un comportamiento de \"los ricos se hacen más ricos\" que conduce a tamaños de conglomerado desiguales. En este sentido, el enlazamiento simple es la peor estrategia, y Ward da los tamaños más regulares. Sin embargo, la afinidad (o distancia utilizada en la conglomeración) no puede variarse con Ward, por lo que para las métricas no Euclidianas, el enlazamiento promedio es una buena alternativa. El enlazamiento simple, aunque no es robusto frente a los datos ruidosos, puede calcularse de forma muy eficiente y, por tanto, puede ser útil para proporcionar un análisis de conglomerados jerárquicos de conjuntos de datos más grandes. El enlazamiento simple también puede funcionar bien con datos no globulares."

#: ../modules/clustering.rst:607
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_digits_linkage.py`: exploration of the different linkage strategies in a real dataset."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_digits_linkage.py`: exploración de las diferentes estrategias de enlazamientos en un conjunto de datos real."

#: ../modules/clustering.rst:611
msgid "Visualization of cluster hierarchy"
msgstr "Visualización de la jerarquía de conglomerados"

#: ../modules/clustering.rst:613
msgid "It's possible to visualize the tree representing the hierarchical merging of clusters as a dendrogram. Visual inspection can often be useful for understanding the structure of the data, though more so in the case of small sample sizes."
msgstr "Es posible visualizar el árbol que representa la fusión jerárquica de conglomerados como un dendrograma. La inspección visual a menudo puede ser a menudo útil para comprender la estructura de los datos, aunque más en el caso de tamaños de muestra pequeños."

#: ../modules/clustering.rst:624
msgid "Adding connectivity constraints"
msgstr "Añadir restricciones de conectividad"

#: ../modules/clustering.rst:626
msgid "An interesting aspect of :class:`AgglomerativeClustering` is that connectivity constraints can be added to this algorithm (only adjacent clusters can be merged together), through a connectivity matrix that defines for each sample the neighboring samples following a given structure of the data. For instance, in the swiss-roll example below, the connectivity constraints forbid the merging of points that are not adjacent on the swiss roll, and thus avoid forming clusters that extend across overlapping folds of the roll."
msgstr "Un aspecto interesante de :class:`AgglomerativeClustering`es que a este algoritmo se le pueden añadir restricciones de conectividad (sólo se pueden fusionar conglomerados adyacentes), a través de una matriz de conectividad que define para cada muestra las muestras vecinas siguiendo una estructura determinada de los datos. Por ejemplo, en el ejemplo de brazo de gitano que aparece a continuación, las restricciones de conectividad prohíben la fusión de puntos que no sean adyacentes en el brazo gitano, y evitar así la formación de conglomerados que se extiendan a través de partes o pliegues superpuestos del rollo."

#: ../modules/clustering.rst:644
msgid "unstructured structured"
msgstr "unstructured structured"

#: ../modules/clustering.rst:645
msgid "These constraint are useful to impose a certain local structure, but they also make the algorithm faster, especially when the number of the samples is high."
msgstr "Estas restricciones son útiles para imponer una cierta estructura local, pero también hacen que el algoritmo sea más rápido, especialmente cuando el número de muestras es alto."

#: ../modules/clustering.rst:649
msgid "The connectivity constraints are imposed via an connectivity matrix: a scipy sparse matrix that has elements only at the intersection of a row and a column with indices of the dataset that should be connected. This matrix can be constructed from a-priori information: for instance, you may wish to cluster web pages by only merging pages with a link pointing from one to another. It can also be learned from the data, for instance using :func:`sklearn.neighbors.kneighbors_graph` to restrict merging to nearest neighbors as in :ref:`this example <sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py>`, or using :func:`sklearn.feature_extraction.image.grid_to_graph` to enable only merging of neighboring pixels on an image, as in the :ref:`coin <sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py>` example."
msgstr "Las restricciones de conectividad se imponen a través de una matriz de conectividad: una matriz dispersa de scipy que tiene elementos sólo en la intersección de una fila y una columna con índices del conjunto de datos que deben ser conectados. Esta matriz puede construirse a partir de la información de a-priori: por ejemplo, puedes querer conglomerar páginas web mediante la fusión de páginas con un enlace que apunte de una a otra. También se puede aprender de los datos, por ejemplo usando :func:`sklearn.neighbors.kneighbors_graph` para restringir la fusión a los vecinos más cercanos como en :ref:`este ejemplo <sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py>`, o utilizando :func:`sklearn. eature_extraction.image.grid_to_graph` para permitir sólo la fusión de píxeles vecinos en una imagen, como en el ejemplo de la :ref:`moneda <sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py>`."

#: ../modules/clustering.rst:664
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: Ward clustering to split the image of coins in regions."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`: Análisis de conglomerados Ward para dividir la imagen de las monedas en regiones."

#: ../modules/clustering.rst:667
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: Example of Ward algorithm on a swiss-roll, comparison of structured approaches versus unstructured approaches."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_ward_structured_vs_unstructured.py`: Ejemplo del algoritmo Ward en un brazo de gitano, comparación de enfoques estructurados frente a enfoques no estructurados."

#: ../modules/clustering.rst:671
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`: Example of dimensionality reduction with feature agglomeration based on Ward hierarchical clustering."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`: Ejemplo de reducción de la dimensionalidad con aglomeración de características basado en el análisis de conglomerados jerárquicos de Ward."

#: ../modules/clustering.rst:675
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`"

#: ../modules/clustering.rst:677
msgid "**Connectivity constraints with single, average and complete linkage**"
msgstr "**Restricciones de conectividad con enlazamiento simple, promedio y completo**"

#: ../modules/clustering.rst:679
msgid "Connectivity constraints and single, complete or average linkage can enhance the 'rich getting richer' aspect of agglomerative clustering, particularly so if they are built with :func:`sklearn.neighbors.kneighbors_graph`. In the limit of a small number of clusters, they tend to give a few macroscopically occupied clusters and almost empty ones. (see the discussion in :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`). Single linkage is the most brittle linkage option with regard to this issue."
msgstr "Las restricciones de conectividad y el enlazamiento simple, completo o promedio pueden mejorar el aspecto del análisis de conglomerados aglomerativos \"ricos cada vez más ricos\". particularmente si se construyen con :func:`sklearn. eighbors.kneighbors_graph`. En el límite de un pequeño número de conglomerados, tienden a dar unos pocos conglomerados ocupados macroscópicamente y unos casi vacíos. (vea la discusión en :ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering.py`). El enlazamiento simple es la opción de enlazamiento más frágil con respecto a este problema."

#: ../modules/clustering.rst:706
msgid "Varying the metric"
msgstr "Variación de la métrica"

#: ../modules/clustering.rst:708
msgid "Single, average and complete linkage can be used with a variety of distances (or affinities), in particular Euclidean distance (*l2*), Manhattan distance (or Cityblock, or *l1*), cosine distance, or any precomputed affinity matrix."
msgstr "Los enlazamientos simple, promedio y completo pueden utilizarse con una variedad de distancias (o afinidades), en particular la distancia Euclidiana (*l2*), la distancia Manhattan (o Cityblock, o *l1*), la distancia coseno, o cualquier matriz de afinidad precalculada."

#: ../modules/clustering.rst:713
msgid "*l1* distance is often good for sparse features, or sparse noise: i.e. many of the features are zero, as in text mining using occurrences of rare words."
msgstr "La distancia *l1* suele ser buena para las características dispersas, o para el ruido disperso: es decir, muchas de las características son cero, como en la minería de texto que utiliza las ocurrencias de palabras raras."

#: ../modules/clustering.rst:717
msgid "*cosine* distance is interesting because it is invariant to global scalings of the signal."
msgstr "La distancia del *coseno* es interesante porque es invariante a las escalas globales de la señal."

#: ../modules/clustering.rst:720
msgid "The guidelines for choosing a metric is to use one that maximizes the distance between samples in different classes, and minimizes that within each class."
msgstr "La pauta para elegir una métrica es utilizar una que maximice la distancia entre las muestras las distintas clases y minimice la distancia dentro de cada clase."

#: ../modules/clustering.rst:738
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering_metrics.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_agglomerative_clustering_metrics.py`"

#: ../modules/clustering.rst:744
msgid "DBSCAN"
msgstr "DBSCAN"

#: ../modules/clustering.rst:746
msgid "The :class:`DBSCAN` algorithm views clusters as areas of high density separated by areas of low density. Due to this rather generic view, clusters found by DBSCAN can be any shape, as opposed to k-means which assumes that clusters are convex shaped. The central component to the DBSCAN is the concept of *core samples*, which are samples that are in areas of high density. A cluster is therefore a set of core samples, each close to each other (measured by some distance measure) and a set of non-core samples that are close to a core sample (but are not themselves core samples). There are two parameters to the algorithm, ``min_samples`` and ``eps``, which define formally what we mean when we say *dense*. Higher ``min_samples`` or lower ``eps`` indicate higher density necessary to form a cluster."
msgstr "El algoritmo :class:`DBSCAN` considera los conglomerados como áreas de alta densidad separadas por áreas de baja densidad. Debido a esta visión bastante genérica, los conglomerados encontrados por DBSCAN pueden tener cualquier forma, a diferencia de k-medias, que asume que los conglomerados tienen forma convexa. El componente central del DBSCAN es el concepto de *muestras principales*, que son muestras que se encuentran en zonas de alta densidad. Un conglomerado es, por tanto, un conjunto de muestras principales, cada una de ellas cercana a la otra (medida por alguna distancia) y un conjunto de muestras no principales que están cerca de una muestra principal (pero que no son ellas mismas muestras principales). El algoritmo tiene dos parámetros, ``min_samples`` y ``eps``, que definen formalmente lo que queremos decir cuando hablamos de *densidad*. Un ``min_samples`` más alto o un ``eps`` más bajo indican una mayor densidad necesaria para formar un conglomerado."

#: ../modules/clustering.rst:760
msgid "More formally, we define a core sample as being a sample in the dataset such that there exist ``min_samples`` other samples within a distance of ``eps``, which are defined as *neighbors* of the core sample. This tells us that the core sample is in a dense area of the vector space. A cluster is a set of core samples that can be built by recursively taking a core sample, finding all of its neighbors that are core samples, finding all of *their* neighbors that are core samples, and so on. A cluster also has a set of non-core samples, which are samples that are neighbors of a core sample in the cluster but are not themselves core samples. Intuitively, these samples are on the fringes of a cluster."
msgstr "Más formalmente, definimos una muestra principal como una muestra en el conjunto de datos de forma tal que existen ``min_samples`` otras muestras dentro de una distancia de ``eps``, que se definen como *vecinos* de la muestra principal. Esto nos dice que la muestra principal se encuentra en un área densa del espacio vectorial. Un conglomerado es un conjunto de muestras principales que puede construirse tomando recursivamente una muestra principal, encontrando a todos *sus* vecinos que son muestras principales, y así sucesivamente. Un conglomerado también tiene un conjunto de muestras no principales, que son muestras vecinas de una muestra principal en el conglomerado pero no son en sí mismas muestras principales. Intuitivamente, estas muestras están en los márgenes de un conglomerado."

#: ../modules/clustering.rst:771
msgid "Any core sample is part of a cluster, by definition. Any sample that is not a core sample, and is at least ``eps`` in distance from any core sample, is considered an outlier by the algorithm."
msgstr "Cualquier muestra principal forma parte de un conglomerado, por definición. Cualquier muestra que no es una muestra principal, y que esté al menos ``eps`` de distancia de cualquier muestra principal, es considerado un valor atípico por el algoritmo."

#: ../modules/clustering.rst:775
msgid "While the parameter ``min_samples`` primarily controls how tolerant the algorithm is towards noise (on noisy and large data sets it may be desirable to increase this parameter), the parameter ``eps`` is *crucial to choose appropriately* for the data set and distance function and usually cannot be left at the default value. It controls the local neighborhood of the points. When chosen too small, most data will not be clustered at all (and labeled as ``-1`` for \"noise\"). When chosen too large, it causes close clusters to be merged into one cluster, and eventually the entire data set to be returned as a single cluster. Some heuristics for choosing this parameter have been discussed in the literature, for example based on a knee in the nearest neighbor distances plot (as discussed in the references below)."
msgstr "Mientras que el parámetro ``min_samples`` controla principalmente la tolerancia del algoritmo al ruido (en conjuntos de datos ruidosos y grandes puede ser deseable aumentar este parámetro), el parámetro ``eps`` es *crucial para elegir adecuadamente* para el conjunto de datos y la función de distancia y normalmente no puede dejarse en el valor por defecto. Controla el vecindario local de los puntos. Cuando se elige demasiado pequeño, la mayoría de los datos no serán conglomerados en absoluto (y serán etiquetados como ``-1`` para el \"ruido\"). Cuando se elige demasiado grande, hace que los conglomerados cercanos se fusionen en un conglomerado, y finalmente, todo el conjunto de datos será devuelto como un solo conglomerado. Algunas heurísticas para elegir este parámetro han sido discutidas en la literatura, por ejemplo, basándose en una curvatura en el gráfico de distancias del vecino más cercano (como se discute en las referencias más abajo)."

#: ../modules/clustering.rst:787
msgid "In the figure below, the color indicates cluster membership, with large circles indicating core samples found by the algorithm. Smaller circles are non-core samples that are still part of a cluster. Moreover, the outliers are indicated by black points below."
msgstr "En la figura de abajo, el color indica la pertenencia a un conglomerado, los círculos grandes indican las muestras principales encontradas por el algoritmo. Los círculos más pequeños son muestras no principales que siguen formando parte del conglomerado. Además, los valores atípicos se indican con puntos negros abajo."

#: ../modules/clustering.rst:797
msgid "dbscan_results"
msgstr "dbscan_results"

#: ../modules/clustering.rst:800
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_dbscan.py`"

msgid "Implementation"
msgstr "Implementación"

#: ../modules/clustering.rst:804
msgid "The DBSCAN algorithm is deterministic, always generating the same clusters when given the same data in the same order.  However, the results can differ when data is provided in a different order. First, even though the core samples will always be assigned to the same clusters, the labels of those clusters will depend on the order in which those samples are encountered in the data. Second and more importantly, the clusters to which non-core samples are assigned can differ depending on the data order.  This would happen when a non-core sample has a distance lower than ``eps`` to two core samples in different clusters. By the triangular inequality, those two core samples must be more distant than ``eps`` from each other, or they would be in the same cluster. The non-core sample is assigned to whichever cluster is generated first in a pass through the data, and so the results will depend on the data ordering."
msgstr "El algoritmo DBSCAN es determinístico, siempre genera los mismos conglomerados cuando se le dan los mismos datos en el mismo orden. Sin embargo, los resultados pueden diferir cuando los datos se proporcionan en un orden diferente. En primer lugar, aunque las muestras principales siempre serán asignadas a los mismos conglomerados, las etiquetas de esos conglomerados dependerán del orden en que se encuentren esas muestras en los datos. En segundo lugar y más importante, los conglomerados a los que se asignan muestras no principales pueden diferir dependiendo del orden de los datos. Esto ocurriría cuando una muestra no principal tiene una distancia inferior a ``eps`` a dos muestras principales en conglomerados diferentes. Por la desigualdad triangular, esas dos muestras principales deben estar a una distancia mayor que ``eps`` entre sí, o estarían en el mismo conglomerado. La muestra no principal se asigna al conglomerado que se genere primero en una pasada a través de los datos, y por lo tanto los resultados dependerán del orden de los datos."

#: ../modules/clustering.rst:817
msgid "The current implementation uses ball trees and kd-trees to determine the neighborhood of points, which avoids calculating the full distance matrix (as was done in scikit-learn versions before 0.14). The possibility to use custom metrics is retained; for details, see :class:`NearestNeighbors`."
msgstr "La implementación actual utiliza árboles de bolas y árbol kd para determinar el vecindario de los puntos, que evita calcular la matriz a distancia completa (como se hacía en las versiones de scikit-learn anteriores a la 0.14). Se mantiene la posibilidad de utilizar métricas personalizadas; para más detalles, véase :class:`NearestNeighbors`."

#: ../modules/clustering.rst:826
msgid "This implementation is by default not memory efficient because it constructs a full pairwise similarity matrix in the case where kd-trees or ball-trees cannot be used (e.g., with sparse matrices). This matrix will consume :math:`n^2` floats. A couple of mechanisms for getting around this are:"
msgstr "Esta implementación no es eficiente en cuanto a memoria porque construye una matriz de similitud por pares completa en el caso de que no se puedan utilizar los árboles kd o los árboles de bolas (por ejemplo, con matrices dispersas). Esta matriz consumirá :math:`n^2` flotantes (números de punto flotante). Un par de mecanismos para evitar esto son:"

#: ../modules/clustering.rst:831
msgid "Use :ref:`OPTICS <optics>` clustering in conjunction with the `extract_dbscan` method. OPTICS clustering also calculates the full pairwise matrix, but only keeps one row in memory at a time (memory complexity n)."
msgstr "Utilice el análisis de conglomerados :ref:`OPTICS <optics>` junto con el método `extract_dbscan`. El análisis de conglomerados OPTICS también calcula la matriz completa por pares, pero sólo mantiene una fila en memoria a la vez (complejidad de memoria n)."

#: ../modules/clustering.rst:836
msgid "A sparse radius neighborhood graph (where missing entries are presumed to be out of eps) can be precomputed in a memory-efficient way and dbscan can be run over this with ``metric='precomputed'``.  See :meth:`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`."
msgstr "Un grafo de vecindad de radio disperso (donde se presumen entradas faltantes para estar fuera de eps) puede precalcularse de una manera eficiente en cuanto a memoria y dbscan puede funcionar sobre esto con ``metric='precomputed'``. Vea :meth:`sklearn.neighbors.NearestNeighbors.radius_neighbors_graph`."

#: ../modules/clustering.rst:841
msgid "The dataset can be compressed, either by removing exact duplicates if these occur in your data, or by using BIRCH. Then you only have a relatively small number of representatives for a large number of points. You can then provide a ``sample_weight`` when fitting DBSCAN."
msgstr "El conjunto de datos se puede comprimir, ya sea eliminando los duplicados exactos si aparecen en los datos, o utilizando BIRCH. De este modo, sólo tendrá un número relativamente pequeño de representantes para un gran número de puntos. A continuación, puede proporcionar un ``sample_weight`` al ajustar DBSCAN."

#: ../modules/clustering.rst:848
msgid "\"A Density-Based Algorithm for Discovering Clusters in Large Spatial Databases with Noise\" Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, pp. 226–231. 1996"
msgstr "\"A Density-Based Algorithm for Discovering Clglomerados in Large Spatial Databases with Noise\" Ester, M., H. P. Kriegel, J. Sander, and X. Xu, In Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining, Portland, OR, AAAI Press, páginas 226-231. 1996"

#: ../modules/clustering.rst:854
msgid "\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19."
msgstr "\"DBSCAN revisited, revisited: why and how you should (still) use DBSCAN. Schubert, E., Sander, J., Ester, M., Kriegel, H. P., & Xu, X. (2017). In ACM Transactions on Database Systems (TODS), 42(3), 19."

#: ../modules/clustering.rst:861
msgid "OPTICS"
msgstr "OPTICS"

#: ../modules/clustering.rst:863
msgid "The :class:`OPTICS` algorithm shares many similarities with the :class:`DBSCAN` algorithm, and can be considered a generalization of DBSCAN that relaxes the ``eps`` requirement from a single value to a value range. The key difference between DBSCAN and OPTICS is that the OPTICS algorithm builds a *reachability* graph, which assigns each sample both a ``reachability_`` distance, and a spot within the cluster ``ordering_`` attribute; these two attributes are assigned when the model is fitted, and are used to determine cluster membership. If OPTICS is run with the default value of *inf* set for ``max_eps``, then DBSCAN style cluster extraction can be performed repeatedly in linear time for any given ``eps`` value using the ``cluster_optics_dbscan`` method. Setting ``max_eps`` to a lower value will result in shorter run times, and can be thought of as the maximum neighborhood radius from each point to find other potential reachable points."
msgstr "El algoritmo :class:`OPTICS` comparte muchas similitudes con el algoritmo :class:`DBSCAN`, y puede considerarse una generalización de DBSCAN que relaja el requerimiento de ``eps`` de un valor único a un rango de valores. La diferencia clave entre DBSCAN y OPTICS es que el algoritmo OPTICS construye un grafo de *accesibilidad*, que asigna a cada muestra tanto una distancia ``reachability_``, como un punto dentro del atributo ``ordering_`` del conglomerado ; estos dos atributos se asignan cuando se ajusta el modelo, y se utilizan para determinar la pertenencia a un conglomerado. Si OPTICS se ejecuta con el valor predeterminado de *inf* establecido para ``max_eps``, entonces la extracción de conglomerados al estilo DBSCAN puede realizarse repetidamente en tiempo lineal para cualquier valor dado de ``eps`` utilizando el método ``cluster_optics_dbscan``. Establecer ``max_eps`` a un valor más bajo resultará en tiempos de ejecución más cortos, y puede ser considerado como el radio máximo de vecindad de cada punto para encontrar otros puntos potenciales alcanzables."

#: ../modules/clustering.rst:882
msgid "optics_results"
msgstr "optics_results"

#: ../modules/clustering.rst:883
msgid "The *reachability* distances generated by OPTICS allow for variable density extraction of clusters within a single data set. As shown in the above plot, combining *reachability* distances and data set ``ordering_`` produces a *reachability plot*, where point density is represented on the Y-axis, and points are ordered such that nearby points are adjacent. 'Cutting' the reachability plot at a single value produces DBSCAN like results; all points above the 'cut' are classified as noise, and each time that there is a break when reading from left to right signifies a new cluster. The default cluster extraction with OPTICS looks at the steep slopes within the graph to find clusters, and the user can define what counts as a steep slope using the parameter ``xi``. There are also other possibilities for analysis on the graph itself, such as generating hierarchical representations of the data through reachability-plot dendrograms, and the hierarchy of clusters detected by the algorithm can be accessed through the ``cluster_hierarchy_`` parameter. The plot above has been color-coded so that cluster colors in planar space match the linear segment clusters of the reachability plot. Note that the blue and red clusters are adjacent in the reachability plot, and can be hierarchically represented as children of a larger parent cluster."
msgstr "Las distancias de *accesibilidad* generadas por OPTICS permiten la extracción de densidad variable de los conglomerados dentro de un mismo conjunto de datos. Como se muestra en la gráfica anterior, la combinación de distancias de *accesibilidad* y el ``ordering_`` del conjunto de datos produce un *gráfico de accesibilidad*, en el que la densidad de los puntos se representa en el eje Y, y los puntos se ordenan de tal manera que los puntos cercanos sean adyacentes. Al 'Cortar' el gráfico de accesibilidad en un solo valor produce resultados similares a los de DBSCAN; todos los puntos por encima del 'corte' se clasifican como ruido, y cada vez que hay una interrupción al leer de izquierda a derecha significa un nuevo conglomerado. La extracción de conglomerados por defecto con OPTICS mira las pendientes pronunciadas dentro del grafo para encontrar conglomerados, y el usuario puede definir lo que cuenta como una pendiente pronunciada usando el parámetro ``xi``. También existen otras posibilidades de análisis sobre el propio grafo, como generar representaciones jerárquicas de los datos a través de dendrogramas de accesibilidad, y se puede acceder a la jerarquía de conglomerados detectada por el algoritmo a través del parámetro ``cluster_hierarchy_``. El gráfico anterior ha sido codificado por colores de modo que los colores de los conglomerados en el espacio plano coinciden con los conglomerados de segmentos lineales del gráfico de accesibilidad. Tenga en cuenta que los conglomerados azules y rojos están adyacentes en el gráfico de accesibilidad, y pueden representarse jerárquicamente como hijos de un conglomerado padre más grande."

#: ../modules/clustering.rst:904
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_optics.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_optics.py`"

msgid "Comparison with DBSCAN"
msgstr "Comparación con DBSCAN"

#: ../modules/clustering.rst:909
msgid "The results from OPTICS ``cluster_optics_dbscan`` method and DBSCAN are very similar, but not always identical; specifically, labeling of periphery and noise points. This is in part because the first samples of each dense area processed by OPTICS have a large reachability value while being close to other points in their area, and will thus sometimes be marked as noise rather than periphery. This affects adjacent points when they are considered as candidates for being marked as either periphery or noise."
msgstr "Los resultados del método OPTICS ``cluster_optics_dbscan`` y de DBSCAN son muy similares, pero no siempre idénticos; específicamente, el etiquetado de los puntos de periferia y de ruido. Esto se debe, en parte, a que las primeras muestras de cada área densa procesada por OPTICS tienen un gran valor de accesibilidad estando cerca de otros puntos en su área, y por lo tanto, a veces se marcarán como ruido, en lugar de periferia. Esto afecta a los puntos adyacentes cuando se consideran candidatos para ser marcados como periferia o ruido."

#: ../modules/clustering.rst:917
msgid "Note that for any single value of ``eps``, DBSCAN will tend to have a shorter run time than OPTICS; however, for repeated runs at varying ``eps`` values, a single run of OPTICS may require less cumulative runtime than DBSCAN. It is also important to note that OPTICS' output is close to DBSCAN's only if ``eps`` and ``max_eps`` are close."
msgstr "Tenga en cuenta que para cualquier valor único de ``eps``, DBSCAN tenderá a tener un tiempo de ejecución más corto que OPTICS; sin embargo, para ejecuciones repetidas con valores variantes de ``eps``, una sola ejecución de OPTICS puede requerir menos tiempo de ejecución acumulado que DBSCAN. También es importante tener en cuenta que la salida de OPTICS se acerca a la de DBSCAN, sólo si ``eps`` y ``max_eps`` están cerca."

msgid "Computational Complexity"
msgstr "Complejidad computacional"

#: ../modules/clustering.rst:925
msgid "Spatial indexing trees are used to avoid calculating the full distance matrix, and allow for efficient memory usage on large sets of samples. Different distance metrics can be supplied via the ``metric`` keyword."
msgstr "Los árboles de indexación espacial se utilizan para evitar el cálculo de la matriz de distancias completa, y permiten un uso eficiente de la memoria en grandes conjuntos de muestras. Diferentes métricas de distancia pueden ser suministradas a través de la palabra clave ``metric``."

#: ../modules/clustering.rst:929
msgid "For large datasets, similar (but not identical) results can be obtained via `HDBSCAN <https://hdbscan.readthedocs.io>`_. The HDBSCAN implementation is multithreaded, and has better algorithmic runtime complexity than OPTICS, at the cost of worse memory scaling. For extremely large datasets that exhaust system memory using HDBSCAN, OPTICS will maintain :math:`n` (as opposed to :math:`n^2`) memory scaling; however, tuning of the ``max_eps`` parameter will likely need to be used to give a solution in a reasonable amount of wall time."
msgstr "Para grandes conjuntos de datos, se pueden obtener resultados similares (pero no idénticos) mediante `HDBSCAN <https://hdbscan.readthedocs.io>`_. La implementación de HDBSCAN es multihilo, y tiene una mejor complejidad algorítmica en tiempo de ejecución que OPTICS, a costa de un peor escalado de memoria. Para los conjuntos de datos extremadamente grandes que agotan la memoria del sistema utilizando HDBSCAN, OPTICS mantendrá el escalado de memoria :math:`n` (en lugar de :math:`n^2`) ; sin embargo, es probable que sea necesario ajustar el parámetro ``max_eps`` para obtener una solución en una cantidad razonable de tiempo real."

#: ../modules/clustering.rst:940
msgid "\"OPTICS: ordering points to identify the clustering structure.\" Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999."
msgstr "\"OPTICS: ordering points to identify the clustering structure.\" Ankerst, Mihael, Markus M. Breunig, Hans-Peter Kriegel, and Jörg Sander. In ACM Sigmod Record, vol. 28, no. 2, pp. 49-60. ACM, 1999."

#: ../modules/clustering.rst:947
msgid "Birch"
msgstr "Birch"

#: ../modules/clustering.rst:949
msgid "The :class:`Birch` builds a tree called the Clustering Feature Tree (CFT) for the given data. The data is essentially lossy compressed to a set of Clustering Feature nodes (CF Nodes). The CF Nodes have a number of subclusters called Clustering Feature subclusters (CF Subclusters) and these CF Subclusters located in the non-terminal CF Nodes can have CF Nodes as children."
msgstr "El :class:`Birch` construye un árbol llamado el Árbol de Características de Conglomerados (CFT) para los datos dados. Los datos se comprimen esencialmente con pérdidas en un conjunto de nodos de característica de conglomeración (Nodos CF). Los Nodos CF tienen un número de subconglomerados llamados \"Subconglomerados de Características de Conglomeración (Subconglomerados CF) y estos subconglomerados CF ubicados en los Nodos CF no terminales pueden tener Nodos CF como hijos."

#: ../modules/clustering.rst:956
msgid "The CF Subclusters hold the necessary information for clustering which prevents the need to hold the entire input data in memory. This information includes:"
msgstr "Los Subconglomerados CF contienen la información necesaria para el análisis de conglomerados, lo que evita la necesidad de mantener todos los datos de entrada en la memoria. Esta información incluye:"

#: ../modules/clustering.rst:959
msgid "Number of samples in a subcluster."
msgstr "Número de muestras en un subconglomerado."

#: ../modules/clustering.rst:960
msgid "Linear Sum - An n-dimensional vector holding the sum of all samples"
msgstr "Suma lineal - Un vector n-dimensional que contiene la suma de todas las muestras"

#: ../modules/clustering.rst:961
msgid "Squared Sum - Sum of the squared L2 norm of all samples."
msgstr "Suma cuadrado - Suma de la norma L2 al cuadrado de todas las muestras."

#: ../modules/clustering.rst:962
msgid "Centroids - To avoid recalculation linear sum / n_samples."
msgstr "Centroides - Para evitar recálculos de suma lineal / n_muestras."

#: ../modules/clustering.rst:963
msgid "Squared norm of the centroids."
msgstr "Norma al cuadrado de los centroides."

#: ../modules/clustering.rst:965
msgid "The Birch algorithm has two parameters, the threshold and the branching factor. The branching factor limits the number of subclusters in a node and the threshold limits the distance between the entering sample and the existing subclusters."
msgstr "El algoritmo Birch tiene dos parámetros, el umbral y el factor de ramificación. El factor de ramificación limita el número de subconglomerados en un nodo y el umbral limita la distancia entre la muestra que entra y los subconglomerados existentes."

#: ../modules/clustering.rst:970
msgid "This algorithm can be viewed as an instance or data reduction method, since it reduces the input data to a set of subclusters which are obtained directly from the leaves of the CFT. This reduced data can be further processed by feeding it into a global clusterer. This global clusterer can be set by ``n_clusters``. If ``n_clusters`` is set to None, the subclusters from the leaves are directly read off, otherwise a global clustering step labels these subclusters into global clusters (labels) and the samples are mapped to the global label of the nearest subcluster."
msgstr "Este algoritmo puede considerarse como una instancia o un método de reducción de datos, ya que reduce los datos de entrada a un conjunto de subconglomerados que se obtienen directamente de las hojas del CFT. Estos datos reducidos pueden ser procesados posteriormente alimentando un agrupador global. Este agrupador global puede establecerse mediante ``n_clusters``. Si ``n_clusters`` se establece como None, los subconglomerados de las hojas se leen directamente, de lo contrario en un paso del análisis de conglomerados globales etiqueta estos subconglomerados globales (etiquetas) y las muestras se asignan a la etiqueta global del subconglomerado más cercano."

#: ../modules/clustering.rst:978
msgid "**Algorithm description:**"
msgstr "**Descripción del algoritmo:**"

#: ../modules/clustering.rst:980
msgid "A new sample is inserted into the root of the CF Tree which is a CF Node. It is then merged with the subcluster of the root, that has the smallest radius after merging, constrained by the threshold and branching factor conditions. If the subcluster has any child node, then this is done repeatedly till it reaches a leaf. After finding the nearest subcluster in the leaf, the properties of this subcluster and the parent subclusters are recursively updated."
msgstr "Una nueva muestra se inserta en la raíz del árbol CF que es un nodo CF. Luego se fusiona con el subconglomerado de la raíz, que tenga el radio más pequeño después de la fusión, restringido por el umbral y las condiciones del factor de ramificación. Si el subconglomerado tiene algún nodo hijo, entonces se repite la operación hasta que alcanza una hoja. Después de encontrar el subconglomerado más cercano en la hoja, las propiedades de este subconglomerado y los subconglomerados padre se actualizan recursivamente."

#: ../modules/clustering.rst:987
msgid "If the radius of the subcluster obtained by merging the new sample and the nearest subcluster is greater than the square of the threshold and if the number of subclusters is greater than the branching factor, then a space is temporarily allocated to this new sample. The two farthest subclusters are taken and the subclusters are divided into two groups on the basis of the distance between these subclusters."
msgstr "Si el radio del subconglomerado obtenido al fuisionar la nueva muestra y el subconglomerado más cercano es mayor que el cuadrado del umbral y si el número de subconglomerados es mayor que el factor de ramificación, entonces se asigna temporalmente un espacio a esta nueva muestra. Se toman los dos subconglomerados más alejados y se dividen los subconglomerados en dos grupos en función de la distancia entre estos subconglomerados."

#: ../modules/clustering.rst:994
msgid "If this split node has a parent subcluster and there is room for a new subcluster, then the parent is split into two. If there is no room, then this node is again split into two and the process is continued recursively, till it reaches the root."
msgstr "Si este nodo dividido tiene un subconglomerado padre y hay espacio para un nuevo subconglomerado, entonces el padre se divide en dos. Si no hay espacio, entonces este nodo se divide de nuevo en dos y el proceso se continúa recursivamente, hasta que llega a la raíz."

#: ../modules/clustering.rst:999
msgid "**Birch or MiniBatchKMeans?**"
msgstr "**¿Birch o MiniBatchKMeans?**"

#: ../modules/clustering.rst:1001
msgid "Birch does not scale very well to high dimensional data. As a rule of thumb if ``n_features`` is greater than twenty, it is generally better to use MiniBatchKMeans."
msgstr "Birch no se adapta muy bien a los datos de alta dimensión. Como regla general, si ``n_features`` es mayor que veinte, suele ser mejor utilizar MiniBatchKMeans."

#: ../modules/clustering.rst:1003
msgid "If the number of instances of data needs to be reduced, or if one wants a large number of subclusters either as a preprocessing step or otherwise, Birch is more useful than MiniBatchKMeans."
msgstr "Si es necesario reducir el número de instancias de datos, o si se desea un gran número de subconglomerados, ya sea como paso previo al procesamiento o de otro modo, Birch es más útil que MiniBatchKMeans."

#: ../modules/clustering.rst:1008
msgid "**How to use partial_fit?**"
msgstr "**¿Cómo utilizar partial_fit?**"

#: ../modules/clustering.rst:1010
msgid "To avoid the computation of global clustering, for every call of ``partial_fit`` the user is advised"
msgstr "Para evitar el cálculo del agrupamiento global, para cada llamado de ``partial_fit`` se aconseja al usuario"

#: ../modules/clustering.rst:1013
msgid "To set ``n_clusters=None`` initially"
msgstr "Para establecer ``n_clusters=None`` inicialmente"

#: ../modules/clustering.rst:1014
msgid "Train all data by multiple calls to partial_fit."
msgstr "Entrena todos los datos mediante múltiples llamados a partial_fit."

#: ../modules/clustering.rst:1015
msgid "Set ``n_clusters`` to a required value using ``brc.set_params(n_clusters=n_clusters)``."
msgstr "Establezca ``n_clusters`` a un valor requerido usando ``brc.set_params(n_clusters=n_clusters)``."

#: ../modules/clustering.rst:1017
msgid "Call ``partial_fit`` finally with no arguments, i.e. ``brc.partial_fit()`` which performs the global clustering."
msgstr "Llama finalmente a ``partial_fit`` sin argumentos, es decir, ``brc.partial_fit()`` que realiza el agrupamiento global."

#: ../modules/clustering.rst:1025
msgid "Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf"
msgstr "Tian Zhang, Raghu Ramakrishnan, Maron Livny BIRCH: An efficient data clustering method for large databases. https://www.cs.sfu.ca/CourseCentral/459/han/papers/zhang96.pdf"

#: ../modules/clustering.rst:1029
msgid "Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm https://code.google.com/archive/p/jbirch"
msgstr "Roberto Perdisci JBirch - Java implementation of BIRCH clustering algorithm https://code.google.com/archive/p/jbirch"

#: ../modules/clustering.rst:1037
msgid "Clustering performance evaluation"
msgstr "Evaluación del rendimiento del análisis de conglomerados (agrupamiento)"

#: ../modules/clustering.rst:1039
msgid "Evaluating the performance of a clustering algorithm is not as trivial as counting the number of errors or the precision and recall of a supervised classification algorithm. In particular any evaluation metric should not take the absolute values of the cluster labels into account but rather if this clustering define separations of the data similar to some ground truth set of classes or satisfying some assumption such that members belong to the same class are more similar than members of different classes according to some similarity metric."
msgstr "Evaluar el rendimiento de un algoritmo de análisis de conglomerados no es tan trivial como contar el número de errores o la precisión y exhaustividad de un algoritmo de clasificación supervisada. En particular, cualquier métrica de evaluación no debería tener en cuenta los valores absolutos de las etiquetas de los conglomerados, sino más bien si este agrupamiento define separaciones de los datos similares a algún conjunto de clases de la verdad básica o satisface alguna suposición como que los miembros que pertenecen a la misma clase son más similares que los miembros de clases diferentes según alguna métrica de similitud."

#: ../modules/clustering.rst:1054
msgid "Rand index"
msgstr "Índice de Rand"

#: ../modules/clustering.rst:1056
msgid "Given the knowledge of the ground truth class assignments ``labels_true`` and our clustering algorithm assignments of the same samples ``labels_pred``, the **(adjusted or unadjusted) Rand index** is a function that measures the **similarity** of the two assignments, ignoring permutations::"
msgstr "Dado el conocimiento de las asignaciones de clase de la verdad sobre el terreno ``labels_true`` y las asignaciones de nuestro algoritmo de agrupación de las mismas muestras ``labels_pred``, el **índice de Rand (ajustado o no ajustado)** es una función que mide la **similitud** de las dos asignaciones, ignorando las permutaciones::"

#: ../modules/clustering.rst:1068
msgid "The Rand index does not ensure to obtain a value close to 0.0 for a random labelling. The adjusted Rand index **corrects for chance** and will give such a baseline."
msgstr "El índice de Rand no garantiza obtener un valor cercano a 0.0 para un etiquetado aleatorio. El índice de Rand ajustado **corrige por casualidad** y dará esa línea de base."

#: ../modules/clustering.rst:1075
msgid "As with all clustering metrics, one can permute 0 and 1 in the predicted labels, rename 2 to 3, and get the same score::"
msgstr "Como con todas las métricas del análisis de conglomerados, se puede permutar 0 y 1 en las etiquetas predichas, renombrar 2 a 3 y obtener la misma puntuación::"

#: ../modules/clustering.rst:1084
msgid "Furthermore, both :func:`rand_score` :func:`adjusted_rand_score` are **symmetric**: swapping the argument does not change the scores. They can thus be used as **consensus measures**::"
msgstr "Además, :func:`rand_score`y :func:`adjusted_rand_score` son **simétricos**: intercambiar el argumento no cambia las puntuaciones. Por tanto, pueden utilizarse como **medidas de consenso**::"

#: ../modules/clustering.rst:1093 ../modules/clustering.rst:1248
#: ../modules/clustering.rst:1606
msgid "Perfect labeling is scored 1.0::"
msgstr "El etiquetado perfecto tiene una puntuación de 1,0::"

#: ../modules/clustering.rst:1101
msgid "Poorly agreeing labels (e.g. independent labelings) have lower scores, and for the adjusted Rand index the score will be negative or close to zero. However, for the unadjusted Rand index the score, while lower, will not necessarily be close to zero.::"
msgstr "Las etiquetas con poca concordancia (por ejemplo, etiquetas independientes) tienen puntuaciones más bajas, y para el índice de Rand ajustado la puntuación será negativa o cercana a cero. Sin embargo, para el índice de Rand no ajustado la puntuación, aunque sea más baja, no será necesariamente cercana a cero.::"

#: ../modules/clustering.rst:1115 ../modules/clustering.rst:1271
#: ../modules/clustering.rst:1480 ../modules/clustering.rst:1620
#: ../modules/clustering.rst:1706 ../modules/clustering.rst:1759
#: ../modules/clustering.rst:1837 ../modules/clustering.rst:1922
msgid "Advantages"
msgstr "Ventajas"

#: ../modules/clustering.rst:1117
msgid "**Interpretability**: The unadjusted Rand index is proportional to the number of sample pairs whose labels are the same in both `labels_pred` and `labels_true`, or are different in both."
msgstr "**Interpretabilidad**: El índice de Rand no ajustado es proporcional al número de pares de muestras cuyas etiquetas son iguales tanto en `labels_pred` como en `labels_true`, o son diferentes en ambas."

#: ../modules/clustering.rst:1121
msgid "**Random (uniform) label assignments have an adjusted Rand index score close to 0.0** for any value of ``n_clusters`` and ``n_samples`` (which is not the case for the unadjusted Rand index or the V-measure for instance)."
msgstr "**Las asignaciones de etiquetas aleatorias (uniformes) tienen una puntuación del índice de Rand ajustado cercana a 0,0** para cualquier valor de ``n_clusters`` y ``n_muestras`` (lo que no ocurre con el índice de Rand no ajustado o la medida V, por ejemplo)."

#: ../modules/clustering.rst:1126
msgid "**Bounded range**: Lower values indicate different labelings, similar clusterings have a high (adjusted or unadjusted) Rand index, 1.0 is the perfect match score. The score range is [0, 1] for the unadjusted Rand index and [-1, 1] for the adjusted Rand index."
msgstr "**Rango limitado**: Los valores más bajos indican diferentes etiquetados, las agrupaciones similares tienen un índice de Rand alto (ajustado o no ajustado), 1,0 es la puntuación de coincidencia perfecta. El rango de puntuación es [0, 1] para el índice de Rand no ajustado y [-1, 1] para el índice de Rand ajustado."

#: ../modules/clustering.rst:1131
msgid "**No assumption is made on the cluster structure**: The (adjusted or unadjusted) Rand index can be used to compare all kinds of clustering algorithms, and can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with \"folded\" shapes."
msgstr "**No se hace ninguna suposición sobre la estructura del conglomerado**: El índice de Rand (ajustado o no ajustado) puede utilizarse para comparar todos los tipos de algoritmos de análisis de conglomerados, y puede utilizarse para comparar algoritmos de agrupamiento tales como k-medias que asume formas de manchas isotrópicas con resultados de algoritmos de agrupamiento espectral que pueden encontrar conglomerados con formas \"plegadas\"."

#: ../modules/clustering.rst:1140 ../modules/clustering.rst:1284
#: ../modules/clustering.rst:1495 ../modules/clustering.rst:1639
#: ../modules/clustering.rst:1716 ../modules/clustering.rst:1768
#: ../modules/clustering.rst:1843 ../modules/clustering.rst:1932
msgid "Drawbacks"
msgstr "Inconvenientes"

#: ../modules/clustering.rst:1142
msgid "Contrary to inertia, the **(adjusted or unadjusted) Rand index requires knowledge of the ground truth classes** which is almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting)."
msgstr "Al contrario de lo que ocurre con la inercia, el **índice de Rand (ajustado o sin ajustar) requiere el conocimiento de las clases verdaderas**, que casi nunca están disponibles en la práctica o requieren la asignación manual por parte de anotadores humanos (como en el entorno del aprendizaje supervisado)."

#: ../modules/clustering.rst:1147
msgid "However (adjusted or unadjusted) Rand index can also be useful in a purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection (TODO)."
msgstr "Sin embargo, el índice de Rand (ajustado o no ajustado) también puede ser útil en un entorno puramente no supervisado como bloque de construcción para un Índice de Consenso que puede ser utilizado para la selección de modelos de análisis de conglomerados (TODO)."

#: ../modules/clustering.rst:1151
msgid "The **unadjusted Rand index is often close to 1.0** even if the clusterings themselves differ significantly. This can be understood when interpreting the Rand index as the accuracy of element pair labeling resulting from the clusterings: In practice there often is a majority of element pairs that are assigned the ``different`` pair label under both the predicted and the ground truth clustering resulting in a high proportion of pair labels that agree, which leads subsequently to a high score."
msgstr "El **índice de Rand no ajustado suele ser cercano a 1,0** incluso si los conglomerados difieren significativamente. Esto se puede entender cuando se interpreta el índice de Rand como la exactitud del etiquetado de los pares de elementos resultantes de los agrupamientos: En la práctica, a menudo hay una mayoría de pares de elementos a los que se le asigna la etiqueta ``different`` tanto en el conglomerado predicho como en el agrupamiento verdadero sobre el terreno, lo que da lugar a una alta proporción de etiquetas de par que coinciden, lo que conduce posteriormente a una alta puntuación."

#: ../modules/clustering.rst:1162 ../modules/clustering.rst:1521
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis of the impact of the dataset size on the value of clustering measures for random assignments."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Análisis del impacto del tamaño del conjunto de datos en el valor de las medidas del agrupamiento para las asignaciones aleatorias."

#: ../modules/clustering.rst:1168 ../modules/clustering.rst:1307
#: ../modules/clustering.rst:1527 ../modules/clustering.rst:1775
#: ../modules/clustering.rst:1851
msgid "Mathematical formulation"
msgstr "Formulación matemática"

#: ../modules/clustering.rst:1170
msgid "If C is a ground truth class assignment and K the clustering, let us define :math:`a` and :math:`b` as:"
msgstr "Si C es una asignación de la clase basada en la evidencia y K el agrupamiento, definamos :math:`a` y :math:`b` como:"

#: ../modules/clustering.rst:1173
msgid ":math:`a`, the number of pairs of elements that are in the same set in C and in the same set in K"
msgstr ":math:`a`, el número de pares de elementos que están en el mismo conjunto en C y en el mismo conjunto en K"

#: ../modules/clustering.rst:1176
msgid ":math:`b`, the number of pairs of elements that are in different sets in C and in different sets in K"
msgstr ":math:`b`, el número de pares de elementos que están en conjuntos diferentes en C y en conjuntos diferentes en K"

#: ../modules/clustering.rst:1179
msgid "The unadjusted Rand index is then given by:"
msgstr "El índice de Rand no ajustado viene dado entonces por:"

#: ../modules/clustering.rst:1181
msgid "\\text{RI} = \\frac{a + b}{C_2^{n_{samples}}}\n\n"
msgstr "\\text{RI} = \\frac{a + b}{C_2^{n_{samples}}}\n\n"

#: ../modules/clustering.rst:1183
msgid "where :math:`C_2^{n_{samples}}` is the total number of possible pairs in the dataset. It does not matter if the calculation is performed on ordered pairs or unordered pairs as long as the calculation is performed consistently."
msgstr "donde :math:`C_2^{n_{samples}}` es el número total de pares posibles en el conjunto de datos. No importa si el cálculo se realiza sobre pares ordenados o pares no ordenados siempre y cuando el cálculo se realice de forma coherente."

#: ../modules/clustering.rst:1188
msgid "However, the Rand index does not guarantee that random label assignments will get a value close to zero (esp. if the number of clusters is in the same order of magnitude as the number of samples)."
msgstr "Sin embargo, el índice de Rand no garantiza que las asignaciones aleatorias de etiquetas obtengan un valor cercano a cero (especialmente si el número de conglomerado es del mismo orden de magnitud que el número de muestras)."

#: ../modules/clustering.rst:1192
msgid "To counter this effect we can discount the expected RI :math:`E[\\text{RI}]` of random labelings by defining the adjusted Rand index as follows:"
msgstr "Para contrarrestar este efecto podemos descontar el RI esperado :math:`E[\\text{RI}]` de las etiquetas aleatorias definiendo el índice de Rand ajustado de la siguiente manera:"

#: ../modules/clustering.rst:1195
msgid "\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n\n"
msgstr "\\text{ARI} = \\frac{\\text{RI} - E[\\text{RI}]}{\\max(\\text{RI}) - E[\\text{RI}]}\n\n"

#: ../modules/clustering.rst:1199
#, python-format
msgid "`Comparing Partitions <https://link.springer.com/article/10.1007%2FBF01908075>`_ L. Hubert and P. Arabie, Journal of Classification 1985"
msgstr "`Comparing Partitions <https://link.springer.com/article/10.1007%2FBF01908075>`_ L. Hubert and P. Arabie, Journal of Classification 1985"

#: ../modules/clustering.rst:1203
msgid "`Properties of the Hubert-Arabie adjusted Rand index <https://psycnet.apa.org/record/2004-17801-007>`_ D. Steinley, Psychological Methods 2004"
msgstr "`Properties of the Hubert-Arabie adjusted Rand index <https://psycnet.apa.org/record/2004-17801-007>`_ D. Steinley, Psychological Methods 2004"

#: ../modules/clustering.rst:1207
msgid "`Wikipedia entry for the Rand index <https://en.wikipedia.org/wiki/Rand_index>`_"
msgstr "`Wikipedia entry for the Rand index <https://en.wikipedia.org/wiki/Rand_index>`_"

#: ../modules/clustering.rst:1210
msgid "`Wikipedia entry for the adjusted Rand index <https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_"
msgstr "`Wikipedia entry for the adjusted Rand index <https://en.wikipedia.org/wiki/Rand_index#Adjusted_Rand_index>`_"

#: ../modules/clustering.rst:1217
msgid "Mutual Information based scores"
msgstr "Puntuaciones basadas en información mutua"

#: ../modules/clustering.rst:1219
msgid "Given the knowledge of the ground truth class assignments ``labels_true`` and our clustering algorithm assignments of the same samples ``labels_pred``, the **Mutual Information** is a function that measures the **agreement** of the two assignments, ignoring permutations.  Two different normalized versions of this measure are available, **Normalized Mutual Information (NMI)** and **Adjusted Mutual Information (AMI)**. NMI is often used in the literature, while AMI was proposed more recently and is **normalized against chance**::"
msgstr "Dado el conocimiento de las asignaciones de clase basadas en la evidencia ``labels_true`` y nuestras asignaciones del algoritmo de agrupación de las mismas muestras ``labels_pred``, la **Información Mutua** es una función que mide el **acuerdo** de las dos asignaciones, ignorando las permutaciones. Dos versiones normalizadas de esta medida están disponibles, la **Información Mutua Normalizada (Normalized Mutual Information, NMI)** y la **Información Mutua Ajustada (Adjusted Mutual Information, AMI)**. La NMI se utiliza a menudo en la literatura, mientras que la AMI fue propuesto más recientemente y se **normaliza contra el azar**::"

#: ../modules/clustering.rst:1234 ../modules/clustering.rst:1598
msgid "One can permute 0 and 1 in the predicted labels, rename 2 to 3 and get the same score::"
msgstr "Se puede permutar el 0 y 1 en las etiquetas predichas, renombrar el 2 al 3 y obtener la misma puntuación::"

#: ../modules/clustering.rst:1241
msgid "All, :func:`mutual_info_score`, :func:`adjusted_mutual_info_score` and :func:`normalized_mutual_info_score` are symmetric: swapping the argument does not change the score. Thus they can be used as a **consensus measure**::"
msgstr "Todas, :func:`mutual_info_score`, :func:`adjusted_mutual_info_score` y :func:`normalized_mutual_info_score` son simétricos: intercambiar el argumento no cambia la puntuación. Por lo tanto, pueden utilizarse como una **medida de consenso**::"

#: ../modules/clustering.rst:1257
msgid "This is not true for ``mutual_info_score``, which is therefore harder to judge::"
msgstr "Esto no es cierto para ``mutual_info_score``, que por lo tanto es más difícil de juzgar::"

#: ../modules/clustering.rst:1262
msgid "Bad (e.g. independent labelings) have non-positive scores::"
msgstr "Los malos (por ejemplo, los etiquetados independientes) tienen puntuaciones no positivas::"

#: ../modules/clustering.rst:1273
msgid "**Random (uniform) label assignments have a AMI score close to 0.0** for any value of ``n_clusters`` and ``n_samples`` (which is not the case for raw Mutual Information or the V-measure for instance)."
msgstr "**Las asignaciones de etiquetas aleatorias (uniformes) tienen una puntuación AMI cercana a 0,0** para cualquier valor de ``n_clusters`` y ``n_samples`` (lo que no ocurre con la Información Mutua bruta o la medida V, por ejemplo)."

#: ../modules/clustering.rst:1277
msgid "**Upper bound  of 1**:  Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, an AMI of exactly 1 indicates that the two label assignments are equal (with or without permutation)."
msgstr "**Límite superior de 1**:  Los valores cercanos a cero indican dos asignaciones de etiquetas que son en gran medida independientes, mientras que los valores cercanos a uno indican un acuerdo significativo. Además, una AMI de exactamente 1 indica que las dos asignaciones de etiquetas son iguales (con o sin permutación)."

#: ../modules/clustering.rst:1286
msgid "Contrary to inertia, **MI-based measures require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting)."
msgstr "Al contrario de lo que ocurre con la inercia, **las medidas basadas en el IM requieren el conocimiento de las clases verdaderas**, mientras que casi nunca están disponibles en la práctica o requieren la asignación manual por parte de anotadores humanos (como en el entorno del aprendizaje supervisado)."

#: ../modules/clustering.rst:1291
msgid "However MI-based measures can also be useful in purely unsupervised setting as a building block for a Consensus Index that can be used for clustering model selection."
msgstr "Sin embargo, las medidas basadas en el IM también pueden ser útiles en un entorno puramente no supervisado como componente básico para un Índice de Consenso que puede utilizarse para la selección de modelos de agrupamiento."

#: ../modules/clustering.rst:1295
msgid "NMI and MI are not adjusted against chance."
msgstr "La NMI y la MI no están ajustadas al azar."

#: ../modules/clustering.rst:1300
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Analysis of the impact of the dataset size on the value of clustering measures for random assignments. This example also includes the Adjusted Rand Index."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_adjusted_for_chance_measures.py`: Análisis del impacto del tamaño del conjunto de datos en el valor de las medidas de agrupamiento para las asignaciones aleatorias. Este ejemplo también incluye el índice de Rand ajustado."

#: ../modules/clustering.rst:1309
msgid "Assume two label assignments (of the same N objects), :math:`U` and :math:`V`. Their entropy is the amount of uncertainty for a partition set, defined by:"
msgstr "Supongamos dos asignaciones de etiquetas (de los mismos N objetos), :math:`U` y :math:`V`. Su entropía es la cantidad de incertidumbre para un conjunto particionado, definida por:"

#: ../modules/clustering.rst:1312
msgid "H(U) = - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\n\n"
msgstr "H(U) = - \\sum_{i=1}^{|U|}P(i)\\log(P(i))\n\n"

#: ../modules/clustering.rst:1314
msgid "where :math:`P(i) = |U_i| / N` is the probability that an object picked at random from :math:`U` falls into class :math:`U_i`. Likewise for :math:`V`:"
msgstr "donde :math:`P(i) = |U_i| / N` es la probabilidad de que un objeto elegido al azar de :math:`U` pertenezca a la clase :math:`U_i`. Lo mismo ocurre con :math:`V`:"

#: ../modules/clustering.rst:1317
msgid "H(V) = - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\n\n"
msgstr "H(V) = - \\sum_{j=1}^{|V|}P'(j)\\log(P'(j))\n\n"

#: ../modules/clustering.rst:1319
msgid "With :math:`P'(j) = |V_j| / N`. The mutual information (MI) between :math:`U` and :math:`V` is calculated by:"
msgstr "Con :math:`P'(j) = |V_j| / N`. La información mutua (Mutual Information, MI) entre :math:`U` y :math:`V` se calcula mediante:"

#: ../modules/clustering.rst:1322
msgid "\\text{MI}(U, V) = \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\n\n"
msgstr "\\text{MI}(U, V) = \\sum_{i=1}^{|U|}\\sum_{j=1}^{|V|}P(i, j)\\log\\left(\\frac{P(i,j)}{P(i)P'(j)}\\right)\n\n"

#: ../modules/clustering.rst:1324
msgid "where :math:`P(i, j) = |U_i \\cap V_j| / N` is the probability that an object picked at random falls into both classes :math:`U_i` and :math:`V_j`."
msgstr "donde :math:`P(i, j) = |U_i \\cap V_j| / N` es la probabilidad de que un objeto elegido al azar caiga en ambas clases :math:`U_i` y :math:`V_j`."

#: ../modules/clustering.rst:1327
msgid "It also can be expressed in set cardinality formulation:"
msgstr "También puede expresarse en la formulación de la cardinalidad del conjunto:"

#: ../modules/clustering.rst:1329
msgid "\\text{MI}(U, V) = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\n\n"
msgstr "\\text{MI}(U, V) = \\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\frac{|U_i \\cap V_j|}{N}\\log\\left(\\frac{N|U_i \\cap V_j|}{|U_i||V_j|}\\right)\n\n"

#: ../modules/clustering.rst:1331
msgid "The normalized mutual information is defined as"
msgstr "La información mutua normalizada se define como"

#: ../modules/clustering.rst:1333
msgid "\\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\n\n"
msgstr "\\text{NMI}(U, V) = \\frac{\\text{MI}(U, V)}{\\text{mean}(H(U), H(V))}\n\n"

#: ../modules/clustering.rst:1335
msgid "This value of the mutual information and also the normalized variant is not adjusted for chance and will tend to increase as the number of different labels (clusters) increases, regardless of the actual amount of \"mutual information\" between the label assignments."
msgstr "Este valor de la información mutua y también la variante normalizada no está ajustada al azar y tenderá a aumentar a medida que aumente el número de etiquetas diferentes (conglomerados), sin importar la cantidad real de \"información mutua\" entre las asignaciones de etiquetas."

#: ../modules/clustering.rst:1340
msgid "The expected value for the mutual information can be calculated using the following equation [VEB2009]_. In this equation, :math:`a_i = |U_i|` (the number of elements in :math:`U_i`) and :math:`b_j = |V_j|` (the number of elements in :math:`V_j`)."
msgstr "El valor esperado de la información mutua puede calcularse mediante la siguiente ecuación [VEB2009]_. En esta ecuación, :math:`a_i = |U_i|` (el número de elementos en :math:`U_i`) y :math:`b_j = |V_j|` (el número de elementos en :math:`V_j`)."

#: ../modules/clustering.rst:1346
msgid "E[\\text{MI}(U,V)]=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\sum_{n_{ij}=(a_i+b_j-N)^+\n"
"}^{\\min(a_i, b_j)} \\frac{n_{ij}}{N}\\log \\left( \\frac{ N.n_{ij}}{a_i b_j}\\right)\n"
"\\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!\n"
"(N-a_i-b_j+n_{ij})!}\n\n"
msgstr "E[\\text{MI}(U,V)]=\\sum_{i=1}^{|U|} \\sum_{j=1}^{|V|} \\sum_{n_{ij}=(a_i+b_j-N)^+\n"
"}^{\\min(a_i, b_j)} \\frac{n_{ij}}{N}\\log \\left( \\frac{ N.n_{ij}}{a_i b_j}\\right)\n"
"\\frac{a_i!b_j!(N-a_i)!(N-b_j)!}{N!n_{ij}!(a_i-n_{ij})!(b_j-n_{ij})!\n"
"(N-a_i-b_j+n_{ij})!}\n\n"

#: ../modules/clustering.rst:1351
msgid "Using the expected value, the adjusted mutual information can then be calculated using a similar form to that of the adjusted Rand index:"
msgstr "A partir del valor esperado, la información mutua ajustada puede calcularse de forma similar a la del índice de Rand ajustado:"

#: ../modules/clustering.rst:1354
msgid "\\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\n"
msgstr "\\text{AMI} = \\frac{\\text{MI} - E[\\text{MI}]}{\\text{mean}(H(U), H(V)) - E[\\text{MI}]}\n\n"

#: ../modules/clustering.rst:1356
msgid "For normalized mutual information and adjusted mutual information, the normalizing value is typically some *generalized* mean of the entropies of each clustering. Various generalized means exist, and no firm rules exist for preferring one over the others.  The decision is largely a field-by-field basis; for instance, in community detection, the arithmetic mean is most common. Each normalizing method provides \"qualitatively similar behaviours\" [YAT2016]_. In our implementation, this is controlled by the ``average_method`` parameter."
msgstr "Para la información mutua normalizada y la información mutua ajustada, el valor normalizador suele ser alguna media *generalizada* de las entropías de cada conglomerado. Existen varias medias generalizadas, y no hay reglas firmes para preferir una sobre las otras.  La decisión es en gran medida un campo por campo; por ejemplo, en la detección de comunidades, la media aritmética es la más común. Cada método de normalización proporciona \"comportamientos cualitativamente similares\" [YAT2016]_. En nuestra implementación, esto se controla con el parámetro ``average_method``."

#: ../modules/clustering.rst:1364
msgid "Vinh et al. (2010) named variants of NMI and AMI by their averaging method [VEB2010]_. Their 'sqrt' and 'sum' averages are the geometric and arithmetic means; we use these more broadly common names."
msgstr "Vinh et al. (2010) nombraron variantes de la NMI y la AMI por su método de promediación [VEB2010]_. Sus promedios 'sqrt' y 'sum' son las medias geométrica y aritmética; nosotros utilizamos estos nombres más comunes."

#: ../modules/clustering.rst:1370
msgid "Strehl, Alexander, and Joydeep Ghosh (2002). \"Cluster ensembles – a knowledge reuse framework for combining multiple partitions\". Journal of Machine Learning Research 3: 583–617. `doi:10.1162/153244303321897735 <http://strehl.com/download/strehl-jmlr02.pdf>`_."
msgstr "Strehl, Alexander, and Joydeep Ghosh (2002). \"Cluster ensembles – a knowledge reuse framework for combining multiple partitions\". Journal of Machine Learning Research 3: 583–617. `doi:10.1162/153244303321897735 <http://strehl.com/download/strehl-jmlr02.pdf>`_."

#: ../modules/clustering.rst:1375
msgid "`Wikipedia entry for the (normalized) Mutual Information <https://en.wikipedia.org/wiki/Mutual_Information>`_"
msgstr "`Wikipedia entry for the (normalized) Mutual Information <https://en.wikipedia.org/wiki/Mutual_Information>`_"

#: ../modules/clustering.rst:1378
msgid "`Wikipedia entry for the Adjusted Mutual Information <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_"
msgstr "`Wikipedia entry for the Adjusted Mutual Information <https://en.wikipedia.org/wiki/Adjusted_Mutual_Information>`_"

#: ../modules/clustering.rst:1381
msgid "Vinh, Epps, and Bailey, (2009). \"Information theoretic measures for clusterings comparison\". Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09. `doi:10.1145/1553374.1553511 <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_. ISBN 9781605585161."
msgstr "Vinh, Epps, and Bailey, (2009). \"Information theoretic measures for clusterings comparison\". Proceedings of the 26th Annual International Conference on Machine Learning - ICML '09. `doi:10.1145/1553374.1553511 <https://dl.acm.org/citation.cfm?doid=1553374.1553511>`_. ISBN 9781605585161."

#: ../modules/clustering.rst:1387
msgid "Vinh, Epps, and Bailey, (2010). \"Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance\". JMLR <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>"
msgstr "Vinh, Epps, and Bailey, (2010). \"Information Theoretic Measures for Clusterings Comparison: Variants, Properties, Normalization and Correction for Chance\". JMLR <http://jmlr.csail.mit.edu/papers/volume11/vinh10a/vinh10a.pdf>"

#: ../modules/clustering.rst:1392
msgid "Yang, Algesheimer, and Tessone, (2016). \"A comparative analysis of community detection algorithms on artificial networks\". Scientific Reports 6: 30750. `doi:10.1038/srep30750 <https://www.nature.com/articles/srep30750>`_."
msgstr "Yang, Algesheimer, and Tessone, (2016). \"A comparative analysis of community detection algorithms on artificial networks\". Scientific Reports 6: 30750. `doi:10.1038/srep30750 <https://www.nature.com/articles/srep30750>`_."

#: ../modules/clustering.rst:1402
msgid "Homogeneity, completeness and V-measure"
msgstr "Homogeneidad, completitud y medida V"

#: ../modules/clustering.rst:1404
msgid "Given the knowledge of the ground truth class assignments of the samples, it is possible to define some intuitive metric using conditional entropy analysis."
msgstr "Dado el conocimiento de las asignaciones de clase de las muestras basadas en la evidencia, es posible definir alguna métrica intuitiva utilizando el análisis de entropía condicional."

#: ../modules/clustering.rst:1408
msgid "In particular Rosenberg and Hirschberg (2007) define the following two desirable objectives for any cluster assignment:"
msgstr "En particular, Rosenberg y Hirschberg (2007) definen los siguientes dos objetivos deseables para cualquier asignación de conglomerados:"

#: ../modules/clustering.rst:1411
msgid "**homogeneity**: each cluster contains only members of a single class."
msgstr "**homogeneidad**: cada conglomerado contiene sólo miembros de una sola clase."

#: ../modules/clustering.rst:1413
msgid "**completeness**: all members of a given class are assigned to the same cluster."
msgstr "**completitud**: todos los miembros de una clase determinada están asignados al mismo conglomerado."

#: ../modules/clustering.rst:1416
msgid "We can turn those concept as scores :func:`homogeneity_score` and :func:`completeness_score`. Both are bounded below by 0.0 and above by 1.0 (higher is better)::"
msgstr "Podemos convertir estos conceptos en puntuaciones :func:`homogeneity_score` y :func:`completeness_score`. Ambas están delimitadas por debajo de 0,0 y por encima de 1,0 (cuanto más alto, mejor)::"

#: ../modules/clustering.rst:1430
msgid "Their harmonic mean called **V-measure** is computed by :func:`v_measure_score`::"
msgstr "Su media armónica llamada **medidad V** se calcula con :func:`v_measure_score`::"

#: ../modules/clustering.rst:1436
msgid "This function's formula is as follows:"
msgstr "La fórmula de esta función es la siguiente:"

#: ../modules/clustering.rst:1438
msgid "v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}\n\n"
msgstr "v = \\frac{(1 + \\beta) \\times \\text{homogeneity} \\times \\text{completeness}}{(\\beta \\times \\text{homogeneity} + \\text{completeness})}\n\n"

#: ../modules/clustering.rst:1440
msgid "`beta` defaults to a value of 1.0, but for using a value less than 1 for beta::"
msgstr "El valor predeterminado de `beta` es 1,0, pero para utilizar un valor menor que 1 para beta::"

#: ../modules/clustering.rst:1445
msgid "more weight will be attributed to homogeneity, and using a value greater than 1::"
msgstr "se atribuirá más peso a la homogeneidad, y utilizando un valor mayor que 1::"

#: ../modules/clustering.rst:1450
msgid "more weight will be attributed to completeness."
msgstr "se atribuirá más peso a la completitud."

#: ../modules/clustering.rst:1452
msgid "The V-measure is actually equivalent to the mutual information (NMI) discussed above, with the aggregation function being the arithmetic mean [B2011]_."
msgstr "La medida V es, en realidad equivalente a la información mutua (NMI) comentada anteriormente, siendo la función de agregación la media aritmética [B2011]_."

#: ../modules/clustering.rst:1455
msgid "Homogeneity, completeness and V-measure can be computed at once using :func:`homogeneity_completeness_v_measure` as follows::"
msgstr "La homogeneidad, la completitud y la medida V pueden calcularse a la vez utilizando :func:`homogeneity_completeness_v_measure` de la siguiente manera::"

#: ../modules/clustering.rst:1461
msgid "The following clustering assignment is slightly better, since it is homogeneous but not complete::"
msgstr "La siguiente asignación de agrupamiento es ligeramente mejor, ya que es homogénea pero no completa::"

#: ../modules/clustering.rst:1470
msgid ":func:`v_measure_score` is **symmetric**: it can be used to evaluate the **agreement** of two independent assignments on the same dataset."
msgstr ":func:`v_measure_score` es **simétrica**: puede utilizarse para evaluar el **acuerdo** de dos asignaciones independientes en el mismo conjunto de datos."

#: ../modules/clustering.rst:1473
msgid "This is not the case for :func:`completeness_score` and :func:`homogeneity_score`: both are bound by the relationship::"
msgstr "Este no es el caso de :func:`completeness_score` y :func:`homogeneity_score`: ambos están vinculados por la relación::"

#: ../modules/clustering.rst:1482
msgid "**Bounded scores**: 0.0 is as bad as it can be, 1.0 is a perfect score."
msgstr "**Puntuaciones limitadas**: 0,0 es lo peor que puede ser, 1,0 es una puntuación perfecta."

#: ../modules/clustering.rst:1484
msgid "Intuitive interpretation: clustering with bad V-measure can be **qualitatively analyzed in terms of homogeneity and completeness** to better feel what 'kind' of mistakes is done by the assignment."
msgstr "Interpretación intuitiva: el agrupamiento con mala medida V se puede **analizar cualitativamente en términos de homogeneidad y completitud** para sentir mejor qué 'tipo' de errores se hace mediante la asignación."

#: ../modules/clustering.rst:1488 ../modules/clustering.rst:1632
msgid "**No assumption is made on the cluster structure**: can be used to compare clustering algorithms such as k-means which assumes isotropic blob shapes with results of spectral clustering algorithms which can find cluster with \"folded\" shapes."
msgstr "**No se hace ninguna suposición sobre la estructura del conglomerado**: puede utilizarse para comparar todos los tipos de algoritmos de agrupamiento, y puede utilizarse para comparar algoritmos de agrupamiento tales como k-medias que asume formas de manchas isotrópicas con resultados de los algoritmos de agrupamiento espectral, que pueden encontrar conglomerados con formas \"plegadas\"."

#: ../modules/clustering.rst:1497
msgid "The previously introduced metrics are **not normalized with regards to random labeling**: this means that depending on the number of samples, clusters and ground truth classes, a completely random labeling will not always yield the same values for homogeneity, completeness and hence v-measure. In particular **random labeling won't yield zero scores especially when the number of clusters is large**."
msgstr "Las métricas introducidas anteriormente **no están normalizadas con respecto al etiquetado aleatorio**: esto significa que, dependiendo del número de muestras, conglomerados y clases basadas en la evidencia, un etiquetado completamente aleatorio no siempre produce los mismos valores de homogeneidad, completitud y, por tanto, la medida v. En particular, **el etiquetado aleatorio no producirá puntuaciones iguales a cero, especialmente cuando el número de conglomerados sea grande**."

#: ../modules/clustering.rst:1504
msgid "This problem can safely be ignored when the number of samples is more than a thousand and the number of clusters is less than 10. **For smaller sample sizes or larger number of clusters it is safer to use an adjusted index such as the Adjusted Rand Index (ARI)**."
msgstr "Este problema puede ignorarse con seguridad cuando el número de muestras es superior a mil y el número de conglomerados es menor que 10. **Para tamaños de muestra más pequeños o mayor número de conglomerados, es más seguro utilizar un índice ajustado como el Índice de Rand Ajustado (Adjusted Rand Index ARI)**."

#: ../modules/clustering.rst:1514
msgid "These metrics **require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting)."
msgstr "Estas métricas **requieren el conocimiento de las clases basadas en la evidencia**, mientras que casi nunca están disponibles en la práctica o requieren la asignación manual por parte de anotadores humanos (como en el entorno del aprendizaje supervisado)."

#: ../modules/clustering.rst:1529
msgid "Homogeneity and completeness scores are formally given by:"
msgstr "Las puntuaciones de homogeneidad y completitud vienen dadas formalmente por:"

#: ../modules/clustering.rst:1531
msgid "h = 1 - \\frac{H(C|K)}{H(C)}\n\n"
msgstr "h = 1 - \\frac{H(C|K)}{H(C)}\n\n"

#: ../modules/clustering.rst:1533
msgid "c = 1 - \\frac{H(K|C)}{H(K)}\n\n"
msgstr "c = 1 - \\frac{H(K|C)}{H(K)}\n\n"

#: ../modules/clustering.rst:1535
msgid "where :math:`H(C|K)` is the **conditional entropy of the classes given the cluster assignments** and is given by:"
msgstr "donde :math:`H(C|K)` es la **entropía condicional de las clases dadas las asignaciones de los conglomerados** y viene dada por:"

#: ../modules/clustering.rst:1538
msgid "H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\n"
"\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\n"
msgstr "H(C|K) = - \\sum_{c=1}^{|C|} \\sum_{k=1}^{|K|} \\frac{n_{c,k}}{n}\n"
"\\cdot \\log\\left(\\frac{n_{c,k}}{n_k}\\right)\n\n"

#: ../modules/clustering.rst:1541
msgid "and :math:`H(C)` is the **entropy of the classes** and is given by:"
msgstr "y :math:`H(C)` es la **entropía de las clases** y viene dada por:"

#: ../modules/clustering.rst:1543
msgid "H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\n\n"
msgstr "H(C) = - \\sum_{c=1}^{|C|} \\frac{n_c}{n} \\cdot \\log\\left(\\frac{n_c}{n}\\right)\n\n"

#: ../modules/clustering.rst:1545
msgid "with :math:`n` the total number of samples, :math:`n_c` and :math:`n_k` the number of samples respectively belonging to class :math:`c` and cluster :math:`k`, and finally :math:`n_{c,k}` the number of samples from class :math:`c` assigned to cluster :math:`k`."
msgstr "con :math:`n` el número total de muestras, :math:`n_c` y :math:`n_k` el número de muestras que pertenecen respectivamente a la clase :math:`c` y al conglomerado :math:`k`, y finalmente :math:`n_{c,k}` el número de muestras de la clase :math:`c` asignadas al conglomerado :math:`k`."

#: ../modules/clustering.rst:1550
msgid "The **conditional entropy of clusters given class** :math:`H(K|C)` and the **entropy of clusters** :math:`H(K)` are defined in a symmetric manner."
msgstr "La **entropía condicional de los conglomerados dada la clase** :math:`H(K|C)` y la **entropía de los conglomerados** :math:`H(K)` se definen de forma simétrica."

#: ../modules/clustering.rst:1553
msgid "Rosenberg and Hirschberg further define **V-measure** as the **harmonic mean of homogeneity and completeness**:"
msgstr "Rosenberg y Hirschberg definen además la **medida V** como la **media armónica de la homogeneidad y la completitud**:"

#: ../modules/clustering.rst:1556
msgid "v = 2 \\cdot \\frac{h \\cdot c}{h + c}\n\n"
msgstr "v = 2 \\cdot \\frac{h \\cdot c}{h + c}\n\n"

#: ../modules/clustering.rst:1560
msgid "`V-Measure: A conditional entropy-based external cluster evaluation measure <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_ Andrew Rosenberg and Julia Hirschberg, 2007"
msgstr "`V-Measure: A conditional entropy-based external cluster evaluation measure <https://aclweb.org/anthology/D/D07/D07-1043.pdf>`_ Andrew Rosenberg and Julia Hirschberg, 2007"

#: ../modules/clustering.rst:1564
msgid "`Identication and Characterization of Events in Social Media <http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf>`_, Hila Becker, PhD Thesis."
msgstr "`Identication and Characterization of Events in Social Media <http://www.cs.columbia.edu/~hila/hila-thesis-distributed.pdf>`_, Hila Becker, Tesis doctoral."

#: ../modules/clustering.rst:1571
msgid "Fowlkes-Mallows scores"
msgstr "Puntuaciones de Fowlkes-Mallow"

#: ../modules/clustering.rst:1573
msgid "The Fowlkes-Mallows index (:func:`sklearn.metrics.fowlkes_mallows_score`) can be used when the ground truth class assignments of the samples is known. The Fowlkes-Mallows score FMI is defined as the geometric mean of the pairwise precision and recall:"
msgstr "El índice de Fowlkes-Mallows (:func:`sklearn.metrics.fowlkes_mallows_score`) puede utilizarse cuando se conoce la asignación de la clase de las muestras basadas en la evidencia. La puntuación de Fowlkes-Mallows FMI se define como la media geométrica de la precisión y exhaustividad por pares:"

#: ../modules/clustering.rst:1578
msgid "\\text{FMI} = \\frac{\\text{TP}}{\\sqrt{(\\text{TP} + \\text{FP}) (\\text{TP} + \\text{FN})}}\n\n"
msgstr "\\text{FMI} = \\frac{\\text{TP}}{\\sqrt{(\\text{TP} + \\text{FP}) (\\text{TP} + \\text{FN})}}\n\n"

#: ../modules/clustering.rst:1580
msgid "Where ``TP`` is the number of **True Positive** (i.e. the number of pair of points that belong to the same clusters in both the true labels and the predicted labels), ``FP`` is the number of **False Positive** (i.e. the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels) and ``FN`` is the number of **False Negative** (i.e the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels)."
msgstr "Donde ``TP`` es el número de **Verdaderos Positivos** (es decir, el número de pares de puntos que pertenecen a los mismos conglomerados tanto en las etiquetas verdaderas como en las etiquetas predichas), ``FP`` es el número de **Falsos Positivos** (es decir, el número de pares de puntos que pertenecen a los mismos conglomerados en las etiquetas verdaderas y no en las etiquetas predichas) y ``FN`` es el número de **Falsos Negativos** (es decir, el número de puntos que pertenecen a los mismos conglomerados en las etiquetas predichas y no en las etiquetas verdaderas)."

#: ../modules/clustering.rst:1588
msgid "The score ranges from 0 to 1. A high value indicates a good similarity between two clusters."
msgstr "La puntuación oscila entre 0 y 1. Un valor alto indica una buena similitud entre dos conglomerados."

#: ../modules/clustering.rst:1612
msgid "Bad (e.g. independent labelings) have zero scores::"
msgstr "Los malos (por ejemplo, los etiquetados independientes) tienen puntuación cero::"

#: ../modules/clustering.rst:1622
msgid "**Random (uniform) label assignments have a FMI score close to 0.0** for any value of ``n_clusters`` and ``n_samples`` (which is not the case for raw Mutual Information or the V-measure for instance)."
msgstr "**Las asignaciones de etiquetas aleatorias (uniformes) tienen una puntuación FMI cercana a 0,0** para cualquier valor de ``n_clusters`` y ``n_samples`` (lo que no ocurre con la Información Mutua en bruto o la medida V, por ejemplo)."

#: ../modules/clustering.rst:1626
msgid "**Upper-bounded at 1**:  Values close to zero indicate two label assignments that are largely independent, while values close to one indicate significant agreement. Further, values of exactly 0 indicate **purely** independent label assignments and a FMI of exactly 1 indicates that the two label assignments are equal (with or without permutation)."
msgstr "**El límite superior es 1**:  Los valores cercanos a cero indican dos asignaciones de etiquetas que son en gran medida independientes, mientras que los valores cercanos a uno indican un acuerdo significativo. Además, los valores de exactamente 0 indican asignaciones de etiquetas **puramente** independientes y un FMI de exactamente 1 indica que las dos asignaciones de etiquetas son iguales (con o sin permutación)."

#: ../modules/clustering.rst:1641
msgid "Contrary to inertia, **FMI-based measures require the knowledge of the ground truth classes** while almost never available in practice or requires manual assignment by human annotators (as in the supervised learning setting)."
msgstr "Al contrario de lo que ocurre con la inercia, **las medidas basadas en el FMI requieren el conocimiento de las clases basadas en la evidencia**, mientras que casi nunca están disponibles en la práctica o requieren la asignación manual por parte de anotadores humanos (como en el entorno del aprendizaje supervisado)."

#: ../modules/clustering.rst:1648
msgid "E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two hierarchical clusterings\". Journal of the American Statistical Association. http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf"
msgstr "E. B. Fowkles and C. L. Mallows, 1983. \"A method for comparing two hierarchical clusterings\". Journal of the American Statistical Association. http://wildfire.stat.ucla.edu/pdflibrary/fowlkes.pdf"

#: ../modules/clustering.rst:1652
msgid "`Wikipedia entry for the Fowlkes-Mallows Index <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_"
msgstr "`Wikipedia entry for the Fowlkes-Mallows Index <https://en.wikipedia.org/wiki/Fowlkes-Mallows_index>`_"

#: ../modules/clustering.rst:1658
msgid "Silhouette Coefficient"
msgstr "Coeficiente de Silueta"

#: ../modules/clustering.rst:1660
msgid "If the ground truth labels are not known, evaluation must be performed using the model itself. The Silhouette Coefficient (:func:`sklearn.metrics.silhouette_score`) is an example of such an evaluation, where a higher Silhouette Coefficient score relates to a model with better defined clusters. The Silhouette Coefficient is defined for each sample and is composed of two scores:"
msgstr "Si no se conocen las etiquetas basadas en la evidencia, la evaluación debe realizarse utilizando el propio modelo. El Coeficiente de Silueta (:func:`sklearn.metrics.silhouette_score`) es un ejemplo de este tipo de evaluación, donde una mayor puntuación del Coeficiente de Silueta se relaciona con un modelo con conglomerados mejor definidos. El coeficiente de silueta se define para cada muestra y se compone de dos puntuaciones:"

#: ../modules/clustering.rst:1668
msgid "**a**: The mean distance between a sample and all other points in the same class."
msgstr "**a**: La distancia media entre una muestra y todos los demás puntos de la misma clase."

#: ../modules/clustering.rst:1671
msgid "**b**: The mean distance between a sample and all other points in the *next nearest cluster*."
msgstr "**b**: La distancia media entre una muestra y todos los demás puntos en el *conglomerado más cercano*."

#: ../modules/clustering.rst:1674
msgid "The Silhouette Coefficient *s* for a single sample is then given as:"
msgstr "El Coeficiente de Silueta *s* para una sola muestra se da entonces como:"

#: ../modules/clustering.rst:1676
msgid "s = \\frac{b - a}{max(a, b)}\n\n"
msgstr "s = \\frac{b - a}{max(a, b)}\n\n"

#: ../modules/clustering.rst:1678
msgid "The Silhouette Coefficient for a set of samples is given as the mean of the Silhouette Coefficient for each sample."
msgstr "El Coeficiente de Silueta para un conjunto de muestras se da como la media del Coeficiente de Silueta para cada muestra."

#: ../modules/clustering.rst:1687
msgid "In normal usage, the Silhouette Coefficient is applied to the results of a cluster analysis."
msgstr "En el uso normal, el Coeficiente de Silueta se aplica a los resultados de un análisis de conglomerados."

#: ../modules/clustering.rst:1699
msgid "Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis\". Computational and Applied Mathematics 20: 53–65. `doi:10.1016/0377-0427(87)90125-7 <https://doi.org/10.1016/0377-0427(87)90125-7>`_."
msgstr "Peter J. Rousseeuw (1987). \"Silhouettes: a Graphical Aid to the Interpretation and Validation of Cluster Analysis\". Computational and Applied Mathematics 20: 53–65. `doi:10.1016/0377-0427(87)90125-7 <https://doi.org/10.1016/0377-0427(87)90125-7>`_."

#: ../modules/clustering.rst:1708
msgid "The score is bounded between -1 for incorrect clustering and +1 for highly dense clustering. Scores around zero indicate overlapping clusters."
msgstr "La puntuación está limitada entre -1 para un agrupamiento incorrecto y +1 para un conglomerado altamente denso. Las puntuaciones alrededor de cero indican agrupamiento superpuestos."

#: ../modules/clustering.rst:1711 ../modules/clustering.rst:1761
msgid "The score is higher when clusters are dense and well separated, which relates to a standard concept of a cluster."
msgstr "La puntuación es mayor cuando los conglomerados son densos y están bien separados, lo cual se relaciona con el concepto estándar de un conglomerado."

#: ../modules/clustering.rst:1718
msgid "The Silhouette Coefficient is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN."
msgstr "El Coeficiente de Silueta es generalmente más alto para los conglomerados convexos que para otros conceptos de conglomerados, como los conglomerados basados en la densidad como los obtenidos a través de DBSCAN."

#: ../modules/clustering.rst:1724
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py` : In this example the silhouette analysis is used to choose an optimal value for n_clusters."
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_kmeans_silhouette_analysis.py` : En este ejemplo se utiliza el análisis de siluetas para elegir un valor óptimo para n_clusters."

#: ../modules/clustering.rst:1731
msgid "Calinski-Harabasz Index"
msgstr "Índice de Calinski-Harabasz"

#: ../modules/clustering.rst:1734
msgid "If the ground truth labels are not known, the Calinski-Harabasz index (:func:`sklearn.metrics.calinski_harabasz_score`) - also known as the Variance Ratio Criterion - can be used to evaluate the model, where a higher Calinski-Harabasz score relates to a model with better defined clusters."
msgstr "Si no se conocen las etiquetas basadas en la evidencia, se puede utilizar el índice de Calinski-Harabasz (:func:`sklearn.metrics.calinski_harabasz_score`) -también conocido como Criterio de Razón de la Varianza- para evaluar el modelo, donde una puntuación Calinski-Harabasz más alta se relaciona con un modelo con conglomerados mejor definidos."

#: ../modules/clustering.rst:1739
msgid "The index is the ratio of the sum of between-clusters dispersion and of within-cluster dispersion for all clusters (where dispersion is defined as the sum of distances squared):"
msgstr "El índice es la razón de la suma de la dispersión entre conglomerados y de la dispersión dentro del conglomerado para todos los conglomerados (donde la dispersión se define como la suma de las distancias al cuadrado):"

#: ../modules/clustering.rst:1748
msgid "In normal usage, the Calinski-Harabasz index is applied to the results of a cluster analysis:"
msgstr "En el uso normal, el índice de Calinski-Harabasz se aplica a los resultados de un análisis de conglomerados:"

#: ../modules/clustering.rst:1764
msgid "The score is fast to compute."
msgstr "La puntuación es rápida de calcular."

#: ../modules/clustering.rst:1770
msgid "The Calinski-Harabasz index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained through DBSCAN."
msgstr "El índice de Calinski-Harabasz suele ser mayor para los conglomerados convexos que para otros conceptos de conglomerados, entre ellos los conglomerados basados en la densidad como los obtenidos a través de DBSCAN."

#: ../modules/clustering.rst:1777
msgid "For a set of data :math:`E` of size :math:`n_E` which has been clustered into :math:`k` clusters, the Calinski-Harabasz score :math:`s` is defined as the ratio of the between-clusters dispersion mean and the within-cluster dispersion:"
msgstr "Para un conjunto de datos :math:`E` de tamaño :math:`n_E` que ha sido agrupado en :math:`k` conglomerados, la puntuación de Calinski-Harabasz :math:`s` se define como la razón de la dispersión media entre los conglomerados y la dispersión dentro de los conglomerados:"

#: ../modules/clustering.rst:1781
msgid "s = \\frac{\\mathrm{tr}(B_k)}{\\mathrm{tr}(W_k)} \\times \\frac{n_E - k}{k - 1}\n\n"
msgstr "s = \\frac{\\mathrm{tr}(B_k)}{\\mathrm{tr}(W_k)} \\times \\frac{n_E - k}{k - 1}\n\n"

#: ../modules/clustering.rst:1784
msgid "where :math:`\\mathrm{tr}(B_k)` is trace of the between group dispersion matrix and :math:`\\mathrm{tr}(W_k)` is the trace of the within-cluster dispersion matrix defined by:"
msgstr "donde :math:`\\mathrm{tr}(B_k)` es la traza de la matriz de la dispersión entre grupos y :math:`mathrm{tr}(W_k)` es la traza de la matriz de la dispersión dentro del conglomerado definida por:"

#: ../modules/clustering.rst:1788
msgid "W_k = \\sum_{q=1}^k \\sum_{x \\in C_q} (x - c_q) (x - c_q)^T\n\n"
msgstr "W_k = \\sum_{q=1}^k \\sum_{x \\in C_q} (x - c_q) (x - c_q)^T\n\n"

#: ../modules/clustering.rst:1790
msgid "B_k = \\sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\n\n"
msgstr "B_k = \\sum_{q=1}^k n_q (c_q - c_E) (c_q - c_E)^T\n\n"

#: ../modules/clustering.rst:1792
msgid "with :math:`C_q` the set of points in cluster :math:`q`, :math:`c_q` the center of cluster :math:`q`, :math:`c_E` the center of :math:`E`, and :math:`n_q` the number of points in cluster :math:`q`."
msgstr "con :math:`C_q` el conjunto de puntos en el conglomerado :math:`q`, :math:`c_q` el centro del conglomerado :math:`q`, :math:`c_E` el centro de :math:`E`, y :math:`n_q` el número de puntos del conglomerado :math:`q`."

#: ../modules/clustering.rst:1798
msgid "Caliński, T., & Harabasz, J. (1974). `\"A Dendrite Method for Cluster Analysis\" <https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis>`_. Communications in Statistics-theory and Methods 3: 1-27. `doi:10.1080/03610927408827101 <https://doi.org/10.1080/03610927408827101>`_."
msgstr "Caliński, T., & Harabasz, J. (1974). `\"A Dendrite Method for Cluster Analysis\" <https://www.researchgate.net/publication/233096619_A_Dendrite_Method_for_Cluster_Analysis>`_. Communications in Statistics-theory and Methods 3: 1-27. `doi:10.1080/03610927408827101 <https://doi.org/10.1080/03610927408827101>`_."

#: ../modules/clustering.rst:1808
msgid "Davies-Bouldin Index"
msgstr "Índice de Davies-Bouldin"

#: ../modules/clustering.rst:1810
msgid "If the ground truth labels are not known, the Davies-Bouldin index (:func:`sklearn.metrics.davies_bouldin_score`) can be used to evaluate the model, where a lower Davies-Bouldin index relates to a model with better separation between the clusters."
msgstr "Si no se conocen las etiquetas basadas en la evidencia, se puede utilizar el índice de Davies-Bouldin (:func:`sklearn.metrics.davies_bouldin_score`) para evaluar el modelo, donde un índice de Davies-Bouldin más bajo se relaciona con un modelo con mejor separación entre los conglomerados."

#: ../modules/clustering.rst:1815
msgid "This index signifies the average 'similarity' between clusters, where the similarity is a measure that compares the distance between clusters with the size of the clusters themselves."
msgstr "Este índice significa la \"similitud\" promedio entre conglomerados, donde la similitud es una medida que compara la distancia entre conglomerados con el tamaño de los propios conglomerados."

#: ../modules/clustering.rst:1819
msgid "Zero is the lowest possible score. Values closer to zero indicate a better partition."
msgstr "El cero es la puntuación más baja posible. Los valores más cercanos a cero indican una mejor partición."

#: ../modules/clustering.rst:1822
msgid "In normal usage, the Davies-Bouldin index is applied to the results of a cluster analysis as follows:"
msgstr "En el uso normal, el índice de Davies-Bouldin se aplica a los resultados de un análisis de conglomerados de la siguiente manera:"

#: ../modules/clustering.rst:1839
msgid "The computation of Davies-Bouldin is simpler than that of Silhouette scores."
msgstr "El cálculo de Davies-Bouldin es más sencillo que el de las puntuaciones de Silueta."

#: ../modules/clustering.rst:1840
msgid "The index is computed only quantities and features inherent to the dataset."
msgstr "El índice sólo calcula las cantidades y características inherentes al conjunto de datos."

#: ../modules/clustering.rst:1845
msgid "The Davies-Boulding index is generally higher for convex clusters than other concepts of clusters, such as density based clusters like those obtained from DBSCAN."
msgstr "El índice de Davies-Boulding suele ser mayor para los conglomerados convexos que para otros conceptos de conglomerados, como los basados en la densidad, como los obtenidos con DBSCAN."

#: ../modules/clustering.rst:1848
msgid "The usage of centroid distance limits the distance metric to Euclidean space."
msgstr "El uso de la distancia centroide limita la métrica de la distancia al espacio Euclideano."

#: ../modules/clustering.rst:1853
msgid "The index is defined as the average similarity between each cluster :math:`C_i` for :math:`i=1, ..., k` and its most similar one :math:`C_j`. In the context of this index, similarity is defined as a measure :math:`R_{ij}` that trades off:"
msgstr "El índice se define como la similitud promedio entre cada conglomerado :math:`C_i` para :math:`i=1, ..., k` y su más similar :math:`C_j`. En el contexto de este índice, la similitud se define como una medida :math:`R_{ij}` que compensa:"

#: ../modules/clustering.rst:1857
msgid ":math:`s_i`, the average distance between each point of cluster :math:`i` and the centroid of that cluster -- also know as cluster diameter."
msgstr ":math:`s_i` la distancia promedio entre cada punto del conglomerado :math:`i` y el centroide de ese conglomerado -- también conocido como diámetro del conglomerado."

#: ../modules/clustering.rst:1859
msgid ":math:`d_{ij}`, the distance between cluster centroids :math:`i` and :math:`j`."
msgstr ":math:`d_{ij}`, la distancia entre los centroides de los conglomerados :math:`i` y :math:`j`."

#: ../modules/clustering.rst:1861
msgid "A simple choice to construct :math:`R_{ij}` so that it is nonnegative and symmetric is:"
msgstr "Una opción sencilla para construir :math:`R_{ij}` de manera que sea no negativa y simétrica es:"

#: ../modules/clustering.rst:1864
msgid "R_{ij} = \\frac{s_i + s_j}{d_{ij}}\n\n"
msgstr "R_{ij} = \\frac{s_i + s_j}{d_{ij}}\n\n"

#: ../modules/clustering.rst:1867
msgid "Then the Davies-Bouldin index is defined as:"
msgstr "Entonces el índice de Davies-Bouldin se define como:"

#: ../modules/clustering.rst:1869
msgid "DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{i \\neq j} R_{ij}\n\n"
msgstr "DB = \\frac{1}{k} \\sum_{i=1}^k \\max_{i \\neq j} R_{ij}\n\n"

#: ../modules/clustering.rst:1875
msgid "Davies, David L.; Bouldin, Donald W. (1979). \"A Cluster Separation Measure\" IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227. `doi:10.1109/TPAMI.1979.4766909 <https://doi.org/10.1109/TPAMI.1979.4766909>`_."
msgstr "Davies, David L.; Bouldin, Donald W. (1979). \"A Cluster Separation Measure\" IEEE Transactions on Pattern Analysis and Machine Intelligence. PAMI-1 (2): 224-227. `doi:10.1109/TPAMI.1979.4766909 <https://doi.org/10.1109/TPAMI.1979.4766909>`_."

#: ../modules/clustering.rst:1881
msgid "Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). \"On Clustering Validation Techniques\" Journal of Intelligent Information Systems, 17(2-3), 107-145. `doi:10.1023/A:1012801612483 <https://doi.org/10.1023/A:1012801612483>`_."
msgstr "Halkidi, Maria; Batistakis, Yannis; Vazirgiannis, Michalis (2001). \"On Clustering Validation Techniques\" Journal of Intelligent Information Systems, 17(2-3), 107-145. `doi:10.1023/A:1012801612483 <https://doi.org/10.1023/A:1012801612483>`_."

#: ../modules/clustering.rst:1886
msgid "`Wikipedia entry for Davies-Bouldin index <https://en.wikipedia.org/wiki/Davies–Bouldin_index>`_."
msgstr "`Wikipedia entry for Davies-Bouldin index <https://en.wikipedia.org/wiki/Davies–Bouldin_index>`_."

#: ../modules/clustering.rst:1893
msgid "Contingency Matrix"
msgstr "Matriz de contingencia"

#: ../modules/clustering.rst:1895
msgid "Contingency matrix (:func:`sklearn.metrics.cluster.contingency_matrix`) reports the intersection cardinality for every true/predicted cluster pair. The contingency matrix provides sufficient statistics for all clustering metrics where the samples are independent and identically distributed and one doesn't need to account for some instances not being clustered."
msgstr "La matriz de contingencia (:func:`sklearn.metrics.cluster.contingency_matrix`) informa de la cardinalidad de la intersección para cada par de conglomerados verdadero/predicho. La matriz de contingencia proporciona estadísticos suficientes para todas las métricas de agrupamiento (análisis de conglomerados) en las que las muestras son independientes e idénticamente distribuidas y no es necesario tener en cuenta que algunas instancias no están conglomeradas."

#: ../modules/clustering.rst:1901
msgid "Here is an example::"
msgstr "Aquí hay un ejemplo::"

#: ../modules/clustering.rst:1910
msgid "The first row of output array indicates that there are three samples whose true cluster is \"a\". Of them, two are in predicted cluster 0, one is in 1, and none is in 2. And the second row indicates that there are three samples whose true cluster is \"b\". Of them, none is in predicted cluster 0, one is in 1 and two are in 2."
msgstr "La primera fila del arreglo de salida indica que hay tres muestras cuyo verdadero conglomerado es \"a\". De ellas, dos están en el conglomerado predicho 0, una en el 1 y ninguna en el 2. Y la segunda fila indica que hay tres muestras cuyo verdadero conglomerado es \"b\". De ellas, ninguna está en el conglomerado previsto 0, una está en el 1 y dos en el 2."

#: ../modules/clustering.rst:1916
msgid "A :ref:`confusion matrix <confusion_matrix>` for classification is a square contingency matrix where the order of rows and columns correspond to a list of classes."
msgstr "Una :ref:`matriz de confusión <confusion_matrix>` para la clasificación es una matriz de contingencia cuadrada donde el orden de filas y columnas corresponden a una lista de clases."

#: ../modules/clustering.rst:1924
msgid "Allows to examine the spread of each true cluster across predicted clusters and vice versa."
msgstr "Permite examinar la propagación de cada conglomerado verdadero a través de conglomerados predichos, y viceversa."

#: ../modules/clustering.rst:1927
msgid "The contingency table calculated is typically utilized in the calculation of a similarity statistic (like the others listed in this document) between the two clusterings."
msgstr "La tabla de contingencia calculada suele utilizarse en el cálculo de un estadístico de similitud (como los demás que figuran en este documento) entre los dos agrupamientos."

#: ../modules/clustering.rst:1934
msgid "Contingency matrix is easy to interpret for a small number of clusters, but becomes very hard to interpret for a large number of clusters."
msgstr "La matriz de contingencia es fácil de interpretar para un pequeño número de conglomerados, pero se vuelve muy difícil de interpretar para un gran número de conglomerados."

#: ../modules/clustering.rst:1937
msgid "It doesn't give a single metric to use as an objective for clustering optimisation."
msgstr "No da una sola métrica para usar como objetivo para la optimización del agrupamiento."

#: ../modules/clustering.rst:1943
msgid "`Wikipedia entry for contingency matrix <https://en.wikipedia.org/wiki/Contingency_table>`_"
msgstr "`Wikipedia entry for contingency matrix <https://en.wikipedia.org/wiki/Contingency_table>`_"

#: ../modules/clustering.rst:1949
msgid "Pair Confusion Matrix"
msgstr "Matriz de confusión por pares"

#: ../modules/clustering.rst:1951
msgid "The pair confusion matrix (:func:`sklearn.metrics.cluster.pair_confusion_matrix`) is a 2x2 similarity matrix"
msgstr "La matriz de confusión por pares (:func:`sklearn.metrics.cluster.pair_confusion_matrix`) es una matriz de similitud de 2x2"

#: ../modules/clustering.rst:1955
msgid "C = \\left[\\begin{matrix}\n"
"C_{00} & C_{01} \\\\\n"
"C_{10} & C_{11}\n"
"\\end{matrix}\\right]\n\n"
msgstr "C = \\left[\\begin{matrix}\n"
"C_{00} & C_{01} \\\\\n"
"C_{10} & C_{11}\n"
"\\end{matrix}\\right]\n\n"

#: ../modules/clustering.rst:1961
msgid "between two clusterings computed by considering all pairs of samples and counting pairs that are assigned into the same or into different clusters under the true and predicted clusterings."
msgstr "entre dos agrupamientos calculados considerando todos los pares de muestras y contando los pares que se asignan en el mismo o en diferentes conglomerados bajo los agrupamientos verdaderos y predichos."

#: ../modules/clustering.rst:1965
msgid "It has the following entries:"
msgstr "Tiene las siguientes entradas:"

#: ../modules/clustering.rst:1967
msgid ":math:`C_{00}` : number of pairs with both clusterings having the samples not clustered together"
msgstr ":math:`C_{00}` : número de pares con ambos agrupamientos que tienen las muestras no conglomeradas"

#: ../modules/clustering.rst:1970
msgid ":math:`C_{10}` : number of pairs with the true label clustering having the samples clustered together but the other clustering not having the samples clustered together"
msgstr ":math:`C_{10}` : número de pares con el agrupamiento de etiqueta verdadera que tiene las muestras conglomeradas pero el otro agrupamiento no tiene las muestras conglomeradas"

#: ../modules/clustering.rst:1974
msgid ":math:`C_{01}` : number of pairs with the true label clustering not having the samples clustered together but the other clustering having the samples clustered together"
msgstr ":math:`C_{01}` : número de pares con el agrupamiento de la etiqueta verdadera que no tiene las muestras conglomeradas pero el otro agrupamiento tiene las muestras conglomeradas"

#: ../modules/clustering.rst:1978
msgid ":math:`C_{11}` : number of pairs with both clusterings having the samples clustered together"
msgstr ":math:`C_{11}` : número de pares con ambos agrupamientos que tienen las muestras conglomeradas"

#: ../modules/clustering.rst:1981
msgid "Considering a pair of samples that is clustered together a positive pair, then as in binary classification the count of true negatives is :math:`C_{00}`, false negatives is :math:`C_{10}`, true positives is :math:`C_{11}` and false positives is :math:`C_{01}`."
msgstr "Considerando un par de muestras que están conglomeradas como un par positivo, entonces como en la clasificación binaria el conteo de verdaderos negativos es :math:`C_{00}`, falsos negativos :math:`C_{10}`, verdaderos positivos :math:`C_{11}` y falsos positivos :math:`C_{01}`."

#: ../modules/clustering.rst:1986
msgid "Perfectly matching labelings have all non-zero entries on the diagonal regardless of actual label values::"
msgstr "Las etiquetas que corresponden perfectamente tienen todas las entradas no nulas o iguales a cero en la diagonal, independientemente de los valores reales de las etiquetas::"

#: ../modules/clustering.rst:2000
msgid "Labelings that assign all classes members to the same clusters are complete but may not always be pure, hence penalized, and have some off-diagonal non-zero entries::"
msgstr "Las etiquetas que asignan a todos los miembros de las clases a los mismos conglomerados son completos, pero pueden no ser siempre puros, por lo que están penalizados, y tienen algunas entradas no nulas fuera de la diagonal::"

#: ../modules/clustering.rst:2008
msgid "The matrix is not symmetric::"
msgstr "La matriz no es simétrica::"

#: ../modules/clustering.rst:2014
msgid "If classes members are completely split across different clusters, the assignment is totally incomplete, hence the matrix has all zero diagonal entries::"
msgstr "Si los miembros de las clases están completamente divididos en diferentes conglomerados, la asignación es totalmente incompleta, por lo que la matriz tiene todas las entradas diagonales cero::"

#: ../modules/clustering.rst:2024
#, python-format
msgid "L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 <https://link.springer.com/article/10.1007%2FBF01908075>_"
msgstr "L. Hubert and P. Arabie, Comparing Partitions, Journal of Classification 1985 <https://link.springer.com/article/10.1007%2FBF01908075>_"

