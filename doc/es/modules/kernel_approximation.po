msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-30 21:43\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/kernel_approximation.po\n"
"X-Crowdin-File-ID: 4848\n"
"Language: es_ES\n"

#: ../modules/kernel_approximation.rst:4
msgid "Kernel Approximation"
msgstr "Aproximación de núcleo"

#: ../modules/kernel_approximation.rst:6
msgid "This submodule contains functions that approximate the feature mappings that correspond to certain kernels, as they are used for example in support vector machines (see :ref:`svm`). The following feature functions perform non-linear transformations of the input, which can serve as a basis for linear classification or other algorithms."
msgstr "Este submódulo contiene funciones que aproximan los mapeos de características que corresponden a ciertos núcleos (kernels), tal y como se utilizan, por ejemplo, en las máquinas de vectores de soporte (véase :ref:`svm`). Las siguientes funciones de características realizan transformaciones no lineales de la entrada, que pueden servir de base para la clasificación lineal u otros algoritmos."

#: ../modules/kernel_approximation.rst:15
msgid "The advantage of using approximate explicit feature maps compared to the `kernel trick <https://en.wikipedia.org/wiki/Kernel_trick>`_, which makes use of feature maps implicitly, is that explicit mappings can be better suited for online learning and can significantly reduce the cost of learning with very large datasets. Standard kernelized SVMs do not scale well to large datasets, but using an approximate kernel map it is possible to use much more efficient linear SVMs. In particular, the combination of kernel map approximations with :class:`SGDClassifier` can make non-linear learning on large datasets possible."
msgstr "La ventaja de utilizar mapas de características explícitas (explicit feature maps) aproximados en comparación con el truco del núcleo `kernel trick <https://en.wikipedia.org/wiki/Kernel_trick>`_, que hace uso de los mapas de características de forma implícita, es que los mapeos explícitos pueden ser más adecuados para el aprendizaje en línea (online learning) y pueden reducir significativamente el coste del aprendizaje con conjuntos de datos muy grandes. Las SVM de núcleo estándar no se escalan bien en grandes conjuntos de datos, pero utilizando un mapa de núcleo aproximado es posible utilizar SVM lineales mucho más eficientes. En particular, la combinación de aproximaciones del mapa del núcleo con :class:`SGDClassifier` puede hacer posible el aprendizaje no lineal en grandes conjuntos de datos."

#: ../modules/kernel_approximation.rst:25
msgid "Since there has not been much empirical work using approximate embeddings, it is advisable to compare results against exact kernel methods when possible."
msgstr "Dado que no se han realizado muchos trabajos empíricos utilizando embeddings aproximados, es aconsejable comparar los resultados con los métodos de núcleos exactos cuando sea posible."

#: ../modules/kernel_approximation.rst:30
msgid ":ref:`polynomial_regression` for an exact polynomial transformation."
msgstr ":ref:`polynomial_regression` para una transformación polinómica exacta."

#: ../modules/kernel_approximation.rst:37
msgid "Nystroem Method for Kernel Approximation"
msgstr "Método Nystroem para la aproximación de núcleos"

#: ../modules/kernel_approximation.rst:38
msgid "The Nystroem method, as implemented in :class:`Nystroem` is a general method for low-rank approximations of kernels. It achieves this by essentially subsampling the data on which the kernel is evaluated. By default :class:`Nystroem` uses the ``rbf`` kernel, but it can use any kernel function or a precomputed kernel matrix. The number of samples used - which is also the dimensionality of the features computed - is given by the parameter ``n_components``."
msgstr "El método Nystroem, tal y como se implementa en :class:`Nystroem` es un método general para aproximaciones de bajo nivel de los núcleos. Lo consigue esencialmente mediante el submuestreo de los datos sobre los que se evalúa el núcleo. Por defecto, :class:`Nystroem` utiliza el kernel ``rbf``, pero puede utilizar cualquier función del kernel o una matriz del kernel precalculada. El número de muestras utilizadas - que es también la dimensionalidad de las características calculadas - viene dado por el parámetro ``n_components``."

#: ../modules/kernel_approximation.rst:49
msgid "Radial Basis Function Kernel"
msgstr "Núcleo de la función de base radial"

#: ../modules/kernel_approximation.rst:51
msgid "The :class:`RBFSampler` constructs an approximate mapping for the radial basis function kernel, also known as *Random Kitchen Sinks* [RR2007]_. This transformation can be used to explicitly model a kernel map, prior to applying a linear algorithm, for example a linear SVM::"
msgstr "La :class:`RBFSampler` construye un mapeo aproximado para el núcleo de la función de base radial (radial basis function kernel, RBF), también conocido como *Random Kitchen Sinks* [RR2007]_. Esta transformación puede utilizarse para modelar explícitamente un mapa de núcleo, antes de aplicar un algoritmo lineal, por ejemplo una SVM lineal::"

#: ../modules/kernel_approximation.rst:68
msgid "The mapping relies on a Monte Carlo approximation to the kernel values. The ``fit`` function performs the Monte Carlo sampling, whereas the ``transform`` method performs the mapping of the data.  Because of the inherent randomness of the process, results may vary between different calls to the ``fit`` function."
msgstr "El mapeo se basa en una aproximación de Monte Carlo a los valores del núcleo. La función ``fit`` realiza el muestreo de Monte Carlo, mientras que el método ``transform`` realiza el mapeo de los datos.  Debido a la aleatoriedad inherente al proceso, los resultados pueden variar entre diferentes llamadas a la función ``fit``."

#: ../modules/kernel_approximation.rst:74
msgid "The ``fit`` function takes two arguments: ``n_components``, which is the target dimensionality of the feature transform, and ``gamma``, the parameter of the RBF-kernel.  A higher ``n_components`` will result in a better approximation of the kernel and will yield results more similar to those produced by a kernel SVM. Note that \"fitting\" the feature function does not actually depend on the data given to the ``fit`` function. Only the dimensionality of the data is used. Details on the method can be found in [RR2007]_."
msgstr "La función ``fit`` toma dos argumentos: ``n_components``, que es la dimensionalidad objetivo de la transformación de características, y ``gamma``, el parámetro del núcleo RBF.  Un ``n_components`` más alto dará lugar a una mejor aproximación del núcleo y producirá resultados más similares a los producidos por un núcleo SVM. Ten en cuenta que el \"ajuste\" de la función característica no depende realmente de los datos dados a la función ``fit``. Sólo se utiliza la dimensionalidad de los datos. Los detalles del método se pueden encontrar en [RR2007]_."

#: ../modules/kernel_approximation.rst:83
msgid "For a given value of ``n_components`` :class:`RBFSampler` is often less accurate as :class:`Nystroem`. :class:`RBFSampler` is cheaper to compute, though, making use of larger feature spaces more efficient."
msgstr "Para un valor determinado de ``n_components`` :class:`RBFSampler` suele ser menos preciso que :class:`Nystroem`. Sin embargo, :class:`RBFSampler` es menos costoso de calcular, lo que hace más eficiente el uso de espacios de características más grandes."

#: ../modules/kernel_approximation.rst:92
msgid "Comparing an exact RBF kernel (left) with the approximation (right)"
msgstr "Comparación de un núcleo RBF exacto (izquierda) con la aproximación (derecha)"

#: ../modules/kernel_approximation.rst:96
msgid ":ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_approximation.py`"
msgstr ":ref:`sphx_glr_auto_examples_miscellaneous_plot_kernel_approximation.py`"

#: ../modules/kernel_approximation.rst:101
msgid "Additive Chi Squared Kernel"
msgstr "Núcleo aditivo de chi cuadrado"

#: ../modules/kernel_approximation.rst:103
msgid "The additive chi squared kernel is a kernel on histograms, often used in computer vision."
msgstr "El núcleo chi cuadrado aditivo es un núcleo basado en histogramas que se utiliza a menudo en la visión por computadora."

#: ../modules/kernel_approximation.rst:105
msgid "The additive chi squared kernel as used here is given by"
msgstr "El núcleo aditivo de chi-cuadrado utilizado aquí viene dado por"

#: ../modules/kernel_approximation.rst:107
msgid "k(x, y) = \\sum_i \\frac{2x_iy_i}{x_i+y_i}"
msgstr "k(x, y) = \\sum_i \\frac{2x_iy_i}{x_i+y_i}"

#: ../modules/kernel_approximation.rst:111
msgid "This is not exactly the same as :func:`sklearn.metrics.additive_chi2_kernel`. The authors of [VZ2010]_ prefer the version above as it is always positive definite. Since the kernel is additive, it is possible to treat all components :math:`x_i` separately for embedding. This makes it possible to sample the Fourier transform in regular intervals, instead of approximating using Monte Carlo sampling."
msgstr "Esto no es exactamente lo mismo que :func:`sklearn.metrics.additive_chi2_kernel`. Los autores de [VZ2010]_ prefieren la versión anterior ya que siempre es definida positiva. Como el núcleo es aditivo, es posible tratar todos los componentes :math:`x_i` por separado para la incrustación (embedding). Esto hace posible muestrear la transformada de Fourier en intervalos regulares, en lugar de aproximar utilizando el muestreo de Monte Carlo."

#: ../modules/kernel_approximation.rst:119
msgid "The class :class:`AdditiveChi2Sampler` implements this component wise deterministic sampling. Each component is sampled :math:`n` times, yielding :math:`2n+1` dimensions per input dimension (the multiple of two stems from the real and complex part of the Fourier transform). In the literature, :math:`n` is usually chosen to be 1 or 2, transforming the dataset to size ``n_samples * 5 * n_features`` (in the case of :math:`n=2`)."
msgstr "La clase :class:`AdditiveChi2Sampler` implementa este muestreo determinista por componentes. Cada componente se muestrea :math:`n` veces, dando lugar a :math:`2n+1` dimensiones por dimensión de entrada (el múltiplo de dos proviene de la parte real y compleja de la transformada de Fourier). En la bibliografía, :math:`n` suele elegirse como 1 o 2, lo que transforma el conjunto de datos en un tamaño de ``n_samples * 5 * n_features`` (en el caso de :math:`n=2`)."

#: ../modules/kernel_approximation.rst:126
msgid "The approximate feature map provided by :class:`AdditiveChi2Sampler` can be combined with the approximate feature map provided by :class:`RBFSampler` to yield an approximate feature map for the exponentiated chi squared kernel. See the [VZ2010]_ for details and [VVZ2010]_ for combination with the :class:`RBFSampler`."
msgstr "El mapa de características aproximado proporcionado por :class:`AdditiveChi2Sampler` puede combinarse con el mapa de características aproximado proporcionado por :class:`RBFSampler` para obtener un mapa de características aproximado para el núcleo de chi cuadrado exponenciado. Véase el documento [VZ2010]_ para más detalles y [VVZ2010]_ para la combinación con la :clase:`RBFSampler`."

#: ../modules/kernel_approximation.rst:134
msgid "Skewed Chi Squared Kernel"
msgstr "Núcleo de chi cuadrado sesgado"

#: ../modules/kernel_approximation.rst:136
msgid "The skewed chi squared kernel is given by:"
msgstr "El núcleo de chi cuadrado sesgado viene dado por:"

#: ../modules/kernel_approximation.rst:138
msgid "k(x,y) = \\prod_i \\frac{2\\sqrt{x_i+c}\\sqrt{y_i+c}}{x_i + y_i + 2c}"
msgstr "k(x,y) = \\prod_i \\frac{2\\sqrt{x_i+c}\\sqrt{y_i+c}}{x_i + y_i + 2c}"

#: ../modules/kernel_approximation.rst:143
msgid "It has properties that are similar to the exponentiated chi squared kernel often used in computer vision, but allows for a simple Monte Carlo approximation of the feature map."
msgstr "Tiene propiedades similares al núcleo de chi cuadrado exponenciado que se utiliza a menudo en la visión por computadora, pero permite una aproximación simple de Monte Carlo del mapa de características."

#: ../modules/kernel_approximation.rst:147
msgid "The usage of the :class:`SkewedChi2Sampler` is the same as the usage described above for the :class:`RBFSampler`. The only difference is in the free parameter, that is called :math:`c`. For a motivation for this mapping and the mathematical details see [LS2010]_."
msgstr "El uso del :class:`SkewedChi2Sampler` es el mismo que el descrito anteriormente para el :class:`RBFSampler`. La única diferencia está en el parámetro independiente, que se llama :math:`c`. Para conocer la motivación de este mapeo y los detalles matemáticos, véase [LS2010]_."

#: ../modules/kernel_approximation.rst:155
msgid "Polynomial Kernel Approximation via Tensor Sketch"
msgstr "Aproximación de núcleos polinómicos a través de Tensor Sketch"

#: ../modules/kernel_approximation.rst:157
msgid "The :ref:`polynomial kernel <polynomial_kernel>` is a popular type of kernel function given by:"
msgstr "El :ref:`núcleo polinómico <polynomial_kernel>` es un tipo popular de función de núcleo dada por:"

#: ../modules/kernel_approximation.rst:160
msgid "k(x, y) = (\\gamma x^\\top y +c_0)^d"
msgstr "k(x, y) = (\\gamma x^\\top y +c_0)^d"

#: ../modules/kernel_approximation.rst:164
msgid "where:"
msgstr "donde:"

#: ../modules/kernel_approximation.rst:166
msgid "``x``, ``y`` are the input vectors"
msgstr "``x``, ``y`` son los vectores de entrada"

#: ../modules/kernel_approximation.rst:167
msgid "``d`` is the kernel degree"
msgstr "``d`` es el grado (degree) del núcleo"

#: ../modules/kernel_approximation.rst:169
msgid "Intuitively, the feature space of the polynomial kernel of degree `d` consists of all possible degree-`d` products among input features, which enables learning algorithms using this kernel to account for interactions between features."
msgstr "Intuitivamente, el espacio de características del núcleo polinómico de grado `d` consiste en todos los posibles productos de grado `d` entre las características de entrada, lo que permite a los algoritmos de aprendizaje que utilizan este núcleo tener en cuenta las interacciones entre las características."

#: ../modules/kernel_approximation.rst:173
msgid "The TensorSketch [PP2013]_ method, as implemented in :class:`PolynomialCountSketch`, is a scalable, input data independent method for polynomial kernel approximation. It is based on the concept of Count sketch [WIKICS]_ [CCF2002]_ , a dimensionality reduction technique similar to feature hashing, which instead uses several independent hash functions. TensorSketch obtains a Count Sketch of the outer product of two vectors (or a vector with itself), which can be used as an approximation of the polynomial kernel feature space. In particular, instead of explicitly computing the outer product, TensorSketch computes the Count Sketch of the vectors and then uses polynomial multiplication via the Fast Fourier Transform to compute the Count Sketch of their outer product."
msgstr "El método TensorSketch [PP2013]_, implementado en :class:`PolynomialCountSketch`, es un método escalable e independiente de los datos de entrada para la aproximación de núcleos polinómicos. Se basa en el concepto de Count Sketch [WIKICS]_ [CCF2002]_ , una técnica de reducción de la dimensionalidad similar al feature hashing, que en cambio utiliza varias funciones hash independientes. TensorSketch obtiene un Count Sketch del producto exterior (outer product) de dos vectores (o de un vector consigo mismo), que puede utilizarse como una aproximación del espacio de características del núcleo polinómico. En concreto, en lugar de calcular explícitamente el producto exterior, TensorSketch calcula el Count Sketch de los vectores y luego utiliza la multiplicación polinómica a través de la Transformada Rápida de Fourier para calcular el Count Sketch de su producto exterior."

#: ../modules/kernel_approximation.rst:184
msgid "Conveniently, the training phase of TensorSketch simply consists of initializing some random variables. It is thus independent of the input data, i.e. it only depends on the number of input features, but not the data values. In addition, this method can transform samples in :math:`\\mathcal{O}(n_{\\text{samples}}(n_{\\text{features}} + n_{\\text{components}} \\log(n_{\\text{components}})))` time, where :math:`n_{\\text{components}}` is the desired output dimension, determined by ``n_components``."
msgstr "Convenientemente, la fase de entrenamiento de TensorSketch consiste simplemente en inicializar algunas variables aleatorias. Por tanto, es independiente de los datos de entrada, es decir, sólo depende del número de características de entrada, pero no de los valores de los datos. Además, este método puede transformar muestras en tiempo :math:`\\mathcal{O}(n_{\\text{samples}}(n_{\\text{features}} + n_{\\text{components}} \\log(n_{\\text{components}})))`, donde :math:`n_{\\text{components}}` es la dimensión de salida deseada, determinada por ``n_components``."

#: ../modules/kernel_approximation.rst:194
msgid ":ref:`sphx_glr_auto_examples_kernel_approximation_plot_scalable_poly_kernels.py`"
msgstr ":ref:`sphx_glr_auto_examples_kernel_approximation_plot_scalable_poly_kernels.py`"

#: ../modules/kernel_approximation.rst:199
msgid "Mathematical Details"
msgstr "Detalles matemáticos"

#: ../modules/kernel_approximation.rst:201
msgid "Kernel methods like support vector machines or kernelized PCA rely on a property of reproducing kernel Hilbert spaces. For any positive definite kernel function :math:`k` (a so called Mercer kernel), it is guaranteed that there exists a mapping :math:`\\phi` into a Hilbert space :math:`\\mathcal{H}`, such that"
msgstr "Los métodos de núcleo, como las máquinas de vectores soporte o el PCA \"kernelizado\", se basan en una propiedad de reproducción de los espacios de Hilbert del núcleo. Para cualquier función de núcleo definida positiva :math:`k` (también llamada núcleo de Mercer), se garantiza que existe un mapeo :math:`\\phi` en un espacio de Hilbert :math:`\\mathcal{H}`, tal que"

#: ../modules/kernel_approximation.rst:207
msgid "k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle"
msgstr "k(x,y) = \\langle \\phi(x), \\phi(y) \\rangle"

#: ../modules/kernel_approximation.rst:211
msgid "Where :math:`\\langle \\cdot, \\cdot \\rangle` denotes the inner product in the Hilbert space."
msgstr "Donde :math:`\\langle \\cdot, \\cdot \\rangle` indica el producto interno en el espacio Hilbert."

#: ../modules/kernel_approximation.rst:214
msgid "If an algorithm, such as a linear support vector machine or PCA, relies only on the scalar product of data points :math:`x_i`, one may use the value of :math:`k(x_i, x_j)`, which corresponds to applying the algorithm to the mapped data points :math:`\\phi(x_i)`. The advantage of using :math:`k` is that the mapping :math:`\\phi` never has to be calculated explicitly, allowing for arbitrary large features (even infinite)."
msgstr "Si un algoritmo, tal como una máquina de vectores de soporte lineal o PCA, se basa sólo en el producto escalar de los puntos de datos :math:`x_i`, se puede utilizar el valor de :math:`k(x_i, x_j)`, que corresponde a la aplicación del algoritmo a los puntos de datos mapeados :math:`\\phi(x_i)`. La ventaja de utilizar :math:`k` es que el mapeo :math:`\\phi` nunca tiene que ser calculado explícitamente, lo que permite características de tamaño arbitrario (incluso infinito)."

#: ../modules/kernel_approximation.rst:222
msgid "One drawback of kernel methods is, that it might be necessary to store many kernel values :math:`k(x_i, x_j)` during optimization. If a kernelized classifier is applied to new data :math:`y_j`, :math:`k(x_i, y_j)` needs to be computed to make predictions, possibly for many different :math:`x_i` in the training set."
msgstr "Una desventaja de los métodos de núcleo es que puede ser necesario almacenar muchos valores de núcleo :math:`k(x_i, x_j)` durante la optimización. Si un clasificador \"kernelizado\" se aplica a nuevos datos :math:`y_j`, es necesario calcular :math:`k(x_i, y_j)` para hacer predicciones, posiblemente para muchos :math:`x_i` diferentes en el conjunto de entrenamiento."

#: ../modules/kernel_approximation.rst:228
msgid "The classes in this submodule allow to approximate the embedding :math:`\\phi`, thereby working explicitly with the representations :math:`\\phi(x_i)`, which obviates the need to apply the kernel or store training examples."
msgstr "Las clases de este submódulo permiten aproximar el embedding :math:`\\phi`, trabajando así explícitamente con las representaciones :math:`\\phi(x_i)`, lo que evita la necesidad de aplicar el núcleo o almacenar ejemplos de entrenamiento."

#: ../modules/kernel_approximation.rst:236
msgid "`\"Random features for large-scale kernel machines\" <https://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf>`_ Rahimi, A. and Recht, B. - Advances in neural information processing 2007,"
msgstr "`\"Random features for large-scale kernel machines\" <https://www.robots.ox.ac.uk/~vgg/rg/papers/randomfeatures.pdf>`_ Rahimi, A. and Recht, B. - Advances in neural information processing 2007,"

#: ../modules/kernel_approximation.rst:239
msgid "`\"Random Fourier approximations for skewed multiplicative histogram kernels\" <http://www.maths.lth.se/matematiklth/personal/sminchis/papers/lis_dagm10.pdf>`_ Random Fourier approximations for skewed multiplicative histogram kernels - Lecture Notes for Computer Sciencd (DAGM)"
msgstr "`\"Random Fourier approximations for skewed multiplicative histogram kernels\" <http://www.maths.lth.se/matematiklth/personal/sminchis/papers/lis_dagm10.pdf>`_ Random Fourier approximations for skewed multiplicative histogram kernels - Lecture Notes for Computer Sciencd (DAGM)"

#: ../modules/kernel_approximation.rst:243
msgid "`\"Efficient additive kernels via explicit feature maps\" <https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf>`_ Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010"
msgstr "`\"Efficient additive kernels via explicit feature maps\" <https://www.robots.ox.ac.uk/~vgg/publications/2011/Vedaldi11/vedaldi11.pdf>`_ Vedaldi, A. and Zisserman, A. - Computer Vision and Pattern Recognition 2010"

#: ../modules/kernel_approximation.rst:246
msgid "`\"Generalized RBF feature maps for Efficient Detection\" <https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf>`_ Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010"
msgstr "`\"Generalized RBF feature maps for Efficient Detection\" <https://www.robots.ox.ac.uk/~vgg/publications/2010/Sreekanth10/sreekanth10.pdf>`_ Vempati, S. and Vedaldi, A. and Zisserman, A. and Jawahar, CV - 2010"

#: ../modules/kernel_approximation.rst:249
msgid "`\"Fast and scalable polynomial kernels via explicit feature maps\" <https://doi.org/10.1145/2487575.2487591>`_ Pham, N., & Pagh, R. - 2013"
msgstr "`\"Fast and scalable polynomial kernels via explicit feature maps\" <https://doi.org/10.1145/2487575.2487591>`_ Pham, N., & Pagh, R. - 2013"

#: ../modules/kernel_approximation.rst:252
msgid "`\"Finding frequent items in data streams\" <http://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf>`_ Charikar, M., Chen, K., & Farach-Colton - 2002"
msgstr "`\"Finding frequent items in data streams\" <http://www.cs.princeton.edu/courses/archive/spring04/cos598B/bib/CharikarCF.pdf>`_ Charikar, M., Chen, K., & Farach-Colton - 2002"

#: ../modules/kernel_approximation.rst:255
msgid "`\"Wikipedia: Count sketch\" <https://en.wikipedia.org/wiki/Count_sketch>`_"
msgstr "`\"Wikipedia: Count sketch\" <https://en.wikipedia.org/wiki/Count_sketch>`_"

