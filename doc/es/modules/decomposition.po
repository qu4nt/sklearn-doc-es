msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-06 01:40\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/decomposition.po\n"
"X-Crowdin-File-ID: 4828\n"
"Language: es_ES\n"

#: ../modules/decomposition.rst:6
msgid "Decomposing signals in components (matrix factorization problems)"
msgstr "Descomposición de señales en componentes (problemas de factorización de matrices)"

#: ../modules/decomposition.rst:15
msgid "Principal component analysis (PCA)"
msgstr "Análisis de componentes principales (PCA)"

#: ../modules/decomposition.rst:18
msgid "Exact PCA and probabilistic interpretation"
msgstr "PCA exacto e interpretación probabilista"

#: ../modules/decomposition.rst:20
msgid "PCA is used to decompose a multivariate dataset in a set of successive orthogonal components that explain a maximum amount of the variance. In scikit-learn, :class:`PCA` is implemented as a *transformer* object that learns :math:`n` components in its ``fit`` method, and can be used on new data to project it on these components."
msgstr "PCA se utiliza para descomponer un conjunto de datos multivariantes en un conjunto de componentes ortogonales sucesivos que explican una cantidad máxima de la varianza. En scikit-learn, :class:`PCA` es implementado como un objeto *transformador* que aprende componentes :math: `n` en su método ``fit``, y se puede utilizar en nuevos datos para proyectarlos en estos componentes."

#: ../modules/decomposition.rst:26
msgid "PCA centers but does not scale the input data for each feature before applying the SVD. The optional parameter ``whiten=True`` makes it possible to project the data onto the singular space while scaling each component to unit variance. This is often useful if the models down-stream make strong assumptions on the isotropy of the signal: this is for example the case for Support Vector Machines with the RBF kernel and the K-Means clustering algorithm."
msgstr "PCA centra pero no escala los datos de entrada para cada característica antes de aplicar la SVD. El parámetro opcional ``whiten=True`` permite proyectar los datos en el espacio singular mientras se escala cada componente a la varianza unitaria. Esto suele ser útil si los modelos subsiguientes hacen suposiciones firmes sobre la isotropía de la señal: este es el caso, por ejemplo, de las Máquinas de Vectores de Apoyo con el núcleo RBF y el algoritmo de agrupación K-Medias."

#: ../modules/decomposition.rst:34
msgid "Below is an example of the iris dataset, which is comprised of 4 features, projected on the 2 dimensions that explain most variance:"
msgstr "A continuación se muestra un ejemplo del conjunto de datos del iris, que se compone de 4 características, proyectadas en las 2 dimensiones que explican la mayor parte de la varianza:"

#: ../modules/decomposition.rst:43
msgid "The :class:`PCA` object also provides a probabilistic interpretation of the PCA that can give a likelihood of data based on the amount of variance it explains. As such it implements a :term:`score` method that can be used in cross-validation:"
msgstr "El objeto :class:`PCA` también proporciona una interpretación probabilística del PCA que puede dar una verosimilitud de los datos basada en la cantidad de varianza que explica. Como tal, implementa un método :term:`score` que puede utilizarse en la validación cruzada:"

#: ../modules/decomposition.rst:56
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`"

#: ../modules/decomposition.rst:57 ../modules/decomposition.rst:634
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_fa_model_selection.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_fa_model_selection.py`"

#: ../modules/decomposition.rst:63
msgid "Incremental PCA"
msgstr "PCA Incremental"

#: ../modules/decomposition.rst:65
msgid "The :class:`PCA` object is very useful, but has certain limitations for large datasets. The biggest limitation is that :class:`PCA` only supports batch processing, which means all of the data to be processed must fit in main memory. The :class:`IncrementalPCA` object uses a different form of processing and allows for partial computations which almost exactly match the results of :class:`PCA` while processing the data in a minibatch fashion. :class:`IncrementalPCA` makes it possible to implement out-of-core Principal Component Analysis either by:"
msgstr "El objeto :class:`PCA` es muy útil, pero tiene ciertas limitaciones para conjuntos de datos grandes. La mayor limitación es que :class:`PCA` sólo admite el procesamiento por lotes, lo que significa que todos los datos a procesar deben caber en la memoria principal. El objeto :class:`IncrementalPCA` utiliza una forma diferente de procesamiento y permite realizar cálculos parciales que coinciden casi exactamente con los resultados de :class:`PCA` mientras se procesan los datos de en mini lotes. :class:`IncrementalPCA` permite implementar el análisis de componentes principales fuera del núcleo:"

#: ../modules/decomposition.rst:74
msgid "Using its ``partial_fit`` method on chunks of data fetched sequentially from the local hard drive or a network database."
msgstr "Utilizando su método ``partial_fit`` en fragmentos de datos obtenidos secuencialmente desde el disco duro local o de una base de datos en la red."

#: ../modules/decomposition.rst:77
msgid "Calling its fit method on a sparse matrix or a memory mapped file using ``numpy.memmap``."
msgstr "Llamando a su método de ajuste en una matriz dispersa o un archivo mapeado en memoria usando ``numpy.memmap``."

#: ../modules/decomposition.rst:80
msgid ":class:`IncrementalPCA` only stores estimates of component and noise variances, in order update ``explained_variance_ratio_`` incrementally. This is why memory usage depends on the number of samples per batch, rather than the number of samples to be processed in the dataset."
msgstr ":class:`IncrementalPCA` sólo almacena las estimaciones de las varianzas de los componentes y del ruido, con el fin de actualizar la ``explained_variance_ratio_`` de forma incremental. Por ello, el uso de la memoria depende del número de muestras por lote, y no del número de muestras a procesar en el conjunto de datos."

#: ../modules/decomposition.rst:85
msgid "As in :class:`PCA`, :class:`IncrementalPCA` centers but does not scale the input data for each feature before applying the SVD."
msgstr "Como en el :class:`PCA`, el :class:`IncrementalPCA` centra pero no escala los datos de entrada para cada característica antes de aplicar la SVD."

#: ../modules/decomposition.rst:101
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_incremental_pca.py`"

#: ../modules/decomposition.rst:107
msgid "PCA using randomized SVD"
msgstr "PCA usando SVD aleatorio"

#: ../modules/decomposition.rst:109
msgid "It is often interesting to project data to a lower-dimensional space that preserves most of the variance, by dropping the singular vector of components associated with lower singular values."
msgstr "A menudo es interesante proyectar los datos a un espacio de menor dimensión que conserve la mayor parte de la varianza, dejando de lado el vector singular de los componentes asociados a los valores singulares más bajos."

#: ../modules/decomposition.rst:113
msgid "For instance, if we work with 64x64 pixel gray-level pictures for face recognition, the dimensionality of the data is 4096 and it is slow to train an RBF support vector machine on such wide data. Furthermore we know that the intrinsic dimensionality of the data is much lower than 4096 since all pictures of human faces look somewhat alike. The samples lie on a manifold of much lower dimension (say around 200 for instance). The PCA algorithm can be used to linearly transform the data while both reducing the dimensionality and preserve most of the explained variance at the same time."
msgstr "Por ejemplo, si trabajamos con imágenes de 64x64 píxeles de escala de grises para el reconocimiento facial, la dimensionalidad de los datos es de 4096 y se hace lento entrenar una máquina de vectores de soporte RBF en datos tan amplios. Además, sabemos que la dimensionalidad intrínseca de los datos es mucho menor que 4096, ya que todas las imágenes de rostros humanos se parecen en cierto modo. Las muestras se sitúan en una matriz de dimensión mucho menor (por ejemplo, alrededor de 200). El algoritmo PCA puede utilizarse para transformar linealmente los datos y, al mismo tiempo, reducir la dimensionalidad y conservar la mayor parte de la varianza explicada."

#: ../modules/decomposition.rst:124
msgid "The class :class:`PCA` used with the optional parameter ``svd_solver='randomized'`` is very useful in that case: since we are going to drop most of the singular vectors it is much more efficient to limit the computation to an approximated estimate of the singular vectors we will keep to actually perform the transform."
msgstr "La clase :class:`PCA` utilizada con el parámetro opcional ``svd_solver='randomized'`` es muy útil en ese caso: como vamos a descartar la mayoría de los vectores singulares, es mucho más eficiente limitar el cálculo a una estimación aproximada de los vectores singulares que mantendremos para realizar realmente la transformación."

#: ../modules/decomposition.rst:130
msgid "For instance, the following shows 16 sample portraits (centered around 0.0) from the Olivetti dataset. On the right hand side are the first 16 singular vectors reshaped as portraits. Since we only require the top 16 singular vectors of a dataset with size :math:`n_{samples} = 400` and :math:`n_{features} = 64 \\times 64 = 4096`, the computation time is less than 1s:"
msgstr "Por ejemplo, a continuación se presentan 16 retratos de muestra (centrados en 0,0) del conjunto de datos Olivetti. En el lado derecho están los primeros 16 vectores singulares transformados en retratos. Dado que sólo necesitamos los 16 primeros vectores singulares de un conjunto de datos con tamaño :math:`n_{samples} = 400` y :math:`n_{features} = 64 \\times 64 = 4096`, el tiempo de cálculo es inferior a 1s:"

#: ../modules/decomposition.rst:146
msgid "orig_img pca_img"
msgstr "orig_img pca_img"

#: ../modules/decomposition.rst:147
msgid "If we note :math:`n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})` and :math:`n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})`, the time complexity of the randomized :class:`PCA` is :math:`O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})` instead of :math:`O(n_{\\max}^2 \\cdot n_{\\min})` for the exact method implemented in :class:`PCA`."
msgstr "Si observamos :math:`n_{\\max} = \\max(n_{\\mathrm{samples}}, n_{\\mathrm{features}})` y :math:`n_{\\min} = \\min(n_{\\mathrm{samples}}, n_{\\mathrm{features}})`, la complejidad temporal del :class:`PCA` aleatoriado es :math:`O(n_{\\max}^2 \\cdot n_{\\mathrm{components}})` en lugar de :math:`O(n_{\\max}^2 \\cdot n_{\\min})` para el método exacto implementado en :class:`PCA`."

#: ../modules/decomposition.rst:153
msgid "The memory footprint of randomized :class:`PCA` is also proportional to :math:`2 \\cdot n_{\\max} \\cdot n_{\\mathrm{components}}` instead of :math:`n_{\\max} \\cdot n_{\\min}` for the exact method."
msgstr "La huella de memoria del :class:`PCA` aleatorio también es proporcional a :math:`2 \\cdot n_{\\max} en lugar de \\cdot n_{\\mathrm{components}} en lugar de :math:`n_{\\max} \\cdot n_{\\min}` para el método exacto."

#: ../modules/decomposition.rst:157
msgid "Note: the implementation of ``inverse_transform`` in :class:`PCA` with ``svd_solver='randomized'`` is not the exact inverse transform of ``transform`` even when ``whiten=False`` (default)."
msgstr "Nota: la implementación de ``inverse_transform`` en :class:`PCA` con ``svd_solver='randomized'`` no es la transformada inversa exacta de ``transform`` incluso cuando ``whiten=False`` (por defecto)."

#: ../modules/decomposition.rst:164
msgid ":ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`"
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_face_recognition.py`"

#: ../modules/decomposition.rst:165 ../modules/decomposition.rst:269
#: ../modules/decomposition.rst:677 ../modules/decomposition.rst:832
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_faces_decomposition.py`"

#: ../modules/decomposition.rst:169
msgid "`\"Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions\" <https://arxiv.org/abs/0909.4061>`_ Halko, et al., 2009"
msgstr "`\"Finding structure with randomness: Stochastic algorithms for constructing approximate matrix decompositions\" <https://arxiv.org/abs/0909.4061>`_ Halko, et al., 2009"

#: ../modules/decomposition.rst:178
msgid "Kernel PCA"
msgstr "Kernel PCA"

#: ../modules/decomposition.rst:180
msgid ":class:`KernelPCA` is an extension of PCA which achieves non-linear dimensionality reduction through the use of kernels (see :ref:`metrics`). It has many applications including denoising, compression and structured prediction (kernel dependency estimation). :class:`KernelPCA` supports both ``transform`` and ``inverse_transform``."
msgstr ":class:`KernelPCA` es una extensión de PCA que obtiene una reducción de la dimensionalidad no lineal mediante el uso de kernels (ver :ref:`metrics`). Tiene muchas aplicaciones, incluyendo la eliminación de ruido, la compresión y la predicción estructurada (estimación de la dependencia del núcleo). :class:`KernelPCA` soporta tanto la ``transform`` como la ``inverse_transform``."

#: ../modules/decomposition.rst:193
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_kernel_pca.py`"

#: ../modules/decomposition.rst:199
msgid "Sparse principal components analysis (SparsePCA and MiniBatchSparsePCA)"
msgstr "Análisis de componentes principales dispersos (SparsePCA y MiniBatchSparsePCA)"

#: ../modules/decomposition.rst:201
msgid ":class:`SparsePCA` is a variant of PCA, with the goal of extracting the set of sparse components that best reconstruct the data."
msgstr ":class:`SparsePCA` es una variante del PCA, cuyo objetivo es extraer el conjunto de componentes dispersos que mejor reconstruyen los datos."

#: ../modules/decomposition.rst:204
msgid "Mini-batch sparse PCA (:class:`MiniBatchSparsePCA`) is a variant of :class:`SparsePCA` that is faster but less accurate. The increased speed is reached by iterating over small chunks of the set of features, for a given number of iterations."
msgstr "El PCA disperso en mini lotes (:class:`MiniBatchSparsePCA`) es una variante de :class:`SparsePCA` que es más rápida pero menos precisa. El aumento de la velocidad se consigue iterando sobre pequeñas porciones del conjunto de características, para un número determinado de iteraciones."

#: ../modules/decomposition.rst:210
msgid "Principal component analysis (:class:`PCA`) has the disadvantage that the components extracted by this method have exclusively dense expressions, i.e. they have non-zero coefficients when expressed as linear combinations of the original variables. This can make interpretation difficult. In many cases, the real underlying components can be more naturally imagined as sparse vectors; for example in face recognition, components might naturally map to parts of faces."
msgstr "El análisis de componentes principales (:class:`PCA`) tiene el inconveniente de que los componentes extraídos por este método tienen expresiones exclusivamente densas, es decir, tienen coeficientes distintos de cero cuando se expresan como combinaciones lineales de las variables originales. Esto puede dificultar su interpretación. En muchos casos, los componentes reales subyacentes pueden pensarse más naturalmente como vectores dispersos; por ejemplo, en el reconocimiento de rostros, los componentes podrían asignarse naturalmente a partes de rostros."

#: ../modules/decomposition.rst:218
msgid "Sparse principal components yields a more parsimonious, interpretable representation, clearly emphasizing which of the original features contribute to the differences between samples."
msgstr "Los componentes principales dispersos producen una representación más sencilla e interpretable, destacando claramente cuáles son las características originales que contribuyen a las diferencias entre las muestras."

#: ../modules/decomposition.rst:222
msgid "The following example illustrates 16 components extracted using sparse PCA from the Olivetti faces dataset.  It can be seen how the regularization term induces many zeros. Furthermore, the natural structure of the data causes the non-zero coefficients to be vertically adjacent. The model does not enforce this mathematically: each component is a vector :math:`h \\in \\mathbf{R}^{4096}`, and there is no notion of vertical adjacency except during the human-friendly visualization as 64x64 pixel images. The fact that the components shown below appear local is the effect of the inherent structure of the data, which makes such local patterns minimize reconstruction error. There exist sparsity-inducing norms that take into account adjacency and different kinds of structure; see [Jen09]_ for a review of such methods. For more details on how to use Sparse PCA, see the Examples section, below."
msgstr "El siguiente ejemplo ilustra 16 componentes extraídos mediante PCA disperso del conjunto de datos de rostros Olivetti.  Se puede observar cómo el término de regularización induce muchos ceros. Además, la estructura natural de los datos hace que los coeficientes no nulos sean verticalmente adyacentes.El modelo no impone esto matemáticamente: cada componente es un vector :math:`h \\in \\mathbf{R}^{4096}`, y no hay ninguna noción de adyacencia vertical excepto durante la visualización amigable para el ser humano como imágenes de 64x64 píxeles. El hecho de que los componentes mostrados a continuación aparezcan locales es el efecto de la estructura inherente de los datos, que hace que tales patrones locales minimicen el error de reconstrucción.  Existen normas que inducen la dispersión que tienen en cuenta la adyacencia y diferentes tipos de estructura; véase [Jen09]_ para una revisión de tales métodos. Para más detalles sobre el uso de Sparse PCA, véase la sección Ejemplos, más abajo."

#: ../modules/decomposition.rst:241
msgid "pca_img spca_img"
msgstr "pca_img spca_img"

#: ../modules/decomposition.rst:242
msgid "Note that there are many different formulations for the Sparse PCA problem. The one implemented here is based on [Mrl09]_ . The optimization problem solved is a PCA problem (dictionary learning) with an :math:`\\ell_1` penalty on the components:"
msgstr "Tenga en cuenta que hay muchas formulaciones diferentes para el problema de Sparse PCA. La implementada aquí se basa en [Mrl09]_ . El problema de optimización que se resuelve es un problema PCA (diccionario de aprendizaje) con una penalidad :math:`\\ell_1` sobre los componentes:"

#: ../modules/decomposition.rst:247
msgid "(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n"
"             ||X-UV||_2^2+\\alpha||V||_1 \\\\\n"
"             \\text{subject to } & ||U_k||_2 = 1 \\text{ for all }\n"
"             0 \\leq k < n_{components}\n\n"
msgstr "(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\, } & \\frac{1}{2}\n"
"             |||X-UV||_2^2+\\alpha||V||_1 \\\\\n"
"             \\text{subject to } & ||U_k||_2 = 1 \\text{ for all }\n"
"             0 \\leq k < n_{components}\n\n"

#: ../modules/decomposition.rst:254
msgid "The sparsity-inducing :math:`\\ell_1` norm also prevents learning components from noise when few training samples are available. The degree of penalization (and thus sparsity) can be adjusted through the hyperparameter ``alpha``. Small values lead to a gently regularized factorization, while larger values shrink many coefficients to zero."
msgstr "La norma :math:`\\ell_1`, que induce la dispersión, también impide el aprendizaje de componentes a partir del ruido cuando se dispone de pocas muestras. El grado de penalidad (y, por tanto, la dispersión) puede ajustarse mediante el hiperparámetro ``alpha``. Los valores pequeños conducen a una factorización ligeramente regularizada, mientras que los valores más grandes reducen muchos coeficientes a cero."

#: ../modules/decomposition.rst:262
msgid "While in the spirit of an online algorithm, the class :class:`MiniBatchSparsePCA` does not implement ``partial_fit`` because the algorithm is online along the features direction, not the samples direction."
msgstr "Aunque se trata de un algoritmo online, la clase :class:`MiniBatchSparsePCA` no implementa ``partial_fit`` porque el algoritmo está en línea en la dirección de las características, no en la dirección de las muestras."

#: ../modules/decomposition.rst:273
msgid "`\"Online Dictionary Learning for Sparse Coding\" <https://www.di.ens.fr/sierra/pdfs/icml09.pdf>`_ J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009"
msgstr "`\"Online Dictionary Learning for Sparse Coding\" <https://www.di.ens.fr/sierra/pdfs/icml09.pdf>`_ J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009"

#: ../modules/decomposition.rst:276
msgid "`\"Structured Sparse Principal Component Analysis\" <https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf>`_ R. Jenatton, G. Obozinski, F. Bach, 2009"
msgstr "`\"Structured Sparse Principal Component Analysis\" <https://www.di.ens.fr/~fbach/sspca_AISTATS2010.pdf>`_ R. Jenatton, G. Obozinski, F. Bach, 2009"

#: ../modules/decomposition.rst:284
msgid "Truncated singular value decomposition and latent semantic analysis"
msgstr "Descomposición de valor singular truncado y análisis semántico latente"

#: ../modules/decomposition.rst:286
msgid ":class:`TruncatedSVD` implements a variant of singular value decomposition (SVD) that only computes the :math:`k` largest singular values, where :math:`k` is a user-specified parameter."
msgstr ":class:`TruncatedSVD` implementa una variante de la descomposición del valor singular (SVD) que sólo calcula los valores singulares más grandes de :math:`k`, donde :math:`k` es un parámetro especificado por el usuario."

#: ../modules/decomposition.rst:290
msgid "When truncated SVD is applied to term-document matrices (as returned by :class:`~sklearn.feature_extraction.text.CountVectorizer` or :class:`~sklearn.feature_extraction.text.TfidfVectorizer`), this transformation is known as `latent semantic analysis <https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>`_ (LSA), because it transforms such matrices to a \"semantic\" space of low dimensionality. In particular, LSA is known to combat the effects of synonymy and polysemy (both of which roughly mean there are multiple meanings per word), which cause term-document matrices to be overly sparse and exhibit poor similarity under measures such as cosine similarity."
msgstr "Cuando se aplica la SVD truncada a las matrices término-documento (como las devueltas por :class:`~sklearn.feature_extraction.text.CountVectorizer` o :class:`~sklearn.feature_extraction.text.TfidfVectorizer`), esta transformación se conoce como `análisis semántico latente <https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>`_ (LSA), porque transforma dichas matrices a un espacio \"semántico\" de baja dimensionalidad. En particular, el LSA es conocido por combatir los efectos de la sinonimia y la polisemia (ambos significan, a grandes rasgos, que hay múltiples significados por palabra), que hacen que las matrices término-documento sean demasiado escasas y muestren una pobre similitud bajo medidas como la similitud del coseno."

#: ../modules/decomposition.rst:303
msgid "LSA is also known as latent semantic indexing, LSI, though strictly that refers to its use in persistent indexes for information retrieval purposes."
msgstr "El LSA también se conoce como indexación semántica latente, LSI, aunque estrictamente se refiere a su uso en índices persistentes con fines de recuperación de información."

#: ../modules/decomposition.rst:307
msgid "Mathematically, truncated SVD applied to training samples :math:`X` produces a low-rank approximation :math:`X`:"
msgstr "Matemáticamente, la SVD truncada aplicada a las muestras de entrenamiento :math:`X` produce una aproximación de bajo rango :math:`X`:"

#: ../modules/decomposition.rst:310
msgid "X \\approx X_k = U_k \\Sigma_k V_k^\\top\n\n"
msgstr "X \\approx X_k = U_k \\Sigma_k V_k^\\top\n\n"

#: ../modules/decomposition.rst:313
msgid "After this operation, :math:`U_k \\Sigma_k^\\top` is the transformed training set with :math:`k` features (called ``n_components`` in the API)."
msgstr "Después de esta operación, :math:`U_k \\Sigma_k^\\top` es el conjunto de entrenamiento transformado con características :math:`k` (llamadas ``n_components`` en la API)."

#: ../modules/decomposition.rst:317
msgid "To also transform a test set :math:`X`, we multiply it with :math:`V_k`:"
msgstr "Para transformar también un conjunto de pruebas :math:`X`, lo multiplicamos por :math:`V_k`:"

#: ../modules/decomposition.rst:319
msgid "X' = X V_k\n\n"
msgstr "X' = X V_k\n\n"

#: ../modules/decomposition.rst:323
msgid "Most treatments of LSA in the natural language processing (NLP) and information retrieval (IR) literature swap the axes of the matrix :math:`X` so that it has shape ``n_features`` × ``n_samples``. We present LSA in a different way that matches the scikit-learn API better, but the singular values found are the same."
msgstr "La mayoría de los tratamientos de LSA en el procesamiento de lenguaje natural (PLN) y la literatura de recuperación de información (RI) intercambian los ejes de la matriz :math:`X` para que tenga forma ``n_features`` × ``n_samples``. Presentamos la LSA de una manera diferente que se ajusta mejor con la API de la scikit-learn, pero los valores singulares encontrados son los mismos."

#: ../modules/decomposition.rst:330
msgid ":class:`TruncatedSVD` is very similar to :class:`PCA`, but differs in that the matrix :math:`X` does not need to be centered. When the columnwise (per-feature) means of :math:`X` are subtracted from the feature values, truncated SVD on the resulting matrix is equivalent to PCA. In practical terms, this means that the :class:`TruncatedSVD` transformer accepts ``scipy.sparse`` matrices without the need to densify them, as densifying may fill up memory even for medium-sized document collections."
msgstr ":class:`TruncatedSVD` es muy similar a :class:`PCA`, pero difiere en que la matriz :math:`X` no necesita estar centrada. Cuando las medias a nivel de columna (por característica) de :math:`X` se restan de los valores de las características, la SVD truncada en la matriz resultante es equivalente a la PCA. En términos prácticos, esto significa que el transformador :class:`TruncatedSVD` acepta matrices ``scipy.sparse`` sin necesidad de densificarlas, ya que la densificación puede llenar la memoria incluso para colecciones de documentos de tamaño medio."

#: ../modules/decomposition.rst:340
msgid "While the :class:`TruncatedSVD` transformer works with any feature matrix, using it on tf–idf matrices is recommended over raw frequency counts in an LSA/document processing setting. In particular, sublinear scaling and inverse document frequency should be turned on (``sublinear_tf=True, use_idf=True``) to bring the feature values closer to a Gaussian distribution, compensating for LSA's erroneous assumptions about textual data."
msgstr "Aunque el transformador :class:`TruncatedSVD` funciona con cualquier matriz de características, se recomienda su uso con matrices tf-idf en lugar de con recuentos de frecuencias en bruto en un entorno de procesamiento de LSA/documentos. En particular, el escalado sublineal y la frecuencia inversa del documento deberían estar activados (``sublinear_tf=True, use_idf=True``) para acercar los valores de las características a una Distribución Gaussiana, compensando las suposiciones erróneas de LSA sobre los datos textuales."

#: ../modules/decomposition.rst:351
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`"

#: ../modules/decomposition.rst:355
msgid "Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), *Introduction to Information Retrieval*, Cambridge University Press, chapter 18: `Matrix decompositions & latent semantic indexing <https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>`_"
msgstr "Christopher D. Manning, Prabhakar Raghavan and Hinrich Schütze (2008), *Introduction to Information Retrieval*, Cambridge University Press, chapter 18: `Matrix decompositions & latent semantic indexing <https://nlp.stanford.edu/IR-book/pdf/18lsi.pdf>`_"

#: ../modules/decomposition.rst:364
msgid "Dictionary Learning"
msgstr "Diccionario de aprrendizaje"

#: ../modules/decomposition.rst:369
msgid "Sparse coding with a precomputed dictionary"
msgstr "Codificación dispersa con un diccionario precalculado"

#: ../modules/decomposition.rst:371
msgid "The :class:`SparseCoder` object is an estimator that can be used to transform signals into sparse linear combination of atoms from a fixed, precomputed dictionary such as a discrete wavelet basis. This object therefore does not implement a ``fit`` method. The transformation amounts to a sparse coding problem: finding a representation of the data as a linear combination of as few dictionary atoms as possible. All variations of dictionary learning implement the following transform methods, controllable via the ``transform_method`` initialization parameter:"
msgstr "El objeto :class:`SparseCoder` es un estimador que puede utilizarse para transformar las señales en una combinación lineal dispersa de átomos a partir de un diccionario fijo precalculado, como una base wavelet discreta. Por lo tanto, este objeto no implementa un método de ``fit``. La transformación equivale a un problema de codificación dispersa: encontrar una representación de los datos como una combinación lineal del menor número posible de átomos del diccionario. Todas las variaciones del diccionario de aprendizaje implementan los siguientes métodos de transformación, controlables a través del parámetro de inicialización ``transform_method``:"

#: ../modules/decomposition.rst:380
msgid "Orthogonal matching pursuit (:ref:`omp`)"
msgstr "Búsqueda de coincidencias ortogonales (:ref:`omp`)"

#: ../modules/decomposition.rst:382
msgid "Least-angle regression (:ref:`least_angle_regression`)"
msgstr "Regresión de ángulo mínimo (:ref:`least_angle_regression`)"

#: ../modules/decomposition.rst:384
msgid "Lasso computed by least-angle regression"
msgstr "Lasso calculado por regresión de ángulo mínimo"

#: ../modules/decomposition.rst:386
msgid "Lasso using coordinate descent (:ref:`lasso`)"
msgstr "Lasso usando coordenadas de descenso (:ref:`lasso`)"

#: ../modules/decomposition.rst:388
msgid "Thresholding"
msgstr "Fijar umbrales"

#: ../modules/decomposition.rst:390
msgid "Thresholding is very fast but it does not yield accurate reconstructions. They have been shown useful in literature for classification tasks. For image reconstruction tasks, orthogonal matching pursuit yields the most accurate, unbiased reconstruction."
msgstr "La fijación de umbrales es muy rápida pero no produce reconstrucciones precisas. Se ha demostrado su utilidad en la literatura para tareas de clasificación. Para las tareas de reconstrucción de imágenes, la búsqueda de coincidencias ortogonales produce la reconstrucción más precisa y no sesgada."

#: ../modules/decomposition.rst:395
msgid "The dictionary learning objects offer, via the ``split_code`` parameter, the possibility to separate the positive and negative values in the results of sparse coding. This is useful when dictionary learning is used for extracting features that will be used for supervised learning, because it allows the learning algorithm to assign different weights to negative loadings of a particular atom, from to the corresponding positive loading."
msgstr "Los objetos del diccionario de aprendizaje ofrecen, a través del parámetro ``split_code``, la posibilidad de separar los valores positivos y negativos en los resultados de la codificación dispersa. Esto es útil cuando el diccionario de aprendizaje se usa para extraer características que se utilizarán para el aprendizaje supervisado, porque permite que el algoritmo de aprendizaje asigne pesos diferentes a las cargas negativas de un átomo en particular, a partir de la carga positiva correspondiente."

#: ../modules/decomposition.rst:402
msgid "The split code for a single sample has length ``2 * n_components`` and is constructed using the following rule: First, the regular code of length ``n_components`` is computed. Then, the first ``n_components`` entries of the ``split_code`` are filled with the positive part of the regular code vector. The second half of the split code is filled with the negative part of the code vector, only with a positive sign. Therefore, the split_code is non-negative."
msgstr "El código dividido para una sola muestra tiene una longitud de ``2 * n_components`` y se construye utilizando la siguiente regla: Primero, se calcula el código regular de longitud ``n_components``. A continuación, las primeras entradas de ``n_components`` del ``split_code`` se rellenan con la parte positiva del vector de código regular. La segunda mitad del código dividido se rellena con la parte negativa del vector de códigos, sólo que con signo positivo. Por lo tanto, el split_code es no negativo."

#: ../modules/decomposition.rst:413
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_sparse_coding.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_sparse_coding.py`"

#: ../modules/decomposition.rst:417
msgid "Generic dictionary learning"
msgstr "Diccionario genérico de aprendizaje"

#: ../modules/decomposition.rst:419
msgid "Dictionary learning (:class:`DictionaryLearning`) is a matrix factorization problem that amounts to finding a (usually overcomplete) dictionary that will perform well at sparsely encoding the fitted data."
msgstr "El diccionario de aprendizaje (:class:`DictionaryLearning`) es un problema de factorización de matrices que consiste en encontrar un diccionario (normalmente sobrecompleto) que funciona bien en la codificación dispersa de los datos ajustados."

#: ../modules/decomposition.rst:423
msgid "Representing data as sparse combinations of atoms from an overcomplete dictionary is suggested to be the way the mammalian primary visual cortex works. Consequently, dictionary learning applied on image patches has been shown to give good results in image processing tasks such as image completion, inpainting and denoising, as well as for supervised recognition tasks."
msgstr "Se sugiere que la representación de los datos como combinaciones dispersas de átomos de un diccionario sobrecompleto es la forma en que funciona la córtex visual primario de los mamíferos. En consecuencia, el diccionario de aprendizaje aplicado a parches de imágenes ha demostrado dar buenos resultados en tareas de procesamiento de imágenes como la finalización, el repintado y la eliminación de ruido, así como en tareas de reconocimiento supervisado."

#: ../modules/decomposition.rst:429
msgid "Dictionary learning is an optimization problem solved by alternatively updating the sparse code, as a solution to multiple Lasso problems, considering the dictionary fixed, and then updating the dictionary to best fit the sparse code."
msgstr "El diccionario de aprendizaje es un problema de optimización que se resuelve actualizando alternativamente el código disperso, como solución a múltiples problemas de Lasso, considerando el diccionario fijo, y luego actualizando el diccionario para que se ajuste mejor al código disperso."

#: ../modules/decomposition.rst:433
msgid "(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n"
"             ||X-UV||_2^2+\\alpha||U||_1 \\\\\n"
"             \\text{subject to } & ||V_k||_2 = 1 \\text{ for all }\n"
"             0 \\leq k < n_{\\mathrm{atoms}}\n\n"
msgstr "(U^*, V^*) = \\underset{U, V}{\\operatorname{arg\\,min\\,}} & \\frac{1}{2}\n"
"             ||X-UV||_2^2+\\alpha||U||_1 \\\\\n"
"             \\text{subject to } & ||V_k||_2 = 1 \\text{ for all }\n"
"             0 \\leq k < n_{\\mathrm{atoms}}\n\n"

#: ../modules/decomposition.rst:450
msgid "pca_img2 dict_img2"
msgstr "pca_img2 dict_img2"

#: ../modules/decomposition.rst:451
msgid "After using such a procedure to fit the dictionary, the transform is simply a sparse coding step that shares the same implementation with all dictionary learning objects (see :ref:`SparseCoder`)."
msgstr "Después de utilizar este procedimiento para ajustar el diccionario, la transformación es simplemente un paso de codificación dispersa que comparte la misma implementación con todos los objetos del diccionario de aprendizaje(véase :ref:`SparseCoder`)."

#: ../modules/decomposition.rst:455
msgid "It is also possible to constrain the dictionary and/or code to be positive to match constraints that may be present in the data. Below are the faces with different positivity constraints applied. Red indicates negative values, blue indicates positive values, and white represents zeros."
msgstr "También es posible restringir el diccionario y/o el código para que sea positivo para que coincida con las restricciones que puedan estar presentes en los datos. A continuación se muestran las caras con diferentes restricciones de positividad aplicadas. El rojo indica valores negativos, el azul indica valores positivos y el blanco representa ceros."

#: ../modules/decomposition.rst:477
msgid "dict_img_pos1 dict_img_pos2"
msgstr "dict_img_pos1 dict_img_pos2"

#: ../modules/decomposition.rst:480
msgid "dict_img_pos3 dict_img_pos4"
msgstr "dict_img_pos3 dict_img_pos4"

#: ../modules/decomposition.rst:481
msgid "The following image shows how a dictionary learned from 4x4 pixel image patches extracted from part of the image of a raccoon face looks like."
msgstr "La siguiente imagen muestra el aspecto de un diccionario obtenido a partir de fragmentos de imagen de 4x4 píxeles extraídos de una zona de la imagen de la cara de un mapache."

#: ../modules/decomposition.rst:493
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_image_denoising.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_image_denoising.py`"

#: ../modules/decomposition.rst:498
msgid "`\"Online dictionary learning for sparse coding\" <https://www.di.ens.fr/sierra/pdfs/icml09.pdf>`_ J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009"
msgstr "`\"Online dictionary learning for sparse coding\" <https://www.di.ens.fr/sierra/pdfs/icml09.pdf>`_ J. Mairal, F. Bach, J. Ponce, G. Sapiro, 2009"

#: ../modules/decomposition.rst:505
msgid "Mini-batch dictionary learning"
msgstr "Diccionario de aprendizaje por mini lotes"

#: ../modules/decomposition.rst:507
msgid ":class:`MiniBatchDictionaryLearning` implements a faster, but less accurate version of the dictionary learning algorithm that is better suited for large datasets."
msgstr ":class:`MiniBatchDictionaryLearning` implementa una versión más rápida, pero menos precisa, del algoritmo de diccionario de aprendizaje que es más adecuado para conjuntos de datos grandes."

#: ../modules/decomposition.rst:511
msgid "By default, :class:`MiniBatchDictionaryLearning` divides the data into mini-batches and optimizes in an online manner by cycling over the mini-batches for the specified number of iterations. However, at the moment it does not implement a stopping condition."
msgstr "Por defecto, :class:`MiniBatchDictionaryLearning` divide los datos en mini lotes y los optimiza de forma directa, recorriendo los mini lotes durante el número de iteraciones especificado. Sin embargo, por el momento no implementa una condición de parada."

#: ../modules/decomposition.rst:516
msgid "The estimator also implements ``partial_fit``, which updates the dictionary by iterating only once over a mini-batch. This can be used for online learning when the data is not readily available from the start, or for when the data does not fit into the memory."
msgstr "El estimador también implementa ``partial_fit``, que actualiza el diccionario iterando sólo una vez sobre un mini lote. Esto puede utilizarse para el aprendizaje en línea cuando los datos no están disponibles desde el principio, o para cuando los datos no caben en la memoria."

msgid "**Clustering for dictionary learning**"
msgstr "**Análisis de conglomerados para el aprendizaje de diccionario**"

#: ../modules/decomposition.rst:530
msgid "Note that when using dictionary learning to extract a representation (e.g. for sparse coding) clustering can be a good proxy to learn the dictionary. For instance the :class:`MiniBatchKMeans` estimator is computationally efficient and implements on-line learning with a ``partial_fit`` method."
msgstr "Tenga en cuenta que cuando se utiliza el aprendizaje de diccionario para extraer una representación (por ejemplo, para la codificación dispersa) el análisis de conglomerados puede ser un buen sustituto para aprender el diccionario. Por ejemplo, el estimador :class:`MiniBatchKMeans` es computacionalmente eficiente e implementa el aprendizaje en línea con un método ``partial_fit``."

#: ../modules/decomposition.rst:536
msgid "Example: :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`"
msgstr "Ejemplo: :ref:`sphx_glr_auto_examples_cluster_plot_dict_face_patches.py`"

#: ../modules/decomposition.rst:543
msgid "Factor Analysis"
msgstr "Análisis de factores"

#: ../modules/decomposition.rst:545
msgid "In unsupervised learning we only have a dataset :math:`X = \\{x_1, x_2, \\dots, x_n \\}`. How can this dataset be described mathematically? A very simple `continuous latent variable` model for :math:`X` is"
msgstr "En el aprendizaje no supervisado sólo tenemos un conjunto de datos :math:`X = \\{x_1, x_2, \\dots, x_n \\}`. ¿Cómo se puede describir matemáticamente este conjunto de datos? Un modelo muy sencillo de `variable latente continua` para :math:`X` es"

#: ../modules/decomposition.rst:549
msgid "x_i = W h_i + \\mu + \\epsilon\n\n"
msgstr "x_i = W h_i + \\mu + \\epsilon\n\n"

#: ../modules/decomposition.rst:551
msgid "The vector :math:`h_i` is called \"latent\" because it is unobserved. :math:`\\epsilon` is considered a noise term distributed according to a Gaussian with mean 0 and covariance :math:`\\Psi` (i.e. :math:`\\epsilon \\sim \\mathcal{N}(0, \\Psi)`), :math:`\\mu` is some arbitrary offset vector. Such a model is called \"generative\" as it describes how :math:`x_i` is generated from :math:`h_i`. If we use all the :math:`x_i`'s as columns to form a matrix :math:`\\mathbf{X}` and all the :math:`h_i`'s as columns of a matrix :math:`\\mathbf{H}` then we can write (with suitably defined :math:`\\mathbf{M}` and :math:`\\mathbf{E}`):"
msgstr "El vector :math:`h_i` se llama \"latente\" porque no se observa. :math:`\\epsilon` se considera un término de ruido distribuido según una Gaussiana con media 0 y covarianza :math:`\\Psi` (es decir, :math:`\\epsilon \\sim \\mathcal{N}(0, \\Psi)`), :math:`\\mu` es algún vector de desplazamiento arbitrario. Este modelo se llama \"generativo\", ya que describe cómo se genera :math:`x_i` a partir de :math:`h_i`. Si utilizamos todas las :math:`x_i` como columnas para formar una matriz :math:`\\mathbf{X}` y todas las :math:`h_i` como columnas de una matriz :math:`\\mathbf{H}` entonces podemos escribir (con :math:`\\mathbf{M}` and :math:`\\mathbf{E}` adecuadamente definidos):"

#: ../modules/decomposition.rst:559
msgid "\\mathbf{X} = W \\mathbf{H} + \\mathbf{M} + \\mathbf{E}\n\n"
msgstr "\\mathbf{X} = W \\mathbf{H} + \\mathbf{M} + \\mathbf{E}\n\n"

#: ../modules/decomposition.rst:562
msgid "In other words, we *decomposed* matrix :math:`\\mathbf{X}`."
msgstr "En otras palabras, hemos *descompuesto* la matriz :math:`\\mathbf{X}`."

#: ../modules/decomposition.rst:564
msgid "If :math:`h_i` is given, the above equation automatically implies the following probabilistic interpretation:"
msgstr "Si se da :math:`h_i`, la ecuación anterior implica automáticamente la siguiente interpretación probabilística:"

#: ../modules/decomposition.rst:567
msgid "p(x_i|h_i) = \\mathcal{N}(Wh_i + \\mu, \\Psi)\n\n"
msgstr "p(x_i|h_i) = \\mathcal{N}(Wh_i + \\mu, \\Psi)\n\n"

#: ../modules/decomposition.rst:569
msgid "For a complete probabilistic model we also need a prior distribution for the latent variable :math:`h`. The most straightforward assumption (based on the nice properties of the Gaussian distribution) is :math:`h \\sim \\mathcal{N}(0, \\mathbf{I})`.  This yields a Gaussian as the marginal distribution of :math:`x`:"
msgstr "Para un modelo probabilístico completo necesitamos también una distribución a priori para la variable latente :math:`h`. La suposición más directa (basada en las buenas propiedades de la distribución Gaussiana) es :math:`h \\sim \\mathcal{N}(0, \\mathbf{I})`.  Esto da como resultado una Gaussiana como distribución marginal de :math:`x`:"

#: ../modules/decomposition.rst:574
msgid "p(x) = \\mathcal{N}(\\mu, WW^T + \\Psi)\n\n"
msgstr "p(x) = \\mathcal{N}(\\mu, WW^T + \\Psi)\n\n"

#: ../modules/decomposition.rst:576
msgid "Now, without any further assumptions the idea of having a latent variable :math:`h` would be superfluous -- :math:`x` can be completely modelled with a mean and a covariance. We need to impose some more specific structure on one of these two parameters. A simple additional assumption regards the structure of the error covariance :math:`\\Psi`:"
msgstr "Ahora, sin ninguna otra suposición, la idea de tener una variable latente :math:`h` sería superflua, ya que :math:`x` puede modelarse completamente con una media y una covarianza. Por ello, necesitamos imponer alguna estructura más específica a uno de estos dos parámetros. Una simple suposición adicional se refiere a la estructura de la covarianza del error :math:`\\Psi`:"

#: ../modules/decomposition.rst:582
msgid ":math:`\\Psi = \\sigma^2 \\mathbf{I}`: This assumption leads to the probabilistic model of :class:`PCA`."
msgstr ":math:`\\Psi = \\sigma^2 \\mathbf{I}`: Esta suposición conduce al modelo probabilístico de :class:`PCA`."

#: ../modules/decomposition.rst:585
msgid ":math:`\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)`: This model is called :class:`FactorAnalysis`, a classical statistical model. The matrix W is sometimes called the \"factor loading matrix\"."
msgstr ":math:`\\Psi = \\mathrm{diag}(\\psi_1, \\psi_2, \\dots, \\psi_n)`: Este modelo se llama \n"
" :class:`FactorAnalysis`, un modelo estadístico clásico. La matriz W se denomina a veces \"matriz de carga de los factores\"."

#: ../modules/decomposition.rst:589
msgid "Both models essentially estimate a Gaussian with a low-rank covariance matrix. Because both models are probabilistic they can be integrated in more complex models, e.g. Mixture of Factor Analysers. One gets very different models (e.g. :class:`FastICA`) if non-Gaussian priors on the latent variables are assumed."
msgstr "Ambos modelos estiman esencialmente una Gaussiana con una matriz de covarianza de bajo rango. Como ambos modelos son probabilísticos, pueden integrarse en modelos más complejos como, por ejemplo, la Mezcla de Analizadores de Factores. Se obtienen modelos muy diferentes (por ejemplo, :class:`FastICA`) si se asumen no Gaussianos a priori en las variables latentes."

#: ../modules/decomposition.rst:594
msgid "Factor analysis *can* produce similar components (the columns of its loading matrix) to :class:`PCA`. However, one can not make any general statements about these components (e.g. whether they are orthogonal):"
msgstr "El análisis factorial *puede* arrojar componentes similares (las columnas de su matriz de carga) al :class:`PCA`. Sin embargo, no se puede hacer ninguna afirmación general sobre estos componentes (por ejemplo, sobre si son ortogonales):"

#: ../modules/decomposition.rst:607
msgid "pca_img3 fa_img3"
msgstr "pca_img3 fa_img3"

#: ../modules/decomposition.rst:608
msgid "The main advantage for Factor Analysis over :class:`PCA` is that it can model the variance in every direction of the input space independently (heteroscedastic noise):"
msgstr "La principal ventaja del Análisis Factorial sobre el :class:`PCA` es que puede modelar la varianza en cada dirección del espacio de entrada de forma independiente (ruido heteroscedástico):"

#: ../modules/decomposition.rst:617
msgid "This allows better model selection than probabilistic PCA in the presence of heteroscedastic noise:"
msgstr "Esto permite una mejor selección del modelo que el PCA probabilístico en presencia de ruido heteroscedástico:"

#: ../modules/decomposition.rst:625
msgid "Factor Analysis is often followed by a rotation of the factors (with the parameter `rotation`), usually to improve interpretability. For example, Varimax rotation maximizes the sum of the variances of the squared loadings, i.e., it tends to produce sparser factors, which are influenced by only a few features each (the \"simple structure\"). See e.g., the first example below."
msgstr "El análisis factorial suele ir seguido de una rotación de los factores (con el parámetro `rotation`), normalmente para mejorar la interpretabilidad. Por ejemplo, la rotación Varimax maximiza la suma de las varianzas de las cargas al cuadrado, es decir, tiende a producir factores más dispersos, en los que sólo influyen unas pocas características cada uno (la \"estructura simple\"). Véase, por ejemplo, el primer caso a continuación."

#: ../modules/decomposition.rst:633
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_varimax_fa.py`"

#: ../modules/decomposition.rst:640
msgid "Independent component analysis (ICA)"
msgstr "Análisis de componentes independientes (ICA)"

#: ../modules/decomposition.rst:642
msgid "Independent component analysis separates a multivariate signal into additive subcomponents that are maximally independent. It is implemented in scikit-learn using the :class:`Fast ICA <FastICA>` algorithm. Typically, ICA is not used for reducing dimensionality but for separating superimposed signals. Since the ICA model does not include a noise term, for the model to be correct, whitening must be applied. This can be done internally using the whiten argument or manually using one of the PCA variants."
msgstr "El análisis de componentes independientes separa una señal multivariante en subcomponentes aditivos que son independientes al máximo. Se implementa en scikit-learn utilizando el algoritmo :class:`Fast ICA <FastICA>`. Normalmente, el ICA no se utiliza para reducir la dimensionalidad, sino para separar las señales superpuestas. Dado que el modelo ICA no incluye un término de ruido, para que el modelo sea correcto se debe aplicar un whitening. Esto puede hacerse internamente usando el argumento whiten o manualmente usando una de las variantes de PCA."

#: ../modules/decomposition.rst:651
msgid "It is classically used to separate mixed signals (a problem known as *blind source separation*), as in the example below:"
msgstr "Se utiliza tradicionalmente para separar señales mixtas (un problema conocido como *separación ciega de fuentes*), como en el ejemplo siguiente:"

#: ../modules/decomposition.rst:660
msgid "ICA can also be used as yet another non linear decomposition that finds components with some sparsity:"
msgstr "El ICA también puede utilizarse como otra descomposición no lineal que encuentra componentes con cierta dispersión:"

#: ../modules/decomposition.rst:672
msgid "pca_img4 ica_img4"
msgstr "pca_img4 ica_img4"

#: ../modules/decomposition.rst:675
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_ica_blind_source_separation.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_ica_blind_source_separation.py`"

#: ../modules/decomposition.rst:676
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_ica_vs_pca.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_ica_vs_pca.py`"

#: ../modules/decomposition.rst:683
msgid "Non-negative matrix factorization (NMF or NNMF)"
msgstr "Factorización matricial no negativa (NMF o NNMF)"

#: ../modules/decomposition.rst:686
msgid "NMF with the Frobenius norm"
msgstr "NMF con la norma de Frobenius"

#: ../modules/decomposition.rst:688
msgid ":class:`NMF` [1]_ is an alternative approach to decomposition that assumes that the data and the components are non-negative. :class:`NMF` can be plugged in instead of :class:`PCA` or its variants, in the cases where the data matrix does not contain negative values. It finds a decomposition of samples :math:`X` into two matrices :math:`W` and :math:`H` of non-negative elements, by optimizing the distance :math:`d` between :math:`X` and the matrix product :math:`WH`. The most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:"
msgstr "La :class:`NMF` [1]_ es un enfoque alternativo a la descomposición que asume que los datos y los componentes son no negativos. La :class:`NMF` puede utilizarse en lugar del :class:`PCA` o sus variantes, en los casos en que la matriz de datos no contenga valores negativos. Encuentra una descomposición de las muestras :math:`X` en dos matrices :math:`W` y :math:`H` de elementos no negativos, optimizando la distancia :math:`d` entre :math:`X` y el producto matricial :math:`WH`. La función de distancia más utilizada es la norma de Frobenius al cuadrado, que es una extensión obvia de la norma euclidiana a las matrices:"

#: ../modules/decomposition.rst:697
msgid "d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{\\mathrm{Fro}}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n\n"
msgstr "d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{\\mathrm{Fro}}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n\n"

#: ../modules/decomposition.rst:700
msgid "Unlike :class:`PCA`, the representation of a vector is obtained in an additive fashion, by superimposing the components, without subtracting. Such additive models are efficient for representing images and text."
msgstr "A diferencia del :class:`PCA`, la representación de un vector se obtiene de forma aditiva, superponiendo los componentes, sin restar. Estos modelos aditivos son eficaces para representar imágenes y textos."

#: ../modules/decomposition.rst:704
msgid "It has been observed in [Hoyer, 2004] [2]_ that, when carefully constrained, :class:`NMF` can produce a parts-based representation of the dataset, resulting in interpretable models. The following example displays 16 sparse components found by :class:`NMF` from the images in the Olivetti faces dataset, in comparison with the PCA eigenfaces."
msgstr "Se ha observado en [Hoyer, 2004] [2]_ que, cuando se restringe cuidadosamente, la :class:`NMF` puede producir una representación basada en partes del conjunto de datos, dando lugar a modelos interpretables. El siguiente ejemplo muestra 16 componentes dispersos encontrados por :class:`NMF` a partir de las imágenes del conjunto de datos de rostros Olivetti, en comparación con las caras propias del PCA."

#: ../modules/decomposition.rst:720
msgid "pca_img5 nmf_img5"
msgstr "pca_img5 nmf_img5"

#: ../modules/decomposition.rst:721
msgid "The :attr:`init` attribute determines the initialization method applied, which has a great impact on the performance of the method. :class:`NMF` implements the method Nonnegative Double Singular Value Decomposition. NNDSVD [4]_ is based on two SVD processes, one approximating the data matrix, the other approximating positive sections of the resulting partial SVD factors utilizing an algebraic property of unit rank matrices. The basic NNDSVD algorithm is better fit for sparse factorization. Its variants NNDSVDa (in which all zeros are set equal to the mean of all elements of the data), and NNDSVDar (in which the zeros are set to random perturbations less than the mean of the data divided by 100) are recommended in the dense case."
msgstr "El atributo :attr:`init` determina el método de inicialización aplicado, que tiene un gran impacto en el rendimiento del método. La :class:`NMF` implementa el método Descomposición del Valor Singular Doble No Negativo. El NNDSVD [4]_ se basa en dos procesos SVD, uno aproximando la matriz de datos, el otro aproximando secciones positivas de los factores parciales SVD resultantes, utilizando una propiedad algebraica de las matrices de rango unitario. El algoritmo básico NNDSVD se adapta mejor a la factorización dispersa. Sus variantes NNDSVDa (en la que todos los ceros se ajustan a la media de todos los elementos de los datos), y NNDSVDar (en la que los ceros se fijan a perturbaciones aleatorias menores que la media de los datos dividida por 100) se recomiendan en el caso denso."

#: ../modules/decomposition.rst:732
msgid "Note that the Multiplicative Update ('mu') solver cannot update zeros present in the initialization, so it leads to poorer results when used jointly with the basic NNDSVD algorithm which introduces a lot of zeros; in this case, NNDSVDa or NNDSVDar should be preferred."
msgstr "Tenga en cuenta que el solucionador de actualización multiplicativa ('mu') no puede actualizar los ceros presentes en la inicialización, por lo que conduce a resultados más pobres cuando se utiliza conjuntamente con el algoritmo básico NNDSVD que introduce muchos ceros; en este caso, se debe preferir NNDSVDa o NNDSVDar."

#: ../modules/decomposition.rst:737
msgid ":class:`NMF` can also be initialized with correctly scaled random non-negative matrices by setting :attr:`init=\"random\"`. An integer seed or a ``RandomState`` can also be passed to :attr:`random_state` to control reproducibility."
msgstr "La :class:`NMF` también puede inicializarse con matrices aleatorias no negativas correctamente escaladas estableciendo :attr:`init=\"random\"`. También se puede pasar una semilla entera o un ``RandomState`` a :attr:`random_state` para controlar la reproducibilidad."

#: ../modules/decomposition.rst:742
msgid "In :class:`NMF`, L1 and L2 priors can be added to the loss function in order to regularize the model. The L2 prior uses the Frobenius norm, while the L1 prior uses an elementwise L1 norm. As in :class:`ElasticNet`, we control the combination of L1 and L2 with the :attr:`l1_ratio` (:math:`\\rho`) parameter, and the intensity of the regularization with the :attr:`alpha` (:math:`\\alpha`) parameter. Then the priors terms are:"
msgstr "En :class:`NMF`, se pueden añadir los valores a priori L1 y L2 a la función de pérdida para regularizar el modelo. El valor a priori L2 utiliza la norma de Frobenius, mientras que el valor a priori L1 utiliza una norma L1 elemental. Como en :class:`ElasticNet`, controlamos la combinación de L1 y L2 con el parámetro :attr:`l1_ratio` (:math:`\\rho`), y la intensidad de la regularización con el parámetro :attr:`alpha` (:math:`\\alpha`). Entonces los términos a priori son:"

#: ../modules/decomposition.rst:749
msgid "\\alpha \\rho ||W||_1 + \\alpha \\rho ||H||_1\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2\n\n"
msgstr "\\alpha \\rho ||W||_1 + \\alpha \\rho ||H||_1\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2\n\n"

#: ../modules/decomposition.rst:754
msgid "and the regularized objective function is:"
msgstr "y la función objetivo regularizada es:"

#: ../modules/decomposition.rst:756
msgid "d_{\\mathrm{Fro}}(X, WH)\n"
"+ \\alpha \\rho ||W||_1 + \\alpha \\rho ||H||_1\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2\n\n"
msgstr "d_{\\mathrm{Fro}}(X, WH)\n"
"+ \\alpha \\rho ||W||_1 + \\alpha \\rho ||H||_1\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||W||_{\\mathrm{Fro}} ^ 2\n"
"+ \\frac{\\alpha(1-\\rho)}{2} ||H||_{\\mathrm{Fro}} ^ 2\n\n"

#: ../modules/decomposition.rst:762
msgid ":class:`NMF` regularizes both W and H by default. The :attr:`regularization` parameter allows for finer control, with which only W, only H, or both can be regularized."
msgstr "La :class:`NMF` regulariza tanto W como H por defecto. El parámetro :attr:`regularization` permite un control más fino, con el que se puede regularizar sólo W, sólo H, o ambos."

#: ../modules/decomposition.rst:767
msgid "NMF with a beta-divergence"
msgstr "NMF con una divergencia beta"

#: ../modules/decomposition.rst:769
msgid "As described previously, the most widely used distance function is the squared Frobenius norm, which is an obvious extension of the Euclidean norm to matrices:"
msgstr "Como se ha descrito anteriormente, la función de distancia más utilizada es la norma de Frobenius al cuadrado, que es una extensión obvia de la norma Euclidiana a las matrices:"

#: ../modules/decomposition.rst:773
msgid "d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n\n"
msgstr "d_{\\mathrm{Fro}}(X, Y) = \\frac{1}{2} ||X - Y||_{Fro}^2 = \\frac{1}{2} \\sum_{i,j} (X_{ij} - {Y}_{ij})^2\n\n"

#: ../modules/decomposition.rst:776
msgid "Other distance functions can be used in NMF as, for example, the (generalized) Kullback-Leibler (KL) divergence, also referred as I-divergence:"
msgstr "Se pueden utilizar otras funciones de distancia en el NMF como, por ejemplo, la divergencia (generalizada) de Kullback-Leibler (KL), también denominada divergencia I:"

#: ../modules/decomposition.rst:779
msgid "d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} \\log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\n\n"
msgstr "d_{KL}(X, Y) = \\sum_{i,j} (X_{ij} \\log(\\frac{X_{ij}}{Y_{ij}}) - X_{ij} + Y_{ij})\n\n"

#: ../modules/decomposition.rst:782
msgid "Or, the Itakura-Saito (IS) divergence:"
msgstr "O la divergencia Itakura-Saito (IS):"

#: ../modules/decomposition.rst:784
msgid "d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - \\log(\\frac{X_{ij}}{Y_{ij}}) - 1)\n\n"
msgstr "d_{IS}(X, Y) = \\sum_{i,j} (\\frac{X_{ij}}{Y_{ij}} - \\log(\\frac{X_{ij}}{Y_{ij}}) - 1)\n\n"

#: ../modules/decomposition.rst:787
msgid "These three distances are special cases of the beta-divergence family, with :math:`\\beta = 2, 1, 0` respectively [6]_. The beta-divergence are defined by :"
msgstr "Estas tres distancias son casos especiales de la familia de las divergencias beta, con :math:`\\beta = 2, 1, 0` respectivamente [6]_. Las divergencias beta se definen por:"

#: ../modules/decomposition.rst:791
msgid "d_{\\beta}(X, Y) = \\sum_{i,j} \\frac{1}{\\beta(\\beta - 1)}(X_{ij}^\\beta + (\\beta-1)Y_{ij}^\\beta - \\beta X_{ij} Y_{ij}^{\\beta - 1})\n\n"
msgstr "d_{\\beta}(X, Y) = \\sum_{i,j} \\frac{1}{\\beta(\\beta - 1)}(X_{ij}^\\beta + (\\beta-1)Y_{ij}^\\beta - \\beta X_{ij} Y_{ij}^{\\beta - 1})\n\n"

#: ../modules/decomposition.rst:799
msgid "Note that this definition is not valid if :math:`\\beta \\in (0; 1)`, yet it can be continuously extended to the definitions of :math:`d_{KL}` and :math:`d_{IS}` respectively."
msgstr "Ten en cuenta que esta definición no es válida si :math:`\\beta \\in (0; 1)`, pero puede extenderse continuamente a las definiciones de :math:`d_{KL}` y :math:`d_{IS}` respectivamente."

#: ../modules/decomposition.rst:803
msgid ":class:`NMF` implements two solvers, using Coordinate Descent ('cd') [5]_, and Multiplicative Update ('mu') [6]_. The 'mu' solver can optimize every beta-divergence, including of course the Frobenius norm (:math:`\\beta=2`), the (generalized) Kullback-Leibler divergence (:math:`\\beta=1`) and the Itakura-Saito divergence (:math:`\\beta=0`). Note that for :math:`\\beta \\in (1; 2)`, the 'mu' solver is significantly faster than for other values of :math:`\\beta`. Note also that with a negative (or 0, i.e. 'itakura-saito') :math:`\\beta`, the input matrix cannot contain zero values."
msgstr "La :class:`NMF` implementa dos solucionadores, utilizando el Descenso de Coordenadas ('cd') [5]_, y la Actualización Multiplicativa ('mu') [6]_. El solucionador 'mu' puede optimizar todas las divergencias beta, incluyendo por supuesto la norma de Frobenius (:math:`\\beta=2`), la divergencia (generalizada) de Kullback-Leibler (:math:`\\beta=1`) y la divergencia de Itakura-Saito (:math:`\\beta=0`). Observa que para :math:`\\beta \\in (1; 2)`, el solucionador 'mu' es significativamente más rápido que para otros valores de :math:`beta`. Observa también que con un valor negativo (o 0, es decir, 'itakura-saito') de :math:`beta`, la matriz de entrada no puede contener valores cero."

#: ../modules/decomposition.rst:812
msgid "The 'cd' solver can only optimize the Frobenius norm. Due to the underlying non-convexity of NMF, the different solvers may converge to different minima, even when optimizing the same distance function."
msgstr "El solucionador 'cd' sólo puede optimizar la norma de Frobenius. Debido a la no-convexidad subyacente de la NMF, los diferentes solucionadores pueden converger en diferentes mínimos, incluso cuando se optimiza la misma función de distancia."

#: ../modules/decomposition.rst:816
msgid "NMF is best used with the ``fit_transform`` method, which returns the matrix W. The matrix H is stored into the fitted model in the ``components_`` attribute; the method ``transform`` will decompose a new matrix X_new based on these stored components::"
msgstr "La NMF se utiliza mejor con el método ``fit_transform``, que devuelve la matriz W. La matriz H se almacena en el modelo ajustado en el atributo ``components_``; el método ``transform`` descompondrá una nueva matriz X_new basada en estos componentes almacenados::"

#: ../modules/decomposition.rst:833 ../modules/decomposition.rst:953
msgid ":ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`"
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`"

#: ../modules/decomposition.rst:834
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_beta_divergence.py`"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_beta_divergence.py`"

#: ../modules/decomposition.rst:838
msgid "`\"Learning the parts of objects by non-negative matrix factorization\" <http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf>`_ D. Lee, S. Seung, 1999"
msgstr "`\"Learning the parts of objects by non-negative matrix factorization\" <http://www.columbia.edu/~jwp2128/Teaching/E4903/papers/nmf_nature.pdf>`_ D. Lee, S. Seung, 1999"

#: ../modules/decomposition.rst:842
msgid "`\"Non-negative Matrix Factorization with Sparseness Constraints\" <http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf>`_ P. Hoyer, 2004"
msgstr "`\"Non-negative Matrix Factorization with Sparseness Constraints\" <http://www.jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf>`_ P. Hoyer, 2004"

#: ../modules/decomposition.rst:846
msgid "`\"SVD based initialization: A head start for nonnegative matrix factorization\" <http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf>`_ C. Boutsidis, E. Gallopoulos, 2008"
msgstr "`\"SVD based initialization: A head start for nonnegative matrix factorization\" <http://scgroup.hpclab.ceid.upatras.gr/faculty/stratis/Papers/HPCLAB020107.pdf>`_ C. Boutsidis, E. Gallopoulos, 2008"

#: ../modules/decomposition.rst:851
msgid "`\"Fast local algorithms for large scale nonnegative matrix and tensor factorizations.\" <http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf>`_ A. Cichocki, A. Phan, 2009"
msgstr "`\"Fast local algorithms for large scale nonnegative matrix and tensor factorizations.\" <http://www.bsp.brain.riken.jp/publications/2009/Cichocki-Phan-IEICE_col.pdf>`_ A. Cichocki, A. Phan, 2009"

#: ../modules/decomposition.rst:856
msgid "`\"Algorithms for nonnegative matrix factorization with the beta-divergence\" <https://arxiv.org/pdf/1010.1763.pdf>`_ C. Fevotte, J. Idier, 2011"
msgstr "`\"Algorithms for nonnegative matrix factorization with the beta-divergence\" <https://arxiv.org/pdf/1010.1763.pdf>`_ C. Fevotte, J. Idier, 2011"

#: ../modules/decomposition.rst:864
msgid "Latent Dirichlet Allocation (LDA)"
msgstr "Asignación Latente de Dirichlet (LDA)"

#: ../modules/decomposition.rst:866
msgid "Latent Dirichlet Allocation is a generative probabilistic model for collections of discrete dataset such as text corpora. It is also a topic model that is used for discovering abstract topics from a collection of documents."
msgstr "La Asignación Latente de Dirichlet es un modelo probabilístico generativo para colecciones de conjuntos de datos discretos, como los corpus de texto. También es un modelo temático que se utiliza para descubrir temas abstractos a partir de una colección de documentos."

#: ../modules/decomposition.rst:870
msgid "The graphical model of LDA is a three-level generative model:"
msgstr "El modelo gráfico de LDA es un modelo generativo de tres niveles:"

#: ../modules/decomposition.rst:875
msgid "Note on notations presented in the graphical model above, which can be found in Hoffman et al. (2013):"
msgstr "Nota sobre las notaciones presentadas en el modelo gráfico anterior, que se puede encontrar en Hoffman et al. (2013):"

#: ../modules/decomposition.rst:878
msgid "The corpus is a collection of :math:`D` documents."
msgstr "El corpus es una colección de documentos :math:`D`."

#: ../modules/decomposition.rst:879
msgid "A document is a sequence of :math:`N` words."
msgstr "Un documento es una secuencia de palabras :math:`N`."

#: ../modules/decomposition.rst:880
msgid "There are :math:`K` topics in the corpus."
msgstr "Hay temas :math:`K` en el corpus."

#: ../modules/decomposition.rst:881
msgid "The boxes represent repeated sampling."
msgstr "Los recuadros representan un muestreo repetido."

#: ../modules/decomposition.rst:883
msgid "In the graphical model, each node is a random variable and has a role in the generative process. A shaded node indicates an observed variable and an unshaded node indicates a hidden (latent) variable. In this case, words in the corpus are the only data that we observe. The latent variables determine the random mixture of topics in the corpus and the distribution of words in the documents. The goal of LDA is to use the observed words to infer the hidden topic structure."
msgstr "En el modelo gráfico, cada nodo es una variable aleatoria y tiene un papel en el proceso generativo. Un nodo sombreado indica una variable observada y un nodo no sombreado indica una variable oculta (latente). En este caso, las palabras del corpus son los únicos datos que observamos. Las variables latentes determinan la mezcla aleatoria de temas en el corpus y la distribución de palabras en los documentos. El objetivo del LDA es utilizar las palabras observadas para inferir la estructura temática oculta."

#: ../modules/decomposition.rst:891
msgid "When modeling text corpora, the model assumes the following generative process for a corpus with :math:`D` documents and :math:`K` topics, with :math:`K` corresponding to :attr:`n_components` in the API:"
msgstr "Cuando se modelan corpus de texto, el modelo asume el siguiente proceso generativo para un corpus con :math:`D` documentos y :math:`K` temas, con :math:`K` correspondiente a :attr:`n_components` en el API:"

#: ../modules/decomposition.rst:895
msgid "For each topic :math:`k \\in K`, draw :math:`\\beta_k \\sim \\mathrm{Dirichlet}(\\eta)`. This provides a distribution over the words, i.e. the probability of a word appearing in topic :math:`k`. :math:`\\eta` corresponds to :attr:`topic_word_prior`."
msgstr "Para cada tema :math:`k \\in K`, dibujar :math:`\\beta_k \\sim \\mathrm{Dirichlet}(\\eta)`. Esto proporciona una distribución sobre las palabras, es decir, la probabilidad de que una palabra aparezca en el tema :math:`k`. :math:`\\eta` corresponde a :attr:`topic_word_prior`."

#: ../modules/decomposition.rst:900
msgid "For each document :math:`d \\in D`, draw the topic proportions :math:`\\theta_d \\sim \\mathrm{Dirichlet}(\\alpha)`. :math:`\\alpha` corresponds to :attr:`doc_topic_prior`."
msgstr "Para cada documento :math:`d \\in D`, dibujar las proporciones del tema :math:`\\theta_d \\sim \\mathrm{Dirichlet}(\\alpha)`. :math:`\\alpha` corresponde a :attr:`doc_topic_prior`."

#: ../modules/decomposition.rst:904
msgid "For each word :math:`i` in document :math:`d`:"
msgstr "Para cada palabra :math:`i` en el documento :math:`d`:"

#: ../modules/decomposition.rst:906
msgid "Draw the topic assignment :math:`z_{di} \\sim \\mathrm{Multinomial} (\\theta_d)`"
msgstr "Dibuja la asignación del tema :math:`z_{di} \\sim \\mathrm{Multinomial} (\\theta_d)`"

#: ../modules/decomposition.rst:908
msgid "Draw the observed word :math:`w_{ij} \\sim \\mathrm{Multinomial} (\\beta_{z_{di}})`"
msgstr "Dibuja la palabra observada :math:`w_{ij} \\sim \\mathrm{Multinomial} (\\beta_{z_{di}})`"

#: ../modules/decomposition.rst:911
msgid "For parameter estimation, the posterior distribution is:"
msgstr "Para la estimación de los parámetros, la distribución posterior es:"

#: ../modules/decomposition.rst:913
msgid "p(z, \\theta, \\beta |w, \\alpha, \\eta) =\n"
"  \\frac{p(z, \\theta, \\beta|\\alpha, \\eta)}{p(w|\\alpha, \\eta)}\n\n"
msgstr "p(z, \\theta, \\beta |w, \\alpha, \\eta) =\n"
"  \\frac{p(z, \\theta, \\beta|\\alpha, \\eta)}{p(w|\\alpha, \\eta)}\n\n"

#: ../modules/decomposition.rst:917
msgid "Since the posterior is intractable, variational Bayesian method uses a simpler distribution :math:`q(z,\\theta,\\beta | \\lambda, \\phi, \\gamma)` to approximate it, and those variational parameters :math:`\\lambda`, :math:`\\phi`, :math:`\\gamma` are optimized to maximize the Evidence Lower Bound (ELBO):"
msgstr "Como la posterior es inabordable, el método bayesiano variacional utiliza una distribución más simple :math:`q(z,\\theta,\\beta | \\lambda, \\phi, \\gamma)` para aproximarla, y esos parámetros variacionales :math:`lambda`, :math:`\\phi`, :math:`gamma` se optimizan para maximizar el Límite Inferior de Evidencia (ELBO):"

#: ../modules/decomposition.rst:923
msgid "\\log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}\n"
"  E_{q}[\\log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[\\log\\:q(z, \\theta, \\beta)]\n\n"
msgstr "\\log\\: P(w | \\alpha, \\eta) \\geq L(w,\\phi,\\gamma,\\lambda) \\overset{\\triangle}{=}\n"
"  E_{q}[\\log\\:p(w,z,\\theta,\\beta|\\alpha,\\eta)] - E_{q}[\\log\\:q(z, \\theta, \\beta)]\n\n"

#: ../modules/decomposition.rst:927
msgid "Maximizing ELBO is equivalent to minimizing the Kullback-Leibler(KL) divergence between :math:`q(z,\\theta,\\beta)` and the true posterior :math:`p(z, \\theta, \\beta |w, \\alpha, \\eta)`."
msgstr "Maximizar el ELBO es equivalente a minimizar la divergencia de Kullback-Leibler (KL) entre :math:`q(z,\\theta,\\beta)` y la verdadera posterior :math:`p(z, \\theta, \\beta |w, \\alpha, \\eta)`."

#: ../modules/decomposition.rst:931
msgid ":class:`LatentDirichletAllocation` implements the online variational Bayes algorithm and supports both online and batch update methods. While the batch method updates variational variables after each full pass through the data, the online method updates variational variables from mini-batch data points."
msgstr ":class:`LatentDirichletAllocation` implementa el algoritmo Bayes variacional en línea y admite métodos de actualización en línea y por lotes. Mientras que el método por lotes actualiza las variables variacionales después de cada paso completo por los datos, el método en línea actualiza las variables variacionales a partir de puntos de datos de mini lotes."

#: ../modules/decomposition.rst:939
msgid "Although the online method is guaranteed to converge to a local optimum point, the quality of the optimum point and the speed of convergence may depend on mini-batch size and attributes related to learning rate setting."
msgstr "Aunque se garantiza que el método en línea converge a un punto óptimo local, la calidad del punto óptimo y la velocidad de convergencia pueden depender del tamaño del mini lote y de los atributos relacionados con la configuración de la tasa de aprendizaje."

#: ../modules/decomposition.rst:943
msgid "When :class:`LatentDirichletAllocation` is applied on a \"document-term\" matrix, the matrix will be decomposed into a \"topic-term\" matrix and a \"document-topic\" matrix. While \"topic-term\" matrix is stored as :attr:`components_` in the model, \"document-topic\" matrix can be calculated from ``transform`` method."
msgstr "Cuando se aplica :class:`LatentDirichletAllocation` a una matriz \"documento-término\", la matriz se descompone en una matriz \"tema-término\" y una matriz \"documento-tema\". Mientras que la matriz \"tema-término\" se almacena como :attr:`components_` en el modelo, la matriz \"documento-tema\" puede calcularse a partir del método ``transform``."

#: ../modules/decomposition.rst:948
msgid ":class:`LatentDirichletAllocation` also implements ``partial_fit`` method. This is used when data can be fetched sequentially."
msgstr ":class:`LatentDirichletAllocation` también implementa el método ``partial_fit``. Se utiliza cuando los datos pueden obtenerse de forma secuencial."

#: ../modules/decomposition.rst:957
msgid "`\"Latent Dirichlet Allocation\" <http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>`_ D. Blei, A. Ng, M. Jordan, 2003"
msgstr "`\"Latent Dirichlet Allocation\" <http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf>`_ D. Blei, A. Ng, M. Jordan, 2003"

#: ../modules/decomposition.rst:961
msgid "`\"Online Learning for Latent Dirichlet Allocation” <https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf>`_ M. Hoffman, D. Blei, F. Bach, 2010"
msgstr "`\"Online Learning for Latent Dirichlet Allocation” <https://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf>`_ M. Hoffman, D. Blei, F. Bach, 2010"

#: ../modules/decomposition.rst:965
msgid "`\"Stochastic Variational Inference\" <http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf>`_ M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013"
msgstr "`\"Stochastic Variational Inference\" <http://www.columbia.edu/~jwp2128/Papers/HoffmanBleiWangPaisley2013.pdf>`_ M. Hoffman, D. Blei, C. Wang, J. Paisley, 2013"

#: ../modules/decomposition.rst:969
#, python-format
msgid "`\"The varimax criterion for analytic rotation in factor analysis\" <https://link.springer.com/article/10.1007%2FBF02289233>`_ H. F. Kaiser, 1958"
msgstr "`\"The varimax criterion for analytic rotation in factor analysis\" <https://link.springer.com/article/10.1007%2FBF02289233>`_ H. F. Kaiser, 1958"

#: ../modules/decomposition.rst:973
msgid "See also :ref:`nca_dim_reduction` for dimensionality reduction with Neighborhood Components Analysis."
msgstr "Revisa también :ref:`nca_dim_reduction` para la reducción de la dimensionalidad con el Análisis de Componentes de Vecindad."

