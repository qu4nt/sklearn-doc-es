msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-30 04:17\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.po\n"
"X-Crowdin-File-ID: 5002\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.rst:2
msgid ":mod:`sklearn.feature_extraction.text`.HashingVectorizer"
msgstr ":mod:`sklearn.feature_extraction.text`.HashingVectorizer"

#: of sklearn.feature_extraction.text.HashingVectorizer:2
msgid "Convert a collection of text documents to a matrix of token occurrences"
msgstr "Convertir una colección de documentos de texto en una matriz de ocurrencias de tokens"

#: of sklearn.feature_extraction.text.HashingVectorizer:4
msgid "It turns a collection of text documents into a scipy.sparse matrix holding token occurrence counts (or binary occurrence information), possibly normalized as token frequencies if norm='l1' or projected on the euclidean unit sphere if norm='l2'."
msgstr "Convierte una colección de documentos de texto en una matriz scipy.sparse que contiene recuentos de ocurrencia de tokens (o información de ocurrencia binaria), posiblemente normalizada como frecuencias de tokens si norm='l1' o proyectada en la esfera unitaria euclidiana si norm='l2'."

#: of sklearn.feature_extraction.text.HashingVectorizer:9
msgid "This text vectorizer implementation uses the hashing trick to find the token string name to feature integer index mapping."
msgstr "Esta implementación del vectorizador de texto utiliza el truco del hashing para encontrar el nombre de la cadena del token para el mapeo de índices enteros de características."

#: of sklearn.feature_extraction.text.HashingVectorizer:12
msgid "This strategy has several advantages:"
msgstr "Esta estrategia tiene varias ventajas:"

#: of sklearn.feature_extraction.text.HashingVectorizer:14
msgid "it is very low memory scalable to large datasets as there is no need to store a vocabulary dictionary in memory"
msgstr "es muy poco escalable en memoria para grandes conjuntos de datos, ya que no es necesario almacenar un diccionario de vocabulario en memoria"

#: of sklearn.feature_extraction.text.HashingVectorizer:17
msgid "it is fast to pickle and un-pickle as it holds no state besides the constructor parameters"
msgstr "es rápido para hacer y deshacer el pickle ya que no mantiene ningún estado aparte de los parámetros del constructor"

#: of sklearn.feature_extraction.text.HashingVectorizer:20
msgid "it can be used in a streaming (partial fit) or parallel pipeline as there is no state computed during fit."
msgstr "se puede utilizar en una transmisión (ajuste parcial) o en un pipeline paralelo, ya que no se calcula el estado durante el ajuste."

#: of sklearn.feature_extraction.text.HashingVectorizer:23
msgid "There are also a couple of cons (vs using a CountVectorizer with an in-memory vocabulary):"
msgstr "También hay un par de contras (en comparación con el uso de CountVectorizer con un vocabulario en memoria):"

#: of sklearn.feature_extraction.text.HashingVectorizer:26
msgid "there is no way to compute the inverse transform (from feature indices to string feature names) which can be a problem when trying to introspect which features are most important to a model."
msgstr "no hay forma de calcular la transformación inversa (desde los índices de características hasta los nombres de características de cadena), lo que puede ser un problema cuando se trata de introspeccionar qué características son más importantes para un modelo."

#: of sklearn.feature_extraction.text.HashingVectorizer:30
msgid "there can be collisions: distinct tokens can be mapped to the same feature index. However in practice this is rarely an issue if n_features is large enough (e.g. 2 ** 18 for text classification problems)."
msgstr "puede haber colisiones: se pueden mapear distintos tokens al mismo índice de características. Sin embargo, en la práctica, esto rara vez suele ser un problema si n_features es lo suficientemente grande (por ejemplo, 2 ** 18 para problemas de clasificación de texto)."

#: of sklearn.feature_extraction.text.HashingVectorizer:34
msgid "no IDF weighting as this would render the transformer stateful."
msgstr "sin ponderación de la IDF, ya que esto haría que el transformador tuviera estado."

#: of sklearn.feature_extraction.text.HashingVectorizer:36
msgid "The hash function employed is the signed 32-bit version of Murmurhash3."
msgstr "La función hash empleada es la versión firmada de 32 bits de Murmurhash3."

#: of sklearn.feature_extraction.text.HashingVectorizer:38
msgid "Read more in the :ref:`User Guide <text_feature_extraction>`."
msgstr "Lee más en el :ref:`Manual de usuario <text_feature_extraction>`."

#: of sklearn.base.BaseEstimator.get_params
#: sklearn.base.BaseEstimator.set_params
#: sklearn.feature_extraction.text.HashingVectorizer
#: sklearn.feature_extraction.text.HashingVectorizer.fit
#: sklearn.feature_extraction.text.HashingVectorizer.fit_transform
#: sklearn.feature_extraction.text.HashingVectorizer.partial_fit
#: sklearn.feature_extraction.text.HashingVectorizer.transform
#: sklearn.feature_extraction.text._VectorizerMixin.decode
msgid "Parameters"
msgstr "Parámetros"

#: of sklearn.feature_extraction.text.HashingVectorizer:51
msgid "**input**"
msgstr "**input**"

#: of
msgid "string {'filename', 'file', 'content'}, default='content'"
msgstr "string {'filename', 'file', 'content'}, default='content'"

#: of sklearn.feature_extraction.text.HashingVectorizer:43
msgid "If 'filename', the sequence passed as an argument to fit is expected to be a list of filenames that need reading to fetch the raw content to analyze."
msgstr "Si es 'filename', se espera que la secuencia pasada como argumento a fit sea una lista de nombres de archivos que necesitan ser leídos para obtener el contenido en bruto a analizar."

#: of sklearn.feature_extraction.text.HashingVectorizer:47
msgid "If 'file', the sequence items must have a 'read' method (file-like object) that is called to fetch the bytes in memory."
msgstr "Si es 'file', los elementos de la secuencia deben tener un método 'read' de lectura (objeto file-like) que se invoca para obtener los bytes en la memoria."

#: of sklearn.feature_extraction.text.HashingVectorizer:50
msgid "Otherwise the input is expected to be a sequence of items that can be of type string or byte."
msgstr "En caso contrario, se espera que la entrada sea una secuencia de elementos que pueden ser de tipo cadena (string) o byte."

#: of sklearn.feature_extraction.text.HashingVectorizer:55
msgid "**encoding**"
msgstr "**encoding**"

#: of
msgid "string, default='utf-8'"
msgstr "string, default='utf-8'"

#: of sklearn.feature_extraction.text.HashingVectorizer:54
msgid "If bytes or files are given to analyze, this encoding is used to decode."
msgstr "Si se dan bytes o archivos para analizar, se utiliza esta codificación para decodificar."

#: of sklearn.feature_extraction.text.HashingVectorizer:61
msgid "**decode_error**"
msgstr "**decode_error**"

#: of
msgid "{'strict', 'ignore', 'replace'}, default='strict'"
msgstr "{'strict', 'ignore', 'replace'}, default='strict'"

#: of sklearn.feature_extraction.text.HashingVectorizer:58
msgid "Instruction on what to do if a byte sequence is given to analyze that contains characters not of the given `encoding`. By default, it is 'strict', meaning that a UnicodeDecodeError will be raised. Other values are 'ignore' and 'replace'."
msgstr "Instrucción sobre qué hacer si se proporciona una secuencia de bytes para analizar que contiene caracteres que no pertencen al `encoding` (codificación) dado. Por defecto, es 'strict', lo que significa que se producirá un UnicodeDecodeError. Otros valores son 'ignore' y 'replace'."

#: of sklearn.feature_extraction.text.HashingVectorizer:72
msgid "**strip_accents**"
msgstr "**strip_accents**"

#: of
msgid "{'ascii', 'unicode'}, default=None"
msgstr "{'ascii', 'unicode'}, default=None"

#: of sklearn.feature_extraction.text.HashingVectorizer:64
msgid "Remove accents and perform other character normalization during the preprocessing step. 'ascii' is a fast method that only works on characters that have an direct ASCII mapping. 'unicode' is a slightly slower method that works on any characters. None (default) does nothing."
msgstr "Elimina acentos y realiza otra normalización de caracteres durante el paso de preprocesamiento. 'ascii' es un método rápido que sólo funciona sobre caracteres que tienen un mapeo ASCII directo. 'unicode' es un método ligeramente más lento que funciona en cualquier carácter. None (por defecto) no hace nada."

#: of sklearn.feature_extraction.text.HashingVectorizer:71
msgid "Both 'ascii' and 'unicode' use NFKD normalization from :func:`unicodedata.normalize`."
msgstr "Tanto 'ascii' como 'unicode' utilizan la normalización NFKD de :func:`unicodedata.normalize`."

#: of sklearn.feature_extraction.text.HashingVectorizer:75
msgid "**lowercase**"
msgstr "**lowercase**"

#: of
msgid "bool, default=True"
msgstr "bool, default=True"

#: of sklearn.feature_extraction.text.HashingVectorizer:75
msgid "Convert all characters to lowercase before tokenizing."
msgstr "Convierte todos los caracteres en minúsculas antes de la tokenización."

#: of sklearn.feature_extraction.text.HashingVectorizer:80
msgid "**preprocessor**"
msgstr "**preprocessor**"

#: of
msgid "callable, default=None"
msgstr "callable, default=None"

#: of sklearn.feature_extraction.text.HashingVectorizer:78
msgid "Override the preprocessing (string transformation) stage while preserving the tokenizing and n-grams generation steps. Only applies if ``analyzer is not callable``."
msgstr "Anula la etapa de preprocesamiento (transformación de cadenas) conservando las etapas de tokenización y generación de n-gramas. Sólo se aplica si ``analyzer is not callable``."

#: of sklearn.feature_extraction.text.HashingVectorizer:85
msgid "**tokenizer**"
msgstr "**tokenizer**"

#: of sklearn.feature_extraction.text.HashingVectorizer:83
msgid "Override the string tokenization step while preserving the preprocessing and n-grams generation steps. Only applies if ``analyzer == 'word'``."
msgstr "Anula el paso de tokenización de cadenas conservando los pasos de preprocesamiento y generación de n-gramas. Sólo se aplica si ``analyzer == 'word'``."

#: of sklearn.feature_extraction.text.HashingVectorizer:94
msgid "**stop_words**"
msgstr "**stop_words**"

#: of
msgid "string {'english'}, list, default=None"
msgstr "string {'english'}, list, default=None"

#: of sklearn.feature_extraction.text.HashingVectorizer:88
msgid "If 'english', a built-in stop word list for English is used. There are several known issues with 'english' and you should consider an alternative (see :ref:`stop_words`)."
msgstr "Si es 'english', se utiliza una lista de palabras funcionales incorporada para el inglés. Hay varias incidencias conocidas con 'english' y deberías considerar una alternativa (ver :ref:`stop_words`)."

#: of sklearn.feature_extraction.text.HashingVectorizer:92
msgid "If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens. Only applies if ``analyzer == 'word'``."
msgstr "Si es una lista (list), se asume que dicha lista contiene palabras funcionales, las cuales serán eliminadas de los tokens resultantes. Sólo se aplica si ``analyzer == 'word'``."

#: of sklearn.feature_extraction.text.HashingVectorizer:104
msgid "**token_pattern** : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\""
msgstr "**token_pattern** : str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\""

#: of
msgid "str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\""
msgstr "str, default=r\"(?u)\\\\b\\\\w\\\\w+\\\\b\""

#: of sklearn.feature_extraction.text.HashingVectorizer:97
msgid "Regular expression denoting what constitutes a \"token\", only used if ``analyzer == 'word'``. The default regexp selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator)."
msgstr "Expresión regular que denota lo que constituye un \"token\", sólo se utiliza si ``analyzer == 'word'``. La regexp por defecto selecciona tokens de 2 o más caracteres alfanuméricos (la puntuación se ignora por completo y se trata siempre como un separador de tokens)."

#: of sklearn.feature_extraction.text.HashingVectorizer:102
msgid "If there is a capturing group in token_pattern then the captured group content, not the entire match, becomes the token. At most one capturing group is permitted."
msgstr "Si hay un grupo de captura en token_pattern entonces el contenido del grupo capturado, no la correspondencia completa, se convierte en el token. Se permite como máximo un grupo de captura."

#: of sklearn.feature_extraction.text.HashingVectorizer:112
msgid "**ngram_range**"
msgstr "**ngram_range**"

#: of
msgid "tuple (min_n, max_n), default=(1, 1)"
msgstr "tuple (min_n, max_n), default=(1, 1)"

#: of sklearn.feature_extraction.text.HashingVectorizer:107
msgid "The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that min_n <= n <= max_n will be used. For example an ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means unigrams and bigrams, and ``(2, 2)`` means only bigrams. Only applies if ``analyzer is not callable``."
msgstr "El límite inferior y superior del rango de n-valores para los diferentes n-gramas que se van a extraer. Se utilizarán todos los valores de n tales que min_n <= n <= max_n. Por ejemplo, un ``ngram_range`` de ``(1, 1)`` significa sólo unigramas, ``(1, 2)`` significa unigramas y bigramas, y ``(2, 2)`` significa sólo bigramas. Sólo se aplica si ``analyzer is not callable``."

#: of sklearn.feature_extraction.text.HashingVectorizer:126
msgid "**analyzer**"
msgstr "**analyzer**"

#: of
msgid "{'word', 'char', 'char_wb'} or callable, default='word'"
msgstr "{'word', 'char', 'char_wb'} o callable, default='word'"

#: of sklearn.feature_extraction.text.HashingVectorizer:115
msgid "Whether the feature should be made of word or character n-grams. Option 'char_wb' creates character n-grams only from text inside word boundaries; n-grams at the edges of words are padded with space."
msgstr "Si la característica debe estar hecha de n-gramas de palabras o de n-gramas de caracteres. La opción 'char_wb' crea n-gramas de caracteres sólo a partir del texto dentro de los límites de las palabras; los n-gramas en los bordes de las palabras se rellenan con espacio."

#: of sklearn.feature_extraction.text.HashingVectorizer:119
msgid "If a callable is passed it is used to extract the sequence of features out of the raw, unprocessed input."
msgstr "Si se pasa un invocable, se utiliza para extraer la secuencia de características de la entrada en bruto y sin procesar."

#: of sklearn.feature_extraction.text.HashingVectorizer:124
msgid "Since v0.21, if ``input`` is ``filename`` or ``file``, the data is first read from the file and then passed to the given callable analyzer."
msgstr "Desde la v0.21, si ``input`` es ``filename`` o ``file``, los datos se leen primero del archivo y luego se pasan al analizador invocable dado."

#: of sklearn.feature_extraction.text.HashingVectorizer:131
msgid "**n_features**"
msgstr "**n_features**"

#: of
msgid "int, default=(2 ** 20)"
msgstr "int, default=(2 ** 20)"

#: of sklearn.feature_extraction.text.HashingVectorizer:129
msgid "The number of features (columns) in the output matrices. Small numbers of features are likely to cause hash collisions, but large numbers will cause larger coefficient dimensions in linear learners."
msgstr "El número de características (columnas) en las matrices de salida. Es probable que un número pequeño de características provoque colisiones de hash, pero los números grandes causarán dimensiones de coeficiente más grandes en los aprendices lineales."

#: of sklearn.feature_extraction.text.HashingVectorizer:136
msgid "**binary**"
msgstr "**binary**"

#: of
msgid "bool, default=False."
msgstr "bool, default=False."

#: of sklearn.feature_extraction.text.HashingVectorizer:134
msgid "If True, all non zero counts are set to 1. This is useful for discrete probabilistic models that model binary events rather than integer counts."
msgstr "Si es True, todos los contadores distintos de cero se establecen en 1. Esto es útil para modelos probabilísticos discretos que modelan eventos binarios en lugar de conteos enteros."

#: of sklearn.feature_extraction.text.HashingVectorizer:139
msgid "**norm**"
msgstr "**norm**"

#: of
msgid "{'l1', 'l2'}, default='l2'"
msgstr "{'l1', 'l2'}, default='l2'"

#: of sklearn.feature_extraction.text.HashingVectorizer:139
msgid "Norm used to normalize term vectors. None for no normalization."
msgstr "Norma utilizada para normalizar vectores de términos. None para no normalizar."

#: of sklearn.feature_extraction.text.HashingVectorizer:146
msgid "**alternate_sign**"
msgstr "**alternate_sign**"

#: of sklearn.feature_extraction.text.HashingVectorizer:142
msgid "When True, an alternating sign is added to the features as to approximately conserve the inner product in the hashed space even for small n_features. This approach is similar to sparse random projection."
msgstr "Cuando es True, se añade un signo alternativo a las características para conservar aproximadamente el producto interno en el espacio de hash, incluso para n_features pequeñas. Este enfoque es similar a la proyección aleatoria dispersa."

#: of sklearn.feature_extraction.text.HashingVectorizer:156
msgid "**dtype**"
msgstr "**dtype**"

#: of
msgid "type, default=np.float64"
msgstr "type, default=np.float64"

#: of sklearn.feature_extraction.text.HashingVectorizer:149
msgid "Type of the matrix returned by fit_transform() or transform()."
msgstr "Tipo de la matriz devuelta por fit_transform() o transform()."

#: of sklearn.feature_extraction.text.HashingVectorizer:161
msgid ":obj:`CountVectorizer`, :obj:`TfidfVectorizer`"
msgstr ":obj:`CountVectorizer`, :obj:`TfidfVectorizer`"

#: of sklearn.feature_extraction.text.HashingVectorizer:167
msgid "Examples"
msgstr "Ejemplos"

#: of sklearn.feature_extraction.text.HashingVectorizer:181
msgid "Methods"
msgstr "Métodos"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`build_analyzer <sklearn.feature_extraction.text.HashingVectorizer.build_analyzer>`\\"
msgstr ":obj:`build_analyzer <sklearn.feature_extraction.text.HashingVectorizer.build_analyzer>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
#: sklearn.feature_extraction.text._VectorizerMixin.build_analyzer:2
msgid "Return a callable that handles preprocessing, tokenization and n-grams generation."
msgstr "Devuelve un invocable que maneja el preprocesamiento, tokenización y la generación de n-gramas."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`build_preprocessor <sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor>`\\"
msgstr ":obj:`build_preprocessor <sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
#: sklearn.feature_extraction.text._VectorizerMixin.build_preprocessor:2
msgid "Return a function to preprocess the text before tokenization."
msgstr "Devuelve una función para preprocesar el texto antes de la tokenización."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`build_tokenizer <sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer>`\\"
msgstr ":obj:`build_tokenizer <sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
#: sklearn.feature_extraction.text._VectorizerMixin.build_tokenizer:2
msgid "Return a function that splits a string into a sequence of tokens."
msgstr "Devuelve una función que divide una cadena en una secuencia de tokens."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`decode <sklearn.feature_extraction.text.HashingVectorizer.decode>`\\"
msgstr ":obj:`decode <sklearn.feature_extraction.text.HashingVectorizer.decode>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
#: sklearn.feature_extraction.text._VectorizerMixin.decode:2
msgid "Decode the input into a string of unicode symbols."
msgstr "Decodifica la entrada en una cadena de símbolos Unicode."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`fit <sklearn.feature_extraction.text.HashingVectorizer.fit>`\\"
msgstr ":obj:`fit <sklearn.feature_extraction.text.HashingVectorizer.fit>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer.fit:2
#: sklearn.feature_extraction.text.HashingVectorizer.partial_fit:2
#: sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid "Does nothing: this transformer is stateless."
msgstr "No hace nada: este transformador no tiene estado."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`fit_transform <sklearn.feature_extraction.text.HashingVectorizer.fit_transform>`\\"
msgstr ":obj:`fit_transform <sklearn.feature_extraction.text.HashingVectorizer.fit_transform>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer.fit_transform:2
#: sklearn.feature_extraction.text.HashingVectorizer.transform:2
#: sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid "Transform a sequence of documents to a document-term matrix."
msgstr "Transforma una secuencia de documentos en una matriz documento-término (document-term)."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`get_params <sklearn.feature_extraction.text.HashingVectorizer.get_params>`\\"
msgstr ":obj:`get_params <sklearn.feature_extraction.text.HashingVectorizer.get_params>`\\"

#: of sklearn.base.BaseEstimator.get_params:2
#: sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid "Get parameters for this estimator."
msgstr "Obtiene los parámetros para este estimador."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`get_stop_words <sklearn.feature_extraction.text.HashingVectorizer.get_stop_words>`\\"
msgstr ":obj:`get_stop_words <sklearn.feature_extraction.text.HashingVectorizer.get_stop_words>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
#: sklearn.feature_extraction.text._VectorizerMixin.get_stop_words:2
msgid "Build or fetch the effective stop words list."
msgstr "Construye o busca la lista efectiva de palabras funcionales."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`partial_fit <sklearn.feature_extraction.text.HashingVectorizer.partial_fit>`\\"
msgstr ":obj:`partial_fit <sklearn.feature_extraction.text.HashingVectorizer.partial_fit>`\\"

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`set_params <sklearn.feature_extraction.text.HashingVectorizer.set_params>`\\"
msgstr ":obj:`set_params <sklearn.feature_extraction.text.HashingVectorizer.set_params>`\\"

#: of sklearn.base.BaseEstimator.set_params:2
#: sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid "Set the parameters of this estimator."
msgstr "Establece los parámetros de este estimador."

#: of sklearn.feature_extraction.text.HashingVectorizer:195:<autosummary>:1
msgid ":obj:`transform <sklearn.feature_extraction.text.HashingVectorizer.transform>`\\"
msgstr ":obj:`transform <sklearn.feature_extraction.text.HashingVectorizer.transform>`\\"

#: of sklearn.base.BaseEstimator.get_params
#: sklearn.base.BaseEstimator.set_params
#: sklearn.feature_extraction.text.HashingVectorizer.fit_transform
#: sklearn.feature_extraction.text.HashingVectorizer.transform
#: sklearn.feature_extraction.text._VectorizerMixin.build_analyzer
#: sklearn.feature_extraction.text._VectorizerMixin.build_preprocessor
#: sklearn.feature_extraction.text._VectorizerMixin.build_tokenizer
#: sklearn.feature_extraction.text._VectorizerMixin.decode
#: sklearn.feature_extraction.text._VectorizerMixin.get_stop_words
msgid "Returns"
msgstr "Devuelve"

#: of sklearn.feature_extraction.text._VectorizerMixin.build_analyzer:22
msgid "analyzer: callable"
msgstr "analyzer: callable"

#: of sklearn.feature_extraction.text._VectorizerMixin.build_analyzer:10
msgid "A function to handle preprocessing, tokenization and n-grams generation."
msgstr "Una función para manejar el preprocesamiento, tokenización y la generación de n-gramas."

#: of sklearn.feature_extraction.text._VectorizerMixin.build_preprocessor:20
msgid "preprocessor: callable"
msgstr "preprocessor: callable"

#: of sklearn.feature_extraction.text._VectorizerMixin.build_preprocessor:9
msgid "A function to preprocess the text before tokenization."
msgstr "Una función para preprocesar el texto antes de la tokenización."

#: of sklearn.feature_extraction.text._VectorizerMixin.build_tokenizer:20
msgid "tokenizer: callable"
msgstr "tokenizer: callable"

#: of sklearn.feature_extraction.text._VectorizerMixin.build_tokenizer:9
msgid "A function to split a string into a sequence of tokens."
msgstr "Una función para dividir una cadena en una secuencia de tokens."

#: of sklearn.feature_extraction.text._VectorizerMixin.decode:4
msgid "The decoding strategy depends on the vectorizer parameters."
msgstr "La estrategia de decodificación depende de los parámetros del vectorizador."

#: of sklearn.feature_extraction.text._VectorizerMixin.decode:9
msgid "**doc**"
msgstr "**doc**"

#: of
msgid "str"
msgstr "str"

#: of sklearn.feature_extraction.text._VectorizerMixin.decode:9
msgid "The string to decode."
msgstr "La cadena a decodificar."

#: of sklearn.feature_extraction.text._VectorizerMixin.decode:25
msgid "doc: str"
msgstr "doc: str"

#: of sklearn.feature_extraction.text._VectorizerMixin.decode:14
msgid "A string of unicode symbols."
msgstr "Una cadena de símbolos Unicode."

#: of sklearn.feature_extraction.text.HashingVectorizer.fit:20
#: sklearn.feature_extraction.text.HashingVectorizer.fit_transform:10
#: sklearn.feature_extraction.text.HashingVectorizer.fit_transform:30
#: sklearn.feature_extraction.text.HashingVectorizer.partial_fit:22
#: sklearn.feature_extraction.text.HashingVectorizer.transform:10
#: sklearn.feature_extraction.text.HashingVectorizer.transform:26
msgid "**X**"
msgstr "**X**"

#: of
msgid "ndarray of shape [n_samples, n_features]"
msgstr "ndarray de forma [n_samples, n_features]"

#: of sklearn.feature_extraction.text.HashingVectorizer.fit:8
#: sklearn.feature_extraction.text.HashingVectorizer.partial_fit:10
msgid "Training data."
msgstr "Datos de entrenamiento."

#: of
msgid "iterable over raw text documents, length = n_samples"
msgstr "iterable over raw text documents, length = n_samples"

#: of sklearn.feature_extraction.text.HashingVectorizer.fit_transform:8
#: sklearn.feature_extraction.text.HashingVectorizer.transform:8
msgid "Samples. Each sample must be a text document (either bytes or unicode strings, file name or file object depending on the constructor argument) which will be tokenized and hashed."
msgstr "Muestras. Cada muestra debe ser un documento de texto (ya sea bytes o cadenas unicode, nombre de archivo u objeto de archivo, dependiendo del argumento del constructor) que será tokenizado y se le aplicará hashing."

#: of sklearn.feature_extraction.text.HashingVectorizer.fit_transform:14
msgid "**y**"
msgstr "**y**"

#: of
msgid "any"
msgstr "any"

#: of sklearn.feature_extraction.text.HashingVectorizer.fit_transform:13
msgid "Ignored. This parameter exists only for compatibility with sklearn.pipeline.Pipeline."
msgstr "Ignorado. Este parámetro sólo existe por compatibilidad con sklearn.pipeline.Pipeline."

#: of
msgid "sparse matrix of shape (n_samples, n_features)"
msgstr "sparse matrix de forma (n_samples, n_features)"

#: of sklearn.feature_extraction.text.HashingVectorizer.fit_transform:19
#: sklearn.feature_extraction.text.HashingVectorizer.transform:15
msgid "Document-term matrix."
msgstr "Matriz documento-término (document-term)."

#: of sklearn.base.BaseEstimator.get_params:9
msgid "**deep**"
msgstr "**deep**"

#: of sklearn.base.BaseEstimator.get_params:8
msgid "If True, will return the parameters for this estimator and contained subobjects that are estimators."
msgstr "Si es True, devolverá los parámetros para este estimador y los subobjetos contenidos que son estimadores."

#: of sklearn.base.BaseEstimator.get_params:25
msgid "**params**"
msgstr "**params**"

#: of
msgid "dict"
msgstr "dict"

#: of sklearn.base.BaseEstimator.get_params:14
msgid "Parameter names mapped to their values."
msgstr "Nombres de parámetros mapeados a sus valores."

#: of sklearn.feature_extraction.text._VectorizerMixin.get_stop_words:20
msgid "stop_words: list or None"
msgstr "stop_words: list o None"

#: of sklearn.feature_extraction.text._VectorizerMixin.get_stop_words:9
msgid "A list of stop words."
msgstr "Una lista de palabras funcionales."

#: of sklearn.feature_extraction.text.HashingVectorizer.partial_fit:4
msgid "This method is just there to mark the fact that this transformer can work in a streaming setup."
msgstr "Este método sólo está ahí para marcar el hecho de que este transformador puede funcionar en una configuración de transmisión (streaming)."

#: of sklearn.base.BaseEstimator.set_params:4
msgid "The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object."
msgstr "El método funciona tanto con estimadores simples como con objetos anidados (como :class:`~sklearn.pipeline.Pipeline`). Estos últimos tienen parámetros de la forma ``<component>__<parameter>`` para que sea posible actualizar cada componente de un objeto anidado."

#: of sklearn.base.BaseEstimator.set_params:12
msgid "**\\*\\*params**"
msgstr "**\\*\\*params**"

#: of sklearn.base.BaseEstimator.set_params:12
msgid "Estimator parameters."
msgstr "Parámetros del estimador."

#: of sklearn.base.BaseEstimator.set_params:28
msgid "**self**"
msgstr "**self**"

#: of
msgid "estimator instance"
msgstr "estimator instance"

#: of sklearn.base.BaseEstimator.set_params:17
msgid "Estimator instance."
msgstr "Instancia del estimador."

#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:4
msgid "Examples using ``sklearn.feature_extraction.text.HashingVectorizer``"
msgstr "Ejemplos utilizando ``sklearn.feature_extraction.text.HashingVectorizer``"

#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:15
#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:23
msgid ":ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`"
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`"

#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:34
#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:42
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`"

#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:53
#: ../modules/generated/sklearn.feature_extraction.text.HashingVectorizer.examples:61
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"

