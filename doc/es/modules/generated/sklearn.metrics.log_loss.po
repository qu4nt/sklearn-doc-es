msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-04 02:12\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.metrics.log_loss.po\n"
"X-Crowdin-File-ID: 5812\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.metrics.log_loss.rst:2
msgid ":mod:`sklearn.metrics`.log_loss"
msgstr ":mod:`sklearn.metrics`.log_loss"

#: of sklearn.metrics._classification.log_loss:2
msgid "Log loss, aka logistic loss or cross-entropy loss."
msgstr ""

#: of sklearn.metrics._classification.log_loss:4
msgid "This is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks, defined as the negative log-likelihood of a logistic model that returns ``y_pred`` probabilities for its training data ``y_true``. The log loss is only defined for two or more labels. For a single sample with true label :math:`y \\in \\{0,1\\}` and and a probability estimate :math:`p = \\operatorname{Pr}(y = 1)`, the log loss is:"
msgstr ""

#: of sklearn.metrics._classification.log_loss:13
msgid "L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n\n"
msgstr "L_{\\log}(y, p) = -(y \\log (p) + (1 - y) \\log (1 - p))\n\n"

#: of sklearn.metrics._classification.log_loss:16
msgid "Read more in the :ref:`User Guide <log_loss>`."
msgstr "Leer más en el :ref:`Manual de Usuario <log_loss>`."

#: of sklearn.metrics._classification.log_loss
msgid "Parameters"
msgstr "Parámetros"

#: of sklearn.metrics._classification.log_loss:21
msgid "**y_true**"
msgstr "**y_true**"

#: of
msgid "array-like or label indicator matrix"
msgstr "array-like o label indicator matrix"

#: of sklearn.metrics._classification.log_loss:21
msgid "Ground truth (correct) labels for n_samples samples."
msgstr ""

#: of sklearn.metrics._classification.log_loss:29
msgid "**y_pred**"
msgstr "**y_pred**"

#: of
msgid "array-like of float, shape = (n_samples, n_classes) or (n_samples,)"
msgstr "array-like de float, shape = (n_samples, n_classes) o (n_samples,)"

#: of sklearn.metrics._classification.log_loss:24
msgid "Predicted probabilities, as returned by a classifier's predict_proba method. If ``y_pred.shape = (n_samples,)`` the probabilities provided are assumed to be that of the positive class. The labels in ``y_pred`` are assumed to be ordered alphabetically, as done by :class:`preprocessing.LabelBinarizer`."
msgstr ""

#: of sklearn.metrics._classification.log_loss:33
msgid "**eps**"
msgstr "**eps**"

#: of
msgid "float, default=1e-15"
msgstr "float, default=1e-15"

#: of sklearn.metrics._classification.log_loss:32
msgid "Log loss is undefined for p=0 or p=1, so probabilities are clipped to max(eps, min(1 - eps, p))."
msgstr ""

#: of sklearn.metrics._classification.log_loss:37
msgid "**normalize**"
msgstr "**normalize**"

#: of
msgid "bool, default=True"
msgstr "bool, default=True"

#: of sklearn.metrics._classification.log_loss:36
msgid "If true, return the mean loss per sample. Otherwise, return the sum of the per-sample losses."
msgstr ""

#: of sklearn.metrics._classification.log_loss:40
msgid "**sample_weight**"
msgstr "**sample_weight**"

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr "array-like de forma (n_samples,), default=None"

#: of sklearn.metrics._classification.log_loss:40
msgid "Sample weights."
msgstr "Ponderaciones de muestras."

#: of sklearn.metrics._classification.log_loss:47
msgid "**labels**"
msgstr "**labels**"

#: of
msgid "array-like, default=None"
msgstr "array-like, default=None"

#: of sklearn.metrics._classification.log_loss:43
msgid "If not provided, labels will be inferred from y_true. If ``labels`` is ``None`` and ``y_pred`` has shape (n_samples,) the labels are assumed to be binary and are inferred from ``y_true``."
msgstr ""

#: of sklearn.metrics._classification.log_loss
msgid "Returns"
msgstr "Devuelve"

#: of sklearn.metrics._classification.log_loss:59
msgid "**loss**"
msgstr "**loss**"

#: of
msgid "float"
msgstr "float"

#: of sklearn.metrics._classification.log_loss:62
msgid "Notes"
msgstr "Notas"

#: of sklearn.metrics._classification.log_loss:63
msgid "The logarithm used is the natural logarithm (base-e)."
msgstr ""

#: of sklearn.metrics._classification.log_loss:66
msgid "References"
msgstr "Referencias"

#: of sklearn.metrics._classification.log_loss:67
msgid "C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209."
msgstr "C.M. Bishop (2006). Pattern Recognition and Machine Learning. Springer, p. 209."

#: of sklearn.metrics._classification.log_loss:75
msgid "Examples"
msgstr "Ejemplos"

#: ../modules/generated/sklearn.metrics.log_loss.examples:4
msgid "Examples using ``sklearn.metrics.log_loss``"
msgstr "Ejemplos utilizando ``sklearn.metrics.log_loss``"

#: ../modules/generated/sklearn.metrics.log_loss.examples:15
#: ../modules/generated/sklearn.metrics.log_loss.examples:23
msgid ":ref:`sphx_glr_auto_examples_gaussian_process_plot_gpc.py`"
msgstr ":ref:`sphx_glr_auto_examples_gaussian_process_plot_gpc.py`"

