msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-06-08 01:04\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.preprocessing.power_transform.po\n"
"X-Crowdin-File-ID: 5242\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.preprocessing.power_transform.rst:2
msgid ":mod:`sklearn.preprocessing`.power_transform"
msgstr ":mod:`sklearn.preprocessing`.power_transform"

#: of sklearn.preprocessing._data.power_transform:2
msgid "Power transforms are a family of parametric, monotonic transformations that are applied to make data more Gaussian-like. This is useful for modeling issues related to heteroscedasticity (non-constant variance), or other situations where normality is desired."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:7
msgid "Currently, power_transform supports the Box-Cox transform and the Yeo-Johnson transform. The optimal parameter for stabilizing variance and minimizing skewness is estimated through maximum likelihood."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:11
msgid "Box-Cox requires input data to be strictly positive, while Yeo-Johnson supports both positive or negative data."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:14
msgid "By default, zero-mean, unit-variance normalization is applied to the transformed data."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:17
msgid "Read more in the :ref:`User Guide <preprocessing_transformer>`."
msgstr ""

#: of sklearn.preprocessing._data.power_transform
msgid "Parameters"
msgstr "Par√°metros"

#: of sklearn.preprocessing._data.power_transform:22
msgid "**X**"
msgstr "**X**"

#: of
msgid "array-like of shape (n_samples, n_features)"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:22
msgid "The data to be transformed using a power transformation."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:32
msgid "**method**"
msgstr "**method**"

#: of
msgid "{'yeo-johnson', 'box-cox'}, default='yeo-johnson'"
msgstr "{'yeo-johnson', 'box-cox'}, default='yeo-johnson'"

#: of sklearn.preprocessing._data.power_transform:25
msgid "The power transform method. Available methods are:"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:27
msgid "'yeo-johnson' [R742a88cfa144-1]_, works with positive and negative values"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:28
msgid "'box-cox' [R742a88cfa144-2]_, only works with strictly positive values"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:30
msgid "The default value of the `method` parameter changed from 'box-cox' to 'yeo-johnson' in 0.23."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:36
msgid "**standardize**"
msgstr "**standardize**"

#: of
msgid "bool, default=True"
msgstr "bool, default=True"

#: of sklearn.preprocessing._data.power_transform:35
msgid "Set to True to apply zero-mean, unit-variance normalization to the transformed output."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:39
msgid "**copy**"
msgstr "**copy**"

#: of sklearn.preprocessing._data.power_transform:39
msgid "Set to False to perform inplace computation during transformation."
msgstr ""

#: of sklearn.preprocessing._data.power_transform
msgid "Returns"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:50
msgid "**X_trans**"
msgstr "**X_trans**"

#: of
msgid "ndarray of shape (n_samples, n_features)"
msgstr "ndarray de forma (n_samples, n_features)"

#: of sklearn.preprocessing._data.power_transform:44
msgid "The transformed data."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:55
msgid ":obj:`PowerTransformer`"
msgstr ":obj:`PowerTransformer`"

#: of sklearn.preprocessing._data.power_transform:56
msgid "Equivalent transformation with the Transformer API (e.g. as part of a preprocessing :class:`~sklearn.pipeline.Pipeline`)."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:57
msgid ":obj:`quantile_transform`"
msgstr ":obj:`quantile_transform`"

#: of sklearn.preprocessing._data.power_transform:58
msgid "Maps data to a standard normal distribution with the parameter `output_distribution='normal'`."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:62
msgid "Notes"
msgstr "Notas"

#: of sklearn.preprocessing._data.power_transform:63
msgid "NaNs are treated as missing values: disregarded in ``fit``, and maintained in ``transform``."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:66
msgid "For a comparison of the different scalers, transformers, and normalizers, see :ref:`examples/preprocessing/plot_all_scaling.py <sphx_glr_auto_examples_preprocessing_plot_all_scaling.py>`."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:71
msgid "References"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:72
msgid "I.K. Yeo and R.A. Johnson, \"A new family of power transformations to improve normality or symmetry.\" Biometrika, 87(4), pp.954-959, (2000)."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:76
msgid "G.E.P. Box and D.R. Cox, \"An Analysis of Transformations\", Journal of the Royal Statistical Society B, 26, 211-252 (1964)."
msgstr ""

#: of sklearn.preprocessing._data.power_transform:81
msgid "[R742a88cfa144-1]_, [R742a88cfa144-2]_"
msgstr ""

#: of sklearn.preprocessing._data.power_transform:84
msgid "Examples"
msgstr "Ejemplos"

#: of sklearn.preprocessing._data.power_transform:93
msgid "Risk of data leak. Do not use :func:`~sklearn.preprocessing.power_transform` unless you know what you are doing. A common mistake is to apply it to the entire data *before* splitting into training and test sets. This will bias the model evaluation because information would have leaked from the test set to the training set. In general, we recommend using :class:`~sklearn.preprocessing.PowerTransformer` within a :ref:`Pipeline <pipeline>` in order to prevent most risks of data leaking, e.g.: `pipe = make_pipeline(PowerTransformer(), LogisticRegression())`."
msgstr ""

