msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 00:05\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.metrics.precision_recall_fscore_support.po\n"
"X-Crowdin-File-ID: 3522\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.metrics.precision_recall_fscore_support.rst:2
msgid ":mod:`sklearn.metrics`.precision_recall_fscore_support"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:2
msgid "Compute precision, recall, F-measure and support for each class."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:4
msgid "The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number of true positives and ``fp`` the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:9
msgid "The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of true positives and ``fn`` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:13
msgid "The F-beta score can be interpreted as a weighted harmonic mean of the precision and recall, where an F-beta score reaches its best value at 1 and worst score at 0."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:17
msgid "The F-beta score weights recall more than precision by a factor of ``beta``. ``beta == 1.0`` means recall and precision are equally important."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:20
msgid "The support is the number of occurrences of each class in ``y_true``."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:22
msgid "If ``pos_label is None`` and in binary classification, this function returns the average precision, recall and F-measure if ``average`` is one of ``'micro'``, ``'macro'``, ``'weighted'`` or ``'samples'``."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:26
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support
msgid "Parameters"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:31
msgid "**y_true**"
msgstr ""

#: of
msgid "1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:31
msgid "Ground truth (correct) target values."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:34
msgid "**y_pred**"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:34
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:37
msgid "**beta**"
msgstr ""

#: of
msgid "float, default=1.0"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:37
msgid "The strength of recall versus precision in the F-score."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:46
msgid "**labels**"
msgstr ""

#: of
msgid "array-like, default=None"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:40
msgid "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:52
msgid "**pos_label**"
msgstr ""

#: of
msgid "str or int, default=1"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:49
msgid "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:75
msgid "**average**"
msgstr ""

#: of
msgid "{'binary', 'micro', 'macro', 'samples','weighted'},             default=None"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:55
msgid "If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:59
msgid "``'binary'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:59
msgid "Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:62
msgid "``'micro'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:62
msgid "Calculate metrics globally by counting the total true positives, false negatives and false positives."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:65
msgid "``'macro'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:65
msgid "Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:70
msgid "``'weighted'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:68
msgid "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:75
msgid "``'samples'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:73
msgid "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`)."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:79
msgid "**warn_for**"
msgstr ""

#: of
msgid "tuple or set, for internal use"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:78
msgid "This determines which warnings will be made in the case that this function is being used to return only one of its metrics."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:82
msgid "**sample_weight**"
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:82
msgid "Sample weights."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:90
msgid "**zero_division**"
msgstr ""

#: of
msgid "\"warn\", 0 or 1, default=\"warn\""
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:88
msgid "Sets the value to return when there is a zero division:"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:86
msgid "recall: when there are no positive labels"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:87
msgid "precision: when there are no positive predictions"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:88
msgid "f-score: both"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:90
msgid "If set to \"warn\", this acts as 0, but warnings are also raised."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support
msgid "Returns"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:95
msgid "**precision**"
msgstr ""

#: of
msgid "float (if average is not None) or array of float, shape =        [n_unique_labels]"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:98
msgid "**recall**"
msgstr ""

#: of
msgid "float (if average is not None) or array of float, , shape =        [n_unique_labels]"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:101
msgid "**fbeta_score**"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:111
msgid "**support**"
msgstr ""

#: of
msgid "None (if average is not None) or array of int, shape =        [n_unique_labels]"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:104
msgid "The number of occurrences of each label in ``y_true``."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:114
msgid "Notes"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:115
msgid "When ``true positive + false positive == 0``, precision is undefined. When ``true positive + false negative == 0``, recall is undefined. In such cases, by default the metric will be set to 0, as will f-score, and ``UndefinedMetricWarning`` will be raised. This behavior can be modified with ``zero_division``."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:122
msgid "References"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:123
msgid "`Wikipedia entry for the Precision and recall <https://en.wikipedia.org/wiki/Precision_and_recall>`_."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:126
msgid "`Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:129
msgid "`Discriminative Methods for Multi-labeled Classification Advances in Knowledge Discovery and Data Mining (2004), pp. 22-30 by Shantanu Godbole, Sunita Sarawagi <http://www.godbole.net/shantanu/pubs/multilabelsvm-pakdd04.pdf>`_."
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:136
msgid "[R623484488881-1]_, [R623484488881-2]_, [R623484488881-3]_"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:139
msgid "Examples"
msgstr ""

#: of sklearn.metrics._classification.precision_recall_fscore_support:151
msgid "It is possible to compute per-label precisions, recalls, F1-scores and supports instead of averaging:"
msgstr ""

