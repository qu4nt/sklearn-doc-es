msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-24 14:28\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.decomposition.non_negative_factorization.po\n"
"X-Crowdin-File-ID: 5026\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.decomposition.non_negative_factorization.rst:2
msgid ":mod:`sklearn.decomposition`.non_negative_factorization"
msgstr ":mod:`sklearn.decomposition`.non_negative_factorization"

#: of sklearn.decomposition._nmf.non_negative_factorization:2
msgid "Compute Non-negative Matrix Factorization (NMF)."
msgstr "Calcula la Matriz de Factorización no-negativa (NMF)."

#: of sklearn.decomposition._nmf.non_negative_factorization:4
msgid "Find two non-negative matrices (W, H) whose product approximates the non- negative matrix X. This factorization can be used for example for dimensionality reduction, source separation or topic extraction."
msgstr "Encuentra dos matrices no negativas (W, H) cuyo producto se aproxima a la matriz no negativa X. Esta factorización se puede utilizar, por ejemplo, para la reducción de la dimensionalidad, la separación de fuentes o la extracción de temas."

#: of sklearn.decomposition._nmf.non_negative_factorization:8
msgid "The objective function is:"
msgstr "La función objetivo es:"

#: of sklearn.decomposition._nmf.non_negative_factorization:10
msgid "0.5 * ||X - WH||_{Fro}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n"
"+ alpha * l1_{ratio} * ||vec(H)||_1\n\n"
"+ 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n"
"+ 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2"
msgstr "0.5 * ||X - WH||_{Fro}^2 + alpha * l1_{ratio} * ||vec(W)||_1\n\n"
"+ alpha * l1_{ratio} * ||vec(H)||_1\n\n"
"+ 0.5 * alpha * (1 - l1_{ratio}) * ||W||_{Fro}^2\n\n"
"+ 0.5 * alpha * (1 - l1_{ratio}) * ||H||_{Fro}^2"

#: of sklearn.decomposition._nmf.non_negative_factorization:20
msgid "Where:"
msgstr "Donde:"

#: of sklearn.decomposition._nmf.non_negative_factorization:22
msgid ":math:`||A||_{Fro}^2 = \\sum_{i,j} A_{ij}^2` (Frobenius norm)"
msgstr ":math:`|A||_{Fro}^2 = \\su_{i,j} A_{ij}^2` (Norma de Frobenius)"

#: of sklearn.decomposition._nmf.non_negative_factorization:24
msgid ":math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Elementwise L1 norm)"
msgstr ":math:`||vec(A)||_1 = \\sum_{i,j} abs(A_{ij})` (Norma Elemental L1)"

#: of sklearn.decomposition._nmf.non_negative_factorization:26
msgid "For multiplicative-update ('mu') solver, the Frobenius norm :math:`(0.5 * ||X - WH||_{Fro}^2)` can be changed into another beta-divergence loss, by changing the beta_loss parameter."
msgstr "Para el solucionador de actualización multiplicativa ('mu'), la norma de Frobenius :math:`(0.5 * ||X - WH||_{Fro}^2)` puede cambiarse por otra función de pérdida de beta-divergencia, cambiando el parámetro beta_loss."

#: of sklearn.decomposition._nmf.non_negative_factorization:30
msgid "The objective function is minimized with an alternating minimization of W and H. If H is given and update_H=False, it solves for W only."
msgstr "La función objetivo se minimiza con una minimización alternativa de W y H. Si se da H y update_H=False, se resuelve sólo para W."

#: of sklearn.decomposition._nmf.non_negative_factorization
msgid "Parameters"
msgstr "Parámetros"

#: of sklearn.decomposition._nmf.non_negative_factorization:36
msgid "**X**"
msgstr "**X**"

#: of
msgid "array-like of shape (n_samples, n_features)"
msgstr "array-like de forma (n_samples, n_features)"

#: of sklearn.decomposition._nmf.non_negative_factorization:36
msgid "Constant matrix."
msgstr "Matriz constante."

#: of sklearn.decomposition._nmf.non_negative_factorization:39
#: sklearn.decomposition._nmf.non_negative_factorization:137
msgid "**W**"
msgstr "**W**"

#: of
msgid "array-like of shape (n_samples, n_components), default=None"
msgstr "array-like de forma (n_samples, n_components), default=None"

#: of sklearn.decomposition._nmf.non_negative_factorization:39
msgid "If init='custom', it is used as initial guess for the solution."
msgstr "Si init='custom', se utiliza como conjetura inicial para la solución."

#: of sklearn.decomposition._nmf.non_negative_factorization:43
#: sklearn.decomposition._nmf.non_negative_factorization:140
msgid "**H**"
msgstr "**H**"

#: of
msgid "array-like of shape (n_components, n_features), default=None"
msgstr "array-like de forma (n_components, n_features), default=None"

#: of sklearn.decomposition._nmf.non_negative_factorization:42
msgid "If init='custom', it is used as initial guess for the solution. If update_H=False, it is used as a constant, to solve for W only."
msgstr "Si init='custom', se utiliza como conjetura inicial para la solución. Si update_H=False, se utiliza como una constante, para resolver sólo W."

#: of sklearn.decomposition._nmf.non_negative_factorization:47
msgid "**n_components**"
msgstr "**n_components**"

#: of
msgid "int, default=None"
msgstr "int, default=None"

#: of sklearn.decomposition._nmf.non_negative_factorization:46
msgid "Number of components, if n_components is not set all features are kept."
msgstr "Número de componentes, si n_components no se establece se mantienen todas las características."

#: of sklearn.decomposition._nmf.non_negative_factorization:73
msgid "**init**"
msgstr "**init**"

#: of
msgid "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None"
msgstr "{'random', 'nndsvd', 'nndsvda', 'nndsvdar', 'custom'}, default=None"

#: of sklearn.decomposition._nmf.non_negative_factorization:50
msgid "Method used to initialize the procedure."
msgstr "Método utilizado para inicializar el procedimiento."

#: of sklearn.decomposition._nmf.non_negative_factorization:52
msgid "Valid options:"
msgstr "Opciones válidas:"

#: of sklearn.decomposition._nmf.non_negative_factorization:54
msgid "None: 'nndsvd' if n_components < n_features, otherwise 'random'."
msgstr "None: 'nndsvd' si n_components < n_features, en caso contrario 'random'."

#: of sklearn.decomposition._nmf.non_negative_factorization:57
msgid "'random': non-negative random matrices, scaled with:"
msgstr "'random': matrices aleatorias no negativas, escaladas con:"

#: of sklearn.decomposition._nmf.non_negative_factorization:57
msgid "sqrt(X.mean() / n_components)"
msgstr "sqrt(X.mean() / n_components)"

#: of sklearn.decomposition._nmf.non_negative_factorization:60
msgid "'nndsvd': Nonnegative Double Singular Value Decomposition (NNDSVD)"
msgstr "'nndsvd': Descomposición del valor singular doble no negativo (Nonnegative Double Singular Value Decomposition NNDSVD)"

#: of sklearn.decomposition._nmf.non_negative_factorization:60
msgid "initialization (better for sparseness)"
msgstr "inicialización (mejor para la dispersión)"

#: of sklearn.decomposition._nmf.non_negative_factorization:63
msgid "'nndsvda': NNDSVD with zeros filled with the average of X"
msgstr "'nndsvda': NNDSVD con ceros rellenados con el promedio de X"

#: of sklearn.decomposition._nmf.non_negative_factorization:63
msgid "(better when sparsity is not desired)"
msgstr "(mejor cuando no se desea la dispersión)"

#: of sklearn.decomposition._nmf.non_negative_factorization:67
msgid "'nndsvdar': NNDSVD with zeros filled with small random values"
msgstr "'nndsvdar': NNDSVD con ceros rellenados de pequeños valores aleatorios"

#: of sklearn.decomposition._nmf.non_negative_factorization:66
msgid "(generally faster, less accurate alternative to NNDSVDa for when sparsity is not desired)"
msgstr "(alternativa generalmente más rápida y menos precisa a la NNDSVDa para cuando no se desea la dispersión)"

#: of sklearn.decomposition._nmf.non_negative_factorization:69
msgid "'custom': use custom matrices W and H if `update_H=True`. If `update_H=False`, then only custom matrix H is used."
msgstr "'custom': utilizar las matrices personalizadas W y H si `update_H=True`. Si `update_H=False`, entonces sólo se utiliza la matriz personalizada H."

#: of sklearn.decomposition._nmf.non_negative_factorization:72
msgid "The default value of `init` changed from 'random' to None in 0.23."
msgstr "El valor por defecto de `init` cambió de 'random' a None en 0.23."

#: of sklearn.decomposition._nmf.non_negative_factorization:77
msgid "**update_H**"
msgstr "**update_H**"

#: of
msgid "bool, default=True"
msgstr "bool, default=True"

#: of sklearn.decomposition._nmf.non_negative_factorization:76
msgid "Set to True, both W and H will be estimated from initial guesses. Set to False, only W will be estimated."
msgstr "Si se establece en True, tanto W como H se estimarán a partir de las conjeturas iniciales. Si se establece en False, sólo se estimará W."

#: of sklearn.decomposition._nmf.non_negative_factorization:91
msgid "**solver**"
msgstr "**solver**"

#: of
msgid "{'cd', 'mu'}, default='cd'"
msgstr "{'cd', 'mu'}, default='cd'"

#: of sklearn.decomposition._nmf.non_negative_factorization:80
msgid "Numerical solver to use:"
msgstr "Solucionador numérico a utilizar:"

#: of sklearn.decomposition._nmf.non_negative_factorization:83
msgid "'cd' is a Coordinate Descent solver that uses Fast Hierarchical"
msgstr "'cd' es un solucionador de Descenso de Coordenadas que utiliza la Jerarquía Rápida"

#: of sklearn.decomposition._nmf.non_negative_factorization:83
msgid "Alternating Least Squares (Fast HALS)."
msgstr "Mínimos cuadrados alternativos (HALS rápido)."

#: of sklearn.decomposition._nmf.non_negative_factorization:85
msgid "'mu' is a Multiplicative Update solver."
msgstr "'mu' es un solucionador de Actualización Multiplicativa."

#: of sklearn.decomposition._nmf.non_negative_factorization:87
msgid "Coordinate Descent solver."
msgstr "Solucionador de Descenso de Coordenadas."

#: of sklearn.decomposition._nmf.non_negative_factorization:90
msgid "Multiplicative Update solver."
msgstr "Solucionador de Actualización Multiplicativa."

#: of sklearn.decomposition._nmf.non_negative_factorization:100
msgid "**beta_loss**"
msgstr "**beta_loss**"

#: of
msgid "float or {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'"
msgstr "float o {'frobenius', 'kullback-leibler',             'itakura-saito'}, default='frobenius'"

#: of sklearn.decomposition._nmf.non_negative_factorization:94
msgid "Beta divergence to be minimized, measuring the distance between X and the dot product WH. Note that values different from 'frobenius' (or 2) and 'kullback-leibler' (or 1) lead to significantly slower fits. Note that for beta_loss <= 0 (or 'itakura-saito'), the input matrix X cannot contain zeros. Used only in 'mu' solver."
msgstr "Se minimiza la Divergencia Beta, midiendo la distancia entre X y el producto punto WH. Ten en cuenta que los valores diferentes de 'frobenius' (o 2) y 'kullback-leibler' (o 1) conducen ajustes significativamente más lentos. Ten en cuenta que para beta_loss <= 0 (o 'itakura-saito'), la matriz de entrada X no puede contener ceros. Sólo se utiliza en el solucionador 'mu'."

#: of sklearn.decomposition._nmf.non_negative_factorization:103
msgid "**tol**"
msgstr "**tol**"

#: of
msgid "float, default=1e-4"
msgstr "float, default=1e-4"

#: of sklearn.decomposition._nmf.non_negative_factorization:103
msgid "Tolerance of the stopping condition."
msgstr "Tolerancia de la condición de parada."

#: of sklearn.decomposition._nmf.non_negative_factorization:106
msgid "**max_iter**"
msgstr "**max_iter**"

#: of
msgid "int, default=200"
msgstr "int, default=200"

#: of sklearn.decomposition._nmf.non_negative_factorization:106
msgid "Maximum number of iterations before timing out."
msgstr "Número máximo de iteraciones antes de que se agote el tiempo."

#: of sklearn.decomposition._nmf.non_negative_factorization:109
msgid "**alpha**"
msgstr "**alpha**"

#: of
msgid "float, default=0."
msgstr "float, default=0."

#: of sklearn.decomposition._nmf.non_negative_factorization:109
msgid "Constant that multiplies the regularization terms."
msgstr "Constante que multiplica los términos de regularización."

#: of sklearn.decomposition._nmf.non_negative_factorization:116
msgid "**l1_ratio**"
msgstr "**l1_ratio**"

#: of sklearn.decomposition._nmf.non_negative_factorization:112
msgid "The regularization mixing parameter, with 0 <= l1_ratio <= 1. For l1_ratio = 0 the penalty is an elementwise L2 penalty (aka Frobenius Norm). For l1_ratio = 1 it is an elementwise L1 penalty. For 0 < l1_ratio < 1, the penalty is a combination of L1 and L2."
msgstr "El parámetro de mezcla de regularización, con 0 <= l1_ratio <= 1. Para l1_ratio = 0 la penalización es una penalización L2 por elementos (también conocida como Norma de Frobenius). Para l1_ratio = 1 es una penalización L1 por elementos. Para 0 < l1_ratio < 1, la penalización es una combinación de L1 y L2."

#: of sklearn.decomposition._nmf.non_negative_factorization:120
msgid "**regularization**"
msgstr "**regularization**"

#: of
msgid "{'both', 'components', 'transformation'}, default=None"
msgstr "{'both', 'components', 'transformation'}, default=None"

#: of sklearn.decomposition._nmf.non_negative_factorization:119
msgid "Select whether the regularization affects the components (H), the transformation (W), both or none of them."
msgstr "Selecciona si la regularización afecta a los componentes (H), a la transformación (W), a ambos o a ninguno de ellos."

#: of sklearn.decomposition._nmf.non_negative_factorization:126
msgid "**random_state**"
msgstr "**random_state**"

#: of
msgid "int, RandomState instance or None, default=None"
msgstr "int, instancia RandomState o None, default=None"

#: of sklearn.decomposition._nmf.non_negative_factorization:123
msgid "Used for NMF initialisation (when ``init`` == 'nndsvdar' or 'random'), and in Coordinate Descent. Pass an int for reproducible results across multiple function calls. See :term:`Glossary <random_state>`."
msgstr "Se utiliza para la inicialización del NMF (cuando ``init`` == 'nndsvdar' o 'random'), y en el Descenso de Coordenadas. Pasa un entero(int) para obtener resultados reproducibles a través de múltiples llamadas a la función. Véase :term:`Glosario <random_state>`."

#: of sklearn.decomposition._nmf.non_negative_factorization:129
msgid "**verbose**"
msgstr "**verbose**"

#: of
msgid "int, default=0"
msgstr "int, default=0"

#: of sklearn.decomposition._nmf.non_negative_factorization:129
msgid "The verbosity level."
msgstr "El nivel de verbosidad."

#: of sklearn.decomposition._nmf.non_negative_factorization:132
msgid "**shuffle**"
msgstr "**shuffle**"

#: of
msgid "bool, default=False"
msgstr "bool, default=False"

#: of sklearn.decomposition._nmf.non_negative_factorization:132
msgid "If true, randomize the order of coordinates in the CD solver."
msgstr "Si es verdadero (True), aleatoriza el orden de las coordenadas en el solucionador de CD."

#: of sklearn.decomposition._nmf.non_negative_factorization
msgid "Returns"
msgstr "Devuelve"

#: of
msgid "ndarray of shape (n_samples, n_components)"
msgstr "ndarray de forma (n_samples, n_components)"

#: of sklearn.decomposition._nmf.non_negative_factorization:137
#: sklearn.decomposition._nmf.non_negative_factorization:140
msgid "Solution to the non-negative least squares problem."
msgstr "Solución al problema de mínimos cuadrados no negativos."

#: of
msgid "ndarray of shape (n_components, n_features)"
msgstr "ndarray de forma (n_components, n_features)"

#: of sklearn.decomposition._nmf.non_negative_factorization:151
msgid "**n_iter**"
msgstr "**n_iter**"

#: of
msgid "int"
msgstr "int"

#: of sklearn.decomposition._nmf.non_negative_factorization:143
msgid "Actual number of iterations."
msgstr "Número real de iteraciones."

#: of sklearn.decomposition._nmf.non_negative_factorization:154
msgid "References"
msgstr "Referencias"

#: of sklearn.decomposition._nmf.non_negative_factorization:155
msgid "Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for large scale nonnegative matrix and tensor factorizations.\" IEICE transactions on fundamentals of electronics, communications and computer sciences 92.3: 708-721, 2009."
msgstr "Cichocki, Andrzej, and P. H. A. N. Anh-Huy. \"Fast local algorithms for large scale nonnegative matrix and tensor factorizations.\" IEICE transactions on fundamentals of electronics, communications and computer sciences 92.3: 708-721, 2009."

#: of sklearn.decomposition._nmf.non_negative_factorization:160
msgid "Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9)."
msgstr "Fevotte, C., & Idier, J. (2011). Algorithms for nonnegative matrix factorization with the beta-divergence. Neural Computation, 23(9)."

#: of sklearn.decomposition._nmf.non_negative_factorization:168
msgid "Examples"
msgstr "Ejemplos"

