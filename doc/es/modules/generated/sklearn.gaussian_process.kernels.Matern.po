msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-21 20:53\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.gaussian_process.kernels.Matern.po\n"
"X-Crowdin-File-ID: 5534\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.gaussian_process.kernels.Matern.rst:2
msgid ":mod:`sklearn.gaussian_process.kernels`.Matern"
msgstr ":mod:`sklearn.gaussian_process.kernels`.Matern"

#: of sklearn.gaussian_process.kernels.Matern:2
msgid "Matern kernel."
msgstr "Núcleo Matern (Matern kernel)."

#: of sklearn.gaussian_process.kernels.Matern:4
msgid "The class of Matern kernels is a generalization of the :class:`RBF`. It has an additional parameter :math:`\\nu` which controls the smoothness of the resulting function. The smaller :math:`\\nu`, the less smooth the approximated function is. As :math:`\\nu\\rightarrow\\infty`, the kernel becomes equivalent to the :class:`RBF` kernel. When :math:`\\nu = 1/2`, the Matérn kernel becomes identical to the absolute exponential kernel. Important intermediate values are :math:`\\nu=1.5` (once differentiable functions) and :math:`\\nu=2.5` (twice differentiable functions)."
msgstr "La clase de núcleos Matern es una generalización de la :clase:`RBF`. Tiene un parámetro adicional :math:`\\nu` que controla la suavidad de la función resultante. Cuanto más pequeño sea :math:`\\nu`, menos suave será la función aproximada. Como :math:`\\nu\\rightarrow\\infty`, el núcleo se hace equivalente al núcleo :class:`RBF`. Cuando :math:`\\nu = 1/2`, el núcleo Matérn se vuelve idéntico al núcleo exponencial absoluto. Los valores intermedios importantes son :math:`\\nu=1,5` (funciones una vez diferenciables) y :math:`\\nu=2,5` (funciones dos veces diferenciables)."

#: of sklearn.gaussian_process.kernels.Matern:15
msgid "The kernel is given by:"
msgstr "El núcleo es dado por:"

#: of sklearn.gaussian_process.kernels.Matern:17
msgid "k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n"
"\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\n"
"\\Bigg)^\\nu K_\\nu\\Bigg(\n"
"\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)\n\n"
msgstr "k(x_i, x_j) =  \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\n"
"\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\n"
"\\Bigg)^\\nu K_\\nu\\Bigg(\n"
"\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)\n\n"

#: of sklearn.gaussian_process.kernels.Matern:23
msgid "where :math:`d(\\cdot,\\cdot)` is the Euclidean distance, :math:`K_{\\nu}(\\cdot)` is a modified Bessel function and :math:`\\Gamma(\\cdot)` is the gamma function. See [Rc15b4675c755-1]_, Chapter 4, Section 4.2, for details regarding the different variants of the Matern kernel."
msgstr "donde :math:`d(\\cdot,\\cdot)` es la distancia euclidiana, :math:`K_{\\nu}(\\cdot)` es una función de Bessel modificada y :math:`Gamma(\\cdot)` es la función gamma. Consulta [Rc15b4675c755-1]_, capítulo 4, sección 4.2, para conocer los detalles de las distintas variantes del núcleo Matern."

#: of sklearn.gaussian_process.kernels.Matern:29
msgid "Read more in the :ref:`User Guide <gp_kernels>`."
msgstr "Más información en el :ref:`Manual de usuario <gp_kernels>`."

#: of sklearn.gaussian_process.kernels.Kernel.clone_with_theta
#: sklearn.gaussian_process.kernels.Kernel.get_params
#: sklearn.gaussian_process.kernels.Matern
#: sklearn.gaussian_process.kernels.Matern.__call__
#: sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag
msgid "Parameters"
msgstr "Parámetros"

#: of sklearn.gaussian_process.kernels.Matern:38
msgid "**length_scale**"
msgstr "**length_scale**"

#: of
msgid "float or ndarray of shape (n_features,), default=1.0"
msgstr "float or ndarray of shape (n_features,), default=1.0"

#: of sklearn.gaussian_process.kernels.Matern:36
msgid "The length scale of the kernel. If a float, an isotropic kernel is used. If an array, an anisotropic kernel is used where each dimension of l defines the length-scale of the respective feature dimension."
msgstr "La escala de longitud del núcleo. Si es un flotador, se utiliza un núcleo isotrópico. Si es un arreglo, se utiliza un núcleo anisotrópico en el que cada dimensión de l define la escala de longitud de la respectiva dimensión de la característica."

#: of sklearn.gaussian_process.kernels.Matern:43
msgid "**length_scale_bounds**"
msgstr "**length_scale_bounds**"

#: of
msgid "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)"
msgstr "pair of floats >= 0 or \"fixed\", default=(1e-5, 1e5)"

#: of sklearn.gaussian_process.kernels.Matern:41
msgid "The lower and upper bound on 'length_scale'. If set to \"fixed\", 'length_scale' cannot be changed during hyperparameter tuning."
msgstr "El límite inferior y superior de 'length_scale'. Si se establece como \" fixed\", 'length_scale' no puede cambiarse durante el ajuste de los hiperparámetros."

#: of sklearn.gaussian_process.kernels.Matern:58
msgid "**nu**"
msgstr "**nu**"

#: of
msgid "float, default=1.5"
msgstr "float, default=1.5"

#: of sklearn.gaussian_process.kernels.Matern:46
msgid "The parameter nu controlling the smoothness of the learned function. The smaller nu, the less smooth the approximated function is. For nu=inf, the kernel becomes equivalent to the RBF kernel and for nu=0.5 to the absolute exponential kernel. Important intermediate values are nu=1.5 (once differentiable functions) and nu=2.5 (twice differentiable functions). Note that values of nu not in [0.5, 1.5, 2.5, inf] incur a considerably higher computational cost (appr. 10 times higher) since they require to evaluate the modified Bessel function. Furthermore, in contrast to l, nu is kept fixed to its initial value and not optimized."
msgstr "El parámetro nu controla la suavidad de la función aprendida. Cuanto menor sea nu, menos suave será la función aproximada. Para nu=inf, el núcleo es equivalente al núcleo RBF y para nu=0.5 al núcleo exponencial absoluto. Los valores intermedios importantes son nu=1.5 (funciones una vez diferenciables) y nu=2.5 (funciones dos veces diferenciables). Hay que tener en cuenta que los valores de nu que no están en [0.5, 1.5, 2.5, inf] tienen un coste computacional considerablemente mayor (aproximadamente 10 veces más), ya que requieren evaluar la función de Bessel modificada. Además, a diferencia de l, nu se mantiene fijo en su valor inicial y no se optimiza."

#: of sklearn.gaussian_process.kernels.Matern
msgid "Attributes"
msgstr "Atributos"

#: of sklearn.gaussian_process.kernels.Matern:63
msgid "**anisotropic**"
msgstr "**anisotropic**"

#: of sklearn.gaussian_process.kernels.Matern:66
msgid ":obj:`bounds <bounds>`"
msgstr ":obj:`bounds <bounds>`"

#: of sklearn.gaussian_process.kernels.Matern:66
#: sklearn.gaussian_process.kernels.Matern.bounds:2
msgid "Returns the log-transformed bounds on the theta."
msgstr "Devuelve los límites transformados en logaritmo del theta."

#: of sklearn.gaussian_process.kernels.Matern:69
msgid "**hyperparameter_length_scale**"
msgstr "**hyperparameter_length_scale**"

#: of sklearn.gaussian_process.kernels.Matern:72
msgid ":obj:`hyperparameters <hyperparameters>`"
msgstr ":obj:`hyperparameters <hyperparameters>`"

#: of sklearn.gaussian_process.kernels.Matern:72
#: sklearn.gaussian_process.kernels.Matern.hyperparameters:2
msgid "Returns a list of all hyperparameter specifications."
msgstr "Devuelve una lista de todas las especificaciones de los hiperparámetros."

#: of sklearn.gaussian_process.kernels.Matern:75
msgid ":obj:`n_dims <n_dims>`"
msgstr ":obj:`n_dims <n_dims>`"

#: of sklearn.gaussian_process.kernels.Matern:75
#: sklearn.gaussian_process.kernels.Matern.n_dims:2
msgid "Returns the number of non-fixed hyperparameters of the kernel."
msgstr "Devuelve el número de hiperparámetros no fijos del núcleo."

#: of sklearn.gaussian_process.kernels.Matern:78
msgid ":obj:`requires_vector_input <requires_vector_input>`"
msgstr ":obj:`requires_vector_input <requires_vector_input>`"

#: of sklearn.gaussian_process.kernels.Matern:78
msgid "Returns whether the kernel is defined on fixed-length feature vectors or generic objects."
msgstr "Devuelve si el núcleo está definido en vectores de características de longitud fija o en objetos genéricos."

#: of sklearn.gaussian_process.kernels.Matern:86
msgid ":obj:`theta <theta>`"
msgstr ":obj:`theta <theta>`"

#: of sklearn.gaussian_process.kernels.Matern:81
#: sklearn.gaussian_process.kernels.Matern.theta:2
msgid "Returns the (flattened, log-transformed) non-fixed hyperparameters."
msgstr "Devuelve los hiperparámetros no fijos (aplanados y transformados en logaritmos)."

#: of sklearn.gaussian_process.kernels.Matern:89
msgid "References"
msgstr "Referencias"

#: of sklearn.gaussian_process.kernels.Matern:90
msgid "`Carl Edward Rasmussen, Christopher K. I. Williams (2006). \"Gaussian Processes for Machine Learning\". The MIT Press. <http://www.gaussianprocess.org/gpml/>`_"
msgstr "`Carl Edward Rasmussen, Christopher K. I. Williams (2006). \"Gaussian Processes for Machine Learning\". The MIT Press. <http://www.gaussianprocess.org/gpml/>`_"

#: of sklearn.gaussian_process.kernels.Matern:96
msgid "[Rc15b4675c755-1]_"
msgstr "[Rc15b4675c755-1]_"

#: of sklearn.gaussian_process.kernels.Matern:99
msgid "Examples"
msgstr "Ejemplos"

#: of sklearn.gaussian_process.kernels.Matern:114
msgid "Methods"
msgstr "Métodos"

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid ":obj:`__call__ <sklearn.gaussian_process.kernels.Matern.__call__>`\\"
msgstr ":obj:`__call__ <sklearn.gaussian_process.kernels.Matern.__call__>`\\"

#: of sklearn.gaussian_process.kernels.Matern.__call__:2
#: sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid "Return the kernel k(X, Y) and optionally its gradient."
msgstr "Devuelve el núcleo k(X, Y) y opcionalmente su gradiente."

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid ":obj:`clone_with_theta <sklearn.gaussian_process.kernels.Matern.clone_with_theta>`\\"
msgstr ":obj:`clone_with_theta <sklearn.gaussian_process.kernels.Matern.clone_with_theta>`\\"

#: of sklearn.gaussian_process.kernels.Kernel.clone_with_theta:2
#: sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid "Returns a clone of self with given hyperparameters theta."
msgstr "Devuelve un clon de sí mismo con los hiperparámetros dados theta."

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid ":obj:`diag <sklearn.gaussian_process.kernels.Matern.diag>`\\"
msgstr ":obj:`diag <sklearn.gaussian_process.kernels.Matern.diag>`\\"

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
#: sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag:2
msgid "Returns the diagonal of the kernel k(X, X)."
msgstr "Devuelve la diagonal del núcleo k(X, X)."

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid ":obj:`get_params <sklearn.gaussian_process.kernels.Matern.get_params>`\\"
msgstr ":obj:`get_params <sklearn.gaussian_process.kernels.Matern.get_params>`\\"

#: of sklearn.gaussian_process.kernels.Kernel.get_params:2
#: sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid "Get parameters of this kernel."
msgstr "Obtener los parámetros de este núcleo."

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid ":obj:`is_stationary <sklearn.gaussian_process.kernels.Matern.is_stationary>`\\"
msgstr ":obj:`is_stationary <sklearn.gaussian_process.kernels.Matern.is_stationary>`\\"

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
#: sklearn.gaussian_process.kernels.StationaryKernelMixin.is_stationary:2
msgid "Returns whether the kernel is stationary."
msgstr "Devuelve si el núcleo es estacionario."

#: of sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid ":obj:`set_params <sklearn.gaussian_process.kernels.Matern.set_params>`\\"
msgstr ":obj:`set_params <sklearn.gaussian_process.kernels.Matern.set_params>`\\"

#: of sklearn.gaussian_process.kernels.Kernel.set_params:2
#: sklearn.gaussian_process.kernels.Matern:123:<autosummary>:1
msgid "Set the parameters of this kernel."
msgstr "Establece los parámetros de este núcleo."

#: of sklearn.gaussian_process.kernels.Matern.__call__:8
#: sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag:11
msgid "**X**"
msgstr "**X**"

#: of
msgid "ndarray of shape (n_samples_X, n_features)"
msgstr "ndarray of shape (n_samples_X, n_features)"

#: of sklearn.gaussian_process.kernels.Matern.__call__:8
#: sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag:11
msgid "Left argument of the returned kernel k(X, Y)"
msgstr "Argumento izquierdo del núcleo devuelto k(X, Y)"

#: of sklearn.gaussian_process.kernels.Matern.__call__:12
msgid "**Y**"
msgstr "**Y**"

#: of
msgid "ndarray of shape (n_samples_Y, n_features), default=None"
msgstr "ndarray of shape (n_samples_Y, n_features), default=None"

#: of sklearn.gaussian_process.kernels.Matern.__call__:11
msgid "Right argument of the returned kernel k(X, Y). If None, k(X, X) if evaluated instead."
msgstr "Argumento derecho del núcleo devuelto k(X, Y). Si es None, se evalúa k(X, X) en su lugar."

#: of sklearn.gaussian_process.kernels.Matern.__call__:17
msgid "**eval_gradient**"
msgstr "**eval_gradient**"

#: of
msgid "bool, default=False"
msgstr "bool, default=False"

#: of sklearn.gaussian_process.kernels.Matern.__call__:15
msgid "Determines whether the gradient with respect to the log of the kernel hyperparameter is computed. Only supported when Y is None."
msgstr "Determina si se calcula el gradiente con respecto al logaritmo del hiperparámetro del núcleo. Sólo se admite cuando Y es None."

#: of sklearn.gaussian_process.kernels.Kernel.get_params
#: sklearn.gaussian_process.kernels.Kernel.set_params
#: sklearn.gaussian_process.kernels.Matern.__call__
#: sklearn.gaussian_process.kernels.Matern.bounds
#: sklearn.gaussian_process.kernels.Matern.theta
#: sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag
msgid "Returns"
msgstr "Devuelve"

#: of sklearn.gaussian_process.kernels.Matern.__call__:22
msgid "**K**"
msgstr "**K**"

#: of
msgid "ndarray of shape (n_samples_X, n_samples_Y)"
msgstr "ndarray of shape (n_samples_X, n_samples_Y)"

#: of sklearn.gaussian_process.kernels.Matern.__call__:22
msgid "Kernel k(X, Y)"
msgstr "Núcleo k(X, Y)"

#: of sklearn.gaussian_process.kernels.Matern.__call__:38
msgid "**K_gradient**"
msgstr "**K_gradient**"

#: of
msgid "ndarray of shape (n_samples_X, n_samples_X, n_dims),                 optional"
msgstr "ndarray of shape (n_samples_X, n_samples_X, n_dims),                 optional"

#: of sklearn.gaussian_process.kernels.Matern.__call__:25
msgid "The gradient of the kernel k(X, X) with respect to the log of the hyperparameter of the kernel. Only returned when `eval_gradient` is True."
msgstr "El gradiente del núcleo k(X, X) con respecto al logaritmo del hiperparámetro del núcleo. Sólo se devuelve cuando `eval_gradient` es True."

#: of sklearn.gaussian_process.kernels.Matern.bounds:20
msgid "**bounds**"
msgstr "**bounds**"

#: of
msgid "ndarray of shape (n_dims, 2)"
msgstr "ndarray of shape (n_dims, 2)"

#: of sklearn.gaussian_process.kernels.Matern.bounds:9
msgid "The log-transformed bounds on the kernel's hyperparameters theta"
msgstr "Los límites transformados logarítmicamente de los hiperparámetros del núcleo theta"

#: of sklearn.gaussian_process.kernels.Kernel.clone_with_theta:20
#: sklearn.gaussian_process.kernels.Matern.theta:24
msgid "**theta**"
msgstr "**theta**"

#: of
msgid "ndarray of shape (n_dims,)"
msgstr "ndarray of shape (n_dims,)"

#: of sklearn.gaussian_process.kernels.Kernel.clone_with_theta:8
msgid "The hyperparameters"
msgstr "Hiperparámetros"

#: of sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag:4
msgid "The result of this method is identical to np.diag(self(X)); however, it can be evaluated more efficiently since only the diagonal is evaluated."
msgstr "El resultado de este método es idéntico al de np.diag(self(X)); sin embargo, se puede evaluar de forma más eficiente ya que sólo se evalúa la diagonal."

#: of sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag:27
msgid "**K_diag**"
msgstr "**K_diag**"

#: of
msgid "ndarray of shape (n_samples_X,)"
msgstr "ndarray of shape (n_samples_X,)"

#: of sklearn.gaussian_process.kernels.NormalizedKernelMixin.diag:16
msgid "Diagonal of kernel k(X, X)"
msgstr "Diagonal del núcleo k(X, X)"

#: of sklearn.gaussian_process.kernels.Kernel.get_params:9
msgid "**deep**"
msgstr "**deep**"

#: of
msgid "bool, default=True"
msgstr "bool, default=True"

#: of sklearn.gaussian_process.kernels.Kernel.get_params:8
msgid "If True, will return the parameters for this estimator and contained subobjects that are estimators."
msgstr "Si es True, devolverá los parámetros para este estimador y los subobjetos contenidos que son estimadores."

#: of sklearn.gaussian_process.kernels.Kernel.get_params:25
msgid "**params**"
msgstr "**params**"

#: of
msgid "dict"
msgstr "dict"

#: of sklearn.gaussian_process.kernels.Kernel.get_params:14
msgid "Parameter names mapped to their values."
msgstr "Nombres de parámetros mapeados a sus valores."

#: of sklearn.gaussian_process.kernels.Matern.requires_vector_input:2
msgid "Returns whether the kernel is defined on fixed-length feature vectors or generic objects. Defaults to True for backward compatibility."
msgstr "Devuelve si el núcleo está definido en vectores de características de longitud fija o en objetos genéricos. El valor predeterminado es True para la compatibilidad con versiones anteriores."

#: of sklearn.gaussian_process.kernels.Kernel.set_params:4
msgid "The method works on simple kernels as well as on nested kernels. The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object."
msgstr "El método funciona tanto en núcleos simples como en núcleos anidados. Estos últimos tienen parámetros de la forma ``<component>__<parameter>`` para que sea posible actualizar cada componente de un objeto anidado."

#: of sklearn.gaussian_process.kernels.Kernel.set_params:23
msgid "self"
msgstr "self"

#: of sklearn.gaussian_process.kernels.Matern.theta:4
msgid "Note that theta are typically the log-transformed values of the kernel's hyperparameters as this representation of the search space is more amenable for hyperparameter search, as hyperparameters like length-scales naturally live on a log-scale."
msgstr "Nótese que theta son típicamente los valores transformados en logaritmos de los hiperparámetros del núcleo, ya que esta representación del espacio de búsqueda es más adecuada para la búsqueda de hiperparámetros, ya que los hiperparámetros como las escalas de longitud viven naturalmente en una escala logarítmica."

#: of sklearn.gaussian_process.kernels.Matern.theta:13
msgid "The non-fixed, log-transformed hyperparameters of the kernel"
msgstr "Los hiperparámetros no fijos y transformados en logaritmos del núcleo"

#: ../modules/generated/sklearn.gaussian_process.kernels.Matern.examples:4
msgid "Examples using ``sklearn.gaussian_process.kernels.Matern``"
msgstr "Ejemplos usando ``sklearn.gaussian_process.kernels.Matern``"

#: ../modules/generated/sklearn.gaussian_process.kernels.Matern.examples:15
#: ../modules/generated/sklearn.gaussian_process.kernels.Matern.examples:23
msgid ":ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_prior_posterior.py`"
msgstr ":ref:`sphx_glr_auto_examples_gaussian_process_plot_gpr_prior_posterior.py`"

