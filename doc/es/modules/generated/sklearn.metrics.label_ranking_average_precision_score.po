msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-06-25 05:09\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.metrics.label_ranking_average_precision_score.po\n"
"X-Crowdin-File-ID: 4996\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.metrics.label_ranking_average_precision_score.rst:2
msgid ":mod:`sklearn.metrics`.label_ranking_average_precision_score"
msgstr ":mod:`sklearn.metrics`.label_ranking_average_precision_score"

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:2
msgid "Compute ranking-based average precision."
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:4
msgid "Label ranking average precision (LRAP) is the average over each ground truth label assigned to each sample, of the ratio of true vs. total labels with lower score."
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:8
msgid "This metric is used in multilabel ranking problem, where the goal is to give better rank to the labels associated to each sample."
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:11
msgid "The obtained score is always strictly greater than 0 and the best value is 1."
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:14
msgid "Read more in the :ref:`User Guide <label_ranking_average_precision>`."
msgstr "Leer más en el :ref:`Manual de Usuario <label_ranking_average_precision>`."

#: of sklearn.metrics._ranking.label_ranking_average_precision_score
msgid "Parameters"
msgstr "Parámetros"

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:19
msgid "**y_true**"
msgstr "**y_true**"

#: of
msgid "{ndarray, sparse matrix} of shape (n_samples, n_labels)"
msgstr "{ndarray, sparse matrix} de forma (n_samples, n_labels)"

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:19
msgid "True binary labels in binary indicator format."
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:24
msgid "**y_score**"
msgstr "**y_score**"

#: of
msgid "ndarray of shape (n_samples, n_labels)"
msgstr "ndarray de forma (n_samples, n_labels)"

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:22
msgid "Target scores, can either be probability estimates of the positive class, confidence values, or non-thresholded measure of decisions (as returned by \"decision_function\" on some classifiers)."
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:29
msgid "**sample_weight**"
msgstr "**sample_weight**"

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr "array-like de forma (n_samples,), default=None"

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:27
msgid "Sample weights."
msgstr "Ponderaciones de muestras."

#: of sklearn.metrics._ranking.label_ranking_average_precision_score
msgid "Returns"
msgstr ""

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:43
msgid "**score**"
msgstr "**score**"

#: of
msgid "float"
msgstr "float"

#: of sklearn.metrics._ranking.label_ranking_average_precision_score:46
msgid "Examples"
msgstr "Ejemplos"

