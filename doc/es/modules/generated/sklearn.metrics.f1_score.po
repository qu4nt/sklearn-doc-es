msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 06:08\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.metrics.f1_score.po\n"
"X-Crowdin-File-ID: 5910\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.metrics.f1_score.rst:2
msgid ":mod:`sklearn.metrics`.f1_score"
msgstr ""

#: of sklearn.metrics._classification.f1_score:2
msgid "Compute the F1 score, also known as balanced F-score or F-measure."
msgstr ""

#: of sklearn.metrics._classification.f1_score:4
msgid "The F1 score can be interpreted as a weighted average of the precision and recall, where an F1 score reaches its best value at 1 and worst score at 0. The relative contribution of precision and recall to the F1 score are equal. The formula for the F1 score is::"
msgstr ""

#: of sklearn.metrics._classification.f1_score:11
msgid "In the multi-class and multi-label case, this is the average of the F1 score of each class with weighting depending on the ``average`` parameter."
msgstr ""

#: of sklearn.metrics._classification.f1_score:15
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: of sklearn.metrics._classification.f1_score
msgid "Parameters"
msgstr ""

#: of sklearn.metrics._classification.f1_score:20
msgid "**y_true**"
msgstr ""

#: of
msgid "1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: of sklearn.metrics._classification.f1_score:20
msgid "Ground truth (correct) target values."
msgstr ""

#: of sklearn.metrics._classification.f1_score:23
msgid "**y_pred**"
msgstr ""

#: of sklearn.metrics._classification.f1_score:23
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: of sklearn.metrics._classification.f1_score:35
msgid "**labels**"
msgstr ""

#: of
msgid "array-like, default=None"
msgstr ""

#: of sklearn.metrics._classification.f1_score:26
msgid "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order."
msgstr ""

#: of sklearn.metrics._classification.f1_score:34
msgid "Parameter `labels` improved for multiclass problem."
msgstr ""

#: of sklearn.metrics._classification.f1_score:41
msgid "**pos_label**"
msgstr ""

#: of
msgid "str or int, default=1"
msgstr ""

#: of sklearn.metrics._classification.f1_score:38
msgid "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only."
msgstr ""

#: of sklearn.metrics._classification.f1_score:65
msgid "**average**"
msgstr ""

#: of
msgid "{'micro', 'macro', 'samples','weighted', 'binary'} or None,             default='binary'"
msgstr ""

#: of sklearn.metrics._classification.f1_score:44
msgid "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:"
msgstr ""

#: of sklearn.metrics._classification.f1_score:49
msgid "``'binary'``:"
msgstr ""

#: of sklearn.metrics._classification.f1_score:49
msgid "Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: of sklearn.metrics._classification.f1_score:52
msgid "``'micro'``:"
msgstr ""

#: of sklearn.metrics._classification.f1_score:52
msgid "Calculate metrics globally by counting the total true positives, false negatives and false positives."
msgstr ""

#: of sklearn.metrics._classification.f1_score:55
msgid "``'macro'``:"
msgstr ""

#: of sklearn.metrics._classification.f1_score:55
msgid "Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account."
msgstr ""

#: of sklearn.metrics._classification.f1_score:60
msgid "``'weighted'``:"
msgstr ""

#: of sklearn.metrics._classification.f1_score:58
msgid "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall."
msgstr ""

#: of sklearn.metrics._classification.f1_score:65
msgid "``'samples'``:"
msgstr ""

#: of sklearn.metrics._classification.f1_score:63
msgid "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`)."
msgstr ""

#: of sklearn.metrics._classification.f1_score:68
msgid "**sample_weight**"
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.metrics._classification.f1_score:68
msgid "Sample weights."
msgstr ""

#: of sklearn.metrics._classification.f1_score:73
msgid "**zero_division**"
msgstr ""

#: of
msgid "\"warn\", 0 or 1, default=\"warn\""
msgstr ""

#: of sklearn.metrics._classification.f1_score:71
msgid "Sets the value to return when there is a zero division, i.e. when all predictions and labels are negative. If set to \"warn\", this acts as 0, but warnings are also raised."
msgstr ""

#: of sklearn.metrics._classification.f1_score
msgid "Returns"
msgstr ""

#: of sklearn.metrics._classification.f1_score:85
msgid "**f1_score**"
msgstr ""

#: of
msgid "float or array of float, shape = [n_unique_labels]"
msgstr ""

#: of sklearn.metrics._classification.f1_score:78
msgid "F1 score of the positive class in binary classification or weighted average of the F1 scores of each class for the multiclass task."
msgstr ""

#: of sklearn.metrics._classification.f1_score:90
msgid ":obj:`fbeta_score`, :obj:`precision_recall_fscore_support`, :obj:`jaccard_score`"
msgstr ""

#: of sklearn.metrics._classification.f1_score:92
msgid ":obj:`multilabel_confusion_matrix`"
msgstr ""

#: of sklearn.metrics._classification.f1_score:96
msgid "Notes"
msgstr ""

#: of sklearn.metrics._classification.f1_score:97
msgid "When ``true positive + false positive == 0``, precision is undefined. When ``true positive + false negative == 0``, recall is undefined. In such cases, by default the metric will be set to 0, as will f-score, and ``UndefinedMetricWarning`` will be raised. This behavior can be modified with ``zero_division``."
msgstr ""

#: of sklearn.metrics._classification.f1_score:104
msgid "References"
msgstr ""

#: of sklearn.metrics._classification.f1_score:105
msgid "`Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_."
msgstr ""

#: of sklearn.metrics._classification.f1_score:110
msgid "[R6e5c12522f2c-1]_"
msgstr ""

#: of sklearn.metrics._classification.f1_score:113
msgid "Examples"
msgstr ""

#: ../modules/generated/sklearn.metrics.f1_score.examples:4
msgid "Examples using ``sklearn.metrics.f1_score``"
msgstr ""

#: ../modules/generated/sklearn.metrics.f1_score.examples:15
#: ../modules/generated/sklearn.metrics.f1_score.examples:23
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`"
msgstr ""

#: ../modules/generated/sklearn.metrics.f1_score.examples:34
#: ../modules/generated/sklearn.metrics.f1_score.examples:42
msgid ":ref:`sphx_glr_auto_examples_semi_supervised_plot_semi_supervised_newsgroups.py`"
msgstr ""

