msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 06:01\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.metrics.fbeta_score.po\n"
"X-Crowdin-File-ID: 5036\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.metrics.fbeta_score.rst:2
msgid ":mod:`sklearn.metrics`.fbeta_score"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:2
msgid "Compute the F-beta score."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:4
msgid "The F-beta score is the weighted harmonic mean of precision and recall, reaching its optimal value at 1 and its worst value at 0."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:7
msgid "The `beta` parameter determines the weight of recall in the combined score. ``beta < 1`` lends more weight to precision, while ``beta > 1`` favors recall (``beta -> 0`` considers only precision, ``beta -> +inf`` only recall)."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:12
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score
msgid "Parameters"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:17
msgid "**y_true**"
msgstr ""

#: of
msgid "1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:17
msgid "Ground truth (correct) target values."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:20
msgid "**y_pred**"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:20
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:23
msgid "**beta**"
msgstr ""

#: of
msgid "float"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:23
msgid "Determines the weight of recall in the combined score."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:35
msgid "**labels**"
msgstr ""

#: of
msgid "array-like, default=None"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:26
msgid "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:34
msgid "Parameter `labels` improved for multiclass problem."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:41
msgid "**pos_label**"
msgstr ""

#: of
msgid "str or int, default=1"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:38
msgid "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:65
msgid "**average**"
msgstr ""

#: of
msgid "{'micro', 'macro', 'samples', 'weighted', 'binary'} or None             default='binary'"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:44
msgid "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:49
msgid "``'binary'``:"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:49
msgid "Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:52
msgid "``'micro'``:"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:52
msgid "Calculate metrics globally by counting the total true positives, false negatives and false positives."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:55
msgid "``'macro'``:"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:55
msgid "Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:60
msgid "``'weighted'``:"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:58
msgid "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:65
msgid "``'samples'``:"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:63
msgid "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`)."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:68
msgid "**sample_weight**"
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:68
msgid "Sample weights."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:73
msgid "**zero_division**"
msgstr ""

#: of
msgid "\"warn\", 0 or 1, default=\"warn\""
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:71
msgid "Sets the value to return when there is a zero division, i.e. when all predictions and labels are negative. If set to \"warn\", this acts as 0, but warnings are also raised."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score
msgid "Returns"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:85
msgid "**fbeta_score**"
msgstr ""

#: of
msgid "float (if average is not None) or array of float, shape =        [n_unique_labels]"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:78
msgid "F-beta score of the positive class in binary classification or weighted average of the F-beta score of each class for the multiclass task."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:90
msgid ":obj:`precision_recall_fscore_support`, :obj:`multilabel_confusion_matrix`"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:94
msgid "Notes"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:95
msgid "When ``true positive + false positive == 0`` or ``true positive + false negative == 0``, f-score returns 0 and raises ``UndefinedMetricWarning``. This behavior can be modified with ``zero_division``."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:101
msgid "References"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:102
msgid "R. Baeza-Yates and B. Ribeiro-Neto (2011). Modern Information Retrieval. Addison Wesley, pp. 327-328."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:105
msgid "`Wikipedia entry for the F1-score <https://en.wikipedia.org/wiki/F1_score>`_."
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:110
msgid "[R7380c1e4fbce-1]_, [R7380c1e4fbce-2]_"
msgstr ""

#: of sklearn.metrics._classification.fbeta_score:113
msgid "Examples"
msgstr ""

