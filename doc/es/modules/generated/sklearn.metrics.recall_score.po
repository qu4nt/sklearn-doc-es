msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-06-05 03:28\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/generated/sklearn.metrics.recall_score.po\n"
"X-Crowdin-File-ID: 5724\n"
"Language: es_ES\n"

#: ../modules/generated/sklearn.metrics.recall_score.rst:2
msgid ":mod:`sklearn.metrics`.recall_score"
msgstr ""

#: of sklearn.metrics._classification.recall_score:2
msgid "Compute the recall."
msgstr ""

#: of sklearn.metrics._classification.recall_score:4
msgid "The recall is the ratio ``tp / (tp + fn)`` where ``tp`` is the number of true positives and ``fn`` the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples."
msgstr ""

#: of sklearn.metrics._classification.recall_score:8
msgid "The best value is 1 and the worst value is 0."
msgstr ""

#: of sklearn.metrics._classification.recall_score:10
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: of sklearn.metrics._classification.recall_score
msgid "Parameters"
msgstr "Par√°metros"

#: of sklearn.metrics._classification.recall_score:15
msgid "**y_true**"
msgstr "**y_true**"

#: of
msgid "1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: of sklearn.metrics._classification.recall_score:15
msgid "Ground truth (correct) target values."
msgstr ""

#: of sklearn.metrics._classification.recall_score:18
msgid "**y_pred**"
msgstr "**y_pred**"

#: of sklearn.metrics._classification.recall_score:18
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: of sklearn.metrics._classification.recall_score:30
msgid "**labels**"
msgstr "**labels**"

#: of
msgid "array-like, default=None"
msgstr "array-like, default=None"

#: of sklearn.metrics._classification.recall_score:21
msgid "The set of labels to include when ``average != 'binary'``, and their order if ``average is None``. Labels present in the data can be excluded, for example to calculate a multiclass average ignoring a majority negative class, while labels not present in the data will result in 0 components in a macro average. For multilabel targets, labels are column indices. By default, all labels in ``y_true`` and ``y_pred`` are used in sorted order."
msgstr ""

#: of sklearn.metrics._classification.recall_score:29
msgid "Parameter `labels` improved for multiclass problem."
msgstr ""

#: of sklearn.metrics._classification.recall_score:36
msgid "**pos_label**"
msgstr "**pos_label**"

#: of
msgid "str or int, default=1"
msgstr "str o int, default=1"

#: of sklearn.metrics._classification.recall_score:33
msgid "The class to report if ``average='binary'`` and the data is binary. If the data are multiclass or multilabel, this will be ignored; setting ``labels=[pos_label]`` and ``average != 'binary'`` will report scores for that label only."
msgstr ""

#: of sklearn.metrics._classification.recall_score:60
msgid "**average**"
msgstr "**average**"

#: of
msgid "{'micro', 'macro', 'samples', 'weighted', 'binary'}             default='binary'"
msgstr ""

#: of sklearn.metrics._classification.recall_score:39
msgid "This parameter is required for multiclass/multilabel targets. If ``None``, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:"
msgstr ""

#: of sklearn.metrics._classification.recall_score:44
msgid "``'binary'``:"
msgstr ""

#: of sklearn.metrics._classification.recall_score:44
msgid "Only report results for the class specified by ``pos_label``. This is applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: of sklearn.metrics._classification.recall_score:47
msgid "``'micro'``:"
msgstr ""

#: of sklearn.metrics._classification.recall_score:47
msgid "Calculate metrics globally by counting the total true positives, false negatives and false positives."
msgstr ""

#: of sklearn.metrics._classification.recall_score:50
msgid "``'macro'``:"
msgstr ""

#: of sklearn.metrics._classification.recall_score:50
msgid "Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account."
msgstr ""

#: of sklearn.metrics._classification.recall_score:55
msgid "``'weighted'``:"
msgstr ""

#: of sklearn.metrics._classification.recall_score:53
msgid "Calculate metrics for each label, and find their average weighted by support (the number of true instances for each label). This alters 'macro' to account for label imbalance; it can result in an F-score that is not between precision and recall."
msgstr ""

#: of sklearn.metrics._classification.recall_score:60
msgid "``'samples'``:"
msgstr ""

#: of sklearn.metrics._classification.recall_score:58
msgid "Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from :func:`accuracy_score`)."
msgstr ""

#: of sklearn.metrics._classification.recall_score:63
msgid "**sample_weight**"
msgstr "**sample_weight**"

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr "array-like de forma (n_samples,), default=None"

#: of sklearn.metrics._classification.recall_score:63
msgid "Sample weights."
msgstr ""

#: of sklearn.metrics._classification.recall_score:67
msgid "**zero_division**"
msgstr "**zero_division**"

#: of
msgid "\"warn\", 0 or 1, default=\"warn\""
msgstr ""

#: of sklearn.metrics._classification.recall_score:66
msgid "Sets the value to return when there is a zero division. If set to \"warn\", this acts as 0, but warnings are also raised."
msgstr ""

#: of sklearn.metrics._classification.recall_score
msgid "Returns"
msgstr "Devuelve"

#: of sklearn.metrics._classification.recall_score:80
msgid "**recall**"
msgstr "**recall**"

#: of
msgid "float (if average is not None) or array of float of shape"
msgstr ""

#: of sklearn.metrics._classification.recall_score:72
msgid "(n_unique_labels,) Recall of the positive class in binary classification or weighted average of the recall of each class for the multiclass task."
msgstr ""

#: of sklearn.metrics._classification.recall_score:85
msgid ":obj:`precision_recall_fscore_support`, :obj:`balanced_accuracy_score`"
msgstr ":obj:`precision_recall_fscore_support`, :obj:`balanced_accuracy_score`"

#: of sklearn.metrics._classification.recall_score:87
msgid ":obj:`multilabel_confusion_matrix`"
msgstr ":obj:`multilabel_confusion_matrix`"

#: of sklearn.metrics._classification.recall_score:91
msgid "Notes"
msgstr ""

#: of sklearn.metrics._classification.recall_score:92
msgid "When ``true positive + false negative == 0``, recall returns 0 and raises ``UndefinedMetricWarning``. This behavior can be modified with ``zero_division``."
msgstr ""

#: of sklearn.metrics._classification.recall_score:98
msgid "Examples"
msgstr "Ejemplos"

#: ../modules/generated/sklearn.metrics.recall_score.examples:4
msgid "Examples using ``sklearn.metrics.recall_score``"
msgstr "Ejemplos utilizando ``sklearn.metrics.recall_score``"

#: ../modules/generated/sklearn.metrics.recall_score.examples:15
#: ../modules/generated/sklearn.metrics.recall_score.examples:23
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`"
msgstr ":ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`"

