msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 00:14\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/outlier_detection.po\n"
"X-Crowdin-File-ID: 3918\n"
"Language: es_ES\n"

#: ../modules/outlier_detection.rst:5
msgid "Novelty and Outlier Detection"
msgstr ""

#: ../modules/outlier_detection.rst:9
msgid "Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an *inlier*), or should be considered as different (it is an *outlier*). Often, this ability is used to clean real data sets. Two important distinctions must be made:"
msgstr ""

#: ../modules/outlier_detection.rst
msgid "outlier detection"
msgstr ""

#: ../modules/outlier_detection.rst:16
msgid "The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations."
msgstr ""

#: ../modules/outlier_detection.rst
msgid "novelty detection"
msgstr ""

#: ../modules/outlier_detection.rst:22
msgid "The training data is not polluted by outliers and we are interested in detecting whether a **new** observation is an outlier. In this context an outlier is also called a novelty."
msgstr ""

#: ../modules/outlier_detection.rst:26
msgid "Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection. In the context of outlier detection, the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions. On the contrary, in the context of novelty detection, novelties/anomalies can form a dense cluster as long as they are in a low density region of the training data, considered as normal in this context."
msgstr ""

#: ../modules/outlier_detection.rst:37
msgid "The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data::"
msgstr ""

#: ../modules/outlier_detection.rst:43
msgid "new observations can then be sorted as inliers or outliers with a ``predict`` method::"
msgstr ""

#: ../modules/outlier_detection.rst:48
msgid "Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the ``score_samples`` method, while the threshold can be controlled by the ``contamination`` parameter."
msgstr ""

#: ../modules/outlier_detection.rst:54
msgid "The ``decision_function`` method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers::"
msgstr ""

#: ../modules/outlier_detection.rst:60
msgid "Note that :class:`neighbors.LocalOutlierFactor` does not support ``predict``, ``decision_function`` and ``score_samples`` methods by default but only a ``fit_predict`` method, as this estimator was originally meant to be applied for outlier detection. The scores of abnormality of the training samples are accessible through the ``negative_outlier_factor_`` attribute."
msgstr ""

#: ../modules/outlier_detection.rst:66
msgid "If you really want to use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the ``novelty`` parameter set to ``True`` before fitting the estimator. In this case, ``fit_predict`` is not available."
msgstr ""

#: ../modules/outlier_detection.rst:72
msgid "**Novelty detection with Local Outlier Factor**"
msgstr ""

#: ../modules/outlier_detection.rst:74 ../modules/outlier_detection.rst:373
msgid "When ``novelty`` is set to ``True`` be aware that you must only use ``predict``, ``decision_function`` and ``score_samples`` on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the ``negative_outlier_factor_`` attribute."
msgstr ""

#: ../modules/outlier_detection.rst:80
msgid "The behavior of :class:`neighbors.LocalOutlierFactor` is summarized in the following table."
msgstr ""

#: ../modules/outlier_detection.rst:84
msgid "Method"
msgstr ""

#: ../modules/outlier_detection.rst:84
msgid "Outlier detection"
msgstr ""

#: ../modules/outlier_detection.rst:84
msgid "Novelty detection"
msgstr ""

#: ../modules/outlier_detection.rst:86
msgid "``fit_predict``"
msgstr ""

#: ../modules/outlier_detection.rst:86
msgid "OK"
msgstr ""

#: ../modules/outlier_detection.rst:86 ../modules/outlier_detection.rst:87
#: ../modules/outlier_detection.rst:88
msgid "Not available"
msgstr ""

#: ../modules/outlier_detection.rst:87
msgid "``predict``"
msgstr ""

#: ../modules/outlier_detection.rst:87 ../modules/outlier_detection.rst:88
#: ../modules/outlier_detection.rst:89
msgid "Use only on new data"
msgstr ""

#: ../modules/outlier_detection.rst:88
msgid "``decision_function``"
msgstr ""

#: ../modules/outlier_detection.rst:89
msgid "``score_samples``"
msgstr ""

#: ../modules/outlier_detection.rst:89
msgid "Use ``negative_outlier_factor_``"
msgstr ""

#: ../modules/outlier_detection.rst:94
msgid "Overview of outlier detection methods"
msgstr ""

#: ../modules/outlier_detection.rst:96
msgid "A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection."
msgstr ""

#: ../modules/outlier_detection.rst:106
msgid ":class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor` perform reasonably well on the data sets considered here. The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus does not perform very well for outlier detection. That being said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging. :class:`svm.OneClassSVM` may still be used with outlier detection but requires fine-tuning of its hyperparameter `nu` to handle outliers and prevent overfitting. Finally, :class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns an ellipse. For more details on the different estimators refer to the example :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` and the sections hereunder."
msgstr ""

#: ../modules/outlier_detection.rst:121
msgid "See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` for a comparison of the :class:`svm.OneClassSVM`, the :class:`ensemble.IsolationForest`, the :class:`neighbors.LocalOutlierFactor` and :class:`covariance.EllipticEnvelope`."
msgstr ""

#: ../modules/outlier_detection.rst:128
msgid "Novelty Detection"
msgstr ""

#: ../modules/outlier_detection.rst:130
msgid "Consider a data set of :math:`n` observations from the same distribution described by :math:`p` features.  Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods."
msgstr ""

#: ../modules/outlier_detection.rst:139
msgid "In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding :math:`p`-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment."
msgstr ""

#: ../modules/outlier_detection.rst:147
msgid "The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the :ref:`svm` module in the :class:`svm.OneClassSVM` object. It requires the choice of a kernel and a scalar parameter to define a frontier.  The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The `nu` parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier."
msgstr ""

#: ../modules/outlier_detection.rst:159
msgid "`Estimating the support of a high-dimensional distribution <http://www.recognition.mccme.ru/pub/papers/SVM/sch99estimating.pdf>`_ Schölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471."
msgstr ""

#: ../modules/outlier_detection.rst:165
msgid "See :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py` for visualizing the frontier learned around some data by a :class:`svm.OneClassSVM` object."
msgstr ""

#: ../modules/outlier_detection.rst:168
msgid ":ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`"
msgstr ""

#: ../modules/outlier_detection.rst:177
msgid "Outlier Detection"
msgstr ""

#: ../modules/outlier_detection.rst:179
msgid "Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called *outliers*. Yet, in the case of outlier detection, we don't have a clean data set representing the population of regular observations that can be used to train any tool."
msgstr ""

#: ../modules/outlier_detection.rst:187
msgid "Fitting an elliptic envelope"
msgstr ""

#: ../modules/outlier_detection.rst:189
msgid "One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the \"shape\" of the data, and can define outlying observations as observations which stand far enough from the fit shape."
msgstr ""

#: ../modules/outlier_detection.rst:195
msgid "The scikit-learn provides an object :class:`covariance.EllipticEnvelope` that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode."
msgstr ""

#: ../modules/outlier_detection.rst:200
msgid "For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below."
msgstr ""

#: ../modules/outlier_detection.rst:213
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` for an illustration of the difference between using a standard (:class:`covariance.EmpiricalCovariance`) or a robust estimate (:class:`covariance.MinCovDet`) of location and covariance to assess the degree of outlyingness of an observation."
msgstr ""

#: ../modules/outlier_detection.rst:221
msgid "Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum covariance determinant estimator\" Technometrics 41(3), 212 (1999)"
msgstr ""

#: ../modules/outlier_detection.rst:227
msgid "Isolation Forest"
msgstr ""

#: ../modules/outlier_detection.rst:229
msgid "One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The :class:`ensemble.IsolationForest` 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
msgstr ""

#: ../modules/outlier_detection.rst:235
msgid "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node."
msgstr ""

#: ../modules/outlier_detection.rst:239
msgid "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function."
msgstr ""

#: ../modules/outlier_detection.rst:242
msgid "Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
msgstr ""

#: ../modules/outlier_detection.rst:246
msgid "The implementation of :class:`ensemble.IsolationForest` is based on an ensemble of :class:`tree.ExtraTreeRegressor`. Following Isolation Forest original paper, the maximum depth of each tree is set to :math:`\\lceil \\log_2(n) \\rceil` where :math:`n` is the number of samples used to build the tree (see (Liu et al., 2008) for more details)."
msgstr ""

#: ../modules/outlier_detection.rst:252
msgid "This algorithm is illustrated below."
msgstr ""

#: ../modules/outlier_detection.rst:261
msgid "The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which allows you to add more trees to an already fitted model::"
msgstr ""

#: ../modules/outlier_detection.rst:274
msgid "See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for an illustration of the use of IsolationForest."
msgstr ""

#: ../modules/outlier_detection.rst:277
msgid "See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` for a comparison of :class:`ensemble.IsolationForest` with :class:`neighbors.LocalOutlierFactor`, :class:`svm.OneClassSVM` (tuned to perform like an outlier detection method) and a covariance-based outlier detection with :class:`covariance.EllipticEnvelope`."
msgstr ""

#: ../modules/outlier_detection.rst:286
msgid "Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\" Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on."
msgstr ""

#: ../modules/outlier_detection.rst:291
msgid "Local Outlier Factor"
msgstr ""

#: ../modules/outlier_detection.rst:292
msgid "Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm."
msgstr ""

#: ../modules/outlier_detection.rst:295
msgid "The :class:`neighbors.LocalOutlierFactor` (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors."
msgstr ""

#: ../modules/outlier_detection.rst:302
msgid "In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density."
msgstr ""

#: ../modules/outlier_detection.rst:308
msgid "The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 \\%, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below)."
msgstr ""

#: ../modules/outlier_detection.rst:319
msgid "The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood."
msgstr ""

#: ../modules/outlier_detection.rst:325
msgid "When applying LOF for outlier detection, there are no ``predict``, ``decision_function`` and ``score_samples`` methods but only a ``fit_predict`` method. The scores of abnormality of the training samples are accessible through the ``negative_outlier_factor_`` attribute. Note that ``predict``, ``decision_function`` and ``score_samples`` can be used on new unseen data when LOF is applied for novelty detection, i.e. when the ``novelty`` parameter is set to ``True``. See :ref:`novelty_with_lof`."
msgstr ""

#: ../modules/outlier_detection.rst:334
msgid "This strategy is illustrated below."
msgstr ""

#: ../modules/outlier_detection.rst:343
msgid "See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py` for an illustration of the use of :class:`neighbors.LocalOutlierFactor`."
msgstr ""

#: ../modules/outlier_detection.rst:346
msgid "See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` for a comparison with other anomaly detection methods."
msgstr ""

#: ../modules/outlier_detection.rst:351
msgid "Breunig, Kriegel, Ng, and Sander (2000) `LOF: identifying density-based local outliers. <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`_ Proc. ACM SIGMOD"
msgstr ""

#: ../modules/outlier_detection.rst:359
msgid "Novelty detection with Local Outlier Factor"
msgstr ""

#: ../modules/outlier_detection.rst:361
msgid "To use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the ``novelty`` parameter set to ``True`` before fitting the estimator::"
msgstr ""

#: ../modules/outlier_detection.rst:369
msgid "Note that ``fit_predict`` is not available in this case."
msgstr ""

#: ../modules/outlier_detection.rst:371
msgid "**Novelty detection with Local Outlier Factor`**"
msgstr ""

#: ../modules/outlier_detection.rst:379
msgid "Novelty detection with Local Outlier Factor is illustrated below."
msgstr ""

