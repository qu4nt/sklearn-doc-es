msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-28 22:26\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/outlier_detection.po\n"
"X-Crowdin-File-ID: 4840\n"
"Language: es_ES\n"

#: ../modules/outlier_detection.rst:5
msgid "Novelty and Outlier Detection"
msgstr "Detección de novedades y valores atípicos"

#: ../modules/outlier_detection.rst:9
msgid "Many applications require being able to decide whether a new observation belongs to the same distribution as existing observations (it is an *inlier*), or should be considered as different (it is an *outlier*). Often, this ability is used to clean real data sets. Two important distinctions must be made:"
msgstr "Muchas aplicaciones requieren poder decidir si una nueva observación pertenece a la misma distribución que las observaciones existentes (es un *valor típico*), o debe considerarse como diferente (es un *valor atípico*). A menudo, esta capacidad se utiliza para limpiar conjuntos de datos reales. Hay que hacer dos distinciones importantes:"

#: ../modules/outlier_detection.rst
msgid "outlier detection"
msgstr "detección de valores atípicos"

#: ../modules/outlier_detection.rst:16
msgid "The training data contains outliers which are defined as observations that are far from the others. Outlier detection estimators thus try to fit the regions where the training data is the most concentrated, ignoring the deviant observations."
msgstr "Los datos de entrenamiento contienen valores atípicos que se definen como observaciones que se alejan de las demás. Por tanto, los estimadores de detección de valores atípicos intentan ajustarse a las regiones donde los datos de entrenamiento están más concentrados, ignorando las observaciones desviadas."

#: ../modules/outlier_detection.rst
msgid "novelty detection"
msgstr "detección de novedades"

#: ../modules/outlier_detection.rst:22
msgid "The training data is not polluted by outliers and we are interested in detecting whether a **new** observation is an outlier. In this context an outlier is also called a novelty."
msgstr "Los datos de entrenamiento no están contaminados por valores atípicos y nos interesa detectar si una **nueva** observación es un valor atípico. En este contexto, un valor atípico también se denomina novedades."

#: ../modules/outlier_detection.rst:26
msgid "Outlier detection and novelty detection are both used for anomaly detection, where one is interested in detecting abnormal or unusual observations. Outlier detection is then also known as unsupervised anomaly detection and novelty detection as semi-supervised anomaly detection. In the context of outlier detection, the outliers/anomalies cannot form a dense cluster as available estimators assume that the outliers/anomalies are located in low density regions. On the contrary, in the context of novelty detection, novelties/anomalies can form a dense cluster as long as they are in a low density region of the training data, considered as normal in this context."
msgstr "Tanto la detección de valores atípicos como la detección de novedades se utilizan para la detección de anomalías, cuando uno está interesado en detectar observaciones anormales o inusuales. La detección de valores atípicos también se conoce como detección de anomalías no supervisada y la detección de novedades como detección de anomalías semisupervisada. En el contexto de la detección de valores atípicos, los valores atípicos/anomalías no pueden formar un grupo denso, ya que los estimadores disponibles suponen que los valores atípicos/anomalías se encuentran en regiones de baja densidad. Por el contrario, en el contexto de la detección de novedades, las novedades/anomalías pueden formar un conglomerado denso siempre que se encuentren en una región de baja densidad de los datos de entrenamiento, considerada como normal en este contexto."

#: ../modules/outlier_detection.rst:37
msgid "The scikit-learn project provides a set of machine learning tools that can be used both for novelty or outlier detection. This strategy is implemented with objects learning in an unsupervised way from the data::"
msgstr "El proyecto scikit-learn proporciona un conjunto de herramientas de aprendizaje automático que pueden ser utilizadas tanto para la detección de novedades como para la detección de valores atípicos. Esta estrategia se implementa con el aprendizaje de objetos de una manera no supervisada de los datos::"

#: ../modules/outlier_detection.rst:43
msgid "new observations can then be sorted as inliers or outliers with a ``predict`` method::"
msgstr "las observaciones nuevas pueden ser ordenadas como valores típicos y atípicos con un ``predict`` método::"

#: ../modules/outlier_detection.rst:48
msgid "Inliers are labeled 1, while outliers are labeled -1. The predict method makes use of a threshold on the raw scoring function computed by the estimator. This scoring function is accessible through the ``score_samples`` method, while the threshold can be controlled by the ``contamination`` parameter."
msgstr "Los valores típicos se etiquetan con 1, mientras que los valores atípicos se etiquetan con -1. El método de predicción utiliza un umbral en la función de puntuación bruta calculada por el estimador. Esta función de puntuación es accesible a través del método ``score_samples``, mientras que el umbral puede ser controlado por el parámetro ``contamination``."

#: ../modules/outlier_detection.rst:54
msgid "The ``decision_function`` method is also defined from the scoring function, in such a way that negative values are outliers and non-negative ones are inliers::"
msgstr "El método ``decision_function'' también se define a partir de la función de puntuación, de manera que los valores negativos son valores atípicos y los no negativos son valores atípicos::"

#: ../modules/outlier_detection.rst:60
msgid "Note that :class:`neighbors.LocalOutlierFactor` does not support ``predict``, ``decision_function`` and ``score_samples`` methods by default but only a ``fit_predict`` method, as this estimator was originally meant to be applied for outlier detection. The scores of abnormality of the training samples are accessible through the ``negative_outlier_factor_`` attribute."
msgstr "Ten en cuenta que :class:`neighbors.LocalOutlierFactor` no admite los métodos ``predict``, ``decision_function`` y ``score_samples`` por defecto, sino sólo un método ``fit_predict``, ya que este estimador fue concebido originalmente para ser aplicado para la detección de valores atípicos. Las puntuaciones de anormalidad de las muestras de entrenamiento son accesibles a través del atributo ``negative_outlier_factor_``."

#: ../modules/outlier_detection.rst:66
msgid "If you really want to use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you can instantiate the estimator with the ``novelty`` parameter set to ``True`` before fitting the estimator. In this case, ``fit_predict`` is not available."
msgstr "Si realmente deseas utilizar :class:`neighbors.LocalOutlierFactor` para la detección de novedades, es decir, predecir etiquetas o calcular la puntuación de anormalidad de nuevos datos no vistos, puede instanciar el estimador con el parámetro ``novelty`` establecido en ``True`` antes de ajustar el estimador. En este caso, ``fit_predict`` no está disponible."

#: ../modules/outlier_detection.rst:72
msgid "**Novelty detection with Local Outlier Factor**"
msgstr "**Detección de novedades con Local Outlier Factor**"

#: ../modules/outlier_detection.rst:74 ../modules/outlier_detection.rst:373
msgid "When ``novelty`` is set to ``True`` be aware that you must only use ``predict``, ``decision_function`` and ``score_samples`` on new unseen data and not on the training samples as this would lead to wrong results. The scores of abnormality of the training samples are always accessible through the ``negative_outlier_factor_`` attribute."
msgstr "Cuando ``novelty`` se establece en ``True`` hay que tener en cuenta que sólo debe utilizar ``predict``, ``decision_function`` y ``score_samples`` en los nuevos datos no vistos y no en las muestras de entrenamiento ya que esto llevaría a resultados erróneos. Las puntuaciones de anormalidad de las muestras de entrenamiento son siempre accesibles a través del atributo ``negative_outlier_factor_``."

#: ../modules/outlier_detection.rst:80
msgid "The behavior of :class:`neighbors.LocalOutlierFactor` is summarized in the following table."
msgstr "El comportamiento de :class:`neighbors.LocalOutlierFactor` se resume en la siguiente tabla."

#: ../modules/outlier_detection.rst:84
msgid "Method"
msgstr "Method"

#: ../modules/outlier_detection.rst:84
msgid "Outlier detection"
msgstr "Outlier detection"

#: ../modules/outlier_detection.rst:84
msgid "Novelty detection"
msgstr "Novelty detection"

#: ../modules/outlier_detection.rst:86
msgid "``fit_predict``"
msgstr "``fit_predict``"

#: ../modules/outlier_detection.rst:86
msgid "OK"
msgstr "OK"

#: ../modules/outlier_detection.rst:86 ../modules/outlier_detection.rst:87
#: ../modules/outlier_detection.rst:88
msgid "Not available"
msgstr "Not available"

#: ../modules/outlier_detection.rst:87
msgid "``predict``"
msgstr "``predict``"

#: ../modules/outlier_detection.rst:87 ../modules/outlier_detection.rst:88
#: ../modules/outlier_detection.rst:89
msgid "Use only on new data"
msgstr "Use only on new data"

#: ../modules/outlier_detection.rst:88
msgid "``decision_function``"
msgstr "``decision_function``"

#: ../modules/outlier_detection.rst:89
msgid "``score_samples``"
msgstr "``score_samples``"

#: ../modules/outlier_detection.rst:89
msgid "Use ``negative_outlier_factor_``"
msgstr "Use ``negative_outlier_factor_``"

#: ../modules/outlier_detection.rst:94
msgid "Overview of outlier detection methods"
msgstr "Resumen de los métodos de detección de valores atípicos"

#: ../modules/outlier_detection.rst:96
msgid "A comparison of the outlier detection algorithms in scikit-learn. Local Outlier Factor (LOF) does not show a decision boundary in black as it has no predict method to be applied on new data when it is used for outlier detection."
msgstr "Una comparación de los algoritmos de detección de valores atípicos en scikit-learn. Local Outlier Factor (LOF) no muestra un límite de decisión en negro ya que no tiene un método de predicción que se aplique a los nuevos datos cuando se utiliza para la detección de valores atípicos."

#: ../modules/outlier_detection.rst:106
msgid ":class:`ensemble.IsolationForest` and :class:`neighbors.LocalOutlierFactor` perform reasonably well on the data sets considered here. The :class:`svm.OneClassSVM` is known to be sensitive to outliers and thus does not perform very well for outlier detection. That being said, outlier detection in high-dimension, or without any assumptions on the distribution of the inlying data is very challenging. :class:`svm.OneClassSVM` may still be used with outlier detection but requires fine-tuning of its hyperparameter `nu` to handle outliers and prevent overfitting. Finally, :class:`covariance.EllipticEnvelope` assumes the data is Gaussian and learns an ellipse. For more details on the different estimators refer to the example :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` and the sections hereunder."
msgstr ":class:`ensemble.IsolationForest` y :class:`neighbors.LocalOutlierFactor` funcionan razonablemente bien en los conjuntos de datos considerados aquí. Se sabe que el :class:`svm.OneClassSVM` es sensible a los valores atípicos y, por tanto, no funciona muy bien para la detección de valores atípicos. Dicho esto, la detección de valores atípicos en alta dimensión, o sin ningún tipo de suposición sobre la distribución de los datos subyacentes, es un gran reto. :class:`svm.OneClassSVM` aún puede utilizarse con la detección de valores atípicos, pero requiere un ajuste fino de su hiperparámetro `nu` para manejar los valores atípicos y evitar el sobreajuste. Por último, :class:`covariance.EllipticEnvelope` asume que los datos son gaussianos y aprende una elipse. Para más detalles sobre los diferentes estimadores, consulta el ejemplo :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` y las secciones siguientes."

#: ../modules/outlier_detection.rst:121
msgid "See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` for a comparison of the :class:`svm.OneClassSVM`, the :class:`ensemble.IsolationForest`, the :class:`neighbors.LocalOutlierFactor` and :class:`covariance.EllipticEnvelope`."
msgstr "Ver :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` para una comparación del :class:`svm.OneClassSVM`, el :class:`ensemble.IsolationForest`, el :class:`neighbors.LocalOutlierFactor` y :class:`covariance.EllipticEnvelope`."

#: ../modules/outlier_detection.rst:128
msgid "Novelty Detection"
msgstr "Detección de novedades"

#: ../modules/outlier_detection.rst:130
msgid "Consider a data set of :math:`n` observations from the same distribution described by :math:`p` features.  Consider now that we add one more observation to that data set. Is the new observation so different from the others that we can doubt it is regular? (i.e. does it come from the same distribution?) Or on the contrary, is it so similar to the other that we cannot distinguish it from the original observations? This is the question addressed by the novelty detection tools and methods."
msgstr "Considere un conjunto de datos de :math:`n` observaciones de la misma distribución descrita por las características :math:`p`.  Considere ahora que añadimos una observación más a ese conjunto de datos. ¿Es la nueva observación tan diferente de las demás que podemos dudar de que sea regular? (es decir, ¿procede de la misma distribución?) O, por el contrario, ¿es tan parecida a las demás que no podemos distinguirla de las observaciones originales? Esta es la cuestión que abordan las herramientas y métodos de detección de novedades."

#: ../modules/outlier_detection.rst:139
msgid "In general, it is about to learn a rough, close frontier delimiting the contour of the initial observations distribution, plotted in embedding :math:`p`-dimensional space. Then, if further observations lay within the frontier-delimited subspace, they are considered as coming from the same population than the initial observations. Otherwise, if they lay outside the frontier, we can say that they are abnormal with a given confidence in our assessment."
msgstr "En general, se trata de aprender una frontera aproximada y cercana que delimita el contorno de la distribución de las observaciones iniciales, graficada en el espacio :math:`p`-dimensional. Entonces, si las observaciones posteriores se encuentran dentro del subespacio delimitado por la frontera, se considera que proceden de la misma población que las observaciones iniciales. En caso contrario, si se encuentran fuera de la frontera, podemos decir que son anormales con una confianza determinada en nuestra valoración."

#: ../modules/outlier_detection.rst:147
msgid "The One-Class SVM has been introduced by Schölkopf et al. for that purpose and implemented in the :ref:`svm` module in the :class:`svm.OneClassSVM` object. It requires the choice of a kernel and a scalar parameter to define a frontier.  The RBF kernel is usually chosen although there exists no exact formula or algorithm to set its bandwidth parameter. This is the default in the scikit-learn implementation. The `nu` parameter, also known as the margin of the One-Class SVM, corresponds to the probability of finding a new, but regular, observation outside the frontier."
msgstr "La SVM de una clase ha sido introducida por Schölkopf et al. con este fin y se ha implementado en el módulo :ref:`svm` en el objeto :class:`svm.OneClassSVM`. Requiere la elección de un kernel y un parámetro escalar para definir una frontera.  Normalmente se elige el kernel RBF aunque no existe una fórmula o algoritmo exacto para establecer su parámetro de ancho de banda. Este es el valor por defecto en la implementación de scikit-learn. El parámetro `nu`, también conocido como el margen de la SVM de una clase, corresponde a la probabilidad de encontrar una nueva, pero regular, observación fuera de la frontera."

#: ../modules/outlier_detection.rst:159
msgid "`Estimating the support of a high-dimensional distribution <http://www.recognition.mccme.ru/pub/papers/SVM/sch99estimating.pdf>`_ Schölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471."
msgstr "`Estimating the support of a high-dimensional distribution <http://www.recognition.mccme.ru/pub/papers/SVM/sch99estimating.pdf>`_ Schölkopf, Bernhard, et al. Neural computation 13.7 (2001): 1443-1471."

#: ../modules/outlier_detection.rst:165
msgid "See :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py` for visualizing the frontier learned around some data by a :class:`svm.OneClassSVM` object."
msgstr "Consulta :ref:`sphx_glr_auto_examples_svm_plot_oneclass.py` para visualizar la frontera aprendida alrededor de unos datos por un objeto :class:`svm.OneClassSVM`."

#: ../modules/outlier_detection.rst:168
msgid ":ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`"
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_species_distribution_modeling.py`"

#: ../modules/outlier_detection.rst:177
msgid "Outlier Detection"
msgstr "Detección de valores atípicos"

#: ../modules/outlier_detection.rst:179
msgid "Outlier detection is similar to novelty detection in the sense that the goal is to separate a core of regular observations from some polluting ones, called *outliers*. Yet, in the case of outlier detection, we don't have a clean data set representing the population of regular observations that can be used to train any tool."
msgstr "La detección de valores atípicos es similar a la detección de novedades en el sentido de que el objetivo es separar un núcleo de observaciones regulares de otras contaminantes, denominadas *valores atípicos*. Sin embargo, en el caso de la detección de valores atípicos, no disponemos de un conjunto de datos limpio que represente la población de observaciones regulares que pueda utilizarse para entrenar cualquier herramienta."

#: ../modules/outlier_detection.rst:187
msgid "Fitting an elliptic envelope"
msgstr "Ajuste de una envolvente elíptica"

#: ../modules/outlier_detection.rst:189
msgid "One common way of performing outlier detection is to assume that the regular data come from a known distribution (e.g. data are Gaussian distributed). From this assumption, we generally try to define the \"shape\" of the data, and can define outlying observations as observations which stand far enough from the fit shape."
msgstr "Una forma habitual de realizar la detección de valores atípicos es suponer que los datos regulares proceden de una distribución conocida (por ejemplo, los datos tienen una distribución gaussiana). A partir de esta suposición, generalmente se intenta definir la \"forma\" de los datos, y se pueden definir las observaciones atípicas como observaciones que se alejan lo suficiente de la forma ajustada."

#: ../modules/outlier_detection.rst:195
msgid "The scikit-learn provides an object :class:`covariance.EllipticEnvelope` that fits a robust covariance estimate to the data, and thus fits an ellipse to the central data points, ignoring points outside the central mode."
msgstr "El scikit-learn proporciona un objeto :class:`covariance.EllipticEnvelope` que ajusta una estimación robusta de la covarianza a los datos, y por tanto ajusta una elipse a los puntos centrales de los datos, ignorando los puntos fuera del modo central."

#: ../modules/outlier_detection.rst:200
msgid "For instance, assuming that the inlier data are Gaussian distributed, it will estimate the inlier location and covariance in a robust way (i.e. without being influenced by outliers). The Mahalanobis distances obtained from this estimate is used to derive a measure of outlyingness. This strategy is illustrated below."
msgstr "Por ejemplo, suponiendo que los datos de los inlays se distribuyen de forma gaussiana, estimará la ubicación de los inlays y la covarianza de forma robusta (es decir, sin que se vean influidos por los inlays). Las distancias de Mahalanobis obtenidas a partir de esta estimación se utilizan para obtener una medida de los valores atípicos. Esta estrategia se ilustra a continuación."

#: ../modules/outlier_detection.rst:213
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` for an illustration of the difference between using a standard (:class:`covariance.EmpiricalCovariance`) or a robust estimate (:class:`covariance.MinCovDet`) of location and covariance to assess the degree of outlyingness of an observation."
msgstr "Consulta :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` para ver una ilustración de la diferencia entre utilizar una estimación estándar (:class:`covariance.EmpiricalCovariance`) o una estimación robusta (:class:`covariance.MinCovDet`) de la localización y la covarianza para evaluar el grado de perificidad de una observación."

#: ../modules/outlier_detection.rst:221
msgid "Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum covariance determinant estimator\" Technometrics 41(3), 212 (1999)"
msgstr "Rousseeuw, P.J., Van Driessen, K. \"A fast algorithm for the minimum covariance determinant estimator\" Technometrics 41(3), 212 (1999)"

#: ../modules/outlier_detection.rst:227
msgid "Isolation Forest"
msgstr "Bosque de aislamiento"

#: ../modules/outlier_detection.rst:229
msgid "One efficient way of performing outlier detection in high-dimensional datasets is to use random forests. The :class:`ensemble.IsolationForest` 'isolates' observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature."
msgstr "Una forma eficiente de realizar la detección de valores atípicos en conjuntos de datos de alta dimensión es utilizar bosques aleatorios. La :class:`ensemble.IsolationForest` 'aísla' las observaciones seleccionando aleatoriamente una característica y, a continuación, seleccionando aleatoriamente un valor de división entre los valores máximo y mínimo de la característica seleccionada."

#: ../modules/outlier_detection.rst:235
msgid "Since recursive partitioning can be represented by a tree structure, the number of splittings required to isolate a sample is equivalent to the path length from the root node to the terminating node."
msgstr "Dado que la partición recursiva puede representarse mediante una estructura de árbol, el número de particiones necesarias para aislar una muestra es equivalente a la longitud del camino desde el nodo raíz hasta el nodo final."

#: ../modules/outlier_detection.rst:239
msgid "This path length, averaged over a forest of such random trees, is a measure of normality and our decision function."
msgstr "Esta longitud del recorrido, promediada sobre un bosque de tales árboles aleatorios, es una medida de normalidad y nuestra función de decisión."

#: ../modules/outlier_detection.rst:242
msgid "Random partitioning produces noticeably shorter paths for anomalies. Hence, when a forest of random trees collectively produce shorter path lengths for particular samples, they are highly likely to be anomalies."
msgstr "La partición aleatoria produce trayectorias notablemente más cortas para las anomalías. Por lo tanto, cuando un bosque de árboles aleatorios produce colectivamente trayectorias más cortas para determinadas muestras, es muy probable que se trate de anomalías."

#: ../modules/outlier_detection.rst:246
msgid "The implementation of :class:`ensemble.IsolationForest` is based on an ensemble of :class:`tree.ExtraTreeRegressor`. Following Isolation Forest original paper, the maximum depth of each tree is set to :math:`\\lceil \\log_2(n) \\rceil` where :math:`n` is the number of samples used to build the tree (see (Liu et al., 2008) for more details)."
msgstr "La implementación de :class:`ensemble.IsolationForest` se basa en un conjunto de :class:`tree.ExtraTreeRegressor`. Siguiendo el documento original de Isolation Forest, la profundidad máxima de cada árbol se establece en :math:`\\lceil \\log_2(n) \\rceil` donde :math:`n` es el número de muestras utilizadas para construir el árbol (ver en (Liu et al., 2008) para más detalles)."

#: ../modules/outlier_detection.rst:252
msgid "This algorithm is illustrated below."
msgstr "Este algoritmo se ilustra a continuación."

#: ../modules/outlier_detection.rst:261
msgid "The :class:`ensemble.IsolationForest` supports ``warm_start=True`` which allows you to add more trees to an already fitted model::"
msgstr "La :class:`ensemble.IsolationForest` soporta ``warm_start=True`` que permite añadir más árboles a un modelo ya ajustado::"

#: ../modules/outlier_detection.rst:274
msgid "See :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` for an illustration of the use of IsolationForest."
msgstr "Consultar :ref:`sphx_glr_auto_examples_ensemble_plot_isolation_forest.py` para una ilustración del uso de IsolationForest."

#: ../modules/outlier_detection.rst:277
msgid "See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` for a comparison of :class:`ensemble.IsolationForest` with :class:`neighbors.LocalOutlierFactor`, :class:`svm.OneClassSVM` (tuned to perform like an outlier detection method) and a covariance-based outlier detection with :class:`covariance.EllipticEnvelope`."
msgstr "Consulta :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` para ver una comparación de :class:`ensemble.IsolationForest` con :class:`neighbors.LocalOutlierFactor`, :class:`svm.OneClassSVM` (ajustado para funcionar como un método de detección de valores atípicos) y una detección de valores atípicos basada en la covarianza con :class:`covariance.EllipticEnvelope`."

#: ../modules/outlier_detection.rst:286
msgid "Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\" Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on."
msgstr "Liu, Fei Tony, Ting, Kai Ming and Zhou, Zhi-Hua. \"Isolation forest.\" Data Mining, 2008. ICDM'08. Eighth IEEE International Conference on."

#: ../modules/outlier_detection.rst:291
msgid "Local Outlier Factor"
msgstr "Local Outlier Factor"

#: ../modules/outlier_detection.rst:292
msgid "Another efficient way to perform outlier detection on moderately high dimensional datasets is to use the Local Outlier Factor (LOF) algorithm."
msgstr "Otra forma eficaz de realizar la detección de valores atípicos en conjuntos de datos moderadamente dimensionales es utilizar el algoritmo del factor local de valores atípicos (LOF)."

#: ../modules/outlier_detection.rst:295
msgid "The :class:`neighbors.LocalOutlierFactor` (LOF) algorithm computes a score (called local outlier factor) reflecting the degree of abnormality of the observations. It measures the local density deviation of a given data point with respect to its neighbors. The idea is to detect the samples that have a substantially lower density than their neighbors."
msgstr "El algoritmo :class:`neighbors.LocalOutlierFactor` (LOF) calcula una puntuación (denominada factor local de valores atípicos) que refleja el grado de anormalidad de las observaciones. Mide la desviación de la densidad local de un punto de datos dado con respecto a sus vecinos. La idea es detectar las muestras que tienen una densidad sustancialmente inferior a la de sus vecinos."

#: ../modules/outlier_detection.rst:302
msgid "In practice the local density is obtained from the k-nearest neighbors. The LOF score of an observation is equal to the ratio of the average local density of his k-nearest neighbors, and its own local density: a normal instance is expected to have a local density similar to that of its neighbors, while abnormal data are expected to have much smaller local density."
msgstr "En la práctica, la densidad local se obtiene a partir de los k vecinos más cercanos. La puntuación LOF de una observación es igual a la relación entre la densidad local media de sus k vecinos más cercanos y su propia densidad local: se espera que una instancia normal tenga una densidad local similar a la de sus vecinos, mientras que se espera que los datos anormales tengan una densidad local mucho menor."

#: ../modules/outlier_detection.rst:308
msgid "The number k of neighbors considered, (alias parameter n_neighbors) is typically chosen 1) greater than the minimum number of objects a cluster has to contain, so that other objects can be local outliers relative to this cluster, and 2) smaller than the maximum number of close by objects that can potentially be local outliers. In practice, such informations are generally not available, and taking n_neighbors=20 appears to work well in general. When the proportion of outliers is high (i.e. greater than 10 \\%, as in the example below), n_neighbors should be greater (n_neighbors=35 in the example below)."
msgstr "El número k de vecinos considerado, (parámetro alias n_neighbors) suele elegirse 1) mayor que el número mínimo de objetos que debe contener un clúster, para que otros objetos puedan ser valores atípicos locales en relación con este conglomerado, y 2) menor que el número máximo de objetos cercanos que pueden ser potencialmente atípicos locales. En la práctica, esta información no suele estar disponible, y tomar n_neighbors=20 parece funcionar bien en general. Cuando la proporción de valores atípicos es alta (es decir, superior al 10 \\%, como en el ejemplo siguiente), n_neighbors debe ser mayor (n_neighbors=35 en el ejemplo siguiente)."

#: ../modules/outlier_detection.rst:319
msgid "The strength of the LOF algorithm is that it takes both local and global properties of datasets into consideration: it can perform well even in datasets where abnormal samples have different underlying densities. The question is not, how isolated the sample is, but how isolated it is with respect to the surrounding neighborhood."
msgstr "El punto fuerte del algoritmo LOF es que tiene en cuenta tanto las propiedades locales como las globales de los conjuntos de datos: puede funcionar bien incluso en conjuntos de datos en los que las muestras anómalas tienen diferentes densidades subyacentes. La cuestión no es cuán aislada está la muestra, sino cuán aislada está con respecto al vecindario circundante."

#: ../modules/outlier_detection.rst:325
msgid "When applying LOF for outlier detection, there are no ``predict``, ``decision_function`` and ``score_samples`` methods but only a ``fit_predict`` method. The scores of abnormality of the training samples are accessible through the ``negative_outlier_factor_`` attribute. Note that ``predict``, ``decision_function`` and ``score_samples`` can be used on new unseen data when LOF is applied for novelty detection, i.e. when the ``novelty`` parameter is set to ``True``. See :ref:`novelty_with_lof`."
msgstr "Cuando se aplica LOF para la detección de valores atípicos, no existen los métodos ``predict``, ``decision_function`` y ``score_samples``, sino sólo un método ``fit_predict``. Las puntuaciones de anormalidad de las muestras de entrenamiento son accesibles a través del atributo ``negative_outlier_factor_``. Tenga en cuenta que ``predict``, ``decision_function`` y ``score_samples`` pueden utilizarse en nuevos datos no vistos cuando se aplica LOF para la detección de novedades, es decir, cuando el parámetro ``novelty`` se establece en ``True``. Ver en :ref:`novelty_with_lof`."

#: ../modules/outlier_detection.rst:334
msgid "This strategy is illustrated below."
msgstr "Esta estrategia se ilustra a continuación."

#: ../modules/outlier_detection.rst:343
msgid "See :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py` for an illustration of the use of :class:`neighbors.LocalOutlierFactor`."
msgstr "Consulta :ref:`sphx_glr_auto_examples_neighbors_plot_lof_outlier_detection.py` para ver una ilustración del uso de :class:`neighbors.LocalOutlierFactor`."

#: ../modules/outlier_detection.rst:346
msgid "See :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` for a comparison with other anomaly detection methods."
msgstr "Consulta :ref:`sphx_glr_auto_examples_miscellaneous_plot_anomaly_comparison.py` para una comparación con otros métodos de detección de anomalías."

#: ../modules/outlier_detection.rst:351
msgid "Breunig, Kriegel, Ng, and Sander (2000) `LOF: identifying density-based local outliers. <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`_ Proc. ACM SIGMOD"
msgstr "Breunig, Kriegel, Ng, and Sander (2000) `LOF: identifying density-based local outliers. <http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf>`_ Proc. ACM SIGMOD"

#: ../modules/outlier_detection.rst:359
msgid "Novelty detection with Local Outlier Factor"
msgstr "Detección de novedades con Local Outlier Factor"

#: ../modules/outlier_detection.rst:361
msgid "To use :class:`neighbors.LocalOutlierFactor` for novelty detection, i.e. predict labels or compute the score of abnormality of new unseen data, you need to instantiate the estimator with the ``novelty`` parameter set to ``True`` before fitting the estimator::"
msgstr "Para utilizar :class:`neighbors.LocalOutlierFactor` para la detección de novedades, es decir, para predecir etiquetas o calcular la puntuación de anormalidad de nuevos datos no vistos, es necesario instanciar el estimador con el parámetro ``novelty`` establecido en ``True`` antes de ajustar el estimador::"

#: ../modules/outlier_detection.rst:369
msgid "Note that ``fit_predict`` is not available in this case."
msgstr "Toma en cuenta que ``fit_predict`` no está disponible en este caso."

#: ../modules/outlier_detection.rst:371
msgid "**Novelty detection with Local Outlier Factor`**"
msgstr "**Detección de novedades con Local Outlier Factor`**"

#: ../modules/outlier_detection.rst:379
msgid "Novelty detection with Local Outlier Factor is illustrated below."
msgstr "A continuación, se ilustra la detección de novedades con Local Outlier Factor."

