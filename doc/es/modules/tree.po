msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-13 23:04\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/tree.po\n"
"X-Crowdin-File-ID: 4866\n"
"Language: es_ES\n"

#: ../modules/tree.rst:5
msgid "Decision Trees"
msgstr "Árboles de decisión"

#: ../modules/tree.rst:9
msgid "**Decision Trees (DTs)** are a non-parametric supervised learning method used for :ref:`classification <tree_classification>` and :ref:`regression <tree_regression>`. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation."
msgstr "**Los árboles de decisión (DTs por sus siglas en inglés)** son un método de aprendizaje supervisado no paramétrico utilizado para :ref:`clasificación <tree_classification>` y :ref:`regresión <tree_regression>`. El objetivo es crear un modelo que prediga el valor de una variable objetivo mediante el aprendizaje de reglas de decisión simples inferidas a partir de las características de los datos. Un árbol puede ser visto como una aproximación constante a trozos."

#: ../modules/tree.rst:15
msgid "For instance, in the example below, decision trees learn from data to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model."
msgstr "En el siguiente ejemplo, los árboles de decisión aprenden de los datos para aproximar una curva sinusoidal con un conjunto de reglas de decisión si-entonces-en otro caso (if-then-else). Cuanto más profundo sea el árbol, más complejas serán las reglas de decisión y más ajustado será el modelo."

#: ../modules/tree.rst:24
msgid "Some advantages of decision trees are:"
msgstr "Algunas ventajas de los árboles de decisión son:"

#: ../modules/tree.rst:26
msgid "Simple to understand and to interpret. Trees can be visualised."
msgstr "Sencillo de entender e interpretar. Los árboles pueden ser visualizados."

#: ../modules/tree.rst:28
msgid "Requires little data preparation. Other techniques often require data normalisation, dummy variables need to be created and blank values to be removed. Note however that this module does not support missing values."
msgstr "Requiere poca preparación de los datos. Otras técnicas a menudo requieren la normalización de datos, la creación de variables dummy y la eliminación de valores en blanco. Sin embargo, ten en cuenta que este módulo no admite valores faltantes."

#: ../modules/tree.rst:33
msgid "The cost of using the tree (i.e., predicting data) is logarithmic in the number of data points used to train the tree."
msgstr "El costo de usar el árbol (es decir, de predecir datos) es logarítmico en el número de puntos de datos utilizados para entrenar el árbol."

#: ../modules/tree.rst:36
msgid "Able to handle both numerical and categorical data. However scikit-learn implementation does not support categorical variables for now. Other techniques are usually specialised in analysing datasets that have only one type of variable. See :ref:`algorithms <tree_algorithms>` for more information."
msgstr "Capaz de manejar datos tanto numéricos como categóricos. Sin embargo, la implementación de scikit-learn no admite variables categóricas por ahora. Otras técnicas suelen estar especializadas en el análisis de conjuntos de datos que sólo tienen un tipo de variable. Véase :ref:`algoritmos <tree_algorithms>` para más información."

#: ../modules/tree.rst:42
msgid "Able to handle multi-output problems."
msgstr "Capaz de manejar problemas de salida múltiple."

#: ../modules/tree.rst:44
msgid "Uses a white box model. If a given situation is observable in a model, the explanation for the condition is easily explained by boolean logic. By contrast, in a black box model (e.g., in an artificial neural network), results may be more difficult to interpret."
msgstr "Utiliza un modelo de caja blanca (white box model). Si una situación dada es observable en un modelo, la explicación de la condición se explica fácilmente por lógica booleana. Por el contrario, en un modelo de caja negra (black box model) (por ejemplo, en una red neuronal artificial), los resultados pueden ser más difíciles de interpretar."

#: ../modules/tree.rst:49
msgid "Possible to validate a model using statistical tests. That makes it possible to account for the reliability of the model."
msgstr "Es posible validar un modelo utilizando pruebas estadísticas. Eso permite tener en cuenta la fiabilidad del modelo."

#: ../modules/tree.rst:52
msgid "Performs well even if its assumptions are somewhat violated by the true model from which the data were generated."
msgstr "Funciona bien incluso si sus supuestos son violados de alguna manera por el verdadero modelo a partir del cual se generaron los datos."

#: ../modules/tree.rst:56
msgid "The disadvantages of decision trees include:"
msgstr "Las desventajas de los árboles de decisión incluyen:"

#: ../modules/tree.rst:58
msgid "Decision-tree learners can create over-complex trees that do not generalise the data well. This is called overfitting. Mechanisms such as pruning, setting the minimum number of samples required at a leaf node or setting the maximum depth of the tree are necessary to avoid this problem."
msgstr "Los algoritmos de aprendizaje de árboles de decisión pueden crear árboles demasiado complejos que no generalicen bien los datos. Esto se denomina sobreajuste. Para evitar este problema son necesarios mecanismos como la poda, establecer el número mínimo de muestras necesarias en un nodo de la hoja o establecer la profundidad máxima del árbol."

#: ../modules/tree.rst:64
msgid "Decision trees can be unstable because small variations in the data might result in a completely different tree being generated. This problem is mitigated by using decision trees within an ensemble."
msgstr "Los árboles de decisión pueden ser inestables porque pequeñas variaciones en los datos pueden resultar en la generación de un árbol completamente diferente. Este problema es mitigado mediante el uso de árboles de decisión dentro de un conjunto."

#: ../modules/tree.rst:69
msgid "Predictions of decision trees are neither smooth nor continuous, but piecewise constant approximations as seen in the above figure. Therefore, they are not good at extrapolation."
msgstr "Las predicciones de los árboles de decisión no son suaves ni continuas, sino aproximaciones constantes a trozos, como se ve en la figura anterior. Por lo tanto, no son buenos para la extrapolación."

#: ../modules/tree.rst:73
msgid "The problem of learning an optimal decision tree is known to be NP-complete under several aspects of optimality and even for simple concepts. Consequently, practical decision-tree learning algorithms are based on heuristic algorithms such as the greedy algorithm where locally optimal decisions are made at each node. Such algorithms cannot guarantee to return the globally optimal decision tree.  This can be mitigated by training multiple trees in an ensemble learner, where the features and samples are randomly sampled with replacement."
msgstr "Se sabe que el problema del aprendizaje de un árbol de decisión óptimo es NP-completo bajo varios aspectos de optimalidad e incluso para conceptos simples. En consecuencia, los algoritmos prácticos de aprendizaje de árboles de decisión se basan en algoritmos heurísticos como el algoritmo codicioso, donde se toman decisiones localmente óptimas en cada nodo. Tales algoritmos no pueden garantizar que devuelvan el árbol de decisión globalmente óptimo. Esto puede mitigarse entrenando múltiples árboles en un algoritmo de aprendizaje de conjunto, donde las características y las muestras se muestrean aleatoriamente con reemplazo."

#: ../modules/tree.rst:82
msgid "There are concepts that are hard to learn because decision trees do not express them easily, such as XOR, parity or multiplexer problems."
msgstr "Hay conceptos difíciles de aprender porque los árboles de decisión no los expresan fácilmente, como los problemas de XOR, paridad o multiplexores."

#: ../modules/tree.rst:85
msgid "Decision tree learners create biased trees if some classes dominate. It is therefore recommended to balance the dataset prior to fitting with the decision tree."
msgstr "Los algoritmos de aprendizaje de árboles de decisión crean árboles sesgados si algunas clases dominan. Por lo tanto, se recomienda equilibrar el conjunto de datos antes de ajustarlo con el árbol de decisión."

#: ../modules/tree.rst:93
msgid "Classification"
msgstr "Clasificación"

#: ../modules/tree.rst:95
msgid ":class:`DecisionTreeClassifier` is a class capable of performing multi-class classification on a dataset."
msgstr ":class:`DecisionTreeClassifier` es una clase capaz de realizar una clasificación multiclase en un conjunto de datos."

#: ../modules/tree.rst:98
msgid "As with other classifiers, :class:`DecisionTreeClassifier` takes as input two arrays: an array X, sparse or dense, of shape ``(n_samples, n_features)`` holding the training samples, and an array Y of integer values, shape ``(n_samples,)``, holding the class labels for the training samples::"
msgstr "Al igual que otros clasificadores, :class:`DecisionTreeClassifier` toma como entrada dos arreglos: un arreglo X, disperso o denso, de forma ``(n_samples, n_features)`` que contiene las muestras de entrenamiento, y un arreglo Y de valores enteros, de forma ``(n_samples,)``, que contiene las etiquetas de clase para las muestras de entrenamiento::"

#: ../modules/tree.rst:109
msgid "After being fitted, the model can then be used to predict the class of samples::"
msgstr "Después de ser ajustado, el modelo puede ser utilizado para predecir la clase de muestras::"

#: ../modules/tree.rst:114
msgid "In case that there are multiple classes with the same and highest probability, the classifier will predict the class with the lowest index amongst those classes."
msgstr "En caso de que haya múltiples clases con la misma y más alta probabilidad, el clasificador predecirá la clase con el índice más bajo entre esas clases."

#: ../modules/tree.rst:118
msgid "As an alternative to outputting a specific class, the probability of each class can be predicted, which is the fraction of training samples of the class in a leaf::"
msgstr "Como alternativa a la salida de una clase específica, se puede predecir la probabilidad de cada clase, que es la fracción de las muestras de entrenamiento de la clase en una hoja::"

#: ../modules/tree.rst:125
msgid ":class:`DecisionTreeClassifier` is capable of both binary (where the labels are [-1, 1]) classification and multiclass (where the labels are [0, ..., K-1]) classification."
msgstr ":class:`DecisionTreeClassifier` es capaz de realizar tanto una clasificación binaria (donde las etiquetas son [-1, 1]) como una clasificación multiclase (donde las etiquetas son [0, ..., K-1])."

#: ../modules/tree.rst:129
msgid "Using the Iris dataset, we can construct a tree as follows::"
msgstr "Usando el conjunto de datos Iris, podemos construir un árbol como sigue::"

#: ../modules/tree.rst:137
msgid "Once trained, you can plot the tree with the :func:`plot_tree` function::"
msgstr "Una vez entrenado, puedes graficar el árbol con la función :func:`plot_tree`::"

#: ../modules/tree.rst:147
msgid "We can also export the tree in `Graphviz <https://www.graphviz.org/>`_ format using the :func:`export_graphviz` exporter. If you use the `conda <https://conda.io>`_ package manager, the graphviz binaries and the python package can be installed with `conda install python-graphviz`."
msgstr "También podemos exportar el árbol en formato `Graphviz <https://www.graphviz.org/>`_ usando el exportador :func:`export_graphviz`. Si utilizas el gestor de paquetes `conda <https://conda.io>`_, los binarios de graphviz y el paquete python pueden instalarse con `conda install python-graphviz`."

#: ../modules/tree.rst:152
msgid "Alternatively binaries for graphviz can be downloaded from the graphviz project homepage, and the Python wrapper installed from pypi with `pip install graphviz`."
msgstr "Alternativamente, los binarios para graphviz pueden ser descargados desde la página web del proyecto graphviz, y la capa de Python instalada desde pypi con `pip install graphviz`."

#: ../modules/tree.rst:155
msgid "Below is an example graphviz export of the above tree trained on the entire iris dataset; the results are saved in an output file `iris.pdf`::"
msgstr "A continuación se muestra un ejemplo de exportación en graphviz del árbol anterior entrenado en todo el conjunto de datos iris; los resultados se guardan en un archivo de salida `iris.pdf`::"

#: ../modules/tree.rst:164
msgid "The :func:`export_graphviz` exporter also supports a variety of aesthetic options, including coloring nodes by their class (or value for regression) and using explicit variable and class names if desired. Jupyter notebooks also render these plots inline automatically::"
msgstr "El exportador :func:`export_graphviz` también soporta una variedad de opciones estéticas, como colorear los nodos por su clase (o valor para la regresión) y usar nombres explícitos de variables y clases si así lo deseas. Los cuadernos de Jupyter también representan estos gráficos dentro de la línea automáticamente::"

#: ../modules/tree.rst:192
msgid "Alternatively, the tree can also be exported in textual format with the function :func:`export_text`. This method doesn't require the installation of external libraries and is more compact:"
msgstr "Alternativamente, el árbol también puede exportarse en formato de texto con la función :func:`export_text`. Este método no requiere la instalación de bibliotecas externas y es más compacto:"

#: ../modules/tree.rst:215
msgid ":ref:`sphx_glr_auto_examples_tree_plot_iris_dtc.py`"
msgstr ":ref:`sphx_glr_auto_examples_tree_plot_iris_dtc.py`"

#: ../modules/tree.rst:216
msgid ":ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`"
msgstr ":ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py`"

#: ../modules/tree.rst:221
msgid "Regression"
msgstr "Regresión"

#: ../modules/tree.rst:228
msgid "Decision trees can also be applied to regression problems, using the :class:`DecisionTreeRegressor` class."
msgstr "Los árboles de decisión también pueden ser aplicados en problemas de regresión, utilizando la clase :class:`DecisionTreeRegressor`."

#: ../modules/tree.rst:231
msgid "As in the classification setting, the fit method will take as argument arrays X and y, only that in this case y is expected to have floating point values instead of integer values::"
msgstr "Como en la configuración de la clasificación, el método de ajuste tomará como argumento los arreglos X e y, sólo que en este caso se espera que y tenga valores de punto flotante en lugar de valores enteros::"

#: ../modules/tree.rst:245
msgid ":ref:`sphx_glr_auto_examples_tree_plot_tree_regression.py`"
msgstr ":ref:`sphx_glr_auto_examples_tree_plot_tree_regression.py`"

#: ../modules/tree.rst:251
msgid "Multi-output problems"
msgstr "Problemas de salida múltiple"

#: ../modules/tree.rst:253
msgid "A multi-output problem is a supervised learning problem with several outputs to predict, that is when Y is a 2d array of shape ``(n_samples, n_outputs)``."
msgstr "Un problema de salida múltiple es un problema de aprendizaje supervisado con varias salidas a predecir, que es cuando Y es un arreglo 2d de la forma ``(n_samples, n_outputs)``."

#: ../modules/tree.rst:256
msgid "When there is no correlation between the outputs, a very simple way to solve this kind of problem is to build n independent models, i.e. one for each output, and then to use those models to independently predict each one of the n outputs. However, because it is likely that the output values related to the same input are themselves correlated, an often better way is to build a single model capable of predicting simultaneously all n outputs. First, it requires lower training time since only a single estimator is built. Second, the generalization accuracy of the resulting estimator may often be increased."
msgstr "Cuando no hay correlación entre las salidas, una forma muy sencilla de resolver este tipo de problema es construir n modelos independientes, es decir, uno para cada salida, y luego utilizar esos modelos para predecir independientemente cada una de las n salidas. Sin embargo, dado que es probable que los valores de salida relacionados con la misma entrada estén correlacionados, una forma a menudo mejor es construir un único modelo capaz de predecir simultáneamente todas las n salidas. En primer lugar, requiere menos tiempo de entrenamiento, ya que sólo se construye un único estimador. En segundo lugar, la precisión de la generalización del estimador resultante a menudo puede aumentar."

#: ../modules/tree.rst:265
msgid "With regard to decision trees, this strategy can readily be used to support multi-output problems. This requires the following changes:"
msgstr "Con respecto a los árboles de decisión, esta estrategia puede utilizarse fácilmente para soportar los problemas de salida múltiple. Esto requiere los siguientes cambios:"

#: ../modules/tree.rst:268
msgid "Store n output values in leaves, instead of 1;"
msgstr "Almacena n valores de salida en hojas, en lugar de 1;"

#: ../modules/tree.rst:269
msgid "Use splitting criteria that compute the average reduction across all n outputs."
msgstr "Utiliza criterios de separación que calculen la reducción promedio en todas las n salidas."

#: ../modules/tree.rst:272
msgid "This module offers support for multi-output problems by implementing this strategy in both :class:`DecisionTreeClassifier` and :class:`DecisionTreeRegressor`. If a decision tree is fit on an output array Y of shape ``(n_samples, n_outputs)`` then the resulting estimator will:"
msgstr "Este módulo ofrece soporte para problemas de salida múltiple implementando esta estrategia tanto en :class:`DecisionTreeClassifier` como en :class:`DecisionTreeRegressor`. Si un árbol de decisión se ajusta a un arreglo de salida Y de la forma ``(n_samples, n_outputs)`` entonces el estimador resultante será:"

#: ../modules/tree.rst:277
msgid "Output n_output values upon ``predict``;"
msgstr "Valores de salida n_output en ``predict``;"

#: ../modules/tree.rst:279
msgid "Output a list of n_output arrays of class probabilities upon ``predict_proba``."
msgstr "Da salida a una lista de arreglos n_output de probabilidades de clase sobre ``predict_proba``."

#: ../modules/tree.rst:283
msgid "The use of multi-output trees for regression is demonstrated in :ref:`sphx_glr_auto_examples_tree_plot_tree_regression_multioutput.py`. In this example, the input X is a single real value and the outputs Y are the sine and cosine of X."
msgstr "El uso de árboles de salida múltiple para regresión se demuestra en :ref:`sphx_glr_auto_examples_tree_plot_tree_regression_multioutput.py`. En este ejemplo, la entrada X es un único valor real y las salidas Y son el seno y el coseno de X."

#: ../modules/tree.rst:292
msgid "The use of multi-output trees for classification is demonstrated in :ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`. In this example, the inputs X are the pixels of the upper half of faces and the outputs Y are the pixels of the lower half of those faces."
msgstr "El uso de árboles de salida múltiple para la clasificación se demuestra en :ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`. En este ejemplo, las entradas X son los píxeles de la mitad superior de las caras y las salidas Y son los píxeles de la mitad inferior de esas caras."

#: ../modules/tree.rst:304
msgid ":ref:`sphx_glr_auto_examples_tree_plot_tree_regression_multioutput.py`"
msgstr ":ref:`sphx_glr_auto_examples_tree_plot_tree_regression_multioutput.py`"

#: ../modules/tree.rst:305
msgid ":ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`"
msgstr ":ref:`sphx_glr_auto_examples_miscellaneous_plot_multioutput_face_completion.py`"

#: ../modules/tree.rst:309
msgid "M. Dumont et al,  `Fast multi-class image annotation with random subwindows and multiple output randomized trees <http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf>`_, International Conference on Computer Vision Theory and Applications 2009"
msgstr "M. Dumont et al,  `Fast multi-class image annotation with random subwindows and multiple output randomized trees <http://www.montefiore.ulg.ac.be/services/stochastic/pubs/2009/DMWG09/dumont-visapp09-shortpaper.pdf>`_, International Conference on Computer Vision Theory and Applications 2009"

#: ../modules/tree.rst:317
msgid "Complexity"
msgstr "Complejidad"

#: ../modules/tree.rst:319
msgid "In general, the run time cost to construct a balanced binary tree is :math:`O(n_{samples}n_{features}\\log(n_{samples}))` and query time :math:`O(\\log(n_{samples}))`.  Although the tree construction algorithm attempts to generate balanced trees, they will not always be balanced.  Assuming that the subtrees remain approximately balanced, the cost at each node consists of searching through :math:`O(n_{features})` to find the feature that offers the largest reduction in entropy.  This has a cost of :math:`O(n_{features}n_{samples}\\log(n_{samples}))` at each node, leading to a total cost over the entire trees (by summing the cost at each node) of :math:`O(n_{features}n_{samples}^{2}\\log(n_{samples}))`."
msgstr "En general el costo de tiempo de ejecución para construir un árbol binario balanceado es :math:`O(n_{samples}n_{features}\\log(n_{samples}))` y el tiempo de consulta :math:`O(\\log(n_{samples}))`. Aunque el algoritmo de construcción de árboles intenta generar árboles equilibrados, no siempre lo estarán. Asumiendo que los subárboles permanecen aproximadamente equilibrados, el costo en cada nodo consiste en buscar a través de :math:`O(n_{features})` para encontrar la característica que ofrece la mayor reducción de entropía. Esto tiene un costo de :math:`O(n_{features}n_{samples}\\log(n_{samples}))` en cada nodo, lo que lleva a un costo total sobre todos los árboles (sumando el costo en cada nodo) de :math:`O(n_{features}n_{samples}^{2}\\log(n_{samples}))`."

#: ../modules/tree.rst:332
msgid "Tips on practical use"
msgstr "Consejos sobre uso práctico"

#: ../modules/tree.rst:334
msgid "Decision trees tend to overfit on data with a large number of features. Getting the right ratio of samples to number of features is important, since a tree with few samples in high dimensional space is very likely to overfit."
msgstr "Los árboles de decisión tienden a sobreajustarse en datos con un gran número de características. Es importante obtener la proporción adecuada de muestras con respecto al número de características, ya que un árbol con pocas muestras en un espacio de alta dimensión es muy probable que se sobreajuste."

#: ../modules/tree.rst:338
msgid "Consider performing  dimensionality reduction (:ref:`PCA <PCA>`, :ref:`ICA <ICA>`, or :ref:`feature_selection`) beforehand to give your tree a better chance of finding features that are discriminative."
msgstr "Considera la posibilidad de realizar una reducción de la dimensionalidad (:ref:`PCA <PCA>`, :ref:`ICA <ICA>`, o :ref:`feature_selection`) de antemano para dar a tu árbol una mejor oportunidad de encontrar características que sean discriminatorias."

#: ../modules/tree.rst:342
msgid ":ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` will help in gaining more insights about how the decision tree makes predictions, which is important for understanding the important features in the data."
msgstr ":ref:`sphx_glr_auto_examples_tree_plot_unveil_tree_structure.py` ayudará a obtener más información sobre cómo el árbol de decisión hace las predicciones, lo cual es importante para comprender las características importantes de los datos."

#: ../modules/tree.rst:346
msgid "Visualise your tree as you are training by using the ``export`` function.  Use ``max_depth=3`` as an initial tree depth to get a feel for how the tree is fitting to your data, and then increase the depth."
msgstr "Visualiza tu árbol mientras estás entrenando usando la función ``export``. Usa ``max_depth=3`` como profundidad inicial del árbol para tener una idea de cómo se ajusta a tus datos, y luego aumenta la profundidad."

#: ../modules/tree.rst:350
msgid "Remember that the number of samples required to populate the tree doubles for each additional level the tree grows to.  Use ``max_depth`` to control the size of the tree to prevent overfitting."
msgstr "Recuerda que el número de muestras requeridas para poblar el árbol se duplica por cada nivel adicional al que crece el árbol. Usa ``max_depth`` para controlar el tamaño del árbol para prevenir el sobreajuste."

#: ../modules/tree.rst:354
msgid "Use ``min_samples_split`` or ``min_samples_leaf`` to ensure that multiple samples inform every decision in the tree, by controlling which splits will be considered. A very small number will usually mean the tree will overfit, whereas a large number will prevent the tree from learning the data. Try ``min_samples_leaf=5`` as an initial value. If the sample size varies greatly, a float number can be used as percentage in these two parameters. While ``min_samples_split`` can create arbitrarily small leaves, ``min_samples_leaf`` guarantees that each leaf has a minimum size, avoiding low-variance, over-fit leaf nodes in regression problems.  For classification with few classes, ``min_samples_leaf=1`` is often the best choice."
msgstr "Utiliza ``min_samples_split`` o ``min_samples_leaf`` para asegurarte de que las muestras múltiples informan cada decisión en el árbol, controlando qué divisiones serán consideradas. Un número muy pequeño normalmente significa que el árbol se sobreajustará, mientras que un número grande impedirá que el árbol aprenda de los datos. Prueba ``min_samples_leaf=5`` como un valor inicial. Si el tamaño de la muestra varía considerablemente, se puede utilizar un número de punto flotante como porcentaje en estos dos parámetros. Mientras que ``min_samples_split`` puede crear hojas arbitrariamente pequeñas, ``min_samples_leaf`` garantiza que cada hoja tiene un tamaño mínimo, evitando nodos hoja de baja varianza y sobreajuste en problemas de regresión.  Para la clasificación con pocas clases, ``min_samples_leaf=1`` es a menudo la mejor opción."

#: ../modules/tree.rst:366
msgid "Note that ``min_samples_split`` considers samples directly and independent of ``sample_weight``, if provided (e.g. a node with m weighted samples is still treated as having exactly m samples). Consider ``min_weight_fraction_leaf`` or ``min_impurity_decrease`` if accounting for sample weights is required at splits."
msgstr "Ten en cuenta que ``min_samples_split`` considera las muestras directamente e independientemente de ``sample_weight``, si se proporciona (por ejemplo, un nodo con m muestras ponderadas se sigue tratando como si tuviera exactamente m muestras). Considera ``min_weight_fraction_leaf`` o ``min_impurity_decrease`` si se deben tener en cuenta los pesos de las muestras en las divisiones."

#: ../modules/tree.rst:371
msgid "Balance your dataset before training to prevent the tree from being biased toward the classes that are dominant. Class balancing can be done by sampling an equal number of samples from each class, or preferably by normalizing the sum of the sample weights (``sample_weight``) for each class to the same value. Also note that weight-based pre-pruning criteria, such as ``min_weight_fraction_leaf``, will then be less biased toward dominant classes than criteria that are not aware of the sample weights, like ``min_samples_leaf``."
msgstr "Equilibra tu conjunto de datos antes del entrenamiento para evitar que el árbol esté sesgado hacia las clases que son dominantes. El equilibrio de las clases puede hacerse muestreando un número igual de muestras de cada clase, o preferiblemente normalizando la suma de las ponderaciones de las muestras (``sample_weight``) para cada clase al mismo valor. También ten en cuenta que los criterios de pre-poda basados en ponderaciones, tales como ``min_weight_fraction_leaf``, entonces estarán menos sesgados hacia las clases dominantes que los criterios que no tienen en cuenta las ponderaciones de las muestras, como ``min_samples_leaf``."

#: ../modules/tree.rst:380
msgid "If the samples are weighted, it will be easier to optimize the tree structure using weight-based pre-pruning criterion such as ``min_weight_fraction_leaf``, which ensure that leaf nodes contain at least a fraction of the overall sum of the sample weights."
msgstr "Si las muestras están ponderadas, será más fácil optimizar la estructura del árbol usando un criterio de pre-poda basado en ponderaciones como ``min_weight_fraction_leaf``, que asegura que los nodos hoja contengan al menos una fracción de la suma total de las ponderaciones de las muestras."

#: ../modules/tree.rst:385
msgid "All decision trees use ``np.float32`` arrays internally. If training data is not in this format, a copy of the dataset will be made."
msgstr "Todos los árboles de decisión usan internamente arreglos ``np.float32``. Si los datos de entrenamiento no están en este formato, se hará una copia del conjunto de datos."

#: ../modules/tree.rst:388
msgid "If the input matrix X is very sparse, it is recommended to convert to sparse ``csc_matrix`` before calling fit and sparse ``csr_matrix`` before calling predict. Training time can be orders of magnitude faster for a sparse matrix input compared to a dense matrix when features have zero values in most of the samples."
msgstr "Si la matriz de entrada X es muy dispersa, se recomienda convertirla a dispersa ``csc_matrix`` antes de llamar a fit y dispersa ``csr_matrix`` antes de llamar a predict. El tiempo de entrenamiento pueden ser órdenes de magnitud más rápido para una matriz de entrada dispersa en comparación con una matriz densa cuando las características tienen valores cero en la mayoría de las muestras."

#: ../modules/tree.rst:398
msgid "Tree algorithms: ID3, C4.5, C5.0 and CART"
msgstr "Algoritmos de árbol: ID3, C4.5, C5.0 y CART"

#: ../modules/tree.rst:400
msgid "What are all the various decision tree algorithms and how do they differ from each other? Which one is implemented in scikit-learn?"
msgstr "¿Cuáles son los diferentes algoritmos de árboles de decisión y cómo se diferencian unos de otros? ¿Cuál está implementado en scikit-learn?"

#: ../modules/tree.rst:403
msgid "ID3_ (Iterative Dichotomiser 3) was developed in 1986 by Ross Quinlan. The algorithm creates a multiway tree, finding for each node (i.e. in a greedy manner) the categorical feature that will yield the largest information gain for categorical targets. Trees are grown to their maximum size and then a pruning step is usually applied to improve the ability of the tree to generalise to unseen data."
msgstr "ID3_ (Iterative Dichotomiser 3, Dicotomizador iterativo 3) fue desarrollado en 1986 por Ross Quinlan. El algoritmo crea un árbol multidireccional, encontrando para cada nodo (es decir, de forma codiciosa) la característica categórica que producirá la mayor ganancia de información para los objetivos categóricos. Los árboles crecen hasta su tamaño máximo y luego, se suele aplicar un paso de poda para mejorar la capacidad del árbol de generalizar a los datos no vistos."

#: ../modules/tree.rst:410
msgid "C4.5 is the successor to ID3 and removed the restriction that features must be categorical by dynamically defining a discrete attribute (based on numerical variables) that partitions the continuous attribute value into a discrete set of intervals. C4.5 converts the trained trees (i.e. the output of the ID3 algorithm) into sets of if-then rules. These accuracy of each rule is then evaluated to determine the order in which they should be applied. Pruning is done by removing a rule's precondition if the accuracy of the rule improves without it."
msgstr "C4.5 es el sucesor de ID3 y elimina la restricción de que las características sean categóricas definiendo dinámicamente un atributo discreto (basado en variables numéricas) que particiona el valor del atributo continuo en un conjunto discreto de intervalos. C4.5 convierte los árboles entrenados (es decir, la salida del algoritmo ID3) en conjuntos de reglas if-then (si-entonces). Esa precisión de cada regla se evalúa para determinar el orden en que deben aplicarse. La poda se realiza eliminando la condición previa de una regla si la precisión de ésta mejora sin ella."

#: ../modules/tree.rst:419
msgid "C5.0 is Quinlan's latest version release under a proprietary license. It uses less memory and builds smaller rulesets than C4.5 while being more accurate."
msgstr "C5.0 es la última versión publicada por Quinlan bajo una licencia propietaria. Utiliza menos memoria y construye conjuntos de reglas más pequeños que C4.5, a la vez que es más preciso."

#: ../modules/tree.rst:423
msgid "CART_ (Classification and Regression Trees) is very similar to C4.5, but it differs in that it supports numerical target variables (regression) and does not compute rule sets. CART constructs binary trees using the feature and threshold that yield the largest information gain at each node."
msgstr "CART_ (Classification and Regression Trees, Árboles de Clasificación y Regresión) es muy similar a C4.5, pero difiere en que soporta variables objetivo numéricas (regresión) y no calcula conjuntos de reglas. CART construye árboles binarios usando la característica y el umbral que producen la mayor ganancia de información en cada nodo."

#: ../modules/tree.rst:428
msgid "scikit-learn uses an optimised version of the CART algorithm; however, scikit-learn implementation does not support categorical variables for now."
msgstr "scikit-learn utiliza una versión optimizada del algoritmo CART; sin embargo, la implementación de scikit-learn no soporta variables categóricas por ahora."

#: ../modules/tree.rst:438
msgid "Mathematical formulation"
msgstr "Formulación matemática"

#: ../modules/tree.rst:440
msgid "Given training vectors :math:`x_i \\in R^n`, i=1,..., l and a label vector :math:`y \\in R^l`, a decision tree recursively partitions the feature space such that the samples with the same labels or similar target values are grouped together."
msgstr "Dados los vectores de entrenamiento :math:`x_i \\in R^n`, i=1,..., l y un vector de etiquetas :math:`y \\in R^l`, un árbol de decisión particiona recursivamente el espacio de características de tal manera que las muestras con las mismas etiquetas o valores objetivo similares se agrupan."

#: ../modules/tree.rst:445
msgid "Let the data at node :math:`m` be represented by :math:`Q_m` with :math:`N_m` samples. For each candidate split :math:`\\theta = (j, t_m)` consisting of a feature :math:`j` and threshold :math:`t_m`, partition the data into :math:`Q_m^{left}(\\theta)` and :math:`Q_m^{right}(\\theta)` subsets"
msgstr "Deja que los datos en el nodo :math:`m` sean representados por :math:`Q_m` con :math:`N_m` muestras. Para cada división candidata :math:`\\theta = (j, t_m)` que consiste en una característica :math:`j` y un umbral :math:`t_m`, particiona los datos en subconjuntos :math:`Q_m^{left}(\\theta)` y :math:`Q_m^{right}(\\theta)`"

#: ../modules/tree.rst:450
msgid "Q_m^{left}(\\theta) = \\{(x, y) | x_j <= t_m\\}\n\n"
"Q_m^{right}(\\theta) = Q_m \\setminus Q_m^{left}(\\theta)"
msgstr "Q_m^{left}(\\theta) = \\{(x, y) | x_j <= t_m\\}\n\n"
"Q_m^{right}(\\theta) = Q_m \\setminus Q_m^{left}(\\theta)"

#: ../modules/tree.rst:456
msgid "The quality of a candidate split of node :math:`m` is then computed using an impurity function or loss function :math:`H()`, the choice of which depends on the task being solved (classification or regression)"
msgstr "La calidad de una división candidata del nodo :math:`m` se calcula entonces utilizando una función de impureza o función de pérdida :math:`H()`, cuya elección depende de la tarea que se esté resolviendo (clasificación o regresión)"

#: ../modules/tree.rst:460
msgid "G(Q_m, \\theta) = \\frac{N_m^{left}}{N_m} H(Q_m^{left}(\\theta))\n"
"+ \\frac{N_m^{right}}{N_m} H(Q_m^{right}(\\theta))"
msgstr "G(Q_m, \\theta) = \\frac{N_m^{left}}{N_m} H(Q_m^{left}(\\theta))\n"
"+ \\frac{N_m^{right}}{N_m} H(Q_m^{right}(\\theta))"

#: ../modules/tree.rst:465
msgid "Select the parameters that minimises the impurity"
msgstr "Selecciona los parámetros que minimizan la impureza"

#: ../modules/tree.rst:467
msgid "\\theta^* = \\operatorname{argmin}_\\theta  G(Q_m, \\theta)"
msgstr "\\theta^* = \\operatorname{argmin}_\\theta  G(Q_m, \\theta)"

#: ../modules/tree.rst:471
msgid "Recurse for subsets :math:`Q_m^{left}(\\theta^*)` and :math:`Q_m^{right}(\\theta^*)` until the maximum allowable depth is reached, :math:`N_m < \\min_{samples}` or :math:`N_m = 1`."
msgstr "Recurre a los subconjuntos :math:`Q_m^{left}(\\theta^*)` y :math:`Q_m^{right}(\\theta^*)` hasta alcanzar la profundidad máxima permitida, :math:`N_m < \\min_{samples}` o :math:`N_m = 1`."

#: ../modules/tree.rst:476
msgid "Classification criteria"
msgstr "Criterios de clasificación"

#: ../modules/tree.rst:478
msgid "If a target is a classification outcome taking on values 0,1,...,K-1, for node :math:`m`, let"
msgstr "Si un objetivo es un resultado de clasificación que toma valores 0,1,...,K-1, para el nodo :math:`m`, sea"

#: ../modules/tree.rst:481
msgid "p_{mk} = 1/ N_m \\sum_{y \\in Q_m} I(y = k)"
msgstr "p_{mk} = 1/ N_m \\sum_{y \\in Q_m} I(y = k)"

#: ../modules/tree.rst:485
msgid "be the proportion of class k observations in node :math:`m`. If :math:`m` is a terminal node, `predict_proba` for this region is set to :math:`p_{mk}`. Common measures of impurity are the following."
msgstr "la proporción de observaciones de la clase k en el nodo :math:`m`. Si :math:`m` es un nodo terminal, `predict_proba` para esta región se establece en :math:`p_{mk}`. Las medidas comunes de impureza son las siguientes."

#: ../modules/tree.rst:489
msgid "Gini:"
msgstr "Gini:"

#: ../modules/tree.rst:491
msgid "H(Q_m) = \\sum_k p_{mk} (1 - p_{mk})"
msgstr "H(Q_m) = \\sum_k p_{mk} (1 - p_{mk})"

#: ../modules/tree.rst:495
msgid "Entropy:"
msgstr "Entropía:"

#: ../modules/tree.rst:497
msgid "H(Q_m) = - \\sum_k p_{mk} \\log(p_{mk})"
msgstr "H(Q_m) = - \\sum_k p_{mk} \\log(p_{mk})"

#: ../modules/tree.rst:501
msgid "Misclassification:"
msgstr "Clasificación errónea:"

#: ../modules/tree.rst:503
msgid "H(Q_m) = 1 - \\max(p_{mk})"
msgstr "H(Q_m) = 1 - \\max(p_{mk})"

#: ../modules/tree.rst:508
msgid "Regression criteria"
msgstr "Criterios de Regresión"

#: ../modules/tree.rst:510
msgid "If the target is a continuous value, then for node :math:`m`, common criteria to minimize as for determining locations for future splits are Mean Squared Error (MSE or L2 error), Poisson deviance as well as Mean Absolute Error (MAE or L1 error). MSE and Poisson deviance both set the predicted value of terminal nodes to the learned mean value :math:`\\bar{y}_m` of the node whereas the MAE sets the predicted value of terminal nodes to the median :math:`median(y)_m`."
msgstr "Si el objetivo es un valor continuo, entonces para el nodo :math:`m`, los criterios comunes a minimizar para determinar las ubicaciones de las futuras divisiones son el Error cuadrático medio (MSE o error L2), la desviación de Poisson así como el Error medio absoluto (MAE o error L1). El MSE y la desviación de Poisson establecen el valor predicho de los nodos terminales al valor medio aprendido :math:`\\bar{y}_m` del nodo mientras que el MAE establece el valor predicho de los nodos terminales a la mediana :math:`median(y)_m`."

#: ../modules/tree.rst:518
msgid "Mean Squared Error:"
msgstr "Error medio cuadrático:"

#: ../modules/tree.rst:520
msgid "\\bar{y}_m = \\frac{1}{N_m} \\sum_{y \\in Q_m} y\n\n"
"H(Q_m) = \\frac{1}{N_m} \\sum_{y \\in Q_m} (y - \\bar{y}_m)^2"
msgstr "\\bar{y}_m = \\frac{1}{N_m} \\sum_{y \\in Q_m} y\n\n"
"H(Q_m) = \\frac{1}{N_m} \\sum_{y \\in Q_m} (y - \\bar{y}_m)^2"

#: ../modules/tree.rst:526
msgid "Half Poisson deviance:"
msgstr "Desviación media de Poisson:"

#: ../modules/tree.rst:528
msgid "H(Q_m) = \\frac{1}{N_m} \\sum_{y \\in Q_m} (y \\log\\frac{y}{\\bar{y}_m}\n"
"- y + \\bar{y}_m)"
msgstr "H(Q_m) = \\frac{1}{N_m} \\sum_{y \\in Q_m} (y \\log\\frac{y}{\\bar{y}_m}\n"
"- y + \\bar{y}_m)"

#: ../modules/tree.rst:533
msgid "Setting `criterion=\"poisson\"` might be a good choice if your target is a count or a frequency (count per some unit). In any case, :math:`y >= 0` is a necessary condition to use this criterion. Note that it fits much slower than the MSE criterion."
msgstr "Establecer `criterion=\"poisson\"` puede ser una buena opción si tu objetivo es un conteo o una frecuencia (conteo por alguna unidad). En cualquier caso, :math:`y >= 0` es una condición necesaria para utilizar este criterio. Ten en cuenta que se ajusta mucho más lento que el criterio MSE."

#: ../modules/tree.rst:538
msgid "Mean Absolute Error:"
msgstr "Error medio absoluto:"

#: ../modules/tree.rst:540
msgid "median(y)_m = \\underset{y \\in Q_m}{\\mathrm{median}}(y)\n\n"
"H(Q_m) = \\frac{1}{N_m} \\sum_{y \\in Q_m} |y - median(y)_m|"
msgstr "median(y)_m = \\underset{y \\in Q_m}{\\mathrm{median}}(y)\n\n"
"H(Q_m) = \\frac{1}{N_m} \\sum_{y \\in Q_m} |y - median(y)_m|"

#: ../modules/tree.rst:546
msgid "Note that it fits much slower than the MSE criterion."
msgstr "Ten en cuenta que se ajusta mucho más lento que el criterio del MSE."

#: ../modules/tree.rst:552
msgid "Minimal Cost-Complexity Pruning"
msgstr "Poda de costo-complejidad mínima"

#: ../modules/tree.rst:554
msgid "Minimal cost-complexity pruning is an algorithm used to prune a tree to avoid over-fitting, described in Chapter 3 of [BRE]_. This algorithm is parameterized by :math:`\\alpha\\ge0` known as the complexity parameter. The complexity parameter is used to define the cost-complexity measure, :math:`R_\\alpha(T)` of a given tree :math:`T`:"
msgstr "La poda de costo-complejidad mínima es un algoritmo utilizado para podar un árbol para evitar el sobreajuste, descrito en el capítulo 3 de [BRE]_. Este algoritmo está parametrizado por :math:`\\alpha\\ge0` conocido como el parámetro de complejidad. El parámetro de complejidad se utiliza para definir la medida de costo-complejidad, :math:`R_\\alpha(T)` de un árbol :math:`T` dado:"

#: ../modules/tree.rst:560
msgid "R_\\alpha(T) = R(T) + \\alpha|\\widetilde{T}|"
msgstr "R_\\alpha(T) = R(T) + \\alpha|\\widetilde{T}|"

#: ../modules/tree.rst:564
msgid "where :math:`|\\widetilde{T}|` is the number of terminal nodes in :math:`T` and :math:`R(T)` is traditionally defined as the total misclassification rate of the terminal nodes. Alternatively, scikit-learn uses the total sample weighted impurity of the terminal nodes for :math:`R(T)`. As shown above, the impurity of a node depends on the criterion. Minimal cost-complexity pruning finds the subtree of :math:`T` that minimizes :math:`R_\\alpha(T)`."
msgstr "donde :math:`|\\widetilde{T}|` es el número de nodos terminales en :math:`T` y :math:`R(T)` se define tradicionalmente como la tasa total de clasificación errónea de los nodos terminales. Alternativamente, scikit-learn utiliza la impureza total ponderada de la muestra de los nodos terminales para :math:`R(T)`. Como se muestra arriba, la impureza de un nodo depende del criterio. La poda de costo-complejidad mínima encuentra el subárbol de :math:`T` que minimiza :math:`R_alpha(T)`."

#: ../modules/tree.rst:571
msgid "The cost complexity measure of a single node is :math:`R_\\alpha(t)=R(t)+\\alpha`. The branch, :math:`T_t`, is defined to be a tree where node :math:`t` is its root. In general, the impurity of a node is greater than the sum of impurities of its terminal nodes, :math:`R(T_t)<R(t)`. However, the cost complexity measure of a node, :math:`t`, and its branch, :math:`T_t`, can be equal depending on :math:`\\alpha`. We define the effective :math:`\\alpha` of a node to be the value where they are equal, :math:`R_\\alpha(T_t)=R_\\alpha(t)` or :math:`\\alpha_{eff}(t)=\\frac{R(t)-R(T_t)}{|T|-1}`. A non-terminal node with the smallest value of :math:`\\alpha_{eff}` is the weakest link and will be pruned. This process stops when the pruned tree's minimal :math:`\\alpha_{eff}` is greater than the ``ccp_alpha`` parameter."
msgstr "La medida de costo-complejidad de un único nodo es :math:`R_\\alpha(t)=R(t)+\\alpha`. La rama, :math:`T_t`, se define como un árbol donde el nodo :math:`t` es su raíz. En general, la impureza de un nodo es mayor que la suma de impurezas de sus nodos terminales, :math:`R(T_t)<R(t)`. Sin embargo, la medida de costo-complejidad de un nodo, :math:`t`, y su rama, :math:`T_t`, pueden ser iguales dependiendo de :math:`alpha`. Definimos el :math:`alpha` efectivo de un nodo como el valor donde son iguales, :math:`R_\\\\alpha(T_t)=R_\\\\alpha(t)` o :math:`\\\\alpha_{eff}(t)=\\\\frac{R(t)-R(T_t)}{|T|-1}`. Un nodo no terminal con el valor más pequeño de :math:`\\alpha_{eff}` es el enlace más débil y será podado. Este proceso se detiene cuando el valor mínimo de :math:`\\alpha_{eff}` del árbol podado es mayor que el parámetro ``ccp_alpha``."

#: ../modules/tree.rst:586
msgid ":ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`"
msgstr ":ref:`sphx_glr_auto_examples_tree_plot_cost_complexity_pruning.py`"

#: ../modules/tree.rst:590
msgid "L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984."
msgstr "L. Breiman, J. Friedman, R. Olshen, and C. Stone. Classification and Regression Trees. Wadsworth, Belmont, CA, 1984."

#: ../modules/tree.rst:593
msgid "https://en.wikipedia.org/wiki/Decision_tree_learning"
msgstr "https://en.wikipedia.org/wiki/Decision_tree_learning"

#: ../modules/tree.rst:595
msgid "https://en.wikipedia.org/wiki/Predictive_analytics"
msgstr "https://en.wikipedia.org/wiki/Predictive_analytics"

#: ../modules/tree.rst:597
msgid "J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993."
msgstr "J.R. Quinlan. C4. 5: programs for machine learning. Morgan Kaufmann, 1993."

#: ../modules/tree.rst:600
msgid "T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009."
msgstr "T. Hastie, R. Tibshirani and J. Friedman. Elements of Statistical Learning, Springer, 2009."

