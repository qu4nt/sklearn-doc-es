msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-29 02:34\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/lda_qda.po\n"
"X-Crowdin-File-ID: 4808\n"
"Language: es_ES\n"

#: ../modules/lda_qda.rst:5
msgid "Linear and Quadratic Discriminant Analysis"
msgstr "Análisis Discriminante Lineal y Cuadrático"

#: ../modules/lda_qda.rst:9
msgid "Linear Discriminant Analysis (:class:`~discriminant_analysis.LinearDiscriminantAnalysis`) and Quadratic Discriminant Analysis (:class:`~discriminant_analysis.QuadraticDiscriminantAnalysis`) are two classic classifiers, with, as their names suggest, a linear and a quadratic decision surface, respectively."
msgstr "El análisis lineal de discriminantes (:class:`~discriminant_analysis.LinearDiscriminantAnalysis`) y el análisis discriminante cuadrático (:class:`~discriminant_analysis.QuadraticDiscriminantAnalysis`) son dos clasificadores clásicos que tienen, como su nombre lo indica, una superficie de decisión lineal y cuadrática, respectivamente."

#: ../modules/lda_qda.rst:16
msgid "These classifiers are attractive because they have closed-form solutions that can be easily computed, are inherently multiclass, have proven to work well in practice, and have no hyperparameters to tune."
msgstr "Estos clasificadores son atractivos porque tienen soluciones de forma cerrada que pueden ser calculadas fácilmente, son inherentemente multiclase, se ha demostrado que funcionan bien en la practica, y no tienen hiperparámetros que ajustar."

#: ../modules/lda_qda.rst:25
msgid "ldaqda"
msgstr "ldaqda"

#: ../modules/lda_qda.rst:26
msgid "The plot shows decision boundaries for Linear Discriminant Analysis and Quadratic Discriminant Analysis. The bottom row demonstrates that Linear Discriminant Analysis can only learn linear boundaries, while Quadratic Discriminant Analysis can learn quadratic boundaries and is therefore more flexible."
msgstr "El gráfico muestra los límites de decisión para el Análisis Discriminante Lineal y el Análisis Discriminante Cuadrático. La fila de abajo demuestra que el Análisis Discriminante Lineal solo puede aprender límites lineales, mientras que el Análisis Discriminante Cuadrático puede aprender límites cuadráticos y es por lo tanto mas flexible."

msgid "Examples:"
msgstr "Ejemplos:"

#: ../modules/lda_qda.rst:34
msgid ":ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`: Comparison of LDA and QDA on synthetic data."
msgstr ":ref:`sphx_glr_auto_examples_classification_plot_lda_qda.py`: Comparación de ADL y ADQ en datos sintéticos."

#: ../modules/lda_qda.rst:38
msgid "Dimensionality reduction using Linear Discriminant Analysis"
msgstr "Reducción de dimensionalidad usando el Análisis Discriminante Lineal"

#: ../modules/lda_qda.rst:40
msgid ":class:`~discriminant_analysis.LinearDiscriminantAnalysis` can be used to perform supervised dimensionality reduction, by projecting the input data to a linear subspace consisting of the directions which maximize the separation between classes (in a precise sense discussed in the mathematics section below). The dimension of the output is necessarily less than the number of classes, so this is in general a rather strong dimensionality reduction, and only makes sense in a multiclass setting."
msgstr "El :class:`~discriminant_analysis.LinearDiscriminantAnalysis` puede ser utilizado para realizar una reducción de dimensionalidad supervisada, mediante la proyección de los datos de entrada a un subespacio lineal consistiendo de las direcciones que maximizen la separación entre las clases (en un sentido preciso discutido en la sección de matemáticas abajo). La dimensión de la salida es necesariamente menor que el número de las clases, por lo que esto es en general una reducción de dimensionalidad bastante fuerte, y solo tiene sentido en un ajustamiento multiclase."

#: ../modules/lda_qda.rst:48
msgid "This is implemented in the `transform` method. The desired dimensionality can be set using the ``n_components`` parameter. This parameter has no influence on the `fit` and `predict` methods."
msgstr "Esto está implementado en el método `transform`. La dimensionalidad deseada puede establecerse usando el parámetro ``n_components``. Este parámetro no tiene influencia en los métodos `fit` y `predict`."

#: ../modules/lda_qda.rst:54
msgid ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`: Comparison of LDA and PCA for dimensionality reduction of the Iris dataset"
msgstr ":ref:`sphx_glr_auto_examples_decomposition_plot_pca_vs_lda.py`: Comparación de ADL y PCA para la reducción de dimensionalidad del conjunto de datos Iris"

#: ../modules/lda_qda.rst:60
msgid "Mathematical formulation of the LDA and QDA classifiers"
msgstr "Formulación matemática de los clasificadores ADL y ADQ"

#: ../modules/lda_qda.rst:62
msgid "Both LDA and QDA can be derived from simple probabilistic models which model the class conditional distribution of the data :math:`P(X|y=k)` for each class :math:`k`. Predictions can then be obtained by using Bayes' rule, for each training sample :math:`x \\in \\mathcal{R}^d`:"
msgstr "Tanto ADL como ADQ pueden ser derivados de modelos probabilisticos sencillos que modelan la distribución condicionada por la clase de los datos :math:`P(X|y=k)` por cada clase :math:`k`. Las predicciones pueden entonces ser obtenidas usando la regla de Bayes, para cada muestra de entrenamiento :math:`x \\in \\mathcal{R}^d`:"

#: ../modules/lda_qda.rst:67
msgid "P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}\n\n"
msgstr "P(y=k | x) = \\frac{P(x | y=k) P(y=k)}{P(x)} = \\frac{P(x | y=k) P(y = k)}{ \\sum_{l} P(x | y=l) \\cdot P(y=l)}\n\n"

#: ../modules/lda_qda.rst:70
msgid "and we select the class :math:`k` which maximizes this posterior probability."
msgstr "y seleccionamos la clase :math:`k` que maximiza esta probabilidad posterior."

#: ../modules/lda_qda.rst:72
msgid "More specifically, for linear and quadratic discriminant analysis, :math:`P(x|y)` is modeled as a multivariate Gaussian distribution with density:"
msgstr "Más específicamente, para el análisis discriminante lineal y cuadrático, :math:`P(x|y)` está modelado como una distribución Gaussiana multivariante con densidad:"

#: ../modules/lda_qda.rst:76
msgid "P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n\n"
msgstr "P(x | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k)\\right)\n\n"

#: ../modules/lda_qda.rst:78
msgid "where :math:`d` is the number of features."
msgstr "donde :math:`d` es el número de características."

#: ../modules/lda_qda.rst:81
msgid "QDA"
msgstr "ADQ"

#: ../modules/lda_qda.rst:83
msgid "According to the model above, the log of the posterior is:"
msgstr "De acuerdo con el modelo anterior, el log del posterior es:"

#: ../modules/lda_qda.rst:85
msgid "\\log P(y=k | x) &= \\log P(x | y=k) + \\log P(y = k) + Cst \\\\\n"
"&= -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst,"
msgstr "\\log P(y=k | x) &= \\log P(x | y=k) + \\log P(y = k) + Cst \\\\\n"
"&= -\\frac{1}{2} \\log |\\Sigma_k| -\\frac{1}{2} (x-\\mu_k)^t \\Sigma_k^{-1} (x-\\mu_k) + \\log P(y = k) + Cst,"

#: ../modules/lda_qda.rst:90
msgid "where the constant term :math:`Cst` corresponds to the denominator :math:`P(x)`, in addition to other constant terms from the Gaussian. The predicted class is the one that maximises this log-posterior."
msgstr "donde el termino constante :math:`Cst` corresponde al denominador :math:`P(x)`, junto a los otros terminos constantes del Gaussiano. La clase predicha es aquella que maximiza este log-posterior."

#: ../modules/lda_qda.rst:94
msgid "**Relation with Gaussian Naive Bayes**"
msgstr "**Relación con Bayes ingenuo Gaussiano**"

#: ../modules/lda_qda.rst:96
msgid "If in the QDA model one assumes that the covariance matrices are diagonal, then the inputs are assumed to be conditionally independent in each class, and the resulting classifier is equivalent to the Gaussian Naive Bayes classifier :class:`naive_bayes.GaussianNB`."
msgstr "Si en el modelo ADQ uno asumo que las matrices de covarianza son diagonales, entonces se asume que las entradas son condicionalmente independientes en cada clase, y que el clasificador resultante es equivalente al clasificador Bayesiano Ingenuo Gaussiano :class:`naive_bayes.GaussianNB`."

#: ../modules/lda_qda.rst:102
msgid "LDA"
msgstr "ADL"

#: ../modules/lda_qda.rst:104
msgid "LDA is a special case of QDA, where the Gaussians for each class are assumed to share the same covariance matrix: :math:`\\Sigma_k = \\Sigma` for all :math:`k`. This reduces the log posterior to:"
msgstr "El ADL es un caso especial de ADQ, donde se asume que los Gaussianos de cada clase comparten la misma matriz de covarianza: :math:`\\Sigma_k = \\Sigma` para todo :math:`k`. Esto reduce el log posterior a:"

#: ../modules/lda_qda.rst:108
msgid "\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.\n\n"
msgstr "\\log P(y=k | x) = -\\frac{1}{2} (x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k) + \\log P(y = k) + Cst.\n\n"

#: ../modules/lda_qda.rst:110
msgid "The term :math:`(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)` corresponds to the `Mahalanobis Distance <https://en.wikipedia.org/wiki/Mahalanobis_distance>`_ between the sample :math:`x` and the mean :math:`\\mu_k`. The Mahalanobis distance tells how close :math:`x` is from :math:`\\mu_k`, while also accounting for the variance of each feature. We can thus interpret LDA as assigning :math:`x` to the class whose mean is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities."
msgstr "El termino :math:`(x-\\mu_k)^t \\Sigma^{-1} (x-\\mu_k)` corresponde a la Distancia de Mahalanobis <https://es.wikipedia.org/wiki/Distancia_de_Mahalanobis>`_ desde la muestra :math:`x` y la media :math:`\\mu_k`. La distancia de Mahalanobis nos dice que tan cerca esta :math:`x` desde :math:`\\mu_k`, mientras que se toma en cuenta la varianza de cada característica. Podemos entonces interpretar al ADL como asignar :math:`x` a aquella clase cuya media es la mas cercana en terminos de distancia de Mahalanobis, mientras también se contabilizan las probabilidades previas de la clase."

#: ../modules/lda_qda.rst:119
msgid "The log-posterior of LDA can also be written [3]_ as:"
msgstr "El log-posterior de ADL también se puede escribir [3]_ como:"

#: ../modules/lda_qda.rst:121
msgid "\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst."
msgstr "\\log P(y=k | x) = \\omega_k^t x + \\omega_{k0} + Cst."

#: ../modules/lda_qda.rst:125
msgid "where :math:`\\omega_k = \\Sigma^{-1} \\mu_k` and :math:`\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)`. These quantities correspond to the `coef_` and `intercept_` attributes, respectively."
msgstr "donde :math:`\\omega_k = \\Sigma^{-1} \\mu_k` y :math:`\\omega_{k0} = -\\frac{1}{2} \\mu_k^t\\Sigma^{-1}\\mu_k + \\log P (y = k)`. Estas cantidades corresponden a los atributos `coef_` y `intercept_`, respectivamente."

#: ../modules/lda_qda.rst:129
msgid "From the above formula, it is clear that LDA has a linear decision surface. In the case of QDA, there are no assumptions on the covariance matrices :math:`\\Sigma_k` of the Gaussians, leading to quadratic decision surfaces. See [1]_ for more details."
msgstr "De la fórmula anterior, está claro que el ADL tiene una superficie de decisión lineal. En el caso del ADQ, no hay suposiciones sobre las matrices de covarianza :math:`\\Sigma_k` de los gaussianos, lo que lleva a superficies cuadráticas de decisión. Ver [1]_ para más detalles."

#: ../modules/lda_qda.rst:135
msgid "Mathematical formulation of LDA dimensionality reduction"
msgstr "Formulación matemática de la reducción de dimensionalidad del ADL"

#: ../modules/lda_qda.rst:137
msgid "First note that the K means :math:`\\mu_k` are vectors in :math:`\\mathcal{R}^d`, and they lie in an affine subspace :math:`H` of dimension at least :math:`K - 1` (2 points lie on a line, 3 points lie on a plane, etc)."
msgstr "Primero note que la K significa que :math:`\\mu_k` son vectores en :math:`\\mathcal{R}^d`, y que se encuentran en un subespacio afino :math:`H` de dimensión al menos :math:`K - 1` (2 puntos yacen en una linea, 3 points en un plano, etc)."

#: ../modules/lda_qda.rst:142
msgid "As mentioned above, we can interpret LDA as assigning :math:`x` to the class whose mean :math:`\\mu_k` is the closest in terms of Mahalanobis distance, while also accounting for the class prior probabilities. Alternatively, LDA is equivalent to first *sphering* the data so that the covariance matrix is the identity, and then assigning :math:`x` to the closest mean in terms of Euclidean distance (still accounting for the class priors)."
msgstr "Como se mencionó anteriormente, podemos interpretar ADL como asignar :math:`x` a la clase cuya media :math:`\\mu_k` es la mas cercana en terminos de distancia de Mahalanobis, también contabilizando por las probabilidades previas de la clase. Alternativamente, LDA es equivalente a primero *esferificar* los datos para que la matriz de covarianza sea la identidad, luego asignando :math:`x` a la media mas cercana en terminos de distancia Euclidiana (igualmente tomando en cuenta las previas clases)."

#: ../modules/lda_qda.rst:149
msgid "Computing Euclidean distances in this d-dimensional space is equivalent to first projecting the data points into :math:`H`, and computing the distances there (since the other dimensions will contribute equally to each class in terms of distance). In other words, if :math:`x` is closest to :math:`\\mu_k` in the original space, it will also be the case in :math:`H`. This shows that, implicit in the LDA classifier, there is a dimensionality reduction by linear projection onto a :math:`K-1` dimensional space."
msgstr "Calcular distancias Euclídeas en este espacio d-dimensional es equivalente a primero proyectar los puntos de datos en :math:`H`, y calcular las distancias ahí (ya que las otras dimensiones van a contribuir de manera igual a cada clase en termínos de distancia). En otras palabras, si :math:`k` es el mas cercano a :math:`\\mu_k` en el espacio original, también sera el caso en :math:`H`. Esto nos muestra que hay una reducción de dimensionalidad por proyección lineal hacía un espacio de :math:`K-1` dimensiones implícita en el clasificador ADL."

#: ../modules/lda_qda.rst:158
msgid "We can reduce the dimension even more, to a chosen :math:`L`, by projecting onto the linear subspace :math:`H_L` which maximizes the variance of the :math:`\\mu^*_k` after projection (in effect, we are doing a form of PCA for the transformed class means :math:`\\mu^*_k`). This :math:`L` corresponds to the ``n_components`` parameter used in the :func:`~discriminant_analysis.LinearDiscriminantAnalysis.transform` method. See [1]_ for more details."
msgstr "Podemos reducir la dimensión aún mas, a un :math:`L` elegido, si proyectamos hacia el subespacio lineal :math:`H_L` que maximiza la varianza de :math:`\\mu^*_k` después de la proyección (en efecto, estamos haciendo una forma de PCA para las medias de clase transformadas :math:`\\mu^*_k`). Este :math:`L` corresponde al parámetro ``n_components`` usado en el método :func:`~discriminant_analysis.LinearDiscriminantAnalysis.transform`. Ver [1]_ para más detalles."

#: ../modules/lda_qda.rst:167
msgid "Shrinkage and Covariance Estimator"
msgstr "Estimador de Reducción y Covarianza"

#: ../modules/lda_qda.rst:169
msgid "Shrinkage is a form of regularization used to improve the estimation of covariance matrices in situations where the number of training samples is small compared to the number of features. In this scenario, the empirical sample covariance is a poor estimator, and shrinkage helps improving the generalization performance of the classifier. Shrinkage LDA can be used by setting the ``shrinkage`` parameter of the :class:`~discriminant_analysis.LinearDiscriminantAnalysis` class to 'auto'. This automatically determines the optimal shrinkage parameter in an analytic way following the lemma introduced by Ledoit and Wolf [2]_. Note that currently shrinkage only works when setting the ``solver`` parameter to 'lsqr' or 'eigen'."
msgstr "La reducción es una forma de regularización usada para mejorar la estimación de matrices de covarianza en situaciones donde el número de muestras de entrenamiento es pequeño comparado al número de características. En este escenario, la covarianza de muestras empírica es un estimador pobre, y la reducción ayuda a mejorar el rendimiento de generalización del clasificador. El ADL de reducción se puede utilizar estableciendo el parámetro ``shrinkage`` de la clase :class:`~discriminant_analysis.LinearDiscriminantAnalysis` como 'auto'. Esto automáticamente determina que el parámetro óptimo de reducción siguiendo el lemma introducido por Ledoit y Wolf [2]_. Note que por el momento la reducción solo funciona cuando el parámetro ``solver`` esta configurado como 'lsqr' o 'eigen'."

#: ../modules/lda_qda.rst:182
msgid "The ``shrinkage`` parameter can also be manually set between 0 and 1. In particular, a value of 0 corresponds to no shrinkage (which means the empirical covariance matrix will be used) and a value of 1 corresponds to complete shrinkage (which means that the diagonal matrix of variances will be used as an estimate for the covariance matrix). Setting this parameter to a value between these two extrema will estimate a shrunk version of the covariance matrix."
msgstr "El parámetro ``shrinkage`` también puede establecerse manualmente entre 0 y 1. En particular, un valor de 0 corresponde a ninguna reducción (lo cual significa que la matriz de covarianza empírica sera utilizada), y un valor de 1 corresponde a reducción total (lo cual significa que la matriz diagonal de varianzas sera utilizada como un estimado para la matriz de covarianza). Si se pasa un valor entre estos dos extremos a este parámetro, se estimara una versión reducida del matriz de covarianza."

#: ../modules/lda_qda.rst:190
msgid "The shrinked Ledoit and Wolf estimator of covariance may not always be the best choice. For example if the distribution of the data is normally distributed, the Oracle Shrinkage Approximating estimator :class:`sklearn.covariance.OAS` yields a smaller Mean Squared Error than the one given by Ledoit and Wolf's formula used with shrinkage=\"auto\". In LDA, the data are assumed to be gaussian conditionally to the class. If these assumptions hold, using LDA with the OAS estimator of covariance will yield a better classification accuracy than if Ledoit and Wolf or the empirical covariance estimator is used."
msgstr "El estimador de covarianza Ledoit y Wolf reducido quizás no sea siempre la mejor opción. Por ejemplo, si la distribución de lo datos esta normalmente distribuida, el estimador Oracle Shrinkage Approximating :class:`sklearn.covariance.OAS` da un menor error cuadrático medio que el dado por la formula de Ledoit y Wolf usada con shrinkage=\"auto\". En ADL, los datos se presumen que son condicionalmente gaussianos a la clase. Si estos supuestos se cumplen, usar ADL con el estimador de covarianza OAS dará una mejor precisión de clasificación que si se usan los estimadores Lediot y Wolf o de covarianza empírica."

#: ../modules/lda_qda.rst:200
msgid "The covariance estimator can be chosen using with the ``covariance_estimator`` parameter of the :class:`discriminant_analysis.LinearDiscriminantAnalysis` class. A covariance estimator should have a :term:`fit` method and a ``covariance_`` attribute like all covariance estimators in the :mod:`sklearn.covariance` module."
msgstr "El estimador de covarianza se puede escoger utilizando el parámetro ``covariance_estimator`` de la clase :class:`discriminant_analysis.LinearDiscriminantAnalysis`. Un estimador de covarianza debería tener un método :term:`fit` y un atributo ``covariance_`` como todos los estimadores de covarianza en el módulo :mod:`sklearn.covariance`."

#: ../modules/lda_qda.rst:212
msgid "shrinkage"
msgstr "reducción"

#: ../modules/lda_qda.rst:215
msgid ":ref:`sphx_glr_auto_examples_classification_plot_lda.py`: Comparison of LDA classifiers with Empirical, Ledoit Wolf and OAS covariance estimator."
msgstr ":ref:`sphx_glr_auto_examples_classification_plot_lda.py`: Comparación de los clasificadores ADL con los estimadores de covarianza Empíricos, Ledoit Wolf y OAS."

#: ../modules/lda_qda.rst:219
msgid "Estimation algorithms"
msgstr "Algoritmos de estimación"

#: ../modules/lda_qda.rst:221
msgid "Using LDA and QDA requires computing the log-posterior which depends on the class priors :math:`P(y=k)`, the class means :math:`\\mu_k`, and the covariance matrices."
msgstr "Usar ADL y ADQ requiere calcular el log-posterior que depende en los antecedentes de clase :math:`P(y=k)`, los medios de clase :math:`\\mu_k` y las matrices de covarianza."

#: ../modules/lda_qda.rst:225
msgid "The 'svd' solver is the default solver used for :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, and it is the only available solver for :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`. It can perform both classification and transform (for LDA). As it does not rely on the calculation of the covariance matrix, the 'svd' solver may be preferable in situations where the number of features is large. The 'svd' solver cannot be used with shrinkage. For QDA, the use of the SVD solver relies on the fact that the covariance matrix :math:`\\Sigma_k` is, by definition, equal to :math:`\\frac{1}{n - 1} X_k^tX_k = V S^2 V^t` where :math:`V` comes from the SVD of the (centered) matrix: :math:`X_k = U S V^t`. It turns out that we can compute the log-posterior above without having to explictly compute :math:`\\Sigma`: computing :math:`S` and :math:`V` via the SVD of :math:`X` is enough. For LDA, two SVDs are computed: the SVD of the centered input matrix :math:`X` and the SVD of the class-wise mean vectors."
msgstr "El solucionador 'svd' es el solucionador por defecto utilizado por :class:`~sklearn.discriminant_analysis.LinearDiscriminantAnalysis`, y es el único solucionador disponible para :class:`~sklearn.discriminant_analysis.QuadraticDiscriminantAnalysis`. Puede realizar tanto clasificación como transformación (para ADL). Ya que no se basa en el calculo de la matriz de covarianza, el solucionador 'svd' podría ser preferible en situaciones donde el número de características es grande. El solucionador 'svd' no puede ser utilizado con reducción. Para el ADQ, el uso del solucionador SVD se sostiene en el hecho de que la matriz de covarianza :math:`\\Sigma_k` es, por definición, igual a :math:`\\frac{1}{n - 1} X_k^tX_k = V S^2 V^t`, donde :math:`V` viene del SVD de la matriz (centrada): :math:`X_k = U S V^t`. Resulta que podemos calcular el log-posterior anterior sin necesitar calcular explícitamente :math:`\\Sigma`: calcular :math:`S` y :math:`V` mediante el SVD de :math:`X` es suficiente. Para el ADL, se calculan dos SVD: el SVD de la matríz de entrada centrada :math:`X` y el SVD de los vectores medios por clase."

#: ../modules/lda_qda.rst:242
msgid "The 'lsqr' solver is an efficient algorithm that only works for classification. It needs to explicitly compute the covariance matrix :math:`\\Sigma`, and supports shrinkage and custom covariance estimators. This solver computes the coefficients :math:`\\omega_k = \\Sigma^{-1}\\mu_k` by solving for :math:`\\Sigma \\omega = \\mu_k`, thus avoiding the explicit computation of the inverse :math:`\\Sigma^{-1}`."
msgstr "El solucionador 'lsqr' es un algoritmo eficiente que sólo funciona para la clasificación. Necesita calcular explicitamente la covarianza de matriz :math:`\\Sigma`, y soporta estimadores de reducción y covarianza personalizada. Este solucionador calcula los coeficientes :math:`\\omega_k = \\Sigma^{-1}\\mu_k` resolviendo :math:`\\Sigma \\omega = \\mu_k`, así evitando el calculo explicito de la inversa :math:`\\Sigma^{-1}`."

#: ../modules/lda_qda.rst:250
msgid "The 'eigen' solver is based on the optimization of the between class scatter to within class scatter ratio. It can be used for both classification and transform, and it supports shrinkage. However, the 'eigen' solver needs to compute the covariance matrix, so it might not be suitable for situations with a high number of features."
msgstr "El solucionador 'eigen' esta basado en la optimización de la razón entre la dispersión entre clases y la dispersión dentro de clases. Puede ser utilizada tanto para clasificación como transformación, y soporta la reducción. Sin embargo, el solucionador 'eigen' necesita calcular la matriz de covarianza, así que quizás no sea adecuado para situaciones con un gran número de características."

#: ../modules/lda_qda.rst:258
msgid "\"The Elements of Statistical Learning\", Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008."
msgstr "\"The Elements of Statistical Learning\", Hastie T., Tibshirani R., Friedman J., Section 4.3, p.106-119, 2008."

#: ../modules/lda_qda.rst:261
msgid "Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004."
msgstr "Ledoit O, Wolf M. Honey, I Shrunk the Sample Covariance Matrix. The Journal of Portfolio Management 30(4), 110-119, 2004."

#: ../modules/lda_qda.rst:264
msgid "R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification (Second Edition), section 2.6.2."
msgstr "R. O. Duda, P. E. Hart, D. G. Stork. Pattern Classification (Second Edition), section 2.6.2."

