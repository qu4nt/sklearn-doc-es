msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 04:38\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/partial_dependence.po\n"
"X-Crowdin-File-ID: 4818\n"
"Language: es_ES\n"

#: ../modules/partial_dependence.rst:6
msgid "Partial Dependence and Individual Conditional Expectation plots"
msgstr ""

#: ../modules/partial_dependence.rst:10
msgid "Partial dependence plots (PDP) and individual conditional expectation (ICE) plots can be used to visualize and analyze interaction between the target response [1]_ and a set of input features of interest."
msgstr ""

#: ../modules/partial_dependence.rst:14
msgid "Both PDPs and ICEs assume that the input features of interest are independent from the complement features, and this assumption is often violated in practice. Thus, in the case of correlated features, we will create absurd data points to compute the PDP/ICE."
msgstr ""

#: ../modules/partial_dependence.rst:20
msgid "Partial dependence plots"
msgstr ""

#: ../modules/partial_dependence.rst:22
msgid "Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features (the 'complement' features). Intuitively, we can interpret the partial dependence as the expected target response as a function of the input features of interest."
msgstr ""

#: ../modules/partial_dependence.rst:28
msgid "Due to the limits of human perception the size of the set of input feature of interest must be small (usually, one or two) thus the input features of interest are usually chosen among the most important features."
msgstr ""

#: ../modules/partial_dependence.rst:32
msgid "The figure below shows two one-way and one two-way partial dependence plots for the California housing dataset, with a :class:`HistGradientBoostingRegressor <sklearn.ensemble.HistGradientBoostingRegressor>`:"
msgstr ""

#: ../modules/partial_dependence.rst:41
msgid "One-way PDPs tell us about the interaction between the target response and an input feature of interest feature (e.g. linear, non-linear). The left plot in the above figure shows the effect of the average occupancy on the median house price; we can clearly see a linear relationship among them when the average occupancy is inferior to 3 persons. Similarly, we could analyze the effect of the house age on the median house price (middle plot). Thus, these interpretations are marginal, considering a feature at a time."
msgstr ""

#: ../modules/partial_dependence.rst:49
msgid "PDPs with two input features of interest show the interactions among the two features. For example, the two-variable PDP in the above figure shows the dependence of median house price on joint values of house age and average occupants per household. We can clearly see an interaction between the two features: for an average occupancy greater than two, the house price is nearly independent of the house age, whereas for values less than 2 there is a strong dependence on age."
msgstr ""

#: ../modules/partial_dependence.rst:57
msgid "The :mod:`sklearn.inspection` module provides a convenience function :func:`plot_partial_dependence` to create one-way and two-way partial dependence plots. In the below example we show how to create a grid of partial dependence plots: two one-way PDPs for the features ``0`` and ``1`` and a two-way PDP between the two features::"
msgstr ""

#: ../modules/partial_dependence.rst:73
msgid "You can access the newly created figure and Axes objects using ``plt.gcf()`` and ``plt.gca()``."
msgstr ""

#: ../modules/partial_dependence.rst:76
msgid "For multi-class classification, you need to set the class label for which the PDPs should be created via the ``target`` argument::"
msgstr ""

#: ../modules/partial_dependence.rst:86
msgid "The same parameter ``target`` is used to specify the target in multi-output regression settings."
msgstr ""

#: ../modules/partial_dependence.rst:89
msgid "If you need the raw values of the partial dependence function rather than the plots, you can use the :func:`sklearn.inspection.partial_dependence` function::"
msgstr ""

#: ../modules/partial_dependence.rst:101
msgid "The values at which the partial dependence should be evaluated are directly generated from ``X``. For 2-way partial dependence, a 2D-grid of values is generated. The ``values`` field returned by :func:`sklearn.inspection.partial_dependence` gives the actual values used in the grid for each input feature of interest. They also correspond to the axis of the plots."
msgstr ""

#: ../modules/partial_dependence.rst:111
msgid "Individual conditional expectation (ICE) plot"
msgstr ""

#: ../modules/partial_dependence.rst:113
msgid "Similar to a PDP, an individual conditional expectation (ICE) plot shows the dependence between the target function and an input feature of interest. However, unlike a PDP, which shows the average effect of the input feature, an ICE plot visualizes the dependence of the prediction on a feature for each sample separately with one line per sample. Due to the limits of human perception, only one input feature of interest is supported for ICE plots."
msgstr ""

#: ../modules/partial_dependence.rst:121
msgid "The figures below show four ICE plots for the California housing dataset, with a :class:`HistGradientBoostingRegressor <sklearn.ensemble.HistGradientBoostingRegressor>`. The second figure plots the corresponding PD line overlaid on ICE lines."
msgstr ""

#: ../modules/partial_dependence.rst:131
msgid "While the PDPs are good at showing the average effect of the target features, they can obscure a heterogeneous relationship created by interactions. When interactions are present the ICE plot will provide many more insights. For example, we could observe a linear relationship between the median income and the house price in the PD line. However, the ICE lines show that there are some exceptions, where the house price remains constant in some ranges of the median income."
msgstr ""

#: ../modules/partial_dependence.rst:139
msgid "The :mod:`sklearn.inspection` module's :func:`plot_partial_dependence` convenience function can be used to create ICE plots by setting ``kind='individual'``. In the example below, we show how to create a grid of ICE plots:"
msgstr ""

#: ../modules/partial_dependence.rst:155
msgid "In ICE plots it might not be easy to see the average effect of the input feature of interest. Hence, it is recommended to use ICE plots alongside PDPs. They can be plotted together with ``kind='both'``."
msgstr ""

#: ../modules/partial_dependence.rst:164
msgid "Mathematical Definition"
msgstr ""

#: ../modules/partial_dependence.rst:166
msgid "Let :math:`X_S` be the set of input features of interest (i.e. the `features` parameter) and let :math:`X_C` be its complement."
msgstr ""

#: ../modules/partial_dependence.rst:169
msgid "The partial dependence of the response :math:`f` at a point :math:`x_S` is defined as:"
msgstr ""

#: ../modules/partial_dependence.rst:172
msgid "pd_{X_S}(x_S) &\\overset{def}{=} \\mathbb{E}_{X_C}\\left[ f(x_S, X_C) \\right]\\\\\n"
"              &= \\int f(x_S, x_C) p(x_C) dx_C,"
msgstr ""

#: ../modules/partial_dependence.rst:177
msgid "where :math:`f(x_S, x_C)` is the response function (:term:`predict`, :term:`predict_proba` or :term:`decision_function`) for a given sample whose values are defined by :math:`x_S` for the features in :math:`X_S`, and by :math:`x_C` for the features in :math:`X_C`. Note that :math:`x_S` and :math:`x_C` may be tuples."
msgstr ""

#: ../modules/partial_dependence.rst:183
msgid "Computing this integral for various values of :math:`x_S` produces a PDP plot as above. An ICE line is defined as a single :math:`f(x_{S}, x_{C}^{(i)})` evaluated at :math:`x_{S}`."
msgstr ""

#: ../modules/partial_dependence.rst:188
msgid "Computation methods"
msgstr ""

#: ../modules/partial_dependence.rst:190
msgid "There are two main methods to approximate the integral above, namely the 'brute' and 'recursion' methods. The `method` parameter controls which method to use."
msgstr ""

#: ../modules/partial_dependence.rst:194
msgid "The 'brute' method is a generic method that works with any estimator. Note that computing ICE plots is only supported with the 'brute' method. It approximates the above integral by computing an average over the data `X`:"
msgstr ""

#: ../modules/partial_dependence.rst:198
msgid "pd_{X_S}(x_S) \\approx \\frac{1}{n_\\text{samples}} \\sum_{i=1}^n f(x_S, x_C^{(i)}),"
msgstr ""

#: ../modules/partial_dependence.rst:202
msgid "where :math:`x_C^{(i)}` is the value of the i-th sample for the features in :math:`X_C`. For each value of :math:`x_S`, this method requires a full pass over the dataset `X` which is computationally intensive."
msgstr ""

#: ../modules/partial_dependence.rst:206
msgid "Each of the :math:`f(x_{S}, x_{C}^{(i)})` corresponds to one ICE line evaluated at :math:`x_{S}`. Computing this for multiple values of :math:`x_{S}`, one obtains a full ICE line. As one can see, the average of the ICE lines correspond to the partial dependence line."
msgstr ""

#: ../modules/partial_dependence.rst:211
msgid "The 'recursion' method is faster than the 'brute' method, but it is only supported for PDP plots by some tree-based estimators. It is computed as follows. For a given point :math:`x_S`, a weighted tree traversal is performed: if a split node involves an input feature of interest, the corresponding left or right branch is followed; otherwise both branches are followed, each branch being weighted by the fraction of training samples that entered that branch. Finally, the partial dependence is given by a weighted average of all the visited leaves values."
msgstr ""

#: ../modules/partial_dependence.rst:220
msgid "With the 'brute' method, the parameter `X` is used both for generating the grid of values :math:`x_S` and the complement feature values :math:`x_C`. However with the 'recursion' method, `X` is only used for the grid values: implicitly, the :math:`x_C` values are those of the training data."
msgstr ""

#: ../modules/partial_dependence.rst:225
msgid "By default, the 'recursion' method is used for plotting PDPs on tree-based estimators that support it, and 'brute' is used for the rest."
msgstr ""

#: ../modules/partial_dependence.rst:232
msgid "While both methods should be close in general, they might differ in some specific settings. The 'brute' method assumes the existence of the data points :math:`(x_S, x_C^{(i)})`. When the features are correlated, such artificial samples may have a very low probability mass. The 'brute' and 'recursion' methods will likely disagree regarding the value of the partial dependence, because they will treat these unlikely samples differently. Remember, however, that the primary assumption for interpreting PDPs is that the features should be independent."
msgstr ""

#: ../modules/partial_dependence.rst:244
msgid ":ref:`sphx_glr_auto_examples_inspection_plot_partial_dependence.py`"
msgstr ""

#: ../modules/partial_dependence.rst:247
msgid "Footnotes"
msgstr ""

#: ../modules/partial_dependence.rst:248
msgid "For classification, the target response may be the probability of a class (the positive class for binary classification), or the decision function."
msgstr ""

msgid "References"
msgstr ""

#: ../modules/partial_dependence.rst:254
msgid "T. Hastie, R. Tibshirani and J. Friedman, `The Elements of Statistical Learning <https://web.stanford.edu/~hastie/ElemStatLearn//>`_, Second Edition, Section 10.13.2, Springer, 2009."
msgstr ""

#: ../modules/partial_dependence.rst:258
msgid "C. Molnar, `Interpretable Machine Learning <https://christophm.github.io/interpretable-ml-book/>`_, Section 5.1, 2019."
msgstr ""

#: ../modules/partial_dependence.rst:261
msgid "A. Goldstein, A. Kapelner, J. Bleich, and E. Pitkin, `Peeking Inside the Black Box: Visualizing Statistical Learning With Plots of Individual Conditional Expectation <https://arxiv.org/abs/1309.6392>`_, Journal of Computational and Graphical Statistics, 24(1): 44-65, Springer, 2015."
msgstr ""

