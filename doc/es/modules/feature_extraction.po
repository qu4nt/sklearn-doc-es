msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-28 19:00\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/feature_extraction.po\n"
"X-Crowdin-File-ID: 4846\n"
"Language: es_ES\n"

#: ../modules/feature_extraction.rst:5
msgid "Feature extraction"
msgstr "Extracción de características"

#: ../modules/feature_extraction.rst:9
msgid "The :mod:`sklearn.feature_extraction` module can be used to extract features in a format supported by machine learning algorithms from datasets consisting of formats such as text and image."
msgstr "El módulo :mod:`sklearn.feature_extraction` puede utilizarse para extraer características en un formato soportado por algoritmos de aprendizaje automático a partir de conjuntos de datos que consisten en formatos como texto e imagen."

#: ../modules/feature_extraction.rst:15
msgid "Feature extraction is very different from :ref:`feature_selection`: the former consists in transforming arbitrary data, such as text or images, into numerical features usable for machine learning. The latter is a machine learning technique applied on these features."
msgstr "La extracción de características es muy diferente de la :ref:`feature_selection`: la primera consiste en transformar datos arbitrarios, como texto o imágenes, en características numéricas que se pueden utilizar para el aprendizaje automático. La segunda es una técnica de aprendizaje automático aplicada a estas características."

#: ../modules/feature_extraction.rst:23
msgid "Loading features from dicts"
msgstr "Cargando características desde diccionarios"

#: ../modules/feature_extraction.rst:25
msgid "The class :class:`DictVectorizer` can be used to convert feature arrays represented as lists of standard Python ``dict`` objects to the NumPy/SciPy representation used by scikit-learn estimators."
msgstr "La clase :class:`DictVectorizer` puede utilizarse para convertir arreglos de características representadas como listas de objetos estándar de Python ``dict`` a la representación NumPy/SciPy utilizada por los estimadores de scikit-learn."

#: ../modules/feature_extraction.rst:29
msgid "While not particularly fast to process, Python's ``dict`` has the advantages of being convenient to use, being sparse (absent features need not be stored) and storing feature names in addition to values."
msgstr "Si bien no es particularmente rápido de procesar, el ``dict`` de Python tiene las ventajas de ser cómodo de usar, ser escaso (las características ausentes no necesitan ser almacenadas) y almacenar los nombres de las características además de los valores."

#: ../modules/feature_extraction.rst:33
msgid ":class:`DictVectorizer` implements what is called one-of-K or \"one-hot\" coding for categorical (aka nominal, discrete) features. Categorical features are \"attribute-value\" pairs where the value is restricted to a list of discrete of possibilities without ordering (e.g. topic identifiers, types of objects, tags, names...)."
msgstr ":class:`DictVectorizer` implementa lo que se denomina codificación \"one-of-K\" o \"one-hot\" para características categóricas (también conocidas como nominales, discretas). Las características categóricas son pares \"atributo-valor\" donde el valor está restringido a una lista de posibilidades discretas sin ordenar (p. ej. identificadores de temas, tipos de objetos, etiquetas, nombres...)."

#: ../modules/feature_extraction.rst:39
msgid "In the following, \"city\" is a categorical attribute while \"temperature\" is a traditional numerical feature::"
msgstr "En lo siguiente, \"city\" es un atributo categórico mientras que \"temperature\" es una característica numérica tradicional::"

#: ../modules/feature_extraction.rst:59
msgid ":class:`DictVectorizer` accepts multiple string values for one feature, like, e.g., multiple categories for a movie."
msgstr ":class:`DictVectorizer` acepta múltiples valores de cadena para una característica, como, por ejemplo, múltiples categorías para una película."

#: ../modules/feature_extraction.rst:62
msgid "Assume a database classifies each movie using some categories (not mandatories) and its year of release."
msgstr "Suponte que una base de datos clasifica cada película usando algunas categorías (no obligatorias) y su año de estreno."

#: ../modules/feature_extraction.rst:80
msgid ":class:`DictVectorizer` is also a useful representation transformation for training sequence classifiers in Natural Language Processing models that typically work by extracting feature windows around a particular word of interest."
msgstr ":class:`DictVectorizer` también es una transformación de representación útil para el entrenamiento de clasificadores de secuencias en modelos de Procesamiento de Lenguaje Natural que normalmente trabajan extrayendo ventanas de características alrededor de una palabra de interés particular."

#: ../modules/feature_extraction.rst:85
msgid "For example, suppose that we have a first algorithm that extracts Part of Speech (PoS) tags that we want to use as complementary tags for training a sequence classifier (e.g. a chunker). The following dict could be such a window of features extracted around the word 'sat' in the sentence 'The cat sat on the mat.'::"
msgstr "Por ejemplo, supongamos que tenemos un primer algoritmo que extrae las etiquetas Part of Speech (PoS) que queremos usar como etiquetas complementarias para entrenar un clasificador de secuencias (p. ej. un chunker). El siguiente diccionario podría ser una ventana de características extraídas alrededor de la palabra \"sat\" en la frase 'The cat sat on the mat.'::"

#: ../modules/feature_extraction.rst:103
msgid "This description can be vectorized into a sparse two-dimensional matrix suitable for feeding into a classifier (maybe after being piped into a :class:`~text.TfidfTransformer` for normalization)::"
msgstr "Esta descripción puede ser vectorizada en una matriz bidimensional dispersa adecuada para alimentar a un clasificador (tal vez después de ser canalizada en un :class:`~text.TfidfTransformer` para su normalización)::"

#: ../modules/feature_extraction.rst:117
msgid "As you can imagine, if one extracts such a context around each individual word of a corpus of documents the resulting matrix will be very wide (many one-hot-features) with most of them being valued to zero most of the time. So as to make the resulting data structure able to fit in memory the ``DictVectorizer`` class uses a ``scipy.sparse`` matrix by default instead of a ``numpy.ndarray``."
msgstr "Como puedes imaginar, si se extrae un contexto de este tipo alrededor de cada palabra individual de un corpus de documentos, la matriz resultante será muy amplia (muchas características one-hot) y la mayoría de ellas con valor cero la mayor parte del tiempo. Para que la estructura de datos resultante pueda encajar en la memoria, la clase ``DictVectorizer`` utiliza por defecto una matriz ``scipy. parse`` en lugar de una ``numpy.ndarray``."

#: ../modules/feature_extraction.rst:128
msgid "Feature hashing"
msgstr "Hashing de características"

#: ../modules/feature_extraction.rst:132
msgid "The class :class:`FeatureHasher` is a high-speed, low-memory vectorizer that uses a technique known as `feature hashing <https://en.wikipedia.org/wiki/Feature_hashing>`_, or the \"hashing trick\". Instead of building a hash table of the features encountered in training, as the vectorizers do, instances of :class:`FeatureHasher` apply a hash function to the features to determine their column index in sample matrices directly. The result is increased speed and reduced memory usage, at the expense of inspectability; the hasher does not remember what the input features looked like and has no ``inverse_transform`` method."
msgstr "La clase :class:`FeatureHasher` es un vectorizador de alta velocidad y baja memoria que utiliza una técnica conocida como `hashing de características <https://en.wikipedia.org/wiki/Feature_hashing>`_, o el \"truco de hashing. En lugar de construir una tabla hash de las características encontradas en el entrenamiento, como hacen los vectorizadores, las instancias de :class:`FeatureHasher` aplican una función hash a las características para determinar su índice de columna en las matrices de muestra directamente. El resultado es una mayor velocidad y un menor uso de la memoria, a expensas de la inspeccionabilidad; el hasher no recuerda el aspecto de las características de entrada y no tiene un método ``inverse_transform``."

#: ../modules/feature_extraction.rst:145
msgid "Since the hash function might cause collisions between (unrelated) features, a signed hash function is used and the sign of the hash value determines the sign of the value stored in the output matrix for a feature. This way, collisions are likely to cancel out rather than accumulate error, and the expected mean of any output feature's value is zero. This mechanism is enabled by default with ``alternate_sign=True`` and is particularly useful for small hash table sizes (``n_features < 10000``). For large hash table sizes, it can be disabled, to allow the output to be passed to estimators like :class:`~sklearn.naive_bayes.MultinomialNB` or :class:`~sklearn.feature_selection.chi2` feature selectors that expect non-negative inputs."
msgstr "Dado que la función hash puede causar colisiones entre características (no relacionadas), se utiliza una función hash con signo y el signo del valor hash determina el signo del valor almacenado en la matriz de salida para una característica. De esta manera, es probable que las colisiones se cancelen en lugar de acumular errores, y que la media esperada del valor de cualquier característica de salida es cero. Este mecanismo está activado por defecto con ``alternate_sign=True`` y es particularmente útil para tamaños de tabla hash pequeños (``n_features < 10000``). Para tamaños de tabla hash grandes, se puede desactivar, para permitir que la salida se pase a estimadores como :class:`~sklearn.naive_bayes.MultinomialNB` o :class:`~sklearn.feature_selection.chi2` selectores de características que esperan entradas no negativas."

#: ../modules/feature_extraction.rst:157
msgid ":class:`FeatureHasher` accepts either mappings (like Python's ``dict`` and its variants in the ``collections`` module), ``(feature, value)`` pairs, or strings, depending on the constructor parameter ``input_type``. Mapping are treated as lists of ``(feature, value)`` pairs, while single strings have an implicit value of 1, so ``['feat1', 'feat2', 'feat3']`` is interpreted as ``[('feat1', 1), ('feat2', 1), ('feat3', 1)]``. If a single feature occurs multiple times in a sample, the associated values will be summed (so ``('feat', 2)`` and ``('feat', 3.5)`` become ``('feat', 5.5)``). The output from :class:`FeatureHasher` is always a ``scipy.sparse`` matrix in the CSR format."
msgstr ":class:`FeatureHasher` acepta cualquier mapeo (como el ``dict`` de Python y sus variantes en el módulo ``collections``), pares ``(feature, value)``, o cadenas, dependiendo del parámetro constructor ``input_type``. El mapeo se trata como listas de pares ``(feature, value)``, mientras que las cadenas simples tienen un valor implícito de 1, por lo que ``['feat1', 'feat2', 'feat3']`` se interpreta como ``[('feat1', 1), ('feat2', 1), ('feat3', 1)]``. Si una misma característica aparece varias veces en una muestra, los valores asociados serán sumados (así ``('feat', 2)`` y ``('feat', 3.5 )`` se convierten en ``('feat', 5.5)``). La salida de :class:`FeatureHasher` es siempre una matriz ``scipy.sparse`` en el formato CSR."

#: ../modules/feature_extraction.rst:171
msgid "Feature hashing can be employed in document classification, but unlike :class:`~text.CountVectorizer`, :class:`FeatureHasher` does not do word splitting or any other preprocessing except Unicode-to-UTF-8 encoding; see :ref:`hashing_vectorizer`, below, for a combined tokenizer/hasher."
msgstr "El hashing de características puede emplearse en la clasificación de documentos, pero a diferencia de :class:`~text.CountVectorizer`, :class:`FeatureHasher` no hace la división de palabras ni ningún otro preprocesamiento excepto la codificación Unicode-to-UTF-8; ver :ref:`hashing_vectorizer`, más abajo, para un tokenizador/hasher combinado."

#: ../modules/feature_extraction.rst:177
msgid "As an example, consider a word-level natural language processing task that needs features extracted from ``(token, part_of_speech)`` pairs. One could use a Python generator function to extract features::"
msgstr "Como ejemplo, considera una tarea de procesamiento de lenguaje natural a nivel de palabras que necesite características extraídas de pares ``(token, part_of_speech )``. Se podría utilizar una función generadora de Python para extraer características::"

#: ../modules/feature_extraction.rst:193
msgid "Then, the ``raw_X`` to be fed to ``FeatureHasher.transform`` can be constructed using::"
msgstr "Entonces, el ``raw_X`` para ser suministrado a ``FeatureHasher.transform`` puede ser construido usando::"

#: ../modules/feature_extraction.rst:198
msgid "and fed to a hasher with::"
msgstr "y suministrado a un hasher con::"

#: ../modules/feature_extraction.rst:203
msgid "to get a ``scipy.sparse`` matrix ``X``."
msgstr "para obtener una matriz ``scipy.sparse`` ``X``."

#: ../modules/feature_extraction.rst:205
msgid "Note the use of a generator comprehension, which introduces laziness into the feature extraction: tokens are only processed on demand from the hasher."
msgstr "Nota que el uso de una comprensión generadora, introduce la holgura en la extracción de características: los tokens sólo se procesan bajo demanda del hasher."

#: ../modules/feature_extraction.rst:210
msgid "Implementation details"
msgstr "Detalles de implementación"

#: ../modules/feature_extraction.rst:212
msgid ":class:`FeatureHasher` uses the signed 32-bit variant of MurmurHash3. As a result (and because of limitations in ``scipy.sparse``), the maximum number of features supported is currently :math:`2^{31} - 1`."
msgstr ":class:`FeatureHasher` utiliza la variante de 32 bits con signo de MurmurHash3. Como resultado (y debido a las limitaciones en ``scipy. parse``), el número máximo de características soportadas es actualmente :math:`2^{31} - 1`."

#: ../modules/feature_extraction.rst:216
msgid "The original formulation of the hashing trick by Weinberger et al. used two separate hash functions :math:`h` and :math:`\\xi` to determine the column index and sign of a feature, respectively. The present implementation works under the assumption that the sign bit of MurmurHash3 is independent of its other bits."
msgstr "La formulación original del truco de hashing de Weinberger et al. usó dos funciones hash separadas :math:`h` y :math:`\\xi` para determinar el índice de columna y el signo de una característica, respectivamente. La implementación actual funciona bajo el supuesto de que el bit de signo de MurmurHash3 es independiente de sus otros bits."

#: ../modules/feature_extraction.rst:222
msgid "Since a simple modulo is used to transform the hash function to a column index, it is advisable to use a power of two as the ``n_features`` parameter; otherwise the features will not be mapped evenly to the columns."
msgstr "Dado que se utiliza un modulo simple para transformar la función hash en un índice de columnas, es recomendable utilizar una potencia de dos como el parámetro ``n_features``; de lo contrario, las características no serán asignadas uniformemente a las columnas."

#: ../modules/feature_extraction.rst:229
msgid "Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). `Feature hashing for large scale multitask learning <https://alex.smola.org/papers/2009/Weinbergeretal09.pdf>`_. Proc. ICML."
msgstr "Kilian Weinberger, Anirban Dasgupta, John Langford, Alex Smola and Josh Attenberg (2009). `Feature hashing for large scale multitask learning <https://alex.smola.org/papers/2009/Weinbergeretal09.pdf>`_. Proc. ICML."

#: ../modules/feature_extraction.rst:233
msgid "`MurmurHash3 <https://github.com/aappleby/smhasher>`_."
msgstr "`MurmurHash3 <https://github.com/aappleby/smhasher>`_."

#: ../modules/feature_extraction.rst:239
msgid "Text feature extraction"
msgstr "Extracción de característica de texto"

#: ../modules/feature_extraction.rst:245
msgid "The Bag of Words representation"
msgstr "La representación Bolsa de Palabras"

#: ../modules/feature_extraction.rst:247
msgid "Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length."
msgstr "El análisis de textos es uno de los principales campos de aplicación de los algoritmos de aprendizaje automático. Sin embargo, los datos en bruto, una secuencia de símbolos no puede ser suministrada directamente a los propios algoritmos, ya que la mayoría de ellos esperan vectores de características numéricas con un tamaño fijo en lugar de los documentos de texto sin formato con longitud variable."

#: ../modules/feature_extraction.rst:253
msgid "In order to address this, scikit-learn provides utilities for the most common ways to extract numerical features from text content, namely:"
msgstr "Para abordar esto, scikit-learn proporciona utilidades para las formas más comunes de extraer características numéricas del contenido del texto, a saber:"

#: ../modules/feature_extraction.rst:256
msgid "**tokenizing** strings and giving an integer id for each possible token, for instance by using white-spaces and punctuation as token separators."
msgstr "**tokenizar** las cadenas y dar un identificador entero para cada token posible, por ejemplo, usando espacios en blanco y signos de puntuación como separadores de token."

#: ../modules/feature_extraction.rst:259
msgid "**counting** the occurrences of tokens in each document."
msgstr "**conteo** de las ocurrencias de tokens en cada documento."

#: ../modules/feature_extraction.rst:261
msgid "**normalizing** and weighting with diminishing importance tokens that occur in the majority of samples / documents."
msgstr "**normalizar** y ponderar con importancia decreciente los tokens que aparecen en la mayoría de las muestras / documentos."

#: ../modules/feature_extraction.rst:264
msgid "In this scheme, features and samples are defined as follows:"
msgstr "En este esquema, las características y muestras se definen de la siguiente manera:"

#: ../modules/feature_extraction.rst:266
msgid "each **individual token occurrence frequency** (normalized or not) is treated as a **feature**."
msgstr "cada ** frecuencia de ocurrencia de token individual ** (normalizada o no) se trata como una ** característica **."

#: ../modules/feature_extraction.rst:269
msgid "the vector of all the token frequencies for a given **document** is considered a multivariate **sample**."
msgstr "el vector de todas las frecuencias de tokens para un **documento** dado se considera una **muestra** multivariante."

#: ../modules/feature_extraction.rst:272
msgid "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus."
msgstr "Por lo tanto, un corpus de documentos puede representarse mediante una matriz con una fila por documento y una columna por token (p. ej. palabra) que aparezca en el corpus."

#: ../modules/feature_extraction.rst:275
msgid "We call **vectorization** the general process of turning a collection of text documents into numerical feature vectors. This specific strategy (tokenization, counting and normalization) is called the **Bag of Words** or \"Bag of n-grams\" representation. Documents are described by word occurrences while completely ignoring the relative position information of the words in the document."
msgstr "Llamamos **vectorización** al proceso general de convertir una colección de documentos de texto en vectores de características numéricas. Esta estrategia específica (tokenization, conteo y normalización) se denomina representación **Bolsa de palabras** o \"Bolsa de n-gramas\". Los documentos se describen por las ocurrencias de las palabras, ignorando por completo la información sobre la posición relativa de las palabras en el documento."

#: ../modules/feature_extraction.rst:284
msgid "Sparsity"
msgstr "Dispersión"

#: ../modules/feature_extraction.rst:286
#, python-format
msgid "As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have many feature values that are zeros (typically more than 99% of them)."
msgstr "Como la mayoría de los documentos suelen utilizar un subconjunto muy pequeño de las palabras utilizadas en el corpus, la matriz resultante tendrá muchos valores de características que son ceros (normalmente más del 99% de ellos)."

#: ../modules/feature_extraction.rst:290
msgid "For instance a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually."
msgstr "Por ejemplo, una colección de 10.000 documentos de texto cortos (como correos electrónicos) utilizará un vocabulario con un tamaño del orden de 100.000 palabras únicas en total, mientras que cada documento utilizará entre 100 y 1000 palabras únicas individualmente."

#: ../modules/feature_extraction.rst:294
msgid "In order to be able to store such a matrix in memory but also to speed up algebraic operations matrix / vector, implementations will typically use a sparse representation such as the implementations available in the ``scipy.sparse`` package."
msgstr "Para poder almacenar una matriz de este tipo en la memoria, pero también para acelerar las operaciones algebraicas matriz / vector, las implementaciones suelen utilizar una representación dispersa, como las implementaciones disponibles en el paquete ``scipy.sparse``."

#: ../modules/feature_extraction.rst:301
msgid "Common Vectorizer usage"
msgstr "Uso común del Vectorizador"

#: ../modules/feature_extraction.rst:303
msgid ":class:`CountVectorizer` implements both tokenization and occurrence counting in a single class::"
msgstr ":class:`CountVectorizer` implementa tanto la tokenización como el conteo de ocurrencias en una sola clase::"

#: ../modules/feature_extraction.rst:308
msgid "This model has many parameters, however the default values are quite reasonable (please see  the :ref:`reference documentation <text_feature_extraction_ref>` for the details)::"
msgstr "Este modelo tiene muchos parámetros, sin embargo los valores por defecto son bastante razonables (por favor consulta la :ref:`documentación de referencia <text_feature_extraction_ref>` para los detalles)::"

#: ../modules/feature_extraction.rst:316
msgid "Let's use it to tokenize and count the word occurrences of a minimalistic corpus of text documents::"
msgstr "Vamos a utilizarlo para tokenizar y contar las ocurrencias de palabras de un corpus minimalista de documentos de texto::"

#: ../modules/feature_extraction.rst:330
msgid "The default configuration tokenizes the string by extracting words of at least 2 letters. The specific function that does this step can be requested explicitly::"
msgstr "La configuración por defecto tokeniza la cadena extrayendo palabras de al menos 2 letras. La función específica que realiza este paso puede solicitarse explícitamente::"

#: ../modules/feature_extraction.rst:339
msgid "Each term found by the analyzer during the fit is assigned a unique integer index corresponding to a column in the resulting matrix. This interpretation of the columns can be retrieved as follows::"
msgstr "A cada término encontrado por el analizador durante el ajuste se le asigna un índice entero único correspondiente a una columna en la matriz resultante. Esta interpretación de las columnas se puede recuperar de la siguiente manera::"

#: ../modules/feature_extraction.rst:354
msgid "The converse mapping from feature name to column index is stored in the ``vocabulary_`` attribute of the vectorizer::"
msgstr "El mapeo de conversión del nombre de característica al índice de columna se almacena en el atributo ``vocabulary_`` del vectorizador::"

#: ../modules/feature_extraction.rst:360
msgid "Hence words that were not seen in the training corpus will be completely ignored in future calls to the transform method::"
msgstr "Por lo tanto, las palabras que no se hayan visto en el corpus de entrenamiento se ignorarán por completo en futuras llamadas al método de transformación::"

#: ../modules/feature_extraction.rst:366
msgid "Note that in the previous corpus, the first and the last documents have exactly the same words hence are encoded in equal vectors. In particular we lose the information that the last document is an interrogative form. To preserve some of the local ordering information we can extract 2-grams of words in addition to the 1-grams (individual words)::"
msgstr "Tenga en cuenta que en el corpus anterior, el primer y el último documento tienen exactamente las mismas palabras, por lo tanto, están codificados en vectores iguales. En particular, perdemos la información de que el último documento es una forma interrogativa. Para preservar parte de la información de pedido local, podemos extraer bigramas de palabras además de unigramas (palabras individuales)::"

#: ../modules/feature_extraction.rst:379
msgid "The vocabulary extracted by this vectorizer is hence much bigger and can now resolve ambiguities encoded in local positioning patterns::"
msgstr "El vocabulario extraído por este vectorizador es, por lo tanto, mucho más grande y ahora puede resolver las ambigüedades codificadas en patrones de posicionamiento local::"

#: ../modules/feature_extraction.rst:390
msgid "In particular the interrogative form \"Is this\" is only present in the last document::"
msgstr "En particular, la forma interrogativa \"Is this\" sólo está presente en el último documento::"

#: ../modules/feature_extraction.rst:400
msgid "Using stop words"
msgstr "Usando palabras funcionales (stop words)"

#: ../modules/feature_extraction.rst:402
msgid "Stop words are words like \"and\", \"the\", \"him\", which are presumed to be uninformative in representing the content of a text, and which may be removed to avoid them being construed as signal for prediction.  Sometimes, however, similar words are useful for prediction, such as in classifying writing style or personality."
msgstr "Las palabras funcionales son palabras como \"y\", \"el\", \"él\",  que se suponen poco informativas para representar el contenido de un texto, y que pueden eliminarse para evitar que sean interpretadas como señal para la predicción. Sin embargo, a veces palabras similares son útiles para la predicción, como en la clasificación del estilo de escritura o la personalidad."

#: ../modules/feature_extraction.rst:408
msgid "There are several known issues in our provided 'english' stop word list. It does not aim to be a general, 'one-size-fits-all' solution as some tasks may require a more custom solution. See [NQY18]_ for more details."
msgstr "Hay varios problemas conocidos en nuestra lista de palabras funcionales 'inglés' proporcionada. No tiene como objetivo ser una solución general, de \"talla única\", ya que algunas tareas pueden requerir una solución más personalizada. Consulta [NQY18]_ para más detalles."

#: ../modules/feature_extraction.rst:412
msgid "Please take care in choosing a stop word list. Popular stop word lists may include words that are highly informative to some tasks, such as *computer*."
msgstr "Por favor, ten cuidado al elegir una lista de palabras funcionales. Las listas populares de palabras funcionales pueden incluir palabras que son altamente informativas para algunas tareas, como *computadora*."

#: ../modules/feature_extraction.rst:416
msgid "You should also make sure that the stop word list has had the same preprocessing and tokenization applied as the one used in the vectorizer. The word *we've* is split into *we* and *ve* by CountVectorizer's default tokenizer, so if *we've* is in ``stop_words``, but *ve* is not, *ve* will be retained from *we've* in transformed text.  Our vectorizers will try to identify and warn about some kinds of inconsistencies."
msgstr "También asegúrate de que a la lista de palabras funcionales se le ha aplicado el mismo preprocesamiento y tokenización que el utilizado en el vectorizador. La palabra *we've* está dividida en *we* y *ve* por el tokenizador por defecto CountVectorizer, así que si *we've* está en ``stop_words``, pero *ve* no lo está, *ve* será retenido de *we've* en el texto transformado. Nuestros vectorizadores tratarán de identificar y advertir sobre algunos tipos de inconsistencias."

#: ../modules/feature_extraction.rst:425
msgid "J. Nothman, H. Qin and R. Yurchak (2018). `\"Stop Word Lists in Free Open-source Software Packages\" <https://aclweb.org/anthology/W18-2502>`__. In *Proc. Workshop for NLP Open Source Software*."
msgstr "J. Nothman, H. Qin and R. Yurchak (2018). `\"Stop Word Lists in Free Open-source Software Packages\" <https://aclweb.org/anthology/W18-2502>`__. In *Proc. Workshop for NLP Open Source Software*."

#: ../modules/feature_extraction.rst:433
msgid "Tf–idf term weighting"
msgstr "Ponderación de términos Tf-idf"

#: ../modules/feature_extraction.rst:435
msgid "In a large text corpus, some words will be very present (e.g. \"the\", \"a\", \"is\" in English) hence carrying very little meaningful information about the actual contents of the document. If we were to feed the direct count data directly to a classifier those very frequent terms would shadow the frequencies of rarer yet more interesting terms."
msgstr "En un corpus de texto extenso, algunas palabras estarán muy presentes (p. ej. \"the\", \"a\", \"is\" en inglés), por lo que contienen muy poca información significativa sobre el contenido real del documento. Si tuviéramos que suministrar los datos de recuento directo directamente a un clasificador, esos términos muy frecuentes ensombrecerían las frecuencias de términos más raros pero más interesantes."

#: ../modules/feature_extraction.rst:441
msgid "In order to re-weight the count features into floating point values suitable for usage by a classifier it is very common to use the tf–idf transform."
msgstr "Para reponderar las características de conteo en valores de punto flotante adecuados para su uso por un clasificador, es muy común utilizar la transformación tf-idf."

#: ../modules/feature_extraction.rst:445
msgid "Tf means **term-frequency** while tf–idf means term-frequency times **inverse document-frequency**: :math:`\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}`."
msgstr "Tf significa **frecuencia de términos** mientras que tf-idf significa frecuencia de términos por **frecuencia inversa de documentos**: :math:`\\text{tf-idf(t,d)}=\\text{tf(t,d)} \\times \\text{idf(t)}`."

#: ../modules/feature_extraction.rst:449
msgid "Using the ``TfidfTransformer``'s default settings, ``TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)`` the term frequency, the number of times a term occurs in a given document, is multiplied with idf component, which is computed as"
msgstr "Utilizando la configuración por defecto del ``TfidfTransformer``, ``TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)`` la frecuencia de términos, el número de veces que un término aparece en un documento determinado, se multiplica por el componente idf, que se calcula como"

#: ../modules/feature_extraction.rst:454
msgid ":math:`\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1`,"
msgstr ":math:`\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1`,"

#: ../modules/feature_extraction.rst:456
msgid "where :math:`n` is the total number of documents in the document set, and :math:`\\text{df}(t)` is the number of documents in the document set that contain term :math:`t`. The resulting tf-idf vectors are then normalized by the Euclidean norm:"
msgstr "donde :math:`n` es el número total de documentos en el conjunto de documentos, y :math:`\\text{df}(t)` es el número de documentos en el conjunto de documentos que contienen el término :math:`t`. Los vectores tf-idf resultantes son normalizados por la norma Euclideana:"

#: ../modules/feature_extraction.rst:461
msgid ":math:`v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}`."
msgstr ":math:`v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}`."

#: ../modules/feature_extraction.rst:464
msgid "This was originally a term weighting scheme developed for information retrieval (as a ranking function for search engines results) that has also found good use in document classification and clustering."
msgstr "Originalmente se trataba de un esquema de ponderación de términos desarrollado para la recuperación de la información (como una función de clasificación para los resultados de los motores de búsqueda) que también ha encontrado un buen uso en la clasificación y conglomeración de documentos."

#: ../modules/feature_extraction.rst:468
msgid "The following sections contain further explanations and examples that illustrate how the tf-idfs are computed exactly and how the tf-idfs computed in scikit-learn's :class:`TfidfTransformer` and :class:`TfidfVectorizer` differ slightly from the standard textbook notation that defines the idf as"
msgstr "Las siguientes secciones contienen más explicaciones y ejemplos que ilustran cómo se calculan exactamente los tf-idfs y cómo los tf-idfs calculados en scikit-learn :class:`TfidfTransformer` y :class:`TfidfVectorizer` difieren ligeramente de la notación estándar de los libros de texto que definen el idf como"

#: ../modules/feature_extraction.rst:474
msgid ":math:`\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.`"
msgstr ":math:`\\text{idf}(t) = \\log{\\frac{n}{1+\\text{df}(t)}}.`"

#: ../modules/feature_extraction.rst:477
msgid "In the :class:`TfidfTransformer` and :class:`TfidfVectorizer` with ``smooth_idf=False``, the \"1\" count is added to the idf instead of the idf's denominator:"
msgstr "En :class:`TfidfTransformer` y el :class:`TfidfVectorizer` con ``smooth_idf=False``, el conteo \"1\" se añade al idf en lugar del denominador del idf:"

#: ../modules/feature_extraction.rst:481
msgid ":math:`\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1`"
msgstr ":math:`\\text{idf}(t) = \\log{\\frac{n}{\\text{df}(t)}} + 1`"

#: ../modules/feature_extraction.rst:483
msgid "This normalization is implemented by the :class:`TfidfTransformer` class::"
msgstr "Esta normalización es implementada por la clase :class:`TfidfTransformer`::"

#: ../modules/feature_extraction.rst:491
msgid "Again please see the :ref:`reference documentation <text_feature_extraction_ref>` for the details on all the parameters."
msgstr "De nuevo, por favor consulta la :ref:`documentación de referencia <text_feature_extraction_ref>` para los detalles de todos los parámetros."

#: ../modules/feature_extraction.rst:494
#, python-format
msgid "Let's take an example with the following counts. The first term is present 100% of the time hence not very interesting. The two other features only in less than 50% of the time hence probably more representative of the content of the documents::"
msgstr "Tomemos un ejemplo con los siguientes conteos. El primer término está presente el 100% de las veces, por lo tanto, no es muy interesante. Las otras dos características sólo en menos del 50% de las ocasiones, por lo que probablemente sean más representativas del contenido de los documentos::"

#: ../modules/feature_extraction.rst:519
msgid "Each row is normalized to have unit Euclidean norm:"
msgstr "Cada fila está normalizada para tener una norma Euclideana unitaria:"

#: ../modules/feature_extraction.rst:521
msgid ":math:`v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}`"
msgstr ":math:`v_{norm} = \\frac{v}{||v||_2} = \\frac{v}{\\sqrt{v{_1}^2 + v{_2}^2 + \\dots + v{_n}^2}}`"

#: ../modules/feature_extraction.rst:524
msgid "For example, we can compute the tf-idf of the first term in the first document in the `counts` array as follows:"
msgstr "Por ejemplo, podemos calcular el tf-idf del primer término del primer documento en el arreglo `counts` de la siguiente manera:"

#: ../modules/feature_extraction.rst:527
msgid ":math:`n = 6`"
msgstr ":math:`n = 6`"

#: ../modules/feature_extraction.rst:529
msgid ":math:`\\text{df}(t)_{\\text{term1}} = 6`"
msgstr ":math:`\\text{df}(t)_{\\text{term1}} = 6`"

#: ../modules/feature_extraction.rst:531
msgid ":math:`\\text{idf}(t)_{\\text{term1}} = \\log \\frac{n}{\\text{df}(t)} + 1 = \\log(1)+1 = 1`"
msgstr ":math:`\\text{idf}(t)_{\\text{term1}} = \\log \\frac{n}{\\text{df}(t)} + 1 = \\log(1)+1 = 1`"

#: ../modules/feature_extraction.rst:534
msgid ":math:`\\text{tf-idf}_{\\text{term1}} = \\text{tf} \\times \\text{idf} = 3 \\times 1 = 3`"
msgstr ":math:`\\text{tf-idf}_{\\text{term1}} = \\text{tf} \\times \\text{idf} = 3 \\times 1 = 3`"

#: ../modules/feature_extraction.rst:536
msgid "Now, if we repeat this computation for the remaining 2 terms in the document, we get"
msgstr "Ahora, si repetimos este cálculo para los 2 términos restantes en el documento, obtenemos"

#: ../modules/feature_extraction.rst:539
msgid ":math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times (\\log(6/1)+1) = 0`"
msgstr ":math:`\\text{tf-idf}_{\\text{term2}} = 0 \\times (\\log(6/1)+1) = 0`"

#: ../modules/feature_extraction.rst:541
msgid ":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times (\\log(6/2)+1) \\approx 2.0986`"
msgstr ":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times (\\log(6/2)+1) \\approx 2.0986`"

#: ../modules/feature_extraction.rst:543
msgid "and the vector of raw tf-idfs:"
msgstr "y el vector de tf-idfs en bruto:"

#: ../modules/feature_extraction.rst:545
msgid ":math:`\\text{tf-idf}_{\\text{raw}} = [3, 0, 2.0986].`"
msgstr ":math:`\\text{tf-idf}_{\\text{raw}} = [3, 0, 2.0986].`"

#: ../modules/feature_extraction.rst:548
msgid "Then, applying the Euclidean (L2) norm, we obtain the following tf-idfs for document 1:"
msgstr "Luego, aplicando la norma Euclideana (L2) obtenemos los siguientes tf-idfs para el documento 1:"

#: ../modules/feature_extraction.rst:551
msgid ":math:`\\frac{[3, 0, 2.0986]}{\\sqrt{\\big(3^2 + 0^2 + 2.0986^2\\big)}} = [ 0.819,  0,  0.573].`"
msgstr ":math:`\\frac{[3, 0, 2.0986]}{\\sqrt{\\big(3^2 + 0^2 + 2.0986^2\\big)}} = [ 0.819,  0,  0.573].`"

#: ../modules/feature_extraction.rst:554
msgid "Furthermore, the default parameter ``smooth_idf=True`` adds \"1\" to the numerator and  denominator as if an extra document was seen containing every term in the collection exactly once, which prevents zero divisions:"
msgstr "Además, el parámetro por defecto ``smooth_idf=True`` añade \"1\" al numerador y al denominador como si se viera un documento extra que contiene cada término de la colección exactamente una vez, lo que evita las divisiones cero:"

#: ../modules/feature_extraction.rst:558
msgid ":math:`\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1`"
msgstr ":math:`\\text{idf}(t) = \\log{\\frac{1 + n}{1+\\text{df}(t)}} + 1`"

#: ../modules/feature_extraction.rst:560
msgid "Using this modification, the tf-idf of the third term in document 1 changes to 1.8473:"
msgstr "Usando esta modificación, el tf-idf del tercer término del documento 1 cambia a 1.8473:"

#: ../modules/feature_extraction.rst:563
msgid ":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times \\log(7/3)+1 \\approx 1.8473`"
msgstr ":math:`\\text{tf-idf}_{\\text{term3}} = 1 \\times \\log(7/3)+1 \\approx 1.8473`"

#: ../modules/feature_extraction.rst:565
msgid "And the L2-normalized tf-idf changes to"
msgstr "Y el tf-idf L2-normalizado cambia a"

#: ../modules/feature_extraction.rst:567
msgid ":math:`\\frac{[3, 0, 1.8473]}{\\sqrt{\\big(3^2 + 0^2 + 1.8473^2\\big)}} = [0.8515, 0, 0.5243]`::"
msgstr ":math:`\\frac{[3, 0, 1.8473]}{\\sqrt{\\big(3^2 + 0^2 + 1.8473^2\\big)}} = [0.8515, 0, 0.5243]`::"

#: ../modules/feature_extraction.rst:579
msgid "The weights of each feature computed by the ``fit`` method call are stored in a model attribute::"
msgstr "Los pesos de cada característica calculados por la llamada al método ``fit`` se almacenan en un atributo del modelo::"

#: ../modules/feature_extraction.rst:589
msgid "As tf–idf is very often used for text features, there is also another class called :class:`TfidfVectorizer` that combines all the options of :class:`CountVectorizer` and :class:`TfidfTransformer` in a single model::"
msgstr "Como tf-idf se utiliza muy a menudo para las características de texto, también hay otra clase llamada :class:`TfidfVectorizer` que combina todas las opciones de :class:`CountVectorizer` y :class:`TfidfTransformer` en un solo modelo::"

#: ../modules/feature_extraction.rst:599
msgid "While the tf–idf normalization is often very useful, there might be cases where the binary occurrence markers might offer better features. This can be achieved by using the ``binary`` parameter of :class:`CountVectorizer`. In particular, some estimators such as :ref:`bernoulli_naive_bayes` explicitly model discrete boolean random variables. Also, very short texts are likely to have noisy tf–idf values while the binary occurrence info is more stable."
msgstr "Aunque la normalización tf-idf suele ser muy útil, puede haber casos en los que los marcadores de ocurrencia binarios podrían ofrecer mejores características. Esto se puede lograr usando el parámetro ``binary`` de :class:`CountVectorizer`. En particular, algunos estimadores como :ref:`bernoulli_naive_bayes` modelan explícitamente variables aleatorias booleanas discretas. Además, es probable que los textos muy cortos tengan valores tf-idf ruidosos, mientras que la información de la ocurrencia binaria es más estable."

#: ../modules/feature_extraction.rst:607
msgid "As usual the best way to adjust the feature extraction parameters is to use a cross-validated grid search, for instance by pipelining the feature extractor with a classifier:"
msgstr "Como es habitual, la mejor manera de ajustar los parámetros de extracción de características es utilizar una búsqueda de cuadrícula con validación cruzada, por ejemplo, canalizando el extractor de características con un clasificador:"

#: ../modules/feature_extraction.rst:611
msgid ":ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`"
msgstr ":ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`"

#: ../modules/feature_extraction.rst:615
msgid "Decoding text files"
msgstr "Decodificación de archivos de texto"

#: ../modules/feature_extraction.rst:616
msgid "Text is made of characters, but files are made of bytes. These bytes represent characters according to some *encoding*. To work with text files in Python, their bytes must be *decoded* to a character set called Unicode. Common encodings are ASCII, Latin-1 (Western Europe), KOI8-R (Russian) and the universal encodings UTF-8 and UTF-16. Many others exist."
msgstr "El texto está hecho de caracteres, pero los archivos están hechos de bytes. Estos bytes representan caracteres de acuerdo a alguna *codificación*. Para trabajar con archivos de texto en Python, sus bytes deben ser *decodificados* a un conjunto de caracteres llamado Unicode. Las codificaciones comunes son ASCII, Latin-1 (Europa Occidental), KOI8-R (Ruso) y las codificaciones universales UTF-8 y UTF-16. Existen muchas otras."

#: ../modules/feature_extraction.rst:623
msgid "An encoding can also be called a 'character set', but this term is less accurate: several encodings can exist for a single character set."
msgstr "Una codificación también puede llamarse 'conjunto de caracteres', pero este término es menos preciso: pueden existir varias codificaciones para un mismo conjunto de caracteres."

#: ../modules/feature_extraction.rst:627
msgid "The text feature extractors in scikit-learn know how to decode text files, but only if you tell them what encoding the files are in. The :class:`CountVectorizer` takes an ``encoding`` parameter for this purpose. For modern text files, the correct encoding is probably UTF-8, which is therefore the default (``encoding=\"utf-8\"``)."
msgstr "Los extractores de características de texto en scikit-learn saben cómo decodificar archivos de texto, pero sólo si se les dice en qué codificación están los archivos. El :class:`CountVectorizer` toma un parámetro ``encoding`` para este propósito. Para archivos de texto modernos, la codificación correcta es probablemente UTF-8, que es por lo tanto el valor por defecto (``encoding=\"utf-8\"``)."

#: ../modules/feature_extraction.rst:633
msgid "If the text you are loading is not actually encoded with UTF-8, however, you will get a ``UnicodeDecodeError``. The vectorizers can be told to be silent about decoding errors by setting the ``decode_error`` parameter to either ``\"ignore\"`` or ``\"replace\"``. See the documentation for the Python function ``bytes.decode`` for more details (type ``help(bytes.decode)`` at the Python prompt)."
msgstr "Sin embargo, si el texto que estás cargando no está codificado con UTF-8, obtendrás un ``UnicodeDecodeError``. Se puede indicar a los vectorizadores que guarden silencio sobre los errores de decodificación estableciendo el parámetro ``decode_error`` como ``\"ignore\"`` o ``\"replace\"``. Consulta la documentación de la función de Python ``bytes.decode`` para más detalles (escribe ``help(bytes.decode)`` en la consola de Python)."

#: ../modules/feature_extraction.rst:641
msgid "If you are having trouble decoding text, here are some things to try:"
msgstr "Si estás teniendo problemas para decodificar texto, aquí tienes algunas cosas para probar:"

#: ../modules/feature_extraction.rst:643
msgid "Find out what the actual encoding of the text is. The file might come with a header or README that tells you the encoding, or there might be some standard encoding you can assume based on where the text comes from."
msgstr "Averigua cuál es la codificación real del texto. El archivo puede venir con un encabezado o README que te diga la codificación, o puede haber alguna codificación estándar que puedas asumir basándote en la procedencia del texto."

#: ../modules/feature_extraction.rst:647
msgid "You may be able to find out what kind of encoding it is in general using the UNIX command ``file``. The Python ``chardet`` module comes with a script called ``chardetect.py`` that will guess the specific encoding, though you cannot rely on its guess being correct."
msgstr "Puedes averiguar qué tipo de codificación es en general usando el comando UNIX ``file``. El módulo ``chardet`` de Python viene con un script llamado ``chardetect.py`` que adivinará la codificación específica, aunque no puedes confiar en que su conjetura sea correcta."

#: ../modules/feature_extraction.rst:652
msgid "You could try UTF-8 and disregard the errors. You can decode byte strings with ``bytes.decode(errors='replace')`` to replace all decoding errors with a meaningless character, or set ``decode_error='replace'`` in the vectorizer. This may damage the usefulness of your features."
msgstr "Puedes intentar con UTF-8 e ignorar los errores. Puedes decodificar cadenas de bytes con ``bytes.decode(errors='replace')`` para reemplazar todos los errores de decodificación con un carácter sin sentido, o establecer ``decode_error='replace'`` en el vectorizador. Esto puede dañar la utilidad de tus características."

#: ../modules/feature_extraction.rst:658
msgid "Real text may come from a variety of sources that may have used different encodings, or even be sloppily decoded in a different encoding than the one it was encoded with. This is common in text retrieved from the Web. The Python package `ftfy`_ can automatically sort out some classes of decoding errors, so you could try decoding the unknown text as ``latin-1`` and then using ``ftfy`` to fix errors."
msgstr "El texto real puede provenir de una variedad de fuentes que pueden haber utilizado diferentes codificaciones, o incluso ser decodificado de forma descuidada en una codificación diferente a la que fue codificado. Esto es común en textos recuperados de la web. El paquete de Python `ftfy`_ puede ordenar automáticamente algunas clases de errores de decodificación, así que puedes intentar decodificar el texto desconocido como ``latin-1`` y luego usar ``ftfy`` para corregir errores."

#: ../modules/feature_extraction.rst:665
msgid "If the text is in a mish-mash of encodings that is simply too hard to sort out (which is the case for the 20 Newsgroups dataset), you can fall back on a simple single-byte encoding such as ``latin-1``. Some text may display incorrectly, but at least the same sequence of bytes will always represent the same feature."
msgstr "Si el texto se encuentra en una mezcla de codificaciones que es simplemente demasiado difícil de ordenar (como es el caso del conjunto de datos 20 Newsgroups), puede recurrir a una codificación simple de un solo byte como ``latin-1``. Algún texto puede mostrarse incorrectamente, pero al menos la misma secuencia de bytes siempre representará la misma característica."

#: ../modules/feature_extraction.rst:671
msgid "For example, the following snippet uses ``chardet`` (not shipped with scikit-learn, must be installed separately) to figure out the encoding of three texts. It then vectorizes the texts and prints the learned vocabulary. The output is not shown here."
msgstr "Por ejemplo, el siguiente fragmento de código utiliza ``chardet`` (no se incluye con scikit-learn, debe ser instalado por separado) para determinar la codificación de tres textos. Luego vectoriza los textos e imprime el vocabulario aprendido. La salida no se muestra aquí."

#: ../modules/feature_extraction.rst:686
msgid "(Depending on the version of ``chardet``, it might get the first one wrong.)"
msgstr "(Dependiendo de la versión de ``chardet``, puede que se equivoque en la primera.)"

#: ../modules/feature_extraction.rst:688
msgid "For an introduction to Unicode and character encodings in general, see Joel Spolsky's `Absolute Minimum Every Software Developer Must Know About Unicode <https://www.joelonsoftware.com/articles/Unicode.html>`_."
msgstr "Para una introducción a Unicode y las codificaciones de caracteres en general, consulta Joel Spolsky `Absolute Minimum Every Software Developer Must Know About Unicode <https://www.joelonsoftware.com/articles/Unicode.html>`_."

#: ../modules/feature_extraction.rst:696
msgid "Applications and examples"
msgstr "Aplicaciones y ejemplos"

#: ../modules/feature_extraction.rst:698
msgid "The bag of words representation is quite simplistic but surprisingly useful in practice."
msgstr "La representación bolsa de palabras es bastante simplista pero sorprendentemente útil en la práctica."

#: ../modules/feature_extraction.rst:701
msgid "In particular in a **supervised setting** it can be successfully combined with fast and scalable linear models to train **document classifiers**, for instance:"
msgstr "En particular en una **situación de clasificación supervisada** puede combinarse con éxito con modelos lineales rápidos y escalables para entrenar **clasificadores de documentos**, por ejemplo:"

#: ../modules/feature_extraction.rst:705
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_classification_20newsgroups.py`"

#: ../modules/feature_extraction.rst:707
msgid "In an **unsupervised setting** it can be used to group similar documents together by applying clustering algorithms such as :ref:`k_means`:"
msgstr "En una **situación de clasificación no supervisada* se puede utilizar para agrupar documentos similares aplicando algoritmos de análisis de conglomerados como :ref:`k_means`:"

#: ../modules/feature_extraction.rst:710
msgid ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`"
msgstr ":ref:`sphx_glr_auto_examples_text_plot_document_clustering.py`"

#: ../modules/feature_extraction.rst:712
msgid "Finally it is possible to discover the main topics of a corpus by relaxing the hard assignment constraint of clustering, for instance by using :ref:`NMF`:"
msgstr "Finalmente, es posible descubrir los temas principales de un corpus relajando la restricción de asignación estricta del análisis de conglomerados, por ejemplo, usando: :ref:`NMF`:"

#: ../modules/feature_extraction.rst:716
msgid ":ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`"
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_topics_extraction_with_nmf_lda.py`"

#: ../modules/feature_extraction.rst:720
msgid "Limitations of the Bag of Words representation"
msgstr "Limitaciones de la representación Bolsa de palabras"

#: ../modules/feature_extraction.rst:722
msgid "A collection of unigrams (what bag of words is) cannot capture phrases and multi-word expressions, effectively disregarding any word order dependence. Additionally, the bag of words model doesn't account for potential misspellings or word derivations."
msgstr "Una colección de unigramas (lo que es la bolsa de palabras) no puede capturar frases y expresiones de varias palabras, ignorando efectivamente cualquier dependencia del orden de las palabras. Además, el modelo de bolsa de palabras no tiene en cuenta posibles errores ortográficos ni las derivaciones de las palabras."

#: ../modules/feature_extraction.rst:727
msgid "N-grams to the rescue! Instead of building a simple collection of unigrams (n=1), one might prefer a collection of bigrams (n=2), where occurrences of pairs of consecutive words are counted."
msgstr "¡N-gramas al rescate! En lugar de construir una simple colección de unigramas (n=1), se puede preferir una colección de bigramas (n=2), donde se cuentan las ocurrencias de pares de palabras consecutivas."

#: ../modules/feature_extraction.rst:731
msgid "One might alternatively consider a collection of character n-grams, a representation resilient against misspellings and derivations."
msgstr "Se puede considerar alternativamente una colección de n-gramas de caracteres, una representación resistente a errores ortográficos y derivaciones."

#: ../modules/feature_extraction.rst:734
msgid "For example, let's say we're dealing with a corpus of two documents: ``['words', 'wprds']``. The second document contains a misspelling of the word 'words'. A simple bag of words representation would consider these two as very distinct documents, differing in both of the two possible features. A character 2-gram representation, however, would find the documents matching in 4 out of 8 features, which may help the preferred classifier decide better::"
msgstr "Por ejemplo, digamos que estamos tratando con un corpus de dos documentos: ``['words', 'wprds']``. El segundo documento contiene una falta de ortografía de la palabra 'words'. Una simple representación Bolsa de palabras consideraría estos dos como documentos muy distintos, que difieren en las dos características posibles. Una representación de bigramas de caracteres, sin embargo, encontraría los documentos que coincidan en 4 de las 8 características, lo que puede ayudar al clasificador preferido a decidir mejor::"

#: ../modules/feature_extraction.rst:752
msgid "In the above example, ``char_wb`` analyzer is used, which creates n-grams only from characters inside word boundaries (padded with space on each side). The ``char`` analyzer, alternatively, creates n-grams that span across words::"
msgstr "En el ejemplo anterior, se utiliza el analizador ``char_wb``, que crea n-gramas sólo a partir de caracteres dentro de los límites de las palabras (rellenados con espacio a cada lado). El analizador ``char```, alternativamente, crea n-gramas que se extienden a través de palabras::"

#: ../modules/feature_extraction.rst:773
msgid "The word boundaries-aware variant ``char_wb`` is especially interesting for languages that use white-spaces for word separation as it generates significantly less noisy features than the raw ``char`` variant in that case. For such languages it can increase both the predictive accuracy and convergence speed of classifiers trained using such features while retaining the robustness with regards to misspellings and word derivations."
msgstr "La variante ``char_wb`` que conoce los límites de las palabras es especialmente interesante para los idiomas que usan espacios en blanco para la separación de palabras, ya que en ese caso genera características mucho menos ruidosas que la variante ``char`` en bruto. Para estos idiomas, puede aumentar tanto la precisión predictiva como la velocidad de convergencia de los clasificadores entrenados con estas características, al tiempo que mantiene la robustez con respecto a errores ortográficos y derivaciones de palabras."

#: ../modules/feature_extraction.rst:781
msgid "While some local positioning information can be preserved by extracting n-grams instead of individual words, bag of words and bag of n-grams destroy most of the inner structure of the document and hence most of the meaning carried by that internal structure."
msgstr "Mientras que la información de posicionamiento local se puede preservar extrayendo n-gramas en lugar de palabras individuales, la bolsa de palabras y la bolsa de n-gramas destruyen la mayor parte de la estructura interna del documento y, por tanto, la mayor parte del significado que conlleva esa estructura interna."

#: ../modules/feature_extraction.rst:786
msgid "In order to address the wider task of Natural Language Understanding, the local structure of sentences and paragraphs should thus be taken into account. Many such models will thus be casted as \"Structured output\" problems which are currently outside of the scope of scikit-learn."
msgstr "Con el fin de abordar la tarea más amplia de la comprensión del lenguaje natural, debe tenerse en cuenta la estructura local de frases y párrafos. Muchos de estos modelos serán, por lo tanto, considerados como problemas de \"salida estructurada\" que actualmente están fuera del alcance de scikit-learn."

#: ../modules/feature_extraction.rst:795
msgid "Vectorizing a large text corpus with the hashing trick"
msgstr "Vectorizando un corpus de texto grande con el truco de hashing"

#: ../modules/feature_extraction.rst:797
msgid "The above vectorization scheme is simple but the fact that it holds an **in- memory mapping from the string tokens to the integer feature indices** (the ``vocabulary_`` attribute) causes several **problems when dealing with large datasets**:"
msgstr "El esquema de vectorización anterior es simple, pero el hecho de que contenga **un mapeo de memoria de los tokens de cadena a los índices de características enteras** (el atributo ``vocabulary_`) causa varios **problemas al tratar con conjuntos de datos grandes**:"

#: ../modules/feature_extraction.rst:802
msgid "the larger the corpus, the larger the vocabulary will grow and hence the memory use too,"
msgstr "cuanto más grande sea el corpus, mayor será el vocabulario y, por tanto, también el uso de memoria,"

#: ../modules/feature_extraction.rst:805
msgid "fitting requires the allocation of intermediate data structures of size proportional to that of the original dataset."
msgstr "el ajuste requiere la asignación de estructuras de datos intermedias de tamaño proporcional al del conjunto de datos original."

#: ../modules/feature_extraction.rst:808
msgid "building the word-mapping requires a full pass over the dataset hence it is not possible to fit text classifiers in a strictly online manner."
msgstr "la construcción del mapeo de palabras requiere un pase completo sobre el conjunto de datos, por lo que no es posible ajustar los clasificadores de texto de una manera estrictamente en línea."

#: ../modules/feature_extraction.rst:811
msgid "pickling and un-pickling vectorizers with a large ``vocabulary_`` can be very slow (typically much slower than pickling / un-pickling flat data structures such as a NumPy array of the same size),"
msgstr "pickling y un-pickling vectorizadores con un ``vocabulary_`` extenso puede llegar a ser muy lento (típicamente mucho más lento que el pickling / un-pickling de estructuras de datos planas como un arreglo de NumPy del mismo tamaño),"

#: ../modules/feature_extraction.rst:815
msgid "it is not easily possible to split the vectorization work into concurrent sub tasks as the ``vocabulary_`` attribute would have to be a shared state with a fine grained synchronization barrier: the mapping from token string to feature index is dependent on ordering of the first occurrence of each token hence would have to be shared, potentially harming the concurrent workers' performance to the point of making them slower than the sequential variant."
msgstr "no es posible dividir fácilmente el trabajo de vectorización en subtareas simultáneas, ya que el atributo ``vocabulary_`` tendría que ser un estado compartido con una barrera de sincronización de granularidad fina: el mapeo de la cadena del token al índice de características depende del orden de la primera ocurrencia de cada token, por lo que tendría que ser compartido, perjudicando potencialmente el rendimiento simultáneo de los trabajadores hasta el punto de hacerlos más lentos que la variante secuencial."

#: ../modules/feature_extraction.rst:822
msgid "It is possible to overcome those limitations by combining the \"hashing trick\" (:ref:`Feature_hashing`) implemented by the :class:`~sklearn.feature_extraction.FeatureHasher` class and the text preprocessing and tokenization features of the :class:`CountVectorizer`."
msgstr "Es posible superar esas limitaciones combinando el \"truco de hashing\" (:ref:`Feature_hashing`) implementado por la clase :class:`~sklearn.feature_extraction.FeatureHasher` y el preprocesamiento de texto y tokenización de características de :class:`CountVectorizer`."

#: ../modules/feature_extraction.rst:827
msgid "This combination is implementing in :class:`HashingVectorizer`, a transformer class that is mostly API compatible with :class:`CountVectorizer`. :class:`HashingVectorizer` is stateless, meaning that you don't have to call ``fit`` on it::"
msgstr "Esta combinación está implementada en :class:`HashingVectorizer`, una clase transformadora que es mayormente compatible con la API de :class:`CountVectorizer`. :class:`HashingVectorizer` no tiene estado, lo que significa que no hay que llamar a ``fit`` en ella::"

#: ../modules/feature_extraction.rst:838
msgid "You can see that 16 non-zero feature tokens were extracted in the vector output: this is less than the 19 non-zeros extracted previously by the :class:`CountVectorizer` on the same toy corpus. The discrepancy comes from hash function collisions because of the low value of the ``n_features`` parameter."
msgstr "Puedes ver que se extrajeron 16 tokens de características no nulas en el vector de salida: esto es menor que los 19 no ceros extraídos previamente por :class:`CountVectorizer` en el mismo corpus de juego. La discrepancia proviene de las colisiones de la función hash debido al bajo valor del parámetro ``n_features``."

#: ../modules/feature_extraction.rst:843
msgid "In a real world setting, the ``n_features`` parameter can be left to its default value of ``2 ** 20`` (roughly one million possible features). If memory or downstream models size is an issue selecting a lower value such as ``2 ** 18`` might help without introducing too many additional collisions on typical text classification tasks."
msgstr "En un entorno del mundo real, el parámetro `` n_features '' se puede dejar en su valor predeterminado de `` 2 ** 20 '' (aproximadamente un millón de características posibles). Si la memoria o el tamaño de los modelos posteriores es un problema, seleccionar un valor inferior como `` 2 ** 18 '' podría ayudar sin introducir demasiadas colisiones adicionales en las tareas típicas de clasificación de texto."

#: ../modules/feature_extraction.rst:849
msgid "Note that the dimensionality does not affect the CPU training time of algorithms which operate on CSR matrices (``LinearSVC(dual=True)``, ``Perceptron``, ``SGDClassifier``, ``PassiveAggressive``) but it does for algorithms that work with CSC matrices (``LinearSVC(dual=False)``, ``Lasso()``, etc)."
msgstr "Ten en cuenta que la dimensionalidad no afecta al tiempo de entrenamiento de la CPU de algoritmos que operan con matrices CSR (``LinearSVC(dual=True)``, ``Perceptron``, ``SGDClassifier``, ``PassiveAggressive``), pero sí lo hace para algoritmos que trabajan con matrices CSC (``LinearSVC(dual=False)``, ``Lasso()``, etc)."

#: ../modules/feature_extraction.rst:855
msgid "Let's try again with the default setting::"
msgstr "Intentemos de nuevo con la configuración por defecto::"

#: ../modules/feature_extraction.rst:862
msgid "We no longer get the collisions, but this comes at the expense of a much larger dimensionality of the output space. Of course, other terms than the 19 used here might still collide with each other."
msgstr "Ya no tenemos las colisiones, pero esto se produce a costa de una dimensionalidad mucho mayor del espacio de salida. Por supuesto, otros términos distintos de los 19 utilizados aquí podrían seguir colisionando entre sí."

#: ../modules/feature_extraction.rst:867
msgid "The :class:`HashingVectorizer` also comes with the following limitations:"
msgstr "El :class:`HashingVectorizer` también viene con las siguientes limitaciones:"

#: ../modules/feature_extraction.rst:869
msgid "it is not possible to invert the model (no ``inverse_transform`` method), nor to access the original string representation of the features, because of the one-way nature of the hash function that performs the mapping."
msgstr "no es posible invertir el modelo (no hay método ``inverse_transform``), ni acceder a la representación original de cadenas de las características, debido a la naturaleza unidireccional de la función hash que realiza el mapeo."

#: ../modules/feature_extraction.rst:873
msgid "it does not provide IDF weighting as that would introduce statefulness in the model. A :class:`TfidfTransformer` can be appended to it in a pipeline if required."
msgstr "no proporciona ponderación de IDF, ya que esto introduciría la condición de admitir diferentes estados en el modelo. Un :class:`TfidfTransformer` puede ser anexado a él en un pipeline si es necesario."

#: ../modules/feature_extraction.rst:878
msgid "Performing out-of-core scaling with HashingVectorizer"
msgstr "Realizando escalado fuera del núcleo con HashingVectorizer"

#: ../modules/feature_extraction.rst:880
msgid "An interesting development of using a :class:`HashingVectorizer` is the ability to perform `out-of-core`_ scaling. This means that we can learn from data that does not fit into the computer's main memory."
msgstr "Un desarrollo interesante de usar un :class:`HashingVectorizer` es la capacidad de realizar un escalado `out-of-core`_. Esto significa que podemos aprender de datos que no encajan en la memoria principal del computador."

#: ../modules/feature_extraction.rst:886
msgid "A strategy to implement out-of-core scaling is to stream data to the estimator in mini-batches. Each mini-batch is vectorized using :class:`HashingVectorizer` so as to guarantee that the input space of the estimator has always the same dimensionality. The amount of memory used at any time is thus bounded by the size of a mini-batch. Although there is no limit to the amount of data that can be ingested using such an approach, from a practical point of view the learning time is often limited by the CPU time one wants to spend on the task."
msgstr "Una estrategia para implementar el escalado fuera del núcleo es enviar los datos al estimador en mini lotes. Cada mini lote es vectorizado usando :class:`HashingVectorizer` para garantizar que el espacio de entrada del estimador tenga siempre la misma dimensionalidad. La cantidad de memoria utilizada en cualquier momento está así limitada por el tamaño de un mini lote. Aunque no hay límite para la cantidad de datos que se pueden ingerir utilizando este enfoque, desde un punto de vista práctico el tiempo de aprendizaje a menudo está limitado por el tiempo de CPU que se quiera dedicar a la tarea."

#: ../modules/feature_extraction.rst:894
msgid "For a full-fledged example of out-of-core scaling in a text classification task see :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`."
msgstr "Para un ejemplo completo de escalado fuera del núcleo en una tarea de clasificación de texto, consulta :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`."

#: ../modules/feature_extraction.rst:898
msgid "Customizing the vectorizer classes"
msgstr "Personalización de las clases del vectorizador"

#: ../modules/feature_extraction.rst:900
msgid "It is possible to customize the behavior by passing a callable to the vectorizer constructor::"
msgstr "Es posible personalizar el comportamiento pasando una llamada al constructor del vectorizador::"

#: ../modules/feature_extraction.rst:911
msgid "In particular we name:"
msgstr "En particular, nombramos:"

#: ../modules/feature_extraction.rst:913
msgid "``preprocessor``: a callable that takes an entire document as input (as a single string), and returns a possibly transformed version of the document, still as an entire string. This can be used to remove HTML tags, lowercase the entire document, etc."
msgstr "``preprocessor``: una llamada que toma un documento entero como entrada (como una sola cadena), y devuelve una versión posiblemente transformada del documento, todavía como una cadena completa. Esto se puede utilizar para eliminar etiquetas HTML, poner en minúsculas todo el documento, etc."

#: ../modules/feature_extraction.rst:918
msgid "``tokenizer``: a callable that takes the output from the preprocessor and splits it into tokens, then returns a list of these."
msgstr "``tokenizer``: una llamada que toma la salida del preprocesador y la divide en tokens, luego devuelve una lista de estos."

#: ../modules/feature_extraction.rst:921
msgid "``analyzer``: a callable that replaces the preprocessor and tokenizer. The default analyzers all call the preprocessor and tokenizer, but custom analyzers will skip this. N-gram extraction and stop word filtering take place at the analyzer level, so a custom analyzer may have to reproduce these steps."
msgstr "``analyzer``: una llamada que reemplaza al preprocesador y al tokenizador. Todos los analizadores por defecto llaman al preprocesador y al tokenizador, pero los analizadores personalizados se saltarán esto. La extracción de N-gramas y el filtrado de palabras funcionales tienen lugar en el nivel del analizador, por lo que un analizador personalizado puede tener que reproducir estos pasos."

#: ../modules/feature_extraction.rst:927
msgid "(Lucene users might recognize these names, but be aware that scikit-learn concepts may not map one-to-one onto Lucene concepts.)"
msgstr "(Los usuarios de Lucene pueden reconocer estos nombres, pero ten en cuenta que los conceptos de scikit-learn pueden no coincidir uno a uno con los conceptos de Lucene.)"

#: ../modules/feature_extraction.rst:930
msgid "To make the preprocessor, tokenizer and analyzers aware of the model parameters it is possible to derive from the class and override the ``build_preprocessor``, ``build_tokenizer`` and ``build_analyzer`` factory methods instead of passing custom functions."
msgstr "Para que el preprocesador, el tokenizador y los analizadores sean conscientes de los parámetros del modelo es posible derivar de la clase y anular los métodos de fábrica ``build_preprocessor``, ``build_tokenizer`` y ``build_analyzer`` en lugar de pasar funciones personalizadas."

#: ../modules/feature_extraction.rst:935
msgid "Some tips and tricks:"
msgstr "Algunos consejos y trucos:"

#: ../modules/feature_extraction.rst:937
msgid "If documents are pre-tokenized by an external package, then store them in files (or strings) with the tokens separated by whitespace and pass ``analyzer=str.split``"
msgstr "Si los documentos están pre-tokenizados por un paquete externo, entonces almacénalos en archivos (o cadenas) con los tokens separados por espacios en blanco y pasa ``analyzer=str.split``"

#: ../modules/feature_extraction.rst:940
msgid "Fancy token-level analysis such as stemming, lemmatizing, compound splitting, filtering based on part-of-speech, etc. are not included in the scikit-learn codebase, but can be added by customizing either the tokenizer or the analyzer. Here's a ``CountVectorizer`` with a tokenizer and lemmatizer using `NLTK <https://www.nltk.org/>`_::"
msgstr "El análisis sofisticado a nivel de tokens, como el stemming, la lematización, la división compuesta, el filtrado basado en etiquetas part-of-speech, etc., no se incluyen en la base de código de scikit-learn, pero puede añadirse personalizando el tokenizador o el analizador. Aquí hay un ``CountVectorizer`` con un tokenizador y lematizador usando `NLTK <https://www.nltk.org/>`_::"

#: ../modules/feature_extraction.rst:957
msgid "(Note that this will not filter out punctuation.)"
msgstr "(Nota que esto no filtrará los signos de puntuación.)"

#: ../modules/feature_extraction.rst:960
msgid "The following example will, for instance, transform some British spelling to American spelling::"
msgstr "El siguiente ejemplo transformará, por ejemplo, algunas ortografías Británicas en ortografías Americanas::"

#: ../modules/feature_extraction.rst:980
msgid "for other styles of preprocessing; examples include stemming, lemmatization, or normalizing numerical tokens, with the latter illustrated in:"
msgstr "para otros estilos de preprocesamiento; los ejemplos incluyen stemming, lematización o normalización de tokens numéricos, con estos últimos ilustrados en:"

#: ../modules/feature_extraction.rst:983
msgid ":ref:`sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py`"
msgstr ":ref:`sphx_glr_auto_examples_bicluster_plot_bicluster_newsgroups.py`"

#: ../modules/feature_extraction.rst:986
msgid "Customizing the vectorizer can also be useful when handling Asian languages that do not use an explicit word separator such as whitespace."
msgstr "La personalización del vectorizador también puede ser útil cuando se manejan idiomas asiáticos que no utilizan un separador de palabras explícito, como los espacios en blanco."

#: ../modules/feature_extraction.rst:992
msgid "Image feature extraction"
msgstr "Extracción de características de imagen"

#: ../modules/feature_extraction.rst:997
msgid "Patch extraction"
msgstr "Extracción de parches"

#: ../modules/feature_extraction.rst:999
msgid "The :func:`extract_patches_2d` function extracts patches from an image stored as a two-dimensional array, or three-dimensional with color information along the third axis. For rebuilding an image from all its patches, use :func:`reconstruct_from_patches_2d`. For example let use generate a 4x4 pixel picture with 3 color channels (e.g. in RGB format)::"
msgstr "La función :func:`extract_patches_2d` extrae parches de una imagen almacenada como un arreglo bidimensional o tridimensional con información de color a lo largo del tercer eje. Para reconstruir una imagen a partir de todos sus parches, utiliza :func:`reconstruct_from_patches_2d`. Por ejemplo, generemos una imagen de 4x4 píxeles con 3 canales de color (por ejemplo, en formato RGB)::"

#: ../modules/feature_extraction.rst:1032
msgid "Let us now try to reconstruct the original image from the patches by averaging on overlapping areas::"
msgstr "Intentemos ahora reconstruir la imagen original a partir de los parches promediando en áreas superpuestas::"

#: ../modules/feature_extraction.rst:1038
msgid "The :class:`PatchExtractor` class works in the same way as :func:`extract_patches_2d`, only it supports multiple images as input. It is implemented as an estimator, so it can be used in pipelines. See::"
msgstr "La clase :class:`PatchExtractor` funciona de la misma manera que :func:`extract_patches_2d`, sólo que soporta múltiples imágenes como entrada. Se implementa como un estimador, así que puede ser utilizado en pipelines. Ver ::"

#: ../modules/feature_extraction.rst:1048
msgid "Connectivity graph of an image"
msgstr "Grafo de conectividad de una imagen"

#: ../modules/feature_extraction.rst:1050
msgid "Several estimators in the scikit-learn can use connectivity information between features or samples. For instance Ward clustering (:ref:`hierarchical_clustering`) can cluster together only neighboring pixels of an image, thus forming contiguous patches:"
msgstr "Varios estimadores de scikit-learn pueden utilizar la información de conectividad entre características o muestras. Por ejemplo, el agrupamiento de Ward (:ref:`hierarchical_clustering`) puede agrupar sólo los píxeles vecinos de una imagen, formando así parches contiguos:"

#: ../modules/feature_extraction.rst:1060
msgid "For this purpose, the estimators use a 'connectivity' matrix, giving which samples are connected."
msgstr "Para ello, los estimadores utilizan una matriz \"conectividad\", que indica las muestras que están conectadas."

#: ../modules/feature_extraction.rst:1063
msgid "The function :func:`img_to_graph` returns such a matrix from a 2D or 3D image. Similarly, :func:`grid_to_graph` build a connectivity matrix for images given the shape of these image."
msgstr "La función :func:`img_to_graph` devuelve una matriz de este tipo a partir de una imagen 2D o 3D. Del mismo modo, :func:`grid_to_graph` construye una matriz de conectividad para imágenes dada la forma de estas."

#: ../modules/feature_extraction.rst:1067
msgid "These matrices can be used to impose connectivity in estimators that use connectivity information, such as Ward clustering (:ref:`hierarchical_clustering`), but also to build precomputed kernels, or similarity matrices."
msgstr "Estas matrices pueden utilizarse para imponer conectividad en estimadores que utilizan información de conectividad, tales como el agrupamiento de Ward (:ref:`hierarchical_clustering`), pero también para construir kernels precalculados, o matrices de similaridad."

#: ../modules/feature_extraction.rst:1072
msgid "**Examples**"
msgstr "**Ejemplos**"

#: ../modules/feature_extraction.rst:1074
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_coin_ward_segmentation.py`"

#: ../modules/feature_extraction.rst:1076
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_segmentation_toy.py`"

#: ../modules/feature_extraction.rst:1078
msgid ":ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`"
msgstr ":ref:`sphx_glr_auto_examples_cluster_plot_feature_agglomeration_vs_univariate_selection.py`"

