msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-19 12:08\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/covariance.po\n"
"X-Crowdin-File-ID: 4868\n"
"Language: es_ES\n"

#: ../modules/covariance.rst:5
msgid "Covariance estimation"
msgstr "Estimación de covarianza"

#: ../modules/covariance.rst:10
msgid "Many statistical problems require the estimation of a population's covariance matrix, which can be seen as an estimation of data set scatter plot shape. Most of the time, such an estimation has to be done on a sample whose properties (size, structure, homogeneity) have a large influence on the estimation's quality. The :mod:`sklearn.covariance` package provides tools for accurately estimating a population's covariance matrix under various settings."
msgstr "Muchos problemas estadísticos requieren la estimación de la matriz de covarianza de una población, que puede considerarse como una estimación de la forma del diagrama de dispersión del conjunto de datos. La mayoría de las veces, esta estimación debe realizarse sobre una muestra cuyas propiedades (tamaño, estructura, homogeneidad) tienen una gran influencia en la calidad de la estimación. El paquete :mod:`sklearn.covariance` proporciona herramientas para la estimación precisa de la matriz de covarianza de una población de acuerdo con varias condiciones."

#: ../modules/covariance.rst:18
msgid "We assume that the observations are independent and identically distributed (i.i.d.)."
msgstr "Asumimos que las observaciones son independientes e idénticamente distribuidas (i.i.d.)."

#: ../modules/covariance.rst:23
msgid "Empirical covariance"
msgstr "Covarianza empírica"

#: ../modules/covariance.rst:25
msgid "The covariance matrix of a data set is known to be well approximated by the classical *maximum likelihood estimator* (or \"empirical covariance\"), provided the number of observations is large enough compared to the number of features (the variables describing the observations). More precisely, the Maximum Likelihood Estimator of a sample is an asymptotically unbiased estimator of the corresponding population's covariance matrix."
msgstr "Se sabe que la matriz de covarianza de un conjunto de datos está bien aproximada por el clásico *estimador de máxima verosimilitud* (o \"covarianza empírica\"), siempre que el número de observaciones sea lo suficientemente grande en comparación con el número de características (las variables que describen las observaciones). Más concretamente, el estimador de máxima verosimilitud de una muestra es un estimador asintóticamente no sesgado de la matriz de covarianza de la población correspondiente."

#: ../modules/covariance.rst:33
msgid "The empirical covariance matrix of a sample can be computed using the :func:`empirical_covariance` function of the package, or by fitting an :class:`EmpiricalCovariance` object to the data sample with the :meth:`EmpiricalCovariance.fit` method. Be careful that results depend on whether the data are centered, so one may want to use the ``assume_centered`` parameter accurately. More precisely, if ``assume_centered=False``, then the test set is supposed to have the same mean vector as the training set. If not, both should be centered by the user, and ``assume_centered=True`` should be used."
msgstr "La matriz de covarianza empírica de una muestra puede calcularse utilizando la función :func:`empirical_covariance` del paquete, o ajustando un objeto :class:`EmpiricalCovariance` a la muestra de datos con el método :meth:`EmpiricalCovariance.fit`. Hay que tener en cuenta que los resultados dependen de si los datos están centrados, por lo que es conveniente utilizar el parámetro ``assume_centered`` con precisión. Más concretamente, si ``assume_centered=False``, se supone que el conjunto de prueba tiene el mismo vector medio que el conjunto de entrenamiento. Si no es así, ambos deben ser centrados por el usuario, y se debe utilizar ``assume_centered=True``."

#: ../modules/covariance.rst:45
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for an example on how to fit an :class:`EmpiricalCovariance` object to data."
msgstr "Ver :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` para un ejemplo sobre cómo ajustar un objeto de :class:`EmpiricalCovariance` a los datos."

#: ../modules/covariance.rst:53
msgid "Shrunk Covariance"
msgstr "Covarianza Reducida"

#: ../modules/covariance.rst:56
msgid "Basic shrinkage"
msgstr "Reducción básica"

#: ../modules/covariance.rst:58
msgid "Despite being an asymptotically unbiased estimator of the covariance matrix, the Maximum Likelihood Estimator is not a good estimator of the eigenvalues of the covariance matrix, so the precision matrix obtained from its inversion is not accurate. Sometimes, it even occurs that the empirical covariance matrix cannot be inverted for numerical reasons. To avoid such an inversion problem, a transformation of the empirical covariance matrix has been introduced: the ``shrinkage``."
msgstr "A pesar de ser un estimador asintóticamente no sesgado de la matriz de covarianza, el Estimador de Máxima Verosimilitud no es un buen estimador de los valores propios de la matriz de covarianza, por lo que la matriz de precisión obtenida de su inversión no es exacta. A veces, incluso ocurre que la matriz de covarianza empírica no puede invertirse por razones numéricas. Para evitar este problema de inversión, se ha introducido una transformación de la matriz de covarianza empírica: la ``reducción`` (shrinkage)."

#: ../modules/covariance.rst:66
msgid "In scikit-learn, this transformation (with a user-defined shrinkage coefficient) can be directly applied to a pre-computed covariance with the :func:`shrunk_covariance` method. Also, a shrunk estimator of the covariance can be fitted to data with a :class:`ShrunkCovariance` object and its :meth:`ShrunkCovariance.fit` method. Again, results depend on whether the data are centered, so one may want to use the ``assume_centered`` parameter accurately."
msgstr "En scikit-learn, esta transformación (con un coeficiente de contracción definido por el usuario) se puede aplicar directamente a una covarianza precalculada con el método :func:`shrunk_covariance`. Además, un estimador reducido de la covarianza puede ajustarse a los datos con un objeto :class:`ShrunkCovariance` y su método :meth:`ShrunkCovariance.fit`. Una vez más, los resultados dependen de si los datos están centrados, por lo que uno puede querer utilizar el parámetro ``assume_centered`` con precisión."

#: ../modules/covariance.rst:75
msgid "Mathematically, this shrinkage consists in reducing the ratio between the smallest and the largest eigenvalues of the empirical covariance matrix. It can be done by simply shifting every eigenvalue according to a given offset, which is equivalent of finding the l2-penalized Maximum Likelihood Estimator of the covariance matrix. In practice, shrinkage boils down to a simple a convex transformation : :math:`\\Sigma_{\\rm shrunk} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{{\\rm Tr}\\hat{\\Sigma}}{p}\\rm Id`."
msgstr "Matemáticamente, esta reducción consiste en disminuir la relación entre los valores propios más pequeños y los más grandes de la matriz de covarianza empírica. Puede hacerse simplemente desplazando cada autovalor según un desplazamiento dado, lo que equivale a encontrar el Estimador de Máxima Verosimilitud l2-penalizado de la matriz de covarianza. En la práctica, la reducción se reduce a una simple transformación convexa: :math:`\\Sigma_{\\rm shrunk} = (1-\\alpha)\\hat{\\Sigma} + \\alpha\\frac{\\rm Tr}{hat{\\Sigma}{p}\\rm Id`."

#: ../modules/covariance.rst:84
msgid "Choosing the amount of shrinkage, :math:`\\alpha` amounts to setting a bias/variance trade-off, and is discussed below."
msgstr "La elección de la cantidad de reducción, :math:`\\alpha` equivale a establecer un compromiso de sesgo/varianza, que se discute a continuación."

#: ../modules/covariance.rst:89
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for an example on how to fit a :class:`ShrunkCovariance` object to data."
msgstr "Ver :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` para un ejemplo sobre cómo ajustar un objeto de :class:`EmpiricalCovariance` a los datos."

#: ../modules/covariance.rst:95
msgid "Ledoit-Wolf shrinkage"
msgstr "Reducción de Ledoit-Wolf"

#: ../modules/covariance.rst:97
msgid "In their 2004 paper [1]_, O. Ledoit and M. Wolf propose a formula to compute the optimal shrinkage coefficient :math:`\\alpha` that minimizes the Mean Squared Error between the estimated and the real covariance matrix."
msgstr "En su artículo de 2004 [1]_, O. Ledoit y M. Wolf proponen una fórmula para calcular el coeficiente de reducción óptimo :math:`\\alpha` que minimiza el error cuadrático medio entre la matriz estimada y la matriz de covarianza real."

#: ../modules/covariance.rst:102
msgid "The Ledoit-Wolf estimator of the covariance matrix can be computed on a sample with the :meth:`ledoit_wolf` function of the :mod:`sklearn.covariance` package, or it can be otherwise obtained by fitting a :class:`LedoitWolf` object to the same sample."
msgstr "El estimador Ledoit-Wolf de la matriz de covarianza puede calcularse sobre una muestra con la función :meth:`ledoit_wolf` del paquete :mod:`sklearn.covariance`, o puede obtenerse de otro modo ajustando un objeto :class:`LedoitWolf` a la misma muestra."

#: ../modules/covariance.rst:107
msgid "**Case when population covariance matrix is isotropic**"
msgstr "**Caso cuando la matriz de covarianza de población es isotrópica**"

#: ../modules/covariance.rst:109
msgid "It is important to note that when the number of samples is much larger than the number of features, one would expect that no shrinkage would be necessary. The intuition behind this is that if the population covariance is full rank, when the number of sample grows, the sample covariance will also become positive definite. As a result, no shrinkage would necessary and the method should automatically do this."
msgstr "Es importante señalar que cuando el número de muestras es mucho mayor que el número de características, cabría esperar que no fuera necesaria ninguna reducción. La intuición detrás de esto es que si la covarianza de la población es de rango completo, cuando el número de muestras crece, la covarianza de la muestra también se convertirá en positiva definida. Por lo tanto, no sería necesaria ninguna reducción y el método debería hacerlo automáticamente."

#: ../modules/covariance.rst:116
msgid "This, however, is not the case in the Ledoit-Wolf procedure when the population covariance happens to be a multiple of the identity matrix. In this case, the Ledoit-Wolf shrinkage estimate approaches 1 as the number of samples increases. This indicates that the optimal estimate of the covariance matrix in the Ledoit-Wolf sense is multiple of the identity. Since the population covariance is already a multiple of the identity matrix, the Ledoit-Wolf solution is indeed a reasonable estimate."
msgstr "Sin embargo, esto no ocurre en el procedimiento de Ledoit-Wolf cuando la covarianza de la población resulta ser un múltiplo de la matriz de identidad. En este caso, la estimación de la reducción de Ledoit-Wolf se aproxima a 1 a medida que aumenta el número de muestras. Esto indica que la estimación óptima de la matriz de covarianza en el sentido de Ledoit-Wolf es múltiplo de la identidad. Dado que la covarianza de la población ya es un múltiplo de la matriz de identidad, la solución de Ledoit-Wolf es efectivamente una estimación razonable."

#: ../modules/covariance.rst:126
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for an example on how to fit a :class:`LedoitWolf` object to data and for visualizing the performances of the Ledoit-Wolf estimator in terms of likelihood."
msgstr "Ver :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` para un ejemplo sobre cómo ajustar un objeto :class:`LedoitWolf` a los datos y para visualizar la eficiencia del estimador Ledoit-Wolf en términos de verosimilitud."

#: ../modules/covariance.rst:133
msgid "O. Ledoit and M. Wolf, \"A Well-Conditioned Estimator for Large-Dimensional Covariance Matrices\", Journal of Multivariate Analysis, Volume 88, Issue 2, February 2004, pages 365-411."
msgstr "O. Ledoit y M. Wolf, \"A Well Conditioned Estimator for Large-Dimensional Covariance Matrices\", Journal of Multivariate Analysis, Volumen 88, Número 2, febrero 2004, páginas 365-411."

#: ../modules/covariance.rst:140
msgid "Oracle Approximating Shrinkage"
msgstr "Reducción Aproximante de Oracle"

#: ../modules/covariance.rst:142
msgid "Under the assumption that the data are Gaussian distributed, Chen et al. [2]_ derived a formula aimed at choosing a shrinkage coefficient that yields a smaller Mean Squared Error than the one given by Ledoit and Wolf's formula. The resulting estimator is known as the Oracle Shrinkage Approximating estimator of the covariance."
msgstr "Bajo el supuesto de que los datos tienen una distribución gaussiana, Chen et al. [2]_ derivaron una fórmula destinada a elegir un coeficiente de reducción que produzca un error cuadrático medio menor que el dado por la fórmula de Ledoit y Wolf. El estimador resultante se conoce como estimador de la reducción aproximante de Oracle (OAS) de la covarianza."

#: ../modules/covariance.rst:148
msgid "The OAS estimator of the covariance matrix can be computed on a sample with the :meth:`oas` function of the :mod:`sklearn.covariance` package, or it can be otherwise obtained by fitting an :class:`OAS` object to the same sample."
msgstr "El estimador OAS de la matriz de covarianza puede calcularse sobre una muestra con la función :meth:`oas` del paquete :mod:`sklearn.covariance`, o puede obtenerse de otro modo ajustando un objeto :class:`OAS` a la misma muestra."

#: ../modules/covariance.rst:158
msgid "Bias-variance trade-off when setting the shrinkage: comparing the choices of Ledoit-Wolf and OAS estimators"
msgstr "Compensación entre el sesgo y la varianza al fijar la reducción: comparación de las opciones de los estimadores Ledoit-Wolf y OAS"

#: ../modules/covariance.rst:163
msgid "Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\", IEEE Trans. on Sign. Proc., Volume 58, Issue 10, October 2010."
msgstr "Chen et al., \"Shrinkage Algorithms for MMSE Covariance Estimation\", IEEE Trans. on Sign. Proc., Volume 58, Issue 10, Octubre 2010."

#: ../modules/covariance.rst:168
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` for an example on how to fit an :class:`OAS` object to data."
msgstr "Vea :ref:`sphx_glr_auto_examples_covariance_plot_covariance_estimation.py` para un ejemplo sobre cómo ajustar un objeto de :class:`OAS` a los datos."

#: ../modules/covariance.rst:172
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py` to visualize the Mean Squared Error difference between a :class:`LedoitWolf` and an :class:`OAS` estimator of the covariance."
msgstr "Véase :ref:`sphx_glr_auto_examples_covariance_plot_lw_vs_oas.py` para visualizar la diferencia del error cuadrático medio entre un estimador :class:`LedoitWolf` y un estimador :class:`OAS` de la covarianza."

#: ../modules/covariance.rst:186
msgid "Sparse inverse covariance"
msgstr "Covarianza inversa dispersa"

#: ../modules/covariance.rst:188
msgid "The matrix inverse of the covariance matrix, often called the precision matrix, is proportional to the partial correlation matrix. It gives the partial independence relationship. In other words, if two features are independent conditionally on the others, the corresponding coefficient in the precision matrix will be zero. This is why it makes sense to estimate a sparse precision matrix: the estimation of the covariance matrix is better conditioned by learning independence relations from the data. This is known as *covariance selection*."
msgstr "La matriz inversa de la matriz de covarianza, a menudo llamada matriz de precisión, es proporcional a la matriz de correlación parcial. Proporciona la relación de independencia parcial. En otras palabras, si dos características son independientes condicionalmente de las otras, el coeficiente correspondiente en la matriz de precisión será cero. Por eso tiene sentido estimar una matriz de precisión dispersa: la estimación de la matriz de covarianza está mejor condicionada por el reconocimiento de las relaciones de independencia a partir de los datos. Esto se conoce como *selección de covarianza*."

#: ../modules/covariance.rst:197
msgid "In the small-samples situation, in which ``n_samples`` is on the order of ``n_features`` or smaller, sparse inverse covariance estimators tend to work better than shrunk covariance estimators. However, in the opposite situation, or for very correlated data, they can be numerically unstable. In addition, unlike shrinkage estimators, sparse estimators are able to recover off-diagonal structure."
msgstr "En la situación de muestras pequeñas, en la que ``n_muestras`` es del orden de ``n_caracteres`` o menor, los estimadores de covarianza inversa dispersa tienden a funcionar mejor que los estimadores de covarianza reducida. Sin embargo, en la situación contraria, o para datos muy correlacionados, pueden ser numéricamente inestables. Además, a diferencia de los estimadores de reducción, los estimadores dispersos son capaces de recuperar la estructura no diagonal."

#: ../modules/covariance.rst:204
msgid "The :class:`GraphicalLasso` estimator uses an l1 penalty to enforce sparsity on the precision matrix: the higher its ``alpha`` parameter, the more sparse the precision matrix. The corresponding :class:`GraphicalLassoCV` object uses cross-validation to automatically set the ``alpha`` parameter."
msgstr "El estimador :class:`GraphicalLasso` utiliza una penalización l1 para imponer la dispersión en la matriz de precisión: cuanto mayor sea su parámetro ``alpha``, más dispersa será la matriz de precisión. El objeto :class:`GraphicalLassoCV` correspondiente utiliza la validación cruzada para establecer automáticamente el parámetro ``alpha``."

#: ../modules/covariance.rst:214
msgid "*A comparison of maximum likelihood, shrinkage and sparse estimates of the covariance and precision matrix in the very small samples settings.*"
msgstr "*Una comparación de las estimaciones de máxima verosimilitud, reducción y dispersión de la matriz de covarianza y precisión en las modalidades de muestras muy pequeñas.*"

#: ../modules/covariance.rst:218
msgid "**Structure recovery**"
msgstr "**Recuperación de la estructura**"

#: ../modules/covariance.rst:220
msgid "Recovering a graphical structure from correlations in the data is a challenging thing. If you are interested in such recovery keep in mind that:"
msgstr "Recuperar una estructura gráfica de las correlaciones en los datos es un desafío. Si estás interesado en tal recuperación, ten en cuenta que:"

#: ../modules/covariance.rst:224
msgid "Recovery is easier from a correlation matrix than a covariance matrix: standardize your observations before running :class:`GraphicalLasso`"
msgstr "La recuperación es más fácil a partir de una matriz de correlaciones que de una matriz de covarianza: estandariza tus observaciones antes de ejecutar :class:`GraphicalLasso`"

#: ../modules/covariance.rst:227
msgid "If the underlying graph has nodes with much more connections than the average node, the algorithm will miss some of these connections."
msgstr "Si el gráfico subyacente tiene nodos con muchas más conexiones que el nodo promedio, el algoritmo perderá algunas de estas conexiones."

#: ../modules/covariance.rst:230
msgid "If your number of observations is not large compared to the number of edges in your underlying graph, you will not recover it."
msgstr "Si tu número de observaciones no es elevado en comparación con el número de aristas de tu gráfico subyacente, no lo recuperarás."

#: ../modules/covariance.rst:233
msgid "Even if you are in favorable recovery conditions, the alpha parameter chosen by cross-validation (e.g. using the :class:`GraphicalLassoCV` object) will lead to selecting too many edges. However, the relevant edges will have heavier weights than the irrelevant ones."
msgstr "Incluso en condiciones favorables de recuperación, el parámetro alfa elegido por la validación cruzada (por ejemplo, utilizando el objeto :class:`GraphicalLassoCV`) conducirá a seleccionar demasiadas aristas. Sin embargo, las aristas relevantes tendrán pesos más elevados que las irrelevantes."

#: ../modules/covariance.rst:239
msgid "The mathematical formulation is the following:"
msgstr "La formulación matemática es la siguiente:"

#: ../modules/covariance.rst:241
msgid "\\hat{K} = \\mathrm{argmin}_K \\big(\n"
"            \\mathrm{tr} S K - \\mathrm{log} \\mathrm{det} K\n"
"            + \\alpha \\|K\\|_1\n"
"            \\big)"
msgstr "\\hat{K} = \\mathrm{argmin}_K \\big(\n"
"            \\mathrm{tr} S K - \\mathrm{log} \\mathrm{det} K\n"
"            + \\alpha \\|K\\|_1\n"
"            \\big)"

#: ../modules/covariance.rst:248
msgid "Where :math:`K` is the precision matrix to be estimated, and :math:`S` is the sample covariance matrix. :math:`\\|K\\|_1` is the sum of the absolute values of off-diagonal coefficients of :math:`K`. The algorithm employed to solve this problem is the GLasso algorithm, from the Friedman 2008 Biostatistics paper. It is the same algorithm as in the R ``glasso`` package."
msgstr "Donde :math:`K` es la matriz de precisión a estimar, y :math:`S` es la matriz de covarianza de la muestra. :math:`\\|K\\_1` es la suma de los valores absolutos de los coeficientes fuera de diagonal de :math:`K`. El algoritmo empleado para resolver este problema es el algoritmo GLasso, del trabajo de Friedman 2008 en Biostatistics. Es el mismo algoritmo que aparece en el paquete ``glasso`` de R."

#: ../modules/covariance.rst:257
msgid ":ref:`sphx_glr_auto_examples_covariance_plot_sparse_cov.py`: example on synthetic data showing some recovery of a structure, and comparing to other covariance estimators."
msgstr ":ref:`sphx_glr_auto_examples_covariance_plot_sparse_cov.py`: ejemplo de datos sintéticos que muestran alguna recuperación de una estructura, y compara con otros estimadores de covariancia."

#: ../modules/covariance.rst:261
msgid ":ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`: example on real stock market data, finding which symbols are most linked."
msgstr ":ref:`sphx_glr_auto_examples_applications_plot_stock_market.py`: ejemplo en datos reales del mercado de valores, encontrando qué símbolos están más relacionados."

#: ../modules/covariance.rst:266
msgid "Friedman et al, `\"Sparse inverse covariance estimation with the graphical lasso\" <https://biostatistics.oxfordjournals.org/content/9/3/432.short>`_, Biostatistics 9, pp 432, 2008"
msgstr "Friedman et al, `\"Sparse inverse covariance estimation with the graphical lasso\" <https://biostatistics.oxfordjournals.org/content/9/3/432.short>`_, Biostatistics 9, pp 432, 2008"

#: ../modules/covariance.rst:273
msgid "Robust Covariance Estimation"
msgstr "Estimación de Covarianza Robusta"

#: ../modules/covariance.rst:275
msgid "Real data sets are often subject to measurement or recording errors. Regular but uncommon observations may also appear for a variety of reasons. Observations which are very uncommon are called outliers. The empirical covariance estimator and the shrunk covariance estimators presented above are very sensitive to the presence of outliers in the data. Therefore, one should use robust covariance estimators to estimate the covariance of its real data sets. Alternatively, robust covariance estimators can be used to perform outlier detection and discard/downweight some observations according to further processing of the data."
msgstr "Los conjuntos de datos reales suelen estar sujetos a errores de medición o de registro. También pueden aparecer observaciones regulares pero poco comunes por diversas razones. Las observaciones que son muy poco comunes se denominan valores atípicos. El estimador de la covarianza empírica y los estimadores de la covarianza reducida presentados anteriormente son muy sensibles a la presencia de valores atípicos en los datos. Por lo tanto, hay que utilizar estimadores de covarianza robustos para estimar la covarianza de sus conjuntos de datos reales. Por otra parte, los estimadores de covarianza robustos pueden utilizarse para realizar la detección de valores atípicos y descartar/reducir la ponderación de algunas observaciones de acuerdo con el procesamiento posterior de los datos."

#: ../modules/covariance.rst:287
msgid "The ``sklearn.covariance`` package implements a robust estimator of covariance, the Minimum Covariance Determinant [3]_."
msgstr "El paquete ``sklearn.covariance`` implementa un estimador robusto de la covarianza, el Determinante Mínimo de la Covarianza [3]_."

#: ../modules/covariance.rst:292
msgid "Minimum Covariance Determinant"
msgstr "Determinante Mínimo de la Covarianza"

#: ../modules/covariance.rst:294
msgid "The Minimum Covariance Determinant estimator is a robust estimator of a data set's covariance introduced by P.J. Rousseeuw in [3]_.  The idea is to find a given proportion (h) of \"good\" observations which are not outliers and compute their empirical covariance matrix.  This empirical covariance matrix is then rescaled to compensate the performed selection of observations (\"consistency step\").  Having computed the Minimum Covariance Determinant estimator, one can give weights to observations according to their Mahalanobis distance, leading to a reweighted estimate of the covariance matrix of the data set (\"reweighting step\")."
msgstr "El Estimador del Determinante Mínimo de la Covarianza es un estimador robusto de la covarianza de un conjunto de datos introducido por P.J. Rousseeuw en [3]_.  La idea es encontrar una determinada proporción (h) de observaciones \"buenas\" que no sean valores atípicos y calcular su matriz de covarianza empírica.  Esta matriz de covarianza empírica se reescala para compensar la selección de observaciones realizada (\"paso de consistencia\").  Una vez calculado el Estimador del Determinante Mínimo de la Covarianza, se pueden asignar pesos a las observaciones en función de su distancia de Mahalanobis, lo que conduce a una estimación reponderada de la matriz de covarianza del conjunto de datos (\"paso de reponderación\")."

#: ../modules/covariance.rst:305
msgid "Rousseeuw and Van Driessen [4]_ developed the FastMCD algorithm in order to compute the Minimum Covariance Determinant. This algorithm is used in scikit-learn when fitting an MCD object to data. The FastMCD algorithm also computes a robust estimate of the data set location at the same time."
msgstr "Rousseeuw y Van Driessen [4]_ desarrollaron el algoritmo FastMCD para calcular el Determinante de Covarianza Mínimo. Este algoritmo se utiliza en scikit-learn cuando se ajusta un objeto MCD a los datos. El algoritmo FastMCD también calcula una estimación robusta de la ubicación del conjunto de datos al mismo tiempo."

#: ../modules/covariance.rst:311
msgid "Raw estimates can be accessed as ``raw_location_`` and ``raw_covariance_`` attributes of a :class:`MinCovDet` robust covariance estimator object."
msgstr "Se puede acceder a las estimaciones en bruto como atributos ``raw_location_`` y ``raw_covariance_`` de un objeto estimador de covarianza robusta :class:`MinCovDet`."

#: ../modules/covariance.rst:316
msgid "P. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871, 1984."
msgstr "P. J. Rousseeuw. Least median of squares regression. J. Am Stat Ass, 79:871, 1984."

#: ../modules/covariance.rst:318
msgid "A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS."
msgstr "A Fast Algorithm for the Minimum Covariance Determinant Estimator, 1999, American Statistical Association and the American Society for Quality, TECHNOMETRICS."

#: ../modules/covariance.rst:324
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py` for an example on how to fit a :class:`MinCovDet` object to data and see how the estimate remains accurate despite the presence of outliers."
msgstr "Puedes consultar :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py` para ver un ejemplo sobre cómo ajustar un objeto :class:`MinCovDet` a los datos y ver cómo la estimación sigue siendo precisa a pesar de la presencia de valores atípicos."

#: ../modules/covariance.rst:328
msgid "See :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` to visualize the difference between :class:`EmpiricalCovariance` and :class:`MinCovDet` covariance estimators in terms of Mahalanobis distance (so we get a better estimate of the precision matrix too)."
msgstr "Ver :ref:`sphx_glr_auto_examples_covariance_plot_mahalanobis_distances.py` para visualizar la diferencia entre los estimadores de covarianza :class:`EmpiricalCovariance` y :class:`MinCovDet` en términos de la distancia de Mahalanobis (así obtenemos también una mejor estimación de la matriz de precisión)."

#: ../modules/covariance.rst:348
msgid "Influence of outliers on location and covariance estimates"
msgstr "Influencia de los valores atípicos en las estimaciones de localización y covarianza"

#: ../modules/covariance.rst:349
msgid "Separating inliers from outliers using a Mahalanobis distance"
msgstr "Separar valores típicos de atípicos utilizando una distancia de Mahalanobis"

#: ../modules/covariance.rst:351
msgid "|robust_vs_emp|"
msgstr "|robust_vs_emp|"

#: ../modules/covariance.rst:352
msgid "|mahalanobis|"
msgstr "|mahalanobis|"

