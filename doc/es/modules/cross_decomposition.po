msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-21 19:47\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/cross_decomposition.po\n"
"X-Crowdin-File-ID: 4870\n"
"Language: es_ES\n"

#: ../modules/cross_decomposition.rst:5
msgid "Cross decomposition"
msgstr "Descomposición cruzada"

#: ../modules/cross_decomposition.rst:9
msgid "The cross decomposition module contains **supervised** estimators for dimensionality reduction and regression, belonging to the \"Partial Least Squares\" family."
msgstr "El módulo de descomposición cruzada contiene estimadores **supervisados** para la reducción de la dimensionalidad y la regresión, pertenecientes a la familia de los \"mínimos cuadrados parciales\"."

#: ../modules/cross_decomposition.rst:19
msgid "Cross decomposition algorithms find the fundamental relations between two matrices (X and Y). They are latent variable approaches to modeling the covariance structures in these two spaces. They will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. In other words, PLS projects both `X` and `Y` into a lower-dimensional subspace such that the covariance between `transformed(X)` and `transformed(Y)` is maximal."
msgstr "Los algoritmos de descomposición cruzada encuentran las relaciones fundamentales entre dos matrices (X e Y). Son enfoques de variables latentes para modelar las estructuras de covarianza en estos dos espacios. Tratan de encontrar la dirección multidimensional en el espacio X que explica la máxima dirección de varianza multidimensional en el espacio Y. En otras palabras, PLS proyecta tanto `X` como `Y` en un subespacio de menor dimensión tal que la covarianza entre `transformada(X)` y `transformada(Y)` es máxima."

#: ../modules/cross_decomposition.rst:27
msgid "PLS draws similarities with `Principal Component Regression <https://en.wikipedia.org/wiki/Principal_component_regression>`_ (PCR), where the samples are first projected into a lower-dimensional subspace, and the targets `y` are predicted using `transformed(X)`. One issue with PCR is that the dimensionality reduction is unsupervized, and may lose some important variables: PCR would keep the features with the most variance, but it's possible that features with a small variances are relevant from predicting the target. In a way, PLS allows for the same kind of dimensionality reduction, but by taking into account the targets `y`. An illustration of this fact is given in the following example: * :ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`."
msgstr "El PLS guarda similitudes con la \"Regresión de Componentes Principales\" (PCR), en la que las muestras se proyectan primero en un subespacio de menor dimensión y los objetivos `y` se predicen utilizando `transformed(X)`. Uno de los problemas de la PCR es que la reducción de la dimensionalidad no está supervisada y puede ignorar algunas variables importantes: La PCR mantendría las características con la mayor varianza, pero es posible que las características con una pequeña varianza sean relevantes para predecir el resultado. En cierto modo, PLS permite el mismo tipo de reducción de la dimensionalidad, pero teniendo en cuenta los objetivos `y`. El siguiente ejemplo ilustra este hecho: * :ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`."

#: ../modules/cross_decomposition.rst:39
msgid "Apart from CCA, the PLS estimators are particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among the features. By contrast, standard linear regression would fail in these cases unless it is regularized."
msgstr "Aparte de CCA, los estimadores PLS son especialmente adecuados cuando la matriz de predictores tiene más variables que observaciones, y cuando hay multicolinearidad entre las características. Por el contrario, la regresión lineal estándar fracasaría en estos casos a menos que sea regularizada."

#: ../modules/cross_decomposition.rst:44
msgid "Classes included in this module are :class:`PLSRegression`, :class:`PLSCanonical`, :class:`CCA` and :class:`PLSSVD`"
msgstr "Las clases incluidas en este módulo son :class:`PLSRegression`, :class:`PLSCanonical`, :class:`CCA` and :class:`PLSVD`"

#: ../modules/cross_decomposition.rst:48
msgid "PLSCanonical"
msgstr "PLSCanonical"

#: ../modules/cross_decomposition.rst:50
msgid "We here describe the algorithm used in :class:`PLSCanonical`. The other estimators use variants of this algorithm, and are detailed below. We recommend section [1]_ for more details and comparisons between these algorithms. In [1]_, :class:`PLSCanonical` corresponds to \"PLSW2A\"."
msgstr "Aquí describimos el algoritmo usado en :class:`PLSCanonical`. Los otros estimadores usan variantes de este algoritmo, y se detallan a continuación. Recomendamos la sección [1]_ para más detalles y comparaciones entre estos algoritmos. En [1]_, :class:`PLSCanonical` corresponde a \"PLSW2A\"."

#: ../modules/cross_decomposition.rst:55
msgid "Given two centered matrices :math:`X \\in \\mathbb{R}^{n \\times d}` and :math:`Y \\in \\mathbb{R}^{n \\times t}`, and a number of components :math:`K`, :class:`PLSCanonical` proceeds as follows:"
msgstr "Dadas dos matrices centradas :math:`X \\in \\mathbb{R}^{n \\times d}` y :math:`Y \\in \\mathbb{R}^{n \\times t}`, y un número de componentes :math:`K`, :class:`PLSCanonical` prosigue como se indica a continuación:"

#: ../modules/cross_decomposition.rst:59
msgid "Set :math:`X_1` to :math:`X` and :math:`Y_1` to :math:`Y`. Then, for each :math:`k \\in [1, K]`:"
msgstr "Defina :math:`X_1` como :math:`X` y :math:`Y_1` como :math:`Y`. Entonces, para cada :math:`k \\in [1, K]`:"

#: ../modules/cross_decomposition.rst:62
msgid "a) compute :math:`u_k \\in \\mathbb{R}^d` and :math:`v_k \\in \\mathbb{R}^t`, the first left and right singular vectors of the cross-covariance matrix :math:`C = X_k^T Y_k`. :math:`u_k` and :math:`v_k` are called the *weights*. By definition, :math:`u_k` and :math:`v_k` are choosen so that they maximize the covariance between the projected :math:`X_k` and the projected target, that is :math:`\\text{Cov}(X_k u_k, Y_k v_k)`."
msgstr "a) Calcule :math:`u_k \\in \\mathbb{R}^d` y :math:`v_k \\in \\mathbb{R}^t`, los primeros vectores singulares izquierdo y derecho de la matriz de covarianza cruzada :math:`C = X_k^T Y_k`. :math:`u_k` y :math:`v_k` se denominan los *pesos*. Por definición, :math:`u_k` y :math:`v_k` se eligen de manera que maximicen la covarianza entre la :math:`X_k` proyectada y el objetivo proyectado, es decir :math:`\\text{Cov}(X_k u_k, Y_k v_k)`."

#: ../modules/cross_decomposition.rst:70
msgid "b) Project :math:`X_k` and :math:`Y_k` on the singular vectors to obtain *scores*: :math:`\\xi_k = X_k u_k` and :math:`\\omega_k = Y_k v_k`"
msgstr "b) Proyecta :math:`X_k` y :math:`Y_k` sobre los vectores singulares para obtener *puntuación*: :math:`\\xi_k = X_k u_k` y :math:`\\omega_k = Y_k v_k`"

#: ../modules/cross_decomposition.rst:72
msgid "c) Regress :math:`X_k` on :math:`\\xi_k`, i.e. find a vector :math:`\\gamma_k \\in \\mathbb{R}^d` such that the rank-1 matrix :math:`\\xi_k \\gamma_k^T` is as close as possible to :math:`X_k`. Do the same on :math:`Y_k` with :math:`\\omega_k` to obtain :math:`\\delta_k`. The vectors :math:`\\gamma_k` and :math:`\\delta_k` are called the *loadings*."
msgstr "c) Haz una regresión de :math:`X_k` on :math:`\\xi_k`, es decir, encuentra un vector :math:`\\gamma_k \\in \\mathbb{R}^d` tal que la matriz de rango 1 :math:`\\xi_k \\gamma_k^T` sea lo más cercana posible a :math:`X_k`. Haz lo mismo en :math:`Y_k` con :math:`\\omega_k` para obtener :math:`\\delta_k`. Los vectores :math:`\\gamma_k` y :math:`\\delta_k` se denominan *cargas*."

#: ../modules/cross_decomposition.rst:77
msgid "d) *deflate* :math:`X_k` and :math:`Y_k`, i.e. subtract the rank-1 approximations: :math:`X_{k+1} = X_k - \\xi_k \\gamma_k^T`, and :math:`Y_{k + 1} = Y_k - \\omega_k \\delta_k^T`."
msgstr "d) *disminuye* :math:`X_k` y :math:`Y_k`, es decir, sustrae las aproximaciones de rango 1: :math:`X_{k+1} = X_k - \\xi_k \\gamma_k^T`, y :math:`Y_{k + 1} = Y_k - \\omega_k \\delta_k^T`."

#: ../modules/cross_decomposition.rst:81
msgid "At the end, we have approximated :math:`X` as a sum of rank-1 matrices: :math:`X = \\Xi \\Gamma^T` where :math:`\\Xi \\in \\mathbb{R}^{n \\times K}` contains the scores in its columns, and :math:`\\Gamma^T \\in \\mathbb{R}^{K \\times d}` contains the loadings in its rows. Similarly for :math:`Y`, we have :math:`Y = \\Omega \\Delta^T`."
msgstr "Al final, hemos aproximado :math:`X` como una suma de matrices de rango 1: :math:`X = \\Xi \\Gamma^T` donde :math:`\\Xi \\in \\mathbb{R}^{n \\times K}` contiene las puntuaciones en sus columnas, y :math:`\\Gamma^T \\in \\mathbb{R}^{K \\times d}` contiene las cargas en sus filas. Del mismo modo, para :math:`Y`, tenemos :math:`Y = \\Omega \\Delta^T`."

#: ../modules/cross_decomposition.rst:87
msgid "Note that the scores matrices :math:`\\Xi` and :math:`\\Omega` correspond to the projections of the training data :math:`X` and :math:`Y`, respectively."
msgstr "Observa que las matrices de puntuación :math:`\\Xi` y :math:`Omega` corresponden a las proyecciones de los datos de entrenamiento :math:`X` y :math:`Y`, respectivamente."

#: ../modules/cross_decomposition.rst:90
msgid "Step *a)* may be performed in two ways: either by computing the whole SVD of :math:`C` and only retain the singular vectors with the biggest singular values, or by directly computing the singular vectors using the power method (cf section 11.3 in [1]_), which corresponds to the `'nipals'` option of the `algorithm` parameter."
msgstr "El paso *a)* puede realizarse de dos maneras: calculando toda la SVD de :math:`C` y conservando sólo los vectores singulares con los mayores valores singulares, o calculando directamente los vectores singulares utilizando el método de la potencia (cf. sección 11.3 en [1]_), que corresponde a la opción `'nipals` del parámetro `algorithm`."

#: ../modules/cross_decomposition.rst:97
msgid "Transforming data"
msgstr "Transformación de datos"

#: ../modules/cross_decomposition.rst:99
msgid "To transform :math:`X` into :math:`\\bar{X}`, we need to find a projection matrix :math:`P` such that :math:`\\bar{X} = XP`. We know that for the training data, :math:`\\Xi = XP`, and :math:`X = \\Xi \\Gamma^T`. Setting :math:`P = U(\\Gamma^T U)^{-1}` where :math:`U` is the matrix with the :math:`u_k` in the columns, we have :math:`XP = X U(\\Gamma^T U)^{-1} = \\Xi (\\Gamma^T U) (\\Gamma^T U)^{-1} = \\Xi` as desired. The rotation matrix :math:`P` can be accessed from the `x_rotations_` attribute."
msgstr "Para transformar :math:`X` en :math:`\\bar{X}`, necesitamos encontrar una matriz de proyección :math:`P` tal que :math:`bar{X} = XP`. Sabemos que para los datos de entrenamiento, :math:`\\Xi = XP`, y :math:`X = \\Xi \\Gamma^T`. Estableciendo :math:`P = U(\\Gamma^T U)^{-1}` donde :math:`U` es la matriz con los :math:`u_k` en las columnas, tenemos :math:`XP = X U(\\Gamma^T U)^{-1} = \\Xi (\\Gamma^T U) (\\Gamma^T U)^{-1} = \\Xi` tal y como se quiere. Se puede acceder a la matriz de rotación :math:`P` desde el atributo `x_rotations_`."

#: ../modules/cross_decomposition.rst:107
msgid "Similarly, :math:`Y` can be transformed using the rotation matrix :math:`V(\\Delta^T V)^{-1}`, accessed via the `y_rotations_` attribute."
msgstr "Del mismo modo, :math:`Y` puede ser transformada usando la matriz de rotación :math:`V(\\Delta^T V)^{-1}`, a lo que se tiene acceso a través del atributo `y_rotations_`."

#: ../modules/cross_decomposition.rst:111
msgid "Predicting the targets Y"
msgstr "Predicción de los objetivos Y"

#: ../modules/cross_decomposition.rst:113
msgid "To predict the targets of some data :math:`X`, we are looking for a coefficient matrix :math:`\\beta \\in R^{d \\times t}` such that :math:`Y = X\\beta`."
msgstr "Para predecir los objetivos de algunos datos :math:`X`, estamos buscando una matriz de coeficiente :math:`\\beta \\in R^{d \\times t}` tal que :math:`Y = X\\beta`."

#: ../modules/cross_decomposition.rst:117
msgid "The idea is to try to predict the transformed targets :math:`\\Omega` as a function of the transformed samples :math:`\\Xi`, by computing :math:`\\alpha \\in \\mathbb{R}` such that :math:`\\Omega = \\alpha \\Xi`."
msgstr "La idea es tratar de predecir los objetivos transformados :math:`\\Omega` como una función de las muestras transformadas :math:`\\Xi`, calculando :math:`\\alpha \\in \\mathbb{R}` tal que :math:`\\Omega = \\alpha \\Xi`."

#: ../modules/cross_decomposition.rst:121
msgid "Then, we have :math:`Y = \\Omega \\Delta^T = \\alpha \\Xi \\Delta^T`, and since :math:`\\Xi` is the transformed training data we have that :math:`Y = X \\alpha P \\Delta^T`, and as a result the coefficient matrix :math:`\\beta = \\alpha P \\Delta^T`."
msgstr "Luego, tenemos :math:`Y = \\Omega \\Delta^T = \\alpha \\Xi \\Delta^T`, y ya que :math:`\\Xi` son los datos de entrenamiento transformados, tenemos que :math:`Y = X \\alpha P \\Delta^T`, y como resultado la matriz de coeficiente :math:`\\beta = \\alpha P \\Delta^T`."

#: ../modules/cross_decomposition.rst:126
msgid ":math:`\\beta` can be accessed through the `coef_` attribute."
msgstr ":math:`\\beta` puede ser accedido mediante el atributo `coef_`."

#: ../modules/cross_decomposition.rst:129
msgid "PLSSVD"
msgstr "PLSSVD"

#: ../modules/cross_decomposition.rst:131
msgid ":class:`PLSSVD` is a simplified version of :class:`PLSCanonical` described earlier: instead of iteratively deflating the matrices :math:`X_k` and :math:`Y_k`, :class:`PLSSVD` computes the SVD of :math:`C = X^TY` only *once*, and stores the `n_components` singular vectors corresponding to the biggest singular values in the matrices `U` and `V`, corresponding to the `x_weights_` and `y_weights_` attributes. Here, the transformed data is simply `transformed(X) = XU` and `transformed(Y) = YV`."
msgstr ":class:`PLSSVD` es una versión simplificada de :class:`PLSCanonical` descrita anteriormente: en lugar de disminuir iterativamente las matrices :math:`X_k` y :math:`Y_k`, :class:`PLSSVD` calcula la SVD de :math: `C = X^TY` sólo *una vez*, y almacena los vectores singulares `n_componentes` correspondientes a los mayores valores singulares en las matrices `U` y `V`, correspondientes a los atributos `x_weights_` y `y_weights_`. Aquí, los datos transformados son simplemente `transformed(X) = XU` y `transformed(Y) = YV`."

#: ../modules/cross_decomposition.rst:139
msgid "If `n_components == 1`, :class:`PLSSVD` and :class:`PLSCanonical` are strictly equivalent."
msgstr "Si `n_components == 1`, :class:`PLSVD` y :class:`PLSCanonical` son estrictamente equivalentes."

#: ../modules/cross_decomposition.rst:143
msgid "PLSRegression"
msgstr "PLSRegression"

#: ../modules/cross_decomposition.rst:145
msgid "The :class:`PLSRegression` estimator is similar to :class:`PLSCanonical` with `algorithm='nipals'`, with 2 significant differences:"
msgstr "El estimador :class:`PLSRegression` es similar a :class:`PLSCanonical` con `algorithm='nipals'`, con 2 diferencias importantes:"

#: ../modules/cross_decomposition.rst:149
msgid "at step a) in the power method to compute :math:`u_k` and :math:`v_k`, :math:`v_k` is never normalized."
msgstr "en el paso a) del método de potencia para calcular :math:`u_k` y :math:`v_k`, :math:`v_k` nunca se normaliza."

#: ../modules/cross_decomposition.rst:151
msgid "at step c), the targets :math:`Y_k` are approximated using the projection of :math:`X_k` (i.e. :math:`\\xi_k`) instead of the projection of :math:`Y_k` (i.e. :math:`\\omega_k`). In other words, the loadings computation is different. As a result, the deflation in step d) will also be affected."
msgstr "en el paso c), los objetivos :math:`Y_k` se aproximan utilizando la proyección de :math:`X_k` (es decir, :math:`\\xi_k`) en lugar de la proyección de :math:`Y_k` (es decir, :math:`\\omega_k`). En otras palabras, el cálculo de las cargas es diferente. Como resultado, la disminución en el paso d) también se verá afectada."

#: ../modules/cross_decomposition.rst:157
msgid "These two modifications affect the output of `predict` and `transform`, which are not the same as for :class:`PLSCanonical`. Also, while the number of components is limited by `min(n_samples, n_features, n_targets)` in :class:`PLSCanonical`, here the limit is the rank of :math:`X^TX`, i.e. `min(n_samples, n_features)`."
msgstr "Estas dos modificaciones afectan a la salida de `predict` y `transform`, que no son las mismas que para :class:`PLSCanonical`. Además, mientras que el número de componentes está limitado por `min(n_samples, n_features, n_targets)` en :class:`PLSCanonical`, aquí el límite es el rango de :math:`X^TX`, es decir, `min(n_samples, n_features)`."

#: ../modules/cross_decomposition.rst:163
msgid ":class:`PLSRegression` is also known as PLS1 (single targets) and PLS2 (multiple targets). Much like :class:`~sklearn.linear_model.Lasso`, :class:`PLSRegression` is a form of regularized linear regression where the number of components controls the strength of the regularization."
msgstr ":class:`PLSRegression` también se conoce como PLS1 (objetivos simples) y PLS2 (objetivos múltiples). Al igual que :class:`~sklearn.linear_model.Lasso`, :class:`PLSRegression` es una forma de regresión lineal regularizada donde el número de componentes controla la fuerza de la regularización."

#: ../modules/cross_decomposition.rst:169
msgid "Canonical Correlation Analysis"
msgstr "Análisis de Correlación Canónica"

#: ../modules/cross_decomposition.rst:171
msgid "Canonical Correlation Analysis was developed prior and independently to PLS. But it turns out that :class:`CCA` is a special case of PLS, and corresponds to PLS in \"Mode B\" in the literature."
msgstr "El Análisis de Correlación Canónica se desarrolló previa e independientemente de PLS. Pero resulta que :class:`CCA` es un caso especial de PLS, y corresponde a PLS en el \"Modo B\" en la literatura."

#: ../modules/cross_decomposition.rst:175
msgid ":class:`CCA` differs from :class:`PLSCanonical` in the way the weights :math:`u_k` and :math:`v_k` are computed in the power method of step a). Details can be found in section 10 of [1]_."
msgstr ":class:`CCA` difiere de :class:`PLSCanonical` en la forma en que los pesos :math:`u_k` y :math:`v_k` son calculados en el método de potencia del paso a). Los detalles se pueden encontrar en la sección 10 de [1]_."

#: ../modules/cross_decomposition.rst:179
msgid "Since :class:`CCA` involves the inversion of :math:`X_k^TX_k` and :math:`Y_k^TY_k`, this estimator can be unstable if the number of features or targets is greater than the number of samples."
msgstr "Como :class:`CCA` involucra la inversión de :math:`X_k^TX_k` y :math:`Y_k^TY_k`, este estimador puede ser inestable si el número de características o objetivos es mayor que el número de muestras."

#: ../modules/cross_decomposition.rst:186
msgid "`A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case <https://www.stat.washington.edu/research/reports/2000/tr371.pdf>`_ JA Wegelin"
msgstr "`A survey of Partial Least Squares (PLS) methods, with emphasis on the two-block case <https://www.stat.washington.edu/research/reports/2000/tr371.pdf>`_ JA Wegelin"

#: ../modules/cross_decomposition.rst:193
msgid ":ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`"
msgstr ":ref:`sphx_glr_auto_examples_cross_decomposition_plot_compare_cross_decomposition.py`"

#: ../modules/cross_decomposition.rst:194
msgid ":ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`"
msgstr ":ref:`sphx_glr_auto_examples_cross_decomposition_plot_pcr_vs_pls.py`"

