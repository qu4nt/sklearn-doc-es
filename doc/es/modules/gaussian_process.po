msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-05-05 20:27\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/modules/gaussian_process.po\n"
"X-Crowdin-File-ID: 4820\n"
"Language: es_ES\n"

#: ../modules/gaussian_process.rst:7
msgid "Gaussian Processes"
msgstr "Procesos Gaussianos"

#: ../modules/gaussian_process.rst:11
msgid "**Gaussian Processes (GP)** are a generic supervised learning method designed to solve *regression* and *probabilistic classification* problems."
msgstr "Los **procesos Gaussianos (GP)** son un método genérico de aprendizaje supervisado diseñado para resolver problemas de *regresión* y *clasificación probabilística*."

#: ../modules/gaussian_process.rst:14
msgid "The advantages of Gaussian processes are:"
msgstr "Las ventajas de los procesos Gaussianos son:"

#: ../modules/gaussian_process.rst:16
msgid "The prediction interpolates the observations (at least for regular kernels)."
msgstr "La predicción interpola las observaciones (al menos para los núcleos regulares)."

#: ../modules/gaussian_process.rst:19
msgid "The prediction is probabilistic (Gaussian) so that one can compute empirical confidence intervals and decide based on those if one should refit (online fitting, adaptive fitting) the prediction in some region of interest."
msgstr "La predicción es probabilística (gaussiana), por lo que se pueden calcular intervalos de confianza empíricos y decidir, en base a ellos, si se debe reajustar (ajuste en línea, ajuste adaptativo) la predicción en alguna región de interés."

#: ../modules/gaussian_process.rst:24
msgid "Versatile: different :ref:`kernels <gp_kernels>` can be specified. Common kernels are provided, but it is also possible to specify custom kernels."
msgstr "Versátil: se pueden especificar diferentes :ref:`núcleos <gp_kernels>`. Se proporcionan núcleos comunes, pero también es posible especificar núcleos personalizados."

#: ../modules/gaussian_process.rst:28
msgid "The disadvantages of Gaussian processes include:"
msgstr "Las desventajas de los procesos Gaussianos incluyen:"

#: ../modules/gaussian_process.rst:30
msgid "They are not sparse, i.e., they use the whole samples/features information to perform the prediction."
msgstr "No son dispersos, es decir, utilizan toda la información de las muestras para realizar la predicción."

#: ../modules/gaussian_process.rst:33
msgid "They lose efficiency in high dimensional spaces -- namely when the number of features exceeds a few dozens."
msgstr "Pierden eficacia en espacios de alta dimensión, es decir, cuando el número de características supera algunas decenas."

#: ../modules/gaussian_process.rst:40
msgid "Gaussian Process Regression (GPR)"
msgstr "Regresión de Procesos Gaussianos (GPR)"

#: ../modules/gaussian_process.rst:44
msgid "The :class:`GaussianProcessRegressor` implements Gaussian processes (GP) for regression purposes. For this, the prior of the GP needs to be specified. The prior mean is assumed to be constant and zero (for ``normalize_y=False``) or the training data's mean (for ``normalize_y=True``). The prior's covariance is specified by passing a :ref:`kernel <gp_kernels>` object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed ``optimizer``. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying ``n_restarts_optimizer``. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, `None` can be passed as optimizer."
msgstr "La :clase:`GaussianProcessRegressor` implementa procesos Gaussianos (GP) con fines de regresión. Para ello, es necesario especificar la prioridad del GP. La media a priori se supone constante y cero (para ``normalize_y=False``) o la media de los datos de entrenamiento (para ``normalize_y=True``). La covarianza de la prioridad se especifica pasando un objeto :ref:`kernel <gp_kernels>`. Los hiperparámetros del kernel se optimizan durante el ajuste de GaussianProcessRegressor maximizando la verosimilitud marginal logarítmica (log-marginal-likelihood, o LML) basado en el ``optimizador`` que se le pasa. Como el LML puede tener múltiples óptimos locales, el optimizador puede iniciarse repetidamente especificando ``n_restarts_optimizer``. La primera ejecución se realiza siempre a partir de los valores iniciales de los hiperparámetros del núcleo; las ejecuciones posteriores se realizan a partir de los valores de los hiperparámetros que se han elegido aleatoriamente del rango de valores permitidos. Si los hiperparámetros iniciales deben mantenerse fijos, se puede pasar `None` como optimizador."

#: ../modules/gaussian_process.rst:59
msgid "The noise level in the targets can be specified by passing it via the parameter ``alpha``, either globally as a scalar or per datapoint. Note that a moderate noise level can also be helpful for dealing with numeric issues during fitting as it is effectively implemented as Tikhonov regularization, i.e., by adding it to the diagonal of the kernel matrix. An alternative to specifying the noise level explicitly is to include a WhiteKernel component into the kernel, which can estimate the global noise level from the data (see example below)."
msgstr "El nivel de ruido en los objetivos puede especificarse pasándolo a través del parámetro ``alpha``, ya sea globalmente como escalar o por punto de datos. Ten en cuenta que un nivel de ruido moderado también puede ser útil para tratar los problemas numéricos durante el ajuste, ya que se implementa efectivamente como regularización de Tikhonov, es decir, añadiéndolo a la diagonal de la matriz del núcleo. Una alternativa para la especificación explícita del nivel de ruido es incluir un componente WhiteKernel en el núcleo, que puede estimar el nivel de ruido global a partir de los datos (véase el ejemplo siguiente)."

#: ../modules/gaussian_process.rst:68
msgid "The implementation is based on Algorithm 2.1 of [RW2006]_. In addition to the API of standard scikit-learn estimators, GaussianProcessRegressor:"
msgstr "La implementación se basa en el algoritmo 2.1 de [RW2006]_. Además de la API de los estimadores estándar de scikit-learn, GaussianProcessRegressor:"

#: ../modules/gaussian_process.rst:71
msgid "allows prediction without prior fitting (based on the GP prior)"
msgstr "permite la predicción sin ajuste previo (basado en el previo GP)"

#: ../modules/gaussian_process.rst:73
msgid "provides an additional method ``sample_y(X)``, which evaluates samples drawn from the GPR (prior or posterior) at given inputs"
msgstr "proporciona un método adicional ``sample_y(X)``, que evalúa las muestras extraídas del GPR (a priori o a posteriori) en entradas específicas"

#: ../modules/gaussian_process.rst:76
msgid "exposes a method ``log_marginal_likelihood(theta)``, which can be used externally for other ways of selecting hyperparameters, e.g., via Markov chain Monte Carlo."
msgstr "expone un método ``log_marginal_likelihood(theta)``, que puede ser usado externamente para otras maneras de seleccionar hiperparámetros, por ejemplo, a través de la cadena de Markov Monte Carlo."

#: ../modules/gaussian_process.rst:82
msgid "GPR examples"
msgstr "Ejemplos de GPR"

#: ../modules/gaussian_process.rst:85
msgid "GPR with noise-level estimation"
msgstr "GPR con estimación de nivel de ruido"

#: ../modules/gaussian_process.rst:86
msgid "This example illustrates that GPR with a sum-kernel including a WhiteKernel can estimate the noise level of data. An illustration of the log-marginal-likelihood (LML) landscape shows that there exist two local maxima of LML."
msgstr "Este ejemplo ilustra que la GPR con un núcleo de suma que incluye un WhiteKernel puede estimar el nivel de ruido de los datos. Una descripción de la verosimilitud marginal logarítmica (LML) muestra que existen dos máximos locales de LML."

#: ../modules/gaussian_process.rst:95
msgid "The first corresponds to a model with a high noise level and a large length scale, which explains all variations in the data by noise."
msgstr "El primero corresponde a un modelo con un alto nivel de ruido y una gran escala de longitud, que explica todas las variaciones de los datos debidas al ruido."

#: ../modules/gaussian_process.rst:102
msgid "The second one has a smaller noise level and shorter length scale, which explains most of the variation by the noise-free functional relationship. The second model has a higher likelihood; however, depending on the initial value for the hyperparameters, the gradient-based optimization might also converge to the high-noise solution. It is thus important to repeat the optimization several times for different initializations."
msgstr "El segundo tiene un nivel de ruido menor y una escala de longitud más corta, lo que explica la mayor parte de la variación por la relación funcional sin ruido. El segundo modelo tiene una mayor verosimilitud; sin embargo, dependiendo del valor inicial de los hiperparámetros, la optimización basada en el gradiente también podría converger a la solución de alto ruido. Por lo tanto, es importante repetir la optimización varias veces para diferentes inicializaciones."

#: ../modules/gaussian_process.rst:115
msgid "Comparison of GPR and Kernel Ridge Regression"
msgstr "Comparación de la GPR y la regresión de cresta de núcleo (Kernel Ridge Regression)"

#: ../modules/gaussian_process.rst:117
msgid "Both kernel ridge regression (KRR) and GPR learn a target function by employing internally the \"kernel trick\". KRR learns a linear function in the space induced by the respective kernel which corresponds to a non-linear function in the original space. The linear function in the kernel space is chosen based on the mean-squared error loss with ridge regularization. GPR uses the kernel to define the covariance of a prior distribution over the target functions and uses the observed training data to define a likelihood function. Based on Bayes theorem, a (Gaussian) posterior distribution over target functions is defined, whose mean is used for prediction."
msgstr "Tanto la regresión cresta de núcleo (cuyas siglas en inglés son KRR) como la GPR aprenden una función objetivo empleando internamente el \"truco del núcleo\". KRR aprende una función lineal en el espacio inducido por el núcleo respectivo que corresponde a una función no lineal en el espacio original. La función lineal en el espacio del núcleo se elige en función de la pérdida de error cuadrático medio con regularización de cresta. La GPR utiliza el núcleo para definir la covarianza de una distribución a priori sobre las funciones objetivo y utiliza los datos de entrenamiento observados para definir una función de verosimilitud. Basándose en el teorema de Bayes, se define una distribución posterior (gaussiana) sobre las funciones objetivo, cuya media se utiliza para la predicción."

#: ../modules/gaussian_process.rst:128
msgid "A major difference is that GPR can choose the kernel's hyperparameters based on gradient-ascent on the marginal likelihood function while KRR needs to perform a grid search on a cross-validated loss function (mean-squared error loss). A further difference is that GPR learns a generative, probabilistic model of the target function and can thus provide meaningful confidence intervals and posterior samples along with the predictions while KRR only provides predictions."
msgstr "Una de las principales diferencias es que GPR puede elegir los hiperparámetros del núcleo basándose en el gradiente-ascenso de la función de verosimilitud marginal, mientras que KRR tiene que realizar una búsqueda de cuadrícula en una función de pérdida validada de forma cruzada (pérdida de error cuadrático medio). Otra distinción es que GPR aprende un modelo generativo y probabilístico de la función objetivo y, por tanto, puede proporcionar intervalos de confianza significativos y muestras posteriores junto con las predicciones, mientras que KRR sólo proporciona predicciones."

#: ../modules/gaussian_process.rst:136
msgid "The following figure illustrates both methods on an artificial dataset, which consists of a sinusoidal target function and strong noise. The figure compares the learned model of KRR and GPR based on a ExpSineSquared kernel, which is suited for learning periodic functions. The kernel's hyperparameters control the smoothness (length_scale) and periodicity of the kernel (periodicity). Moreover, the noise level of the data is learned explicitly by GPR by an additional WhiteKernel component in the kernel and by the regularization parameter alpha of KRR."
msgstr "La siguiente figura ilustra ambos métodos en un conjunto de datos artificial, que consiste en una función objetivo sinusoidal y un fuerte ruido. La figura compara el modelo aprendido de KRR y GPR basado en un núcleo ExpSineSquared, que es adecuado para el aprendizaje de funciones periódicas. Los hiperparámetros del núcleo controlan la suavidad (length_scale) y la periodicidad del núcleo (periodicity). Además, el nivel de ruido de los datos es aprendido explícitamente por GPR mediante un componente adicional WhiteKernel en el núcleo y por el parámetro de regularización alfa de KRR."

#: ../modules/gaussian_process.rst:149
msgid "The figure shows that both methods learn reasonable models of the target function. GPR correctly identifies the periodicity of the function to be roughly :math:`2*\\pi` (6.28), while KRR chooses the doubled periodicity :math:`4*\\pi` . Besides that, GPR provides reasonable confidence bounds on the prediction which are not available for KRR. A major difference between the two methods is the time required for fitting and predicting: while fitting KRR is fast in principle, the grid-search for hyperparameter optimization scales exponentially with the number of hyperparameters (\"curse of dimensionality\"). The gradient-based optimization of the parameters in GPR does not suffer from this exponential scaling and is thus considerably faster on this example with 3-dimensional hyperparameter space. The time for predicting is similar; however, generating the variance of the predictive distribution of GPR takes considerably longer than just predicting the mean."
msgstr "La figura muestra que ambos métodos aprenden modelos razonables de la función objetivo. GPR identifica correctamente que la periodicidad de la función es aproximadamente :math:`2*\\pi` (6.28), mientras que KRR elige la periodicidad duplicada :math:`4*\\pi` . Además, GPR proporciona límites de confianza razonables en la predicción que no están disponibles para KRR. Una diferencia importante entre los dos métodos es el tiempo necesario para el ajuste y la predicción: mientras que el ajuste de KRR es rápido en principio, la búsqueda de cuadrícula para la optimización de los hiperparámetros escala exponencialmente con el número de hiperparámetros (\"maldición de la dimensionalidad\"). La optimización de los parámetros basada en el gradiente en GPR no sufre este escalamiento exponencial y, por tanto, es considerablemente más rápida en este ejemplo con un espacio de hiperparámetros tridimensional. El tiempo de predicción es similar; sin embargo, generar la varianza de la distribución de predicción de GPR lleva bastante más tiempo que sólo predecir la media."

#: ../modules/gaussian_process.rst:165
msgid "GPR on Mauna Loa CO2 data"
msgstr "GPR en datos de CO2 de Mauna Loa"

#: ../modules/gaussian_process.rst:167
msgid "This example is based on Section 5.4.3 of [RW2006]_. It illustrates an example of complex kernel engineering and hyperparameter optimization using gradient ascent on the log-marginal-likelihood. The data consists of the monthly average atmospheric CO2 concentrations (in parts per million by volume (ppmv)) collected at the Mauna Loa Observatory in Hawaii, between 1958 and 1997. The objective is to model the CO2 concentration as a function of the time t."
msgstr "Este ejemplo se basa en la Sección 5.4.3 de [RW2006]_. Ilustra un ejemplo de ingeniería de núcleos complejos y optimización de hiperparámetros utilizando el ascenso de gradiente en la verosimilitud marginal logarítmica. Los datos consisten en las concentraciones medias mensuales de CO2 atmosférico (en partes por millón en volumen (ppmv)) recogidas en el Observatorio de Mauna Loa en Hawai, entre 1958 y 1997. El objetivo es modelar la concentración de CO2 como función del tiempo t."

#: ../modules/gaussian_process.rst:175
msgid "The kernel is composed of several terms that are responsible for explaining different properties of the signal:"
msgstr "El núcleo se compone de varios términos que son responsables de explicar diferentes propiedades de la señal:"

#: ../modules/gaussian_process.rst:178
msgid "a long term, smooth rising trend is to be explained by an RBF kernel. The RBF kernel with a large length-scale enforces this component to be smooth; it is not enforced that the trend is rising which leaves this choice to the GP. The specific length-scale and the amplitude are free hyperparameters."
msgstr "una tendencia suave y ascendente a largo plazo debe ser explicada por un núcleo RBF. El núcleo RBF con una gran escala de longitud obliga a que este componente sea suavizado; no se exige que la tendencia sea ascendente, lo que deja esta elección a la GP. La escala de longitud específica y la amplitud son hiperparámetros libres."

#: ../modules/gaussian_process.rst:183
msgid "a seasonal component, which is to be explained by the periodic ExpSineSquared kernel with a fixed periodicity of 1 year. The length-scale of this periodic component, controlling its smoothness, is a free parameter. In order to allow decaying away from exact periodicity, the product with an RBF kernel is taken. The length-scale of this RBF component controls the decay time and is a further free parameter."
msgstr "un componente estacional, que debe ser explicado por el núcleo periódico ExpSineSquared con una periodicidad fija de 1 año. La escala de longitud de este componente periódico, que controla su suavizado (smoothness), es un parámetro libre. Para permitir el decaimiento de la periodicidad exacta, se toma el producto con un núcleo RBF. La escala de longitud de este componente RBF controla el tiempo de deterioro y es otro parámetro libre."

#: ../modules/gaussian_process.rst:190
msgid "smaller, medium term irregularities are to be explained by a RationalQuadratic kernel component, whose length-scale and alpha parameter, which determines the diffuseness of the length-scales, are to be determined. According to [RW2006]_, these irregularities can better be explained by a RationalQuadratic than an RBF kernel component, probably because it can accommodate several length-scales."
msgstr "las irregularidades a corto y a mediano plazo deben explicarse mediante un componente del núcleo RationalQuadratic, cuya escala de longitud y parámetro alfa, que determina la difusividad de las escalas de longitud, deben determinarse. Según [RW2006]_, estas irregularidades se pueden explicar mejor con un componente de núcleo RationalQuadratic que con uno RBF, probablemente porque puede acomodar varias escalas de longitud."

#: ../modules/gaussian_process.rst:197
msgid "a \"noise\" term, consisting of an RBF kernel contribution, which shall explain the correlated noise components such as local weather phenomena, and a WhiteKernel contribution for the white noise. The relative amplitudes and the RBF's length scale are further free parameters."
msgstr "un término de \"ruido\", que consiste en una contribución del núcleo RBF, que explicará los componentes de ruido correlacionados, como los fenómenos meteorológicos locales, y una contribución del WhiteKernel para el ruido blanco. Las amplitudes relativas y la escala de longitud del RBF son otros parámetros libres."

#: ../modules/gaussian_process.rst:202
msgid "Maximizing the log-marginal-likelihood after subtracting the target's mean yields the following kernel with an LML of -83.214:"
msgstr "Al maximizar la verosimilitud marginal logarítmica después de restar la media del objetivo, se obtiene el siguiente núcleo con un LML de -83,214:"

#: ../modules/gaussian_process.rst:213
msgid "Thus, most of the target signal (34.4ppm) is explained by a long-term rising trend (length-scale 41.8 years). The periodic component has an amplitude of 3.27ppm, a decay time of 180 years and a length-scale of 1.44. The long decay time indicates that we have a locally very close to periodic seasonal component. The correlated noise has an amplitude of 0.197ppm with a length scale of 0.138 years and a white-noise contribution of 0.197ppm. Thus, the overall noise level is very small, indicating that the data can be very well explained by the model. The figure shows also that the model makes very confident predictions until around 2015"
msgstr "Así, la mayor parte de la señal objetivo (34,4 partes por millón) se explica por una tendencia ascendente a largo plazo (escala de longitud de 41,8 años). El componente periódico tiene una amplitud de 3,27 ppm, un tiempo de decaimiento de 180 años y una escala de longitud de 1,44. El largo tiempo de deterioro indica que tenemos una componente estacional localmente muy cercana a la periódica. El ruido correlacionado tiene una amplitud de 0,197ppm con una escala de longitud de 0,138 años y una contribución de ruido blanco de 0,197 ppm. Por tanto, el nivel de ruido global es muy pequeño, lo que indica que los datos pueden ser explicados muy bien por el modelo. La figura muestra también que el modelo hace predicciones muy seguras hasta el año 2015 aproximadamente"

#: ../modules/gaussian_process.rst:230
msgid "Gaussian Process Classification (GPC)"
msgstr "Clasificación de Procesos Gaussianos (GPC)"

#: ../modules/gaussian_process.rst:234
msgid "The :class:`GaussianProcessClassifier` implements Gaussian processes (GP) for classification purposes, more specifically for probabilistic classification, where test predictions take the form of class probabilities. GaussianProcessClassifier places a GP prior on a latent function :math:`f`, which is then squashed through a link function to obtain the probabilistic classification. The latent function :math:`f` is a so-called nuisance function, whose values are not observed and are not relevant by themselves. Its purpose is to allow a convenient formulation of the model, and :math:`f` is removed (integrated out) during prediction. GaussianProcessClassifier implements the logistic link function, for which the integral cannot be computed analytically but is easily approximated in the binary case."
msgstr "El :class:`GaussianProcessClassifier` implementa procesos Gaussianos (GP en inglés) con fines de clasificación, más específicamente para la clasificación probabilística, donde las predicciones de prueba toman la forma de probabilidades de clase. GaussianProcessClassifier coloca un GP previo en una función latente :math:`f`, que luego se comprime a través de una función de enlace para obtener la clasificación probabilística. La función latente :math:`f` es una función denominada incómoda (nuisance function), cuyos valores no se observan y no son relevantes por sí mismos. Su propósito es permitir una formulación conveniente del modelo, y :math:`f` se elimina (se integra) durante la predicción. GaussianProcessClassifier implementa la función de enlace logístico, para la cual la integral no puede ser calculada analíticamente pero es fácilmente aproximada en el caso binario."

#: ../modules/gaussian_process.rst:246
msgid "In contrast to the regression setting, the posterior of the latent function :math:`f` is not Gaussian even for a GP prior since a Gaussian likelihood is inappropriate for discrete class labels. Rather, a non-Gaussian likelihood corresponding to the logistic link function (logit) is used. GaussianProcessClassifier approximates the non-Gaussian posterior with a Gaussian based on the Laplace approximation. More details can be found in Chapter 3 of [RW2006]_."
msgstr "En contraste con la situación de la regresión, la posterior de la función latente :math:`f` no es gaussiana incluso para un GP previo, ya que una verosimilitud gaussiana es inapropiada para las etiquetas de clase discretas. En su lugar, se utiliza una probabilidad no gaussiana correspondiente a la función de enlace logístico (logit). GaussianProcessClassifier aproxima la posterior no gaussiana con una gaussiana basada en la aproximación de Laplace. Se pueden encontrar más detalles en el capítulo 3 de [RW2006]_."

#: ../modules/gaussian_process.rst:254
msgid "The GP prior mean is assumed to be zero. The prior's covariance is specified by passing a :ref:`kernel <gp_kernels>` object. The hyperparameters of the kernel are optimized during fitting of GaussianProcessRegressor by maximizing the log-marginal-likelihood (LML) based on the passed ``optimizer``. As the LML may have multiple local optima, the optimizer can be started repeatedly by specifying ``n_restarts_optimizer``. The first run is always conducted starting from the initial hyperparameter values of the kernel; subsequent runs are conducted from hyperparameter values that have been chosen randomly from the range of allowed values. If the initial hyperparameters should be kept fixed, `None` can be passed as optimizer."
msgstr "Se asume que la media de la prioridad GP es cero. La covarianza de la prioridad se especifica pasando un objeto :ref:`kernel <gp_kernels>`. Los hiperparámetros del kernel se optimizan durante el ajuste de GaussianProcessRegressor maximizando la verosimilitud marginal logarítmica (LML) basado en el ``optimizer`` que se le pasa. Como el LML puede tener múltiples óptimos locales, el optimizador puede iniciarse repetidamente especificando ``n_restarts_optimizer``. La primera ejecución se realiza siempre a partir de los valores iniciales de los hiperparámetros del núcleo; las ejecuciones posteriores se realizan a partir de los valores de los hiperparámetros que se han elegido aleatoriamente del rango de valores permitidos. Si los hiperparámetros iniciales deben mantenerse fijos, se puede pasar `None` como optimizador."

#: ../modules/gaussian_process.rst:266
msgid ":class:`GaussianProcessClassifier` supports multi-class classification by performing either one-versus-rest or one-versus-one based training and prediction.  In one-versus-rest, one binary Gaussian process classifier is fitted for each class, which is trained to separate this class from the rest. In \"one_vs_one\", one binary Gaussian process classifier is fitted for each pair of classes, which is trained to separate these two classes. The predictions of these binary predictors are combined into multi-class predictions. See the section on :ref:`multi-class classification <multiclass>` for more details."
msgstr ":class:`GaussianProcessClassifier` admite la clasificación multiclase mediante el entrenamiento y la predicción basados en uno-contra-el-resto o uno-contra-uno.  En uno-contra-resto, se ajusta un clasificador de proceso gaussiano binario para cada clase, que se entrena para separar esta clase del resto. En \"one_vs_one\", se ajusta un clasificador de proceso gaussiano binario para cada par de clases, que se entrena para separar estas dos clases. Las predicciones de estos predictores binarios se combinan en predicciones multiclase. Consulte la sección de :ref:`clasificación multiclase <multiclass>` para obtener más detalles."

#: ../modules/gaussian_process.rst:275
msgid "In the case of Gaussian process classification, \"one_vs_one\" might be computationally  cheaper since it has to solve many problems involving only a subset of the whole training set rather than fewer problems on the whole dataset. Since Gaussian process classification scales cubically with the size of the dataset, this might be considerably faster. However, note that \"one_vs_one\" does not support predicting probability estimates but only plain predictions. Moreover, note that :class:`GaussianProcessClassifier` does not (yet) implement a true multi-class Laplace approximation internally, but as discussed above is based on solving several binary classification tasks internally, which are combined using one-versus-rest or one-versus-one."
msgstr "En el caso de la clasificación del proceso gaussiano, \"one_vs_one\" podría ser computacionalmente más económico, ya que tiene que resolver muchos problemas que implican sólo un subconjunto del conjunto de entrenamiento, en lugar de menos problemas en todo el conjunto de datos. Dado que la clasificación del proceso gaussiano se escala cúbicamente con el tamaño del conjunto de datos, esto podría ser considerablemente más rápido. Sin embargo, ten en cuenta que \"one_vs_one\" no admite la predicción de estimaciones de probabilidad, sino sólo predicciones directas. Además, ten en cuenta que :class:`GaussianProcessClassifier` no implementa (todavía) una verdadera aproximación de Laplace multiclase internamente, sino que, como se ha comentado anteriormente, se basa en la resolución de varias tareas de clasificación binaria internamente, que se combinan utilizando uno-versus-el-resto o uno-versus-uno."

#: ../modules/gaussian_process.rst:287
msgid "GPC examples"
msgstr "Ejemplos de GPC"

#: ../modules/gaussian_process.rst:290
msgid "Probabilistic predictions with GPC"
msgstr "Predicciones probabilísticas con GPC"

#: ../modules/gaussian_process.rst:292
msgid "This example illustrates the predicted probability of GPC for an RBF kernel with different choices of the hyperparameters. The first figure shows the predicted probability of GPC with arbitrarily chosen hyperparameters and with the hyperparameters corresponding to the maximum log-marginal-likelihood (LML)."
msgstr "Este ejemplo ilustra la probabilidad pronosticada de GPC para un núcleo RBF con diferentes elecciones de los hiperparámetros. La primera figura muestra la probabilidad predicha de GPC con hiperparámetros elegidos arbitrariamente y con los hiperparámetros correspondientes a la máxima verosimilitud marginal logarítmica (LML)."

#: ../modules/gaussian_process.rst:297
msgid "While the hyperparameters chosen by optimizing LML have a considerably larger LML, they perform slightly worse according to the log-loss on test data. The figure shows that this is because they exhibit a steep change of the class probabilities at the class boundaries (which is good) but have predicted probabilities close to 0.5 far away from the class boundaries (which is bad) This undesirable effect is caused by the Laplace approximation used internally by GPC."
msgstr "Aunque los hiperparámetros elegidos mediante la optimización de LML tienen un LML considerablemente mayor, su rendimiento es ligeramente inferior de acuerdo con la pérdida logarítmica en los datos de prueba. La figura muestra que esto se debe a que exhiben un cambio pronunciado de las probabilidades de clase en los límites de la clase (lo cual es bueno) pero tienen probabilidades predichas cercanas a 0,5 lejos de los límites de la clase (lo cual es malo) Este efecto indeseable es causado por la aproximación de Laplace utilizada internamente por GPC."

#: ../modules/gaussian_process.rst:305
msgid "The second figure shows the log-marginal-likelihood for different choices of the kernel's hyperparameters, highlighting the two choices of the hyperparameters used in the first figure by black dots."
msgstr "La segunda figura muestra la verosimilitud marginal logarítmica para diferentes opciones de los hiperparámetros del núcleo, destacando las dos opciones de los hiperparámetros utilizados en la primera figura mediante puntos negros."

#: ../modules/gaussian_process.rst:319
msgid "Illustration of GPC on the XOR dataset"
msgstr "Ejemplo de GPC en el conjunto de datos XOR"

#: ../modules/gaussian_process.rst:323
msgid "This example illustrates GPC on XOR data. Compared are a stationary, isotropic kernel (:class:`RBF`) and a non-stationary kernel (:class:`DotProduct`). On this particular dataset, the :class:`DotProduct` kernel obtains considerably better results because the class-boundaries are linear and coincide with the coordinate axes. In practice, however, stationary kernels such as :class:`RBF` often obtain better results."
msgstr "Este ejemplo ilustra la GPC en datos XOR. Se comparan un núcleo estacionario e isotrópico (:class:`RBF`) y un núcleo no estacionario (:class:`DotProduct`). En este conjunto de datos concreto, el núcleo :class:`DotProduct` obtiene resultados considerablemente mejores porque los límites de la clase son lineales y coinciden con los ejes de coordenadas. En la práctica, sin embargo, los núcleos estacionarios como :class:`RBF` suelen obtener mejores resultados."

#: ../modules/gaussian_process.rst:338
msgid "Gaussian process classification (GPC) on iris dataset"
msgstr "Clasificación de procesos gaussianos (GPC) en el conjunto de datos iris"

#: ../modules/gaussian_process.rst:340
msgid "This example illustrates the predicted probability of GPC for an isotropic and anisotropic RBF kernel on a two-dimensional version for the iris-dataset. This illustrates the applicability of GPC to non-binary classification. The anisotropic RBF kernel obtains slightly higher log-marginal-likelihood by assigning different length-scales to the two feature dimensions."
msgstr "Este ejemplo muestra la probabilidad predicha de GPC para un núcleo RBF isotrópico y anisotrópico en una versión bidimensional para el conjunto de datos del iris. Esto muestra la aplicabilidad de la GPC a la clasificación no binaria. El núcleo RBF anisotrópico obtiene una verosimilitud marginal logarítmica (log-marginal-likelihood) ligeramente superior al asignar diferentes escalas de longitud a las dos dimensiones de características."

#: ../modules/gaussian_process.rst:354
msgid "Kernels for Gaussian Processes"
msgstr "Núcleos para procesos gaussianos"

#: ../modules/gaussian_process.rst:357
msgid "Kernels (also called \"covariance functions\" in the context of GPs) are a crucial ingredient of GPs which determine the shape of prior and posterior of the GP. They encode the assumptions on the function being learned by defining the \"similarity\" of two datapoints combined with the assumption that similar datapoints should have similar target values. Two categories of kernels can be distinguished: stationary kernels depend only on the distance of two datapoints and not on their absolute values :math:`k(x_i, x_j)= k(d(x_i, x_j))` and are thus invariant to translations in the input space, while non-stationary kernels depend also on the specific values of the datapoints. Stationary kernels can further be subdivided into isotropic and anisotropic kernels, where isotropic kernels are also invariant to rotations in the input space. For more details, we refer to Chapter 4 of [RW2006]_. For guidance on how to best combine different kernels, we refer to [Duv2014]_."
msgstr "Los núcleos (también llamados \"funciones de covarianza\" en el contexto de las GP) son un ingrediente crucial de las GP que determinan la forma de la probabilidad a priori (prior) y a posteriori (posterior) de la GP. Codifican las suposiciones sobre la función que se aprende definiendo la \"similitud\" de dos puntos de datos combinada con la suposición de que los puntos de datos similares deberían tener valores objetivo similares. Se pueden distinguir dos categorías de núcleos: los núcleos estacionarios dependen sólo de la distancia de dos puntos de datos y no de sus valores absolutos :math:`k(x_i, x_j)= k(d(x_i, x_j))` y, por tanto, son invariables a las traslaciones en el espacio de entrada, mientras que los núcleos no estacionarios dependen también de los valores específicos de los puntos de datos. Los núcleos estacionarios pueden subdividirse en núcleos isotrópicos y anisotrópicos, donde los núcleos isotrópicos también son invariantes a las rotaciones en el espacio de entrada. Para más detalles, nos remitimos al capítulo 4 de [RW2006]_. Para obtener orientación sobre la mejor manera de combinar diferentes núcleos, nos remitimos a [Duv2014]_."

#: ../modules/gaussian_process.rst:372
msgid "Gaussian Process Kernel API"
msgstr "API del núcleo de proceso gaussiano"

#: ../modules/gaussian_process.rst:373
msgid "The main usage of a :class:`Kernel` is to compute the GP's covariance between datapoints. For this, the method ``__call__`` of the kernel can be called. This method can either be used to compute the \"auto-covariance\" of all pairs of datapoints in a 2d array X, or the \"cross-covariance\" of all combinations of datapoints of a 2d array X with datapoints in a 2d array Y. The following identity holds true for all kernels k (except for the :class:`WhiteKernel`): ``k(X) == K(X, Y=X)``"
msgstr "El uso principal de un :class:`Kernel` es calcular la covarianza de la GP entre puntos de datos. Para ello, se puede llamar al método ``__call__`` del núcleo. Este método puede utilizarse para calcular la \"autocovarianza\" de todos los pares de puntos de datos de un arreglo 2D X, o la \"covarianza cruzada\" de todas las combinaciones de puntos de datos de un arreglo 2D X con puntos de datos de una matriz 2D Y. La siguiente identidad es válida para todos los núcleos k (excepto para el :class:`WhiteKernel`): ``k(X) == K(X, Y=X)``"

#: ../modules/gaussian_process.rst:381
msgid "If only the diagonal of the auto-covariance is being used, the method ``diag()`` of a kernel can be called, which is more computationally efficient than the equivalent call to ``__call__``: ``np.diag(k(X, X)) == k.diag(X)``"
msgstr "Si sólo se utiliza la diagonal de la autocovarianza, se puede llamar al método ``diag()`` de un núcleo, que es más eficiente computacionalmente que la llamada equivalente a ``__call__``: ``np.diag(k(X, X)) == k.diag(X)``"

#: ../modules/gaussian_process.rst:385
msgid "Kernels are parameterized by a vector :math:`\\theta` of hyperparameters. These hyperparameters can for instance control length-scales or periodicity of a kernel (see below). All kernels support computing analytic gradients of the kernel's auto-covariance with respect to :math:`log(\\theta)` via setting ``eval_gradient=True`` in the ``__call__`` method. That is, a ``(len(X), len(X), len(theta))`` array is returned where the entry ``[i, j, l]`` contains :math:`\\frac{\\partial k_\\theta(x_i, x_j)}{\\partial log(\\theta_l)}`. This gradient is used by the Gaussian process (both regressor and classifier) in computing the gradient of the log-marginal-likelihood, which in turn is used to determine the value of :math:`\\theta`, which maximizes the log-marginal-likelihood, via gradient ascent. For each hyperparameter, the initial value and the bounds need to be specified when creating an instance of the kernel. The current value of :math:`\\theta` can be get and set via the property ``theta`` of the kernel object. Moreover, the bounds of the hyperparameters can be accessed by the property ``bounds`` of the kernel. Note that both properties (theta and bounds) return log-transformed values of the internally used values since those are typically more amenable to gradient-based optimization. The specification of each hyperparameter is stored in the form of an instance of :class:`Hyperparameter` in the respective kernel. Note that a kernel using a hyperparameter with name \"x\" must have the attributes self.x and self.x_bounds."
msgstr "Los núcleos están parametrizados por un vector :math:`\\theta` de hiperparámetros. Estos hiperparámetros pueden, por ejemplo, controlar las escalas de longitud o la periodicidad de un núcleo (véase más adelante). Todos los núcleos soportan el cálculo de gradientes analíticos de la autocovarianza del núcleo con respecto a :math:`log(\\theta)` mediante la configuración de ``eval_gradient=True`` en el método ``__call__``. Es decir, se devuelve una matriz ``(len(X), len(X), len(theta))`` donde la entrada ``[i, j, l]`` contiene :math:`\\frac{parcial k_\\theta(x_i, x_j)}{\\parcial log(\\theta_l)}`. Este gradiente es utilizado por el proceso gaussiano (tanto el regresor como el clasificador) en el cálculo del gradiente de la verosimilitud marginal logarítmica (log-marginal-likelihood), que a su vez se utiliza para determinar el valor de :math:`\\theta`, que maximiza el logaritmo de probabilidad marginal, a través del ascenso del gradiente. Para cada hiperparámetro, es necesario especificar el valor inicial y los límites al crear una instancia del núcleo. El valor actual de :math:`\\theta` puede obtenerse y establecerse mediante la propiedad ``theta`` del objeto Kernel. Además, se puede acceder a los límites de los hiperparámetros mediante la propiedad ``bounds`` del núcleo. Observa que ambas propiedades (theta y bounds) devuelven valores transformados logarítmicamente de los valores utilizados internamente, ya que suelen ser más susceptibles de una optimización basada en el gradiente. La especificación de cada hiperparámetro se almacena en forma de una instancia de :class:`Hyperparameter` en el núcleo respectivo. Ten en cuenta que un núcleo que utiliza un hiperparámetro con nombre \"x\" debe tener los atributos self.x y self.x_bounds."

#: ../modules/gaussian_process.rst:406
msgid "The abstract base class for all kernels is :class:`Kernel`. Kernel implements a similar interface as :class:`Estimator`, providing the methods ``get_params()``, ``set_params()``, and ``clone()``. This allows setting kernel values also via meta-estimators such as :class:`Pipeline` or :class:`GridSearch`. Note that due to the nested structure of kernels (by applying kernel operators, see below), the names of kernel parameters might become relatively complicated. In general, for a binary kernel operator, parameters of the left operand are prefixed with ``k1__`` and parameters of the right operand with ``k2__``. An additional convenience method is ``clone_with_theta(theta)``, which returns a cloned version of the kernel but with the hyperparameters set to ``theta``. An illustrative example:"
msgstr "La clase base abstracta para todos los núcleos es :class:`Kernel`. El núcleo implementa una interfaz similar a la de :class:`Estimator`, proporcionando los métodos ``get_params()``, ``set_params()``, y ``clone()``. Esto permite establecer los valores del núcleo también a través de metaestimadores como :class:`Pipeline` o :class:`GridSearch`. Ten en cuenta que, debido a la estructura anidada de los núcleos (al aplicar operadores del núcleo, como podrás ver más adelante), los nombres de los parámetros del núcleo pueden resultar relativamente complicados. En general, para un operador de núcleo binario, los parámetros del operando izquierdo llevan el prefijo ``k1__`` y los parámetros del operando derecho el prefijo ``k2__``. Un método adicional de utilidad es ``clone_with_theta(theta)``, que devuelve una versión clonada del núcleo pero con los hiperparámetros ajustados a ``theta``. Un ejemplo de ello:"

#: ../modules/gaussian_process.rst:443
msgid "All Gaussian process kernels are interoperable with :mod:`sklearn.metrics.pairwise` and vice versa: instances of subclasses of :class:`Kernel` can be passed as ``metric`` to ``pairwise_kernels`` from :mod:`sklearn.metrics.pairwise`. Moreover, kernel functions from pairwise can be used as GP kernels by using the wrapper class :class:`PairwiseKernel`. The only caveat is that the gradient of the hyperparameters is not analytic but numeric and all those kernels support only isotropic distances. The parameter ``gamma`` is considered to be a hyperparameter and may be optimized. The other kernel parameters are set directly at initialization and are kept fixed."
msgstr "Todos los núcleos de procesos gaussianos son interoperables con :mod:`sklearn.metrics.pairwise` y viceversa: las instancias de las subclases de :class:`Kernel` pueden pasarse como ``metric`` a ``pairwise_kernels`` de :mod:`sklearn.metrics.pairwise`. Además, las funciones del núcleo de pares (pairwise) pueden utilizarse como núcleos de GP utilizando la clase envolvente :class:`PairwiseKernel`. La única advertencia es que el gradiente de los hiperparámetros no es analítico, sino numérico, y todos esos núcleos sólo admiten distancias isotrópicas. El parámetro ``gamma`` se considera un hiperparámetro y puede ser optimizado. Los demás parámetros del núcleo se establecen directamente en la inicialización y se mantienen fijos."

#: ../modules/gaussian_process.rst:455
msgid "Basic kernels"
msgstr "Núcleos (kernels) básicos"

#: ../modules/gaussian_process.rst:456
msgid "The :class:`ConstantKernel` kernel can be used as part of a :class:`Product` kernel where it scales the magnitude of the other factor (kernel) or as part of a :class:`Sum` kernel, where it modifies the mean of the Gaussian process. It depends on a parameter :math:`constant\\_value`. It is defined as:"
msgstr "El núcleo :class:`ConstantKernel` puede utilizarse como parte de un núcleo :class:`Product` en el que se escala la magnitud del otro factor (núcleo) o como parte de un núcleo :class:`Sum`, en el que se modifica la media del proceso gaussiano. Depende de un parámetro :math:`constant\\_value`. Se define como:"

#: ../modules/gaussian_process.rst:461
msgid "k(x_i, x_j) = constant\\_value \\;\\forall\\; x_1, x_2\n\n"
msgstr "k(x_i, x_j) = constant\\_value \\;\\forall\\; x_1, x_2\n\n"

#: ../modules/gaussian_process.rst:464
msgid "The main use-case of the :class:`WhiteKernel` kernel is as part of a sum-kernel where it explains the noise-component of the signal. Tuning its parameter :math:`noise\\_level` corresponds to estimating the noise-level. It is defined as:"
msgstr "El principal caso de uso del núcleo :class:`WhiteKernel` es como parte de un núcleo de suma en el que explica el componente de ruido de la señal. El ajuste de su parámetro :math:`noise\\_level` corresponde a la estimación del nivel de ruido. Se define como:"

#: ../modules/gaussian_process.rst:469
msgid "k(x_i, x_j) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0\n\n"
msgstr "k(x_i, x_j) = noise\\_level \\text{ if } x_i == x_j \\text{ else } 0\n\n"

#: ../modules/gaussian_process.rst:474
msgid "Kernel operators"
msgstr "Operadores de núcleo"

#: ../modules/gaussian_process.rst:475
msgid "Kernel operators take one or two base kernels and combine them into a new kernel. The :class:`Sum` kernel takes two kernels :math:`k_1` and :math:`k_2` and combines them via :math:`k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)`. The  :class:`Product` kernel takes two kernels :math:`k_1` and :math:`k_2` and combines them via :math:`k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)`. The :class:`Exponentiation` kernel takes one base kernel and a scalar parameter :math:`p` and combines them via :math:`k_{exp}(X, Y) = k(X, Y)^p`. Note that magic methods ``__add__``, ``__mul___`` and ``__pow__`` are overridden on the Kernel objects, so one can use e.g. ``RBF() + RBF()`` as a shortcut for ``Sum(RBF(), RBF())``."
msgstr "Los operadores de núcleo toman uno o dos núcleos base y los combinan en un nuevo núcleo. El núcleo :class:`Suma` toma dos núcleos :math:`k_1` y :math:`k_2` y los combina mediante :math:`k_{sum}(X, Y) = k_1(X, Y) + k_2(X, Y)`. El núcleo :class:`Product` toma dos núcleos :math:`k_1` y :math:`k_2` y los combina mediante :math:`k_{product}(X, Y) = k_1(X, Y) * k_2(X, Y)`. El núcleo :class:`Exponentiation` toma un núcleo base y un parámetro escalar :math:`p` y los combina mediante :math:`k_{exp}(X, Y) = k(X, Y)^p`. Tenga en cuenta que los métodos mágicos ``__add__``, ``__mul___`` y ``__pow__`` se anulan en los objetos Kernel, por lo que se puede utilizar, por ejemplo, ``RBF() + RBF()`` como un atajo para ``Sum(RBF(), RBF())``."

#: ../modules/gaussian_process.rst:488
msgid "Radial-basis function (RBF) kernel"
msgstr "Núcleo de la Función de Base Radial (RBF)"

#: ../modules/gaussian_process.rst:489
msgid "The :class:`RBF` kernel is a stationary kernel. It is also known as the \"squared exponential\" kernel. It is parameterized by a length-scale parameter :math:`l>0`, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs :math:`x` (anisotropic variant of the kernel). The kernel is given by:"
msgstr "El núcleo :class:`RBF` es un núcleo estacionario. También se conoce como núcleo \"exponencial cuadrado\". Está parametrizado por un parámetro de escala de longitud :math:`l>0`, que puede ser un escalar (variante isotrópica del núcleo) o un vector con el mismo número de dimensiones que las entradas :math:`x` (variante anisotrópica del núcleo). El núcleo viene dado por:"

#: ../modules/gaussian_process.rst:495
msgid "k(x_i, x_j) = \\text{exp}\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)\n\n"
msgstr "k(x_i, x_j) = \\text{exp}\\left(- \\frac{d(x_i, x_j)^2}{2l^2} \\right)\n\n"

#: ../modules/gaussian_process.rst:498
msgid "where :math:`d(\\cdot, \\cdot)` is the Euclidean distance. This kernel is infinitely differentiable, which implies that GPs with this kernel as covariance function have mean square derivatives of all orders, and are thus very smooth. The prior and posterior of a GP resulting from an RBF kernel are shown in the following figure:"
msgstr "donde :math:`d(\\cdot, \\cdot)` es la distancia euclidiana. Este núcleo es infinitamente diferenciable, lo que implica que las GP con este núcleo como función de covarianza tienen derivadas cuadráticas medias de todos los órdenes, y por tanto resultan muy suavizadas (smooth). En la siguiente figura se muestran los valores a priori y a posteriori de una GP resultante de un núcleo RBF:"

#: ../modules/gaussian_process.rst:510
msgid "Matérn kernel"
msgstr "Núcleo Matérn"

#: ../modules/gaussian_process.rst:511
msgid "The :class:`Matern` kernel is a stationary kernel and a generalization of the :class:`RBF` kernel. It has an additional parameter :math:`\\nu` which controls the smoothness of the resulting function. It is parameterized by a length-scale parameter :math:`l>0`, which can either be a scalar (isotropic variant of the kernel) or a vector with the same number of dimensions as the inputs :math:`x` (anisotropic variant of the kernel). The kernel is given by:"
msgstr "El núcleo :class:`Matern` es un núcleo estacionario y una generalización del núcleo :class:`RBF`. Tiene un parámetro adicional :math:`\\nu` que controla la suavidad de la función resultante. Está parametrizado por un parámetro de escala de longitud :math:`l>0`, que puede ser un escalar (variante isotrópica del núcleo) o un vector con el mismo número de dimensiones que las entradas :math:`x` (variante anisotrópica del núcleo). El núcleo viene dado por:"

#: ../modules/gaussian_process.rst:515
msgid "k(x_i, x_j) = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)^\\nu K_\\nu\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg),"
msgstr "k(x_i, x_j) = \\frac{1}{\\Gamma(\\nu)2^{\\nu-1}}\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg)^\\nu K_\\nu\\Bigg(\\frac{\\sqrt{2\\nu}}{l} d(x_i , x_j )\\Bigg),"

#: ../modules/gaussian_process.rst:519
msgid "where :math:`d(\\cdot,\\cdot)` is the Euclidean distance, :math:`K_\\nu(\\cdot)` is a modified Bessel function and :math:`\\Gamma(\\cdot)` is the gamma function. As :math:`\\nu\\rightarrow\\infty`, the Matérn kernel converges to the RBF kernel. When :math:`\\nu = 1/2`, the Matérn kernel becomes identical to the absolute exponential kernel, i.e.,"
msgstr "donde :math:`d(\\cdot,\\cdot)` es la distancia euclidiana, :math:`K_\\nu(\\cdot)` es una función de Bessel modificada y :math:`\\Gamma(\\cdot)` es la función gamma. Como :math:`\\nu\\rightarrow\\infty`, el núcleo Matérn converge al núcleo RBF. Cuando :math:`\\nu = 1/2`, el núcleo de Matérn se vuelve idéntico al núcleo exponencial absoluto, es decir"

#: ../modules/gaussian_process.rst:524
msgid "k(x_i, x_j) = \\exp \\Bigg(- \\frac{1}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{1}{2}\n\n"
msgstr "k(x_i, x_j) = \\exp \\Bigg(- \\frac{1}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{1}{2}\n\n"

#: ../modules/gaussian_process.rst:527
msgid "In particular, :math:`\\nu = 3/2`:"
msgstr "En particular, :math:`\\nu = 3/2`:"

#: ../modules/gaussian_process.rst:529
msgid "k(x_i, x_j) =  \\Bigg(1 + \\frac{\\sqrt{3}}{l} d(x_i , x_j )\\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{3}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{3}{2}\n\n"
msgstr "k(x_i, x_j) =  \\Bigg(1 + \\frac{\\sqrt{3}}{l} d(x_i , x_j )\\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{3}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{3}{2}\n\n"

#: ../modules/gaussian_process.rst:532
msgid "and :math:`\\nu = 5/2`:"
msgstr "and :math:`\\nu = 5/2`:"

#: ../modules/gaussian_process.rst:534
msgid "k(x_i, x_j) = \\Bigg(1 + \\frac{\\sqrt{5}}{l} d(x_i , x_j ) +\\frac{5}{3l} d(x_i , x_j )^2 \\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{5}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{5}{2}\n\n"
msgstr "k(x_i, x_j) = \\Bigg(1 + \\frac{\\sqrt{5}}{l} d(x_i , x_j ) +\\frac{5}{3l} d(x_i , x_j )^2 \\Bigg) \\exp \\Bigg(-\\frac{\\sqrt{5}}{l} d(x_i , x_j ) \\Bigg) \\quad \\quad \\nu= \\tfrac{5}{2}\n\n"

#: ../modules/gaussian_process.rst:537
msgid "are popular choices for learning functions that are not infinitely differentiable (as assumed by the RBF kernel) but at least once (:math:`\\nu = 3/2`) or twice differentiable (:math:`\\nu = 5/2`)."
msgstr "son opciones populares para funciones de aprendizaje que no son infinitamente diferenciables (como asume el núcleo RBF), pero al menos una vez (:math:`\\nu = 3/2`) o dos veces diferenciables (:math:`\\nu = 5/2`)."

#: ../modules/gaussian_process.rst:541
msgid "The flexibility of controlling the smoothness of the learned function via :math:`\\nu` allows adapting to the properties of the true underlying functional relation. The prior and posterior of a GP resulting from a Matérn kernel are shown in the following figure:"
msgstr "La flexibilidad de controlar la suavidad de la función aprendida a través de :math:`\\nu` permite adaptarse a las propiedades de la verdadera relación funcional subyacente. En la siguiente figura se muestran los valores a priori y a posteriori de una GP resultante de un núcleo de Matérn:"

#: ../modules/gaussian_process.rst:550
msgid "See [RW2006]_, pp84 for further details regarding the different variants of the Matérn kernel."
msgstr "Ver [RW2006]_, pp84 para más detalles sobre las diferentes variantes del núcleo de Matérn."

#: ../modules/gaussian_process.rst:554
msgid "Rational quadratic kernel"
msgstr "Núcleo cuadrático racional"

#: ../modules/gaussian_process.rst:556
msgid "The :class:`RationalQuadratic` kernel can be seen as a scale mixture (an infinite sum) of :class:`RBF` kernels with different characteristic length-scales. It is parameterized by a length-scale parameter :math:`l>0` and a scale mixture parameter  :math:`\\alpha>0` Only the isotropic variant where :math:`l` is a scalar is supported at the moment. The kernel is given by:"
msgstr "El núcleo :class:`RationalQuadratic` puede verse como una mezcla de escalas (una suma infinita) de núcleos :class:`RBF` con diferentes escalas de longitud características. Está parametrizado por un parámetro de escala de longitud :math:`l>0` y un parámetro de mezcla de escalas :math:`alpha>0` Por el momento sólo se admite la variante isotrópica en la que :math:`l` es un escalar. El núcleo viene dado por:"

#: ../modules/gaussian_process.rst:562
msgid "k(x_i, x_j) = \\left(1 + \\frac{d(x_i, x_j)^2}{2\\alpha l^2}\\right)^{-\\alpha}\n\n"
msgstr "k(x_i, x_j) = \\left(1 + \\frac{d(x_i, x_j)^2}{2\\alpha l^2}\\right)^{-\\alpha}\n\n"

#: ../modules/gaussian_process.rst:565
msgid "The prior and posterior of a GP resulting from a :class:`RationalQuadratic` kernel are shown in the following figure:"
msgstr "En la siguiente figura se muestran los valores a priori y a posteriori de una GP resultante de un núcleo :class:`RationalQuadratic`:"

#: ../modules/gaussian_process.rst:573
msgid "Exp-Sine-Squared kernel"
msgstr "Núcleo exponencial sinusoidal cuadrático"

#: ../modules/gaussian_process.rst:575
msgid "The :class:`ExpSineSquared` kernel allows modeling periodic functions. It is parameterized by a length-scale parameter :math:`l>0` and a periodicity parameter :math:`p>0`. Only the isotropic variant where :math:`l` is a scalar is supported at the moment. The kernel is given by:"
msgstr "El núcleo :class:`ExpSineSquared` permite modelar funciones periódicas. Está parametrizado por un parámetro de escala de longitud :math:`l>0` y un parámetro de periodicidad :math:`p>0`. Sólo la variante isotrópica donde :math:`l` es un escalar es soportada por el momento. El núcleo viene dado por:"

#: ../modules/gaussian_process.rst:580
msgid "k(x_i, x_j) = \\text{exp}\\left(- \\frac{ 2\\sin^2(\\pi d(x_i, x_j) / p) }{ l^ 2} \\right)\n\n"
msgstr "k(x_i, x_j) = \\text{exp}\\left(- \\frac{ 2\\sin^2(\\pi d(x_i, x_j) / p) }{ l^ 2} \\right)\n\n"

#: ../modules/gaussian_process.rst:583
msgid "The prior and posterior of a GP resulting from an ExpSineSquared kernel are shown in the following figure:"
msgstr "Los valores a priori y a posteriori de una GP resultante de un núcleo ExpSineSquared se muestran en la siguiente figura:"

#: ../modules/gaussian_process.rst:591
msgid "Dot-Product kernel"
msgstr "Núcleo de producto punto"

#: ../modules/gaussian_process.rst:593
msgid "The :class:`DotProduct` kernel is non-stationary and can be obtained from linear regression by putting :math:`N(0, 1)` priors on the coefficients of :math:`x_d (d = 1, . . . , D)` and a prior of :math:`N(0, \\sigma_0^2)` on the bias. The :class:`DotProduct` kernel is invariant to a rotation of the coordinates about the origin, but not translations. It is parameterized by a parameter :math:`\\sigma_0^2`. For :math:`\\sigma_0^2 = 0`, the kernel is called the homogeneous linear kernel, otherwise it is inhomogeneous. The kernel is given by"
msgstr "El núcleo :class:`DotProduct` no es estacionario y puede obtenerse a partir de la regresión lineal poniendo los a priori (priors) de :math:`N(0, 1)` en los coeficientes de :math:`x_d (d = 1, . . , D)` y un a priori de :math:`N(0, \\sigma_0^2)` en el sesgo. El núcleo :class:`DotProduct` es invariante a una rotación de las coordenadas sobre el origen, pero no a las traslaciones. Está parametrizado por un parámetro :math:`\\sigma_0^2`. Para :math:`\\sigma_0^2 = 0`, el núcleo se denomina núcleo lineal homogéneo, en caso contrario es no homogéneo. El núcleo viene dado por"

#: ../modules/gaussian_process.rst:600
msgid "k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n\n"
msgstr "k(x_i, x_j) = \\sigma_0 ^ 2 + x_i \\cdot x_j\n\n"

#: ../modules/gaussian_process.rst:603
msgid "The :class:`DotProduct` kernel is commonly combined with exponentiation. An example with exponent 2 is shown in the following figure:"
msgstr "El núcleo :class:`DotProduct` se combina habitualmente con la exponenciación. Un ejemplo con exponente 2 se muestra en la siguiente figura:"

#: ../modules/gaussian_process.rst:611
msgid "References"
msgstr "Referencias"

#: ../modules/gaussian_process.rst:613
msgid "Carl Eduard Rasmussen and Christopher K.I. Williams, \"Gaussian Processes for Machine Learning\", MIT Press 2006, Link to an official complete PDF version of the book `here <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_ ."
msgstr "Carl Eduard Rasmussen y Christopher K.I. Williams, \"Gaussian Processes for Machine Learning\", MIT Press 2006, Vínculo al PDF completo y versión oficial del libro `aquí <http://www.gaussianprocess.org/gpml/chapters/RW.pdf>`_ ."

#: ../modules/gaussian_process.rst:615
msgid "David Duvenaud, \"The Kernel Cookbook: Advice on Covariance functions\", 2014, `Link <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_ ."
msgstr "David Duvenaud, \"The Kernel Cookbook: Advice on Covariance functions\", 2014, `Link <https://www.cs.toronto.edu/~duvenaud/cookbook/>`_ ."

