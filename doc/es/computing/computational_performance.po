msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-08-10 23:05\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/computing/computational_performance.po\n"
"X-Crowdin-File-ID: 4756\n"
"Language: es_ES\n"

#: ../computing/computational_performance.rst:10
msgid "Computational Performance"
msgstr "Rendimiento computacional"

#: ../computing/computational_performance.rst:12
msgid "For some applications the performance (mainly latency and throughput at prediction time) of estimators is crucial. It may also be of interest to consider the training throughput but this is often less important in a production setup (where it often takes place offline)."
msgstr "Para algunas aplicaciones, el rendimiento (principalmente la latencia y el rendimiento en el momento de la predicción) de los estimadores es crucial. También puede ser interesante tener en cuenta el rendimiento del entrenamiento, pero esto suele ser menos importante en una configuración de producción (donde a menudo tiene lugar fuera de línea)."

#: ../computing/computational_performance.rst:17
msgid "We will review here the orders of magnitude you can expect from a number of scikit-learn estimators in different contexts and provide some tips and tricks for overcoming performance bottlenecks."
msgstr "Revisaremos aquí los órdenes de magnitud que se pueden esperar de una serie de estimadores de scikit-learn en diferentes contextos y proporcionaremos algunos consejos y trucos para superar los cuellos de botella de rendimiento."

#: ../computing/computational_performance.rst:21
msgid "Prediction latency is measured as the elapsed time necessary to make a prediction (e.g. in micro-seconds). Latency is often viewed as a distribution and operations engineers often focus on the latency at a given percentile of this distribution (e.g. the 90 percentile)."
msgstr "La latencia de predicción se mide como el tiempo necesario para realizar una predicción (por ejemplo, en microsegundos). La latencia suele verse como una distribución y los ingenieros de operaciones suelen centrarse en la latencia en un percentil determinado de esta distribución (por ejemplo, el percentil 90)."

#: ../computing/computational_performance.rst:26
msgid "Prediction throughput is defined as the number of predictions the software can deliver in a given amount of time (e.g. in predictions per second)."
msgstr "El rendimiento de las predicciones se define como el número de predicciones que el software puede realizar en un tiempo determinado (por ejemplo, en predicciones por segundo)."

#: ../computing/computational_performance.rst:29
msgid "An important aspect of performance optimization is also that it can hurt prediction accuracy. Indeed, simpler models (e.g. linear instead of non-linear, or with fewer parameters) often run faster but are not always able to take into account the same exact properties of the data as more complex ones."
msgstr "Un aspecto importante de la optimización del rendimiento es también que puede perjudicar la precisión de las predicciones. En efecto, los modelos más sencillos (por ejemplo, lineales en lugar de no lineales, o con menos parámetros) suelen funcionar más rápido, pero no siempre son capaces de tener en cuenta las mismas propiedades exactas de los datos que los más complejos."

#: ../computing/computational_performance.rst:35
msgid "Prediction Latency"
msgstr "Latencia de predicción"

#: ../computing/computational_performance.rst:37
msgid "One of the most straight-forward concerns one may have when using/choosing a machine learning toolkit is the latency at which predictions can be made in a production environment."
msgstr "Una de las preocupaciones más directas que uno puede tener al utilizar/elegir un conjunto de herramientas de aprendizaje automático es la latencia con la que se pueden hacer predicciones en un entorno de producción."

#: ../computing/computational_performance.rst:45
msgid "The main factors that influence the prediction latency are"
msgstr "Los principales factores que influyen en la latencia de la predicción son"

#: ../computing/computational_performance.rst:42
msgid "Number of features"
msgstr "Número de características"

#: ../computing/computational_performance.rst:43
msgid "Input data representation and sparsity"
msgstr "Representación de los datos de entrada y dispersión"

#: ../computing/computational_performance.rst:44
msgid "Model complexity"
msgstr "Complejidad del modelo"

#: ../computing/computational_performance.rst:45
msgid "Feature extraction"
msgstr "Extracción de características"

#: ../computing/computational_performance.rst:47
msgid "A last major parameter is also the possibility to do predictions in bulk or one-at-a-time mode."
msgstr "Un último parámetro importante es también la posibilidad de hacer predicciones en modo masivo o uno a uno."

#: ../computing/computational_performance.rst:51
msgid "Bulk versus Atomic mode"
msgstr "Modo Bulk versus Atómico"

#: ../computing/computational_performance.rst:53
msgid "In general doing predictions in bulk (many instances at the same time) is more efficient for a number of reasons (branching predictability, CPU cache, linear algebra libraries optimizations etc.). Here we see on a setting with few features that independently of estimator choice the bulk mode is always faster, and for some of them by 1 to 2 orders of magnitude:"
msgstr "En general, hacer predicciones a granel (muchas instancias al mismo tiempo) es más eficiente por una serie de razones (previsibilidad de bifurcación, caché de la CPU, optimizaciones de las bibliotecas de álgebra lineal, etc.). Aquí vemos, en un entorno con pocas características, que, independientemente de la elección del estimador, el modo masivo es siempre más rápido, y para algunos de ellos en 1 o 2 órdenes de magnitud:"

#: ../computing/computational_performance.rst:64
msgid "atomic_prediction_latency"
msgstr "atomic_prediction_latency"

#: ../computing/computational_performance.rst:70
msgid "bulk_prediction_latency"
msgstr "bulk_prediction_latency"

#: ../computing/computational_performance.rst:71
msgid "To benchmark different estimators for your case you can simply change the ``n_features`` parameter in this example: :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py`. This should give you an estimate of the order of magnitude of the prediction latency."
msgstr "Para comparar diferentes estimadores en su caso, sólo tienes que cambiar el parámetro ``n_features`` en este ejemplo: :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py`. Esto debería darte una estimación del orden de magnitud de la latencia de predicción."

#: ../computing/computational_performance.rst:77
msgid "Configuring Scikit-learn for reduced validation overhead"
msgstr "Configuración de Scikit-learn para reducir la carga de validación"

#: ../computing/computational_performance.rst:79
msgid "Scikit-learn does some validation on data that increases the overhead per call to ``predict`` and similar functions. In particular, checking that features are finite (not NaN or infinite) involves a full pass over the data. If you ensure that your data is acceptable, you may suppress checking for finiteness by setting the environment variable ``SKLEARN_ASSUME_FINITE`` to a non-empty string before importing scikit-learn, or configure it in Python with :func:`set_config`. For more control than these global settings, a :func:`config_context` allows you to set this configuration within a specified context::"
msgstr "Scikit-learn realiza algunas validaciones sobre los datos que aumentan la sobrecarga por llamada a ``predict`` y funciones similares. En particular, la comprobación de que las características son finitas (no NaN o infinitas) implica una pasada completa por los datos. Si se asegura de que sus datos son aceptables, puede suprimir la comprobación de finitud estableciendo la variable de entorno ``SKLEARN_ASSUME_FINITE`` a una cadena no vacía antes de importar scikit-learn, o configurarlo en Python con :func:`set_config`. Para un mayor control que estas configuraciones globales, un :func:`config_context` te permite establecer esta configuración dentro de un contexto especificado::"

#: ../computing/computational_performance.rst:93
msgid "Note that this will affect all uses of :func:`~utils.assert_all_finite` within the context."
msgstr "Nota que esto afectará a todos los usos de :func:`~utils.assert_all_finite` dentro del contexto."

#: ../computing/computational_performance.rst:97
msgid "Influence of the Number of Features"
msgstr "Influencia del número de características"

#: ../computing/computational_performance.rst:99
msgid "Obviously when the number of features increases so does the memory consumption of each example. Indeed, for a matrix of :math:`M` instances with :math:`N` features, the space complexity is in :math:`O(NM)`. From a computing perspective it also means that the number of basic operations (e.g., multiplications for vector-matrix products in linear models) increases too. Here is a graph of the evolution of the prediction latency with the number of features:"
msgstr "Obviamente, cuando el número de características aumenta, también lo hace el consumo de memoria de cada ejemplo. De hecho, para una matriz de :math:`M` instancias con :math:`N` características, la complejidad espacial está en :math:`O(NM)`. Desde el punto de vista informático, esto significa también que el número de operaciones básicas (por ejemplo, las multiplicaciones para los productos vector-matriz en los modelos lineales) también aumenta. He aquí un gráfico de la evolución de la latencia de la predicción con el número de características:"

#: ../computing/computational_performance.rst:112
msgid "influence_of_n_features_on_latency"
msgstr "influence_of_n_features_on_latency"

#: ../computing/computational_performance.rst:113
msgid "Overall you can expect the prediction time to increase at least linearly with the number of features (non-linear cases can happen depending on the global memory footprint and estimator)."
msgstr "En general, se puede esperar que el tiempo de predicción aumente al menos linealmente con el número de características (pueden darse casos no lineales dependiendo de la huella de memoria global y del estimador)."

#: ../computing/computational_performance.rst:118
msgid "Influence of the Input Data Representation"
msgstr "Influencia de la representación de los datos de entrada"

#: ../computing/computational_performance.rst:120
msgid "Scipy provides sparse matrix data structures which are optimized for storing sparse data. The main feature of sparse formats is that you don't store zeros so if your data is sparse then you use much less memory. A non-zero value in a sparse (`CSR or CSC <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_) representation will only take on average one 32bit integer position + the 64 bit floating point value + an additional 32bit per row or column in the matrix. Using sparse input on a dense (or sparse) linear model can speedup prediction by quite a bit as only the non zero valued features impact the dot product and thus the model predictions. Hence if you have 100 non zeros in 1e6 dimensional space, you only need 100 multiply and add operation instead of 1e6."
msgstr "Scipy proporciona estructuras de datos de matrices dispersas que están optimizadas para almacenar datos dispersos. La principal característica de los formatos dispersos es que no se almacenan ceros, por lo que si los datos son dispersos, se utiliza mucha menos memoria. Un valor distinto de cero en una representación dispersa (`CSR o CSC <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_) sólo ocupará, por término medio, una posición entera de 32 bits + el valor de 64 bits en coma flotante + otros 32 bits por fila o columna de la matriz. El uso de una entrada dispersa en un modelo lineal denso (o disperso) puede acelerar la predicción en gran medida, ya que sólo las características de valor no nulo afectan al producto de puntos y, por tanto, a las predicciones del modelo. Por lo tanto, si tiene 100 valores no nulos en un espacio de 1e6 dimensiones, sólo necesitará 100 operaciones de multiplicación y suma en lugar de 1e6."

#: ../computing/computational_performance.rst:131
msgid "Calculation over a dense representation, however, may leverage highly optimised vector operations and multithreading in BLAS, and tends to result in fewer CPU cache misses. So the sparsity should typically be quite high (10% non-zeros max, to be checked depending on the hardware) for the sparse input representation to be faster than the dense input representation on a machine with many CPUs and an optimized BLAS implementation."
msgstr "El cálculo sobre una representación densa, sin embargo, puede aprovechar las operaciones vectoriales altamente optimizadas y el multithreading en BLAS, y tiende a dar lugar a menos pérdidas de caché de la CPU. Por lo tanto, la dispersión debería ser bastante alta (10% de no ceros como máximo, a comprobar dependiendo del hardware) para que la representación de entrada dispersa sea más rápida que la representación de entrada densa en una máquina con muchas CPUs y una implementación de BLAS optimizada."

#: ../computing/computational_performance.rst:138
msgid "Here is sample code to test the sparsity of your input::"
msgstr "Aquí hay un código de ejemplo para probar la escasez de su entrada::"

#: ../computing/computational_performance.rst:144
msgid "As a rule of thumb you can consider that if the sparsity ratio is greater than 90% you can probably benefit from sparse formats. Check Scipy's sparse matrix formats `documentation <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_ for more information on how to build (or convert your data to) sparse matrix formats. Most of the time the ``CSR`` and ``CSC`` formats work best."
msgstr "Como regla general, puedes considerar que si la tasa de dispersión es superior al 90%, probablemente puedas beneficiarte de los formatos dispersos. Consulta la `documentación sobre formatos de matrices dispersas de Scipy <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_ para obtener más información sobre cómo construir (o convertir tus datos a) formatos de matrices dispersas. La mayoría de las veces los formatos ``CSR`` y ``CSC`` funcionan mejor."

#: ../computing/computational_performance.rst:151
msgid "Influence of the Model Complexity"
msgstr "Influencia de la complejidad del modelo"

#: ../computing/computational_performance.rst:153
msgid "Generally speaking, when model complexity increases, predictive power and latency are supposed to increase. Increasing predictive power is usually interesting, but for many applications we would better not increase prediction latency too much. We will now review this idea for different families of supervised models."
msgstr "En general, cuando la complejidad del modelo aumenta, se supone que la potencia de predicción y la latencia aumentan. Aumentar la capacidad de predicción suele ser interesante, pero para muchas aplicaciones es mejor no aumentar demasiado la latencia de la predicción. A continuación revisaremos esta idea para diferentes familias de modelos supervisados."

#: ../computing/computational_performance.rst:159
msgid "For :mod:`sklearn.linear_model` (e.g. Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression...) the decision function that is applied at prediction time is the same (a dot product) , so latency should be equivalent."
msgstr "Para :mod:`sklearn.linear_model` (por ejemplo, Lasso, ElasticNet, SGDClassifier/Regressor, Ridge & RidgeClassifier, PassiveAggressiveClassifier/Regressor, LinearSVC, LogisticRegression...) la función de decisión que se aplica en el momento de la predicción es la misma (un producto punto) , por lo que la latencia debería ser equivalente."

#: ../computing/computational_performance.rst:165
msgid "Here is an example using :class:`~linear_model.SGDClassifier` with the ``elasticnet`` penalty. The regularization strength is globally controlled by the ``alpha`` parameter. With a sufficiently high ``alpha``, one can then increase the ``l1_ratio`` parameter of ``elasticnet`` to enforce various levels of sparsity in the model coefficients. Higher sparsity here is interpreted as less model complexity as we need fewer coefficients to describe it fully. Of course sparsity influences in turn the prediction time as the sparse dot-product takes time roughly proportional to the number of non-zero coefficients."
msgstr "Aquí hay un ejemplo usando :class:`~linear_model.SGDClassifier` con la penalización ``elasticnet``. La fuerza de la regularización es controlada globalmente por el parámetro ``alpha``. Con un parámetro \"alfa\" suficientemente alto, se puede aumentar el parámetro \"l1_ratio\" de \"elasticnet\" para imponer varios niveles de dispersión en los coeficientes del modelo. Una mayor dispersión se interpreta como una menor complejidad del modelo, ya que necesitamos menos coeficientes para describirlo completamente. Por supuesto, la dispersión influye a su vez en el tiempo de predicción, ya que el producto-punto disperso requiere un tiempo aproximadamente proporcional al número de coeficientes distintos de cero."

#: ../computing/computational_performance.rst:181
msgid "en_model_complexity"
msgstr "en_model_complexity"

#: ../computing/computational_performance.rst:182
msgid "For the :mod:`sklearn.svm` family of algorithms with a non-linear kernel, the latency is tied to the number of support vectors (the fewer the faster). Latency and throughput should (asymptotically) grow linearly with the number of support vectors in a SVC or SVR model. The kernel will also influence the latency as it is used to compute the projection of the input vector once per support vector. In the following graph the ``nu`` parameter of :class:`~svm.NuSVR` was used to influence the number of support vectors."
msgstr "Para la familia de algoritmos :mod:`sklearn.svm` con un núcleo no lineal, la latencia está ligada al número de vectores de soporte (cuanto menos, más rápido). La latencia y el rendimiento deberían crecer (asintóticamente) de forma lineal con el número de vectores de soporte en un modelo SVC o SVR. El núcleo también influye en la latencia, ya que se utiliza para calcular la proyección del vector de entrada una vez por vector de soporte. En el siguiente gráfico se utilizó el parámetro ``nu`` de :class:`~svm.NuSVR` para influir en el número de vectores de soporte."

#: ../computing/computational_performance.rst:196
msgid "nusvr_model_complexity"
msgstr "nusvr_model_complexity"

#: ../computing/computational_performance.rst:197
msgid "For :mod:`sklearn.ensemble` of trees (e.g. RandomForest, GBT, ExtraTrees etc) the number of trees and their depth play the most important role. Latency and throughput should scale linearly with the number of trees. In this case we used directly the ``n_estimators`` parameter of :class:`~ensemble.GradientBoostingRegressor`."
msgstr "En el caso de :mod:`sklearn.ensemble` de árboles (por ejemplo, RandomForest, GBT, ExtraTrees, etc.), el número de árboles y su profundidad desempeñan el papel más importante. La latencia y el rendimiento deberían escalar linealmente con el número de árboles. En este caso utilizamos directamente el parámetro ``n_estimators`` de :class:`~ensemble.GradientBoostingRegressor`."

#: ../computing/computational_performance.rst:208
msgid "gbt_model_complexity"
msgstr "gbt_model_complexity"

#: ../computing/computational_performance.rst:209
msgid "In any case be warned that decreasing model complexity can hurt accuracy as mentioned above. For instance a non-linearly separable problem can be handled with a speedy linear model but prediction power will very likely suffer in the process."
msgstr "En cualquier caso, hay que tener en cuenta que la disminución de la complejidad del modelo puede perjudicar la precisión, como se ha mencionado anteriormente. Por ejemplo, un problema separable de forma no lineal puede tratarse con un modelo lineal rápido, pero es muy probable que la capacidad de predicción se vea afectada en el proceso."

#: ../computing/computational_performance.rst:215
msgid "Feature Extraction Latency"
msgstr "Latencia de la extracción de características"

#: ../computing/computational_performance.rst:217
msgid "Most scikit-learn models are usually pretty fast as they are implemented either with compiled Cython extensions or optimized computing libraries. On the other hand, in many real world applications the feature extraction process (i.e. turning raw data like database rows or network packets into numpy arrays) governs the overall prediction time. For example on the Reuters text classification task the whole preparation (reading and parsing SGML files, tokenizing the text and hashing it into a common vector space) is taking 100 to 500 times more time than the actual prediction code, depending on the chosen model."
msgstr "La mayoría de los modelos de scikit-learn suelen ser bastante rápidos, ya que se implementan con extensiones compiladas de Cython o bibliotecas de computación optimizadas. Por otro lado, en muchas aplicaciones del mundo real, el proceso de extracción de características (es decir, la conversión de los datos en bruto, como las filas de la base de datos o los paquetes de red, en arreglos de numpy) rige el tiempo de predicción general. Por ejemplo, en la tarea de clasificación de textos de Reuters, toda la preparación (lectura y análisis sintáctico de los archivos SGML, tokenización del texto y hash en un espacio vectorial común) lleva de 100 a 500 veces más tiempo que el código de predicción real, dependiendo del modelo elegido."

#: ../computing/computational_performance.rst:232
msgid "prediction_time"
msgstr "prediction_time"

#: ../computing/computational_performance.rst:233
msgid "In many cases it is thus recommended to carefully time and profile your feature extraction code as it may be a good place to start optimizing when your overall latency is too slow for your application."
msgstr "Por lo tanto, en muchos casos se recomienda cronometrar y perfilar cuidadosamente su código de extracción de características, ya que puede ser un buen lugar para comenzar a optimizar cuando su latencia general es demasiado lenta para su aplicación."

#: ../computing/computational_performance.rst:238
msgid "Prediction Throughput"
msgstr "Rendimiento de predicción"

#: ../computing/computational_performance.rst:240
msgid "Another important metric to care about when sizing production systems is the throughput i.e. the number of predictions you can make in a given amount of time. Here is a benchmark from the :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py` example that measures this quantity for a number of estimators on synthetic data:"
msgstr "Otra métrica importante que hay que tener en cuenta a la hora de dimensionar los sistemas de producción es el rendimiento, es decir, el número de predicciones que se pueden hacer en un tiempo determinado. A continuación se muestra una referencia del ejemplo :ref:`sphx_glr_auto_examples_applications_plot_prediction_latency.py` que mide esta cantidad para una serie de estimadores sobre datos sintéticos:"

#: ../computing/computational_performance.rst:251
msgid "throughput_benchmark"
msgstr "throughput_benchmark"

#: ../computing/computational_performance.rst:252
msgid "These throughputs are achieved on a single process. An obvious way to increase the throughput of your application is to spawn additional instances (usually processes in Python because of the `GIL <https://wiki.python.org/moin/GlobalInterpreterLock>`_) that share the same model. One might also add machines to spread the load. A detailed explanation on how to achieve this is beyond the scope of this documentation though."
msgstr "Estos rendimientos se consiguen en un solo proceso. Una forma obvia de aumentar el rendimiento de tu aplicación es generar instancias adicionales (normalmente procesos en Python debido al `GIL <https://wiki.python.org/moin/GlobalInterpreterLock>`_) que compartan el mismo modelo. También se pueden añadir máquinas para repartir la carga. Sin embargo, una explicación detallada sobre cómo lograr esto está más allá del alcance de esta documentación."

#: ../computing/computational_performance.rst:261
msgid "Tips and Tricks"
msgstr "Consejos y trucos"

#: ../computing/computational_performance.rst:264
msgid "Linear algebra libraries"
msgstr "Bibliotecas de álgebra lineal"

#: ../computing/computational_performance.rst:266
msgid "As scikit-learn relies heavily on Numpy/Scipy and linear algebra in general it makes sense to take explicit care of the versions of these libraries. Basically, you ought to make sure that Numpy is built using an optimized `BLAS <https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>`_ / `LAPACK <https://en.wikipedia.org/wiki/LAPACK>`_ library."
msgstr "Como scikit-learn depende en gran medida de Numpy/Scipy y del álgebra lineal en general, tiene sentido cuidar explícitamente las versiones de estas bibliotecas. Básicamente, debe asegurarse de que Numpy se construye utilizando una biblioteca optimizada `BLAS <https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms>`_ / `LAPACK <https://en.wikipedia.org/wiki/LAPACK>`_."

#: ../computing/computational_performance.rst:272
msgid "Not all models benefit from optimized BLAS and Lapack implementations. For instance models based on (randomized) decision trees typically do not rely on BLAS calls in their inner loops, nor do kernel SVMs (``SVC``, ``SVR``, ``NuSVC``, ``NuSVR``).  On the other hand a linear model implemented with a BLAS DGEMM call (via ``numpy.dot``) will typically benefit hugely from a tuned BLAS implementation and lead to orders of magnitude speedup over a non-optimized BLAS."
msgstr "No todos los modelos se benefician de las implementaciones optimizadas de BLAS y Lapack. Por ejemplo, los modelos basados en árboles de decisión (aleatorios) no suelen depender de las llamadas a BLAS en sus bucles internos, ni tampoco los SVM de núcleo (``SVC``, ``SVR``, ``NuSVC``, ``NuSVR``).  Por otro lado, un modelo lineal implementado con una llamada BLAS DGEMM (a través de ``numpy.dot``) se beneficiará enormemente de una implementación BLAS ajustada y conducirá a órdenes de magnitud de velocidad sobre un BLAS no optimizado."

#: ../computing/computational_performance.rst:280
msgid "You can display the BLAS / LAPACK implementation used by your NumPy / SciPy / scikit-learn install with the following commands::"
msgstr "Puede mostrar la implementación de BLAS / LAPACK utilizada por su instalación de NumPy / SciPy / scikit-learn con los siguientes comandos::"

#: ../computing/computational_performance.rst:291
msgid "Optimized BLAS / LAPACK implementations include:"
msgstr "Las implementaciones optimizadas de BLAS / LAPACK incluyen:"

#: ../computing/computational_performance.rst:288
msgid "Atlas (need hardware specific tuning by rebuilding on the target machine)"
msgstr "Atlas (necesita un ajuste específico del hardware mediante la reconstrucción en la máquina de destino)"

#: ../computing/computational_performance.rst:289
msgid "OpenBLAS"
msgstr "OpenBLAS"

#: ../computing/computational_performance.rst:290
msgid "MKL"
msgstr "MKL"

#: ../computing/computational_performance.rst:291
msgid "Apple Accelerate and vecLib frameworks (OSX only)"
msgstr "Marcos Apple Accelerate y vecLib (sólo para OSX)"

#: ../computing/computational_performance.rst:293
msgid "More information can be found on the `Scipy install page <https://docs.scipy.org/doc/numpy/user/install.html>`_ and in this `blog post <http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/>`_ from Daniel Nouri which has some nice step by step install instructions for Debian / Ubuntu."
msgstr "Se puede encontrar más información en la página de instalación de `Scipy <https://docs.scipy.org/doc/numpy/user/install.html>`_ y en este `blog post <http://danielnouri.org/notes/2012/12/19/libblas-and-liblapack-issues-and-speed,-with-scipy-and-ubuntu/>`_ de Daniel Nouri que tiene algunas buenas instrucciones de instalación paso a paso para Debian / Ubuntu."

#: ../computing/computational_performance.rst:302
msgid "Limiting Working Memory"
msgstr "Limitación de la memoria de trabajo"

#: ../computing/computational_performance.rst:304
msgid "Some calculations when implemented using standard numpy vectorized operations involve using a large amount of temporary memory.  This may potentially exhaust system memory.  Where computations can be performed in fixed-memory chunks, we attempt to do so, and allow the user to hint at the maximum size of this working memory (defaulting to 1GB) using :func:`set_config` or :func:`config_context`.  The following suggests to limit temporary working memory to 128 MiB::"
msgstr "Algunos cálculos cuando se implementan utilizando operaciones vectoriales estándar de numpy implican el uso de una gran cantidad de memoria temporal.  Esto puede potencialmente agotar la memoria del sistema.  Cuando los cálculos pueden realizarse en trozos de memoria fija, intentamos hacerlo, y permitimos al usuario indicar el tamaño máximo de esta memoria de trabajo (por defecto 1GB) usando :func:`set_config` o :func:`config_context`.  Lo siguiente sugiere limitar la memoria de trabajo temporal a 128 MiB::"

#: ../computing/computational_performance.rst:316
msgid "An example of a chunked operation adhering to this setting is :func:`~metrics.pairwise_distances_chunked`, which facilitates computing row-wise reductions of a pairwise distance matrix."
msgstr "Un ejemplo de operación fragmentada que se adhiere a esta configuración es :func:`~metrics.pairwise_distances_chunked`, que facilita el cálculo de las reducciones por filas de una matriz de distancia por pares."

#: ../computing/computational_performance.rst:321
msgid "Model Compression"
msgstr "Modelo de compresión"

#: ../computing/computational_performance.rst:323
msgid "Model compression in scikit-learn only concerns linear models for the moment. In this context it means that we want to control the model sparsity (i.e. the number of non-zero coordinates in the model vectors). It is generally a good idea to combine model sparsity with sparse input data representation."
msgstr "La compresión de modelos en scikit-learn sólo se refiere a los modelos lineales por el momento. En este contexto, significa que queremos controlar la dispersión del modelo (es decir, el número de coordenadas no nulas en los vectores del modelo). Por lo general, es una buena idea combinar la dispersión del modelo con la representación de datos de entrada dispersos."

#: ../computing/computational_performance.rst:328
msgid "Here is sample code that illustrates the use of the ``sparsify()`` method::"
msgstr "Este es un ejemplo de código que ilustra el uso del método ``sparsify()``::"

#: ../computing/computational_performance.rst:334
msgid "In this example we prefer the ``elasticnet`` penalty as it is often a good compromise between model compactness and prediction power. One can also further tune the ``l1_ratio`` parameter (in combination with the regularization strength ``alpha``) to control this tradeoff."
msgstr "En este ejemplo preferimos la penalización ``elasticnet`` ya que suele ser un buen compromiso entre la compacidad del modelo y la potencia de predicción. También se puede ajustar el parámetro ``l1_ratio`` (en combinación con la fuerza de regularización ``alpha``) para controlar este compromiso."

#: ../computing/computational_performance.rst:339
#, python-format
msgid "A typical `benchmark <https://github.com/scikit-learn/scikit-learn/blob/main/benchmarks/bench_sparsify.py>`_ on synthetic data yields a >30% decrease in latency when both the model and input are sparse (with 0.000024 and 0.027400 non-zero coefficients ratio respectively). Your mileage may vary depending on the sparsity and size of your data and model. Furthermore, sparsifying can be very useful to reduce the memory usage of predictive models deployed on production servers."
msgstr "Un típico `benchmark <https://github.com/scikit-learn/scikit-learn/blob/main/benchmarks/bench_sparsify.py>`_ sobre datos sintéticos arroja una disminución de la latencia superior al 30% cuando tanto el modelo como la entrada son dispersos (con una proporción de coeficientes no nulos de 0.000024 y 0.027400 respectivamente). El kilometraje puede variar en función de la dispersión y el tamaño de los datos y el modelo. Además, la sparsificación puede ser muy útil para reducir el uso de memoria de los modelos predictivos desplegados en los servidores de producción."

#: ../computing/computational_performance.rst:348
msgid "Model Reshaping"
msgstr "Reestructuración de modelos"

#: ../computing/computational_performance.rst:350
msgid "Model reshaping consists in selecting only a portion of the available features to fit a model. In other words, if a model discards features during the learning phase we can then strip those from the input. This has several benefits. Firstly it reduces memory (and therefore time) overhead of the model itself. It also allows to discard explicit feature selection components in a pipeline once we know which features to keep from a previous run. Finally, it can help reduce processing time and I/O usage upstream in the data access and feature extraction layers by not collecting and building features that are discarded by the model. For instance if the raw data come from a database, it can make it possible to write simpler and faster queries or reduce I/O usage by making the queries return lighter records. At the moment, reshaping needs to be performed manually in scikit-learn. In the case of sparse input (particularly in ``CSR`` format), it is generally sufficient to not generate the relevant features, leaving their columns empty."
msgstr "La reestructuración del modelo consiste en seleccionar sólo una parte de las características disponibles para ajustar un modelo. En otras palabras, si un modelo descarta características durante la fase de aprendizaje, podemos eliminarlas de la entrada. Esto tiene varias ventajas. En primer lugar, reduce la sobrecarga de memoria (y, por tanto, de tiempo) del propio modelo. También permite descartar componentes de selección de características explícitas en una cadena de producción una vez que sabemos qué características conservar de una ejecución anterior. Por último, puede ayudar a reducir el tiempo de procesamiento y el uso de E/S en las capas de acceso a los datos y de extracción de características al no recoger y construir características que son descartadas por el modelo. Por ejemplo, si los datos brutos proceden de una base de datos, puede permitir escribir consultas más sencillas y rápidas o reducir el uso de E/S haciendo que las consultas devuelvan registros más ligeros. Por el momento, la remodelación debe realizarse manualmente en scikit-learn. En el caso de entradas dispersas (particularmente en formato ``CSR``), generalmente es suficiente con no generar las características relevantes, dejando sus columnas vacías."

#: ../computing/computational_performance.rst:367
msgid "Links"
msgstr "Enlaces"

#: ../computing/computational_performance.rst:369
msgid ":ref:`scikit-learn developer performance documentation <performance-howto>`"
msgstr ":ref:`documentación sobre el rendimiento de los desarrolladores de scikit-learn <performance-howto>`"

#: ../computing/computational_performance.rst:370
msgid "`Scipy sparse matrix formats documentation <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_"
msgstr "`Scipy sparse matrix formats documentation <https://docs.scipy.org/doc/scipy/reference/sparse.html>`_"

