msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-06 00:24\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/computing/scaling_strategies.po\n"
"X-Crowdin-File-ID: 4752\n"
"Language: es_ES\n"

#: ../computing/scaling_strategies.rst:8
msgid "Strategies to scale computationally: bigger data"
msgstr "Estrategias para escalar computacionalmente: datos más grandes"

#: ../computing/scaling_strategies.rst:10
msgid "For some applications the amount of examples, features (or both) and/or the speed at which they need to be processed are challenging for traditional approaches. In these cases scikit-learn has a number of options you can consider to make your system scale."
msgstr "Para algunas aplicaciones, la cantidad de ejemplos, características (o ambas) y/o la velocidad a la que necesitan ser procesadas son un reto para los enfoques tradicionales. En estos casos, scikit-learn tiene una serie de opciones que puede considerar para hacer que tu sistema sea escalable."

#: ../computing/scaling_strategies.rst:16
msgid "Scaling with instances using out-of-core learning"
msgstr "Escalar con instancias utilizando el aprendizaje fuera del núcleo"

#: ../computing/scaling_strategies.rst:18
msgid "Out-of-core (or \"external memory\") learning is a technique used to learn from data that cannot fit in a computer's main memory (RAM)."
msgstr "El aprendizaje fuera del núcleo (o \"memoria externa\") es una técnica utilizada para aprender de los datos que no caben en la memoria principal (RAM) de una computadora."

#: ../computing/scaling_strategies.rst:21
msgid "Here is a sketch of a system designed to achieve this goal:"
msgstr "He aquí un esbozo de un sistema diseñado para lograr este objetivo:"

#: ../computing/scaling_strategies.rst:23
msgid "a way to stream instances"
msgstr "una forma de transmitir instancias"

#: ../computing/scaling_strategies.rst:24
msgid "a way to extract features from instances"
msgstr "una forma de extraer características de las instancias"

#: ../computing/scaling_strategies.rst:25
msgid "an incremental algorithm"
msgstr "un algoritmo incremental"

#: ../computing/scaling_strategies.rst:28
msgid "Streaming instances"
msgstr "Instancias de streaming"

#: ../computing/scaling_strategies.rst:30
msgid "Basically, 1. may be a reader that yields instances from files on a hard drive, a database, from a network stream etc. However, details on how to achieve this are beyond the scope of this documentation."
msgstr "Básicamente, 1. puede ser un lector que obtenga instancias de archivos en un disco duro, una base de datos, de un flujo de red, etc. Sin embargo, los detalles sobre cómo lograr esto están más allá del alcance de esta documentación."

#: ../computing/scaling_strategies.rst:35
msgid "Extracting features"
msgstr "Extracción de características"

#: ../computing/scaling_strategies.rst:37
msgid "\\2. could be any relevant way to extract features among the different :ref:`feature extraction <feature_extraction>` methods supported by scikit-learn. However, when working with data that needs vectorization and where the set of features or values is not known in advance one should take explicit care. A good example is text classification where unknown terms are likely to be found during training. It is possible to use a stateful vectorizer if making multiple passes over the data is reasonable from an application point of view. Otherwise, one can turn up the difficulty by using a stateless feature extractor. Currently the preferred way to do this is to use the so-called :ref:`hashing trick<feature_hashing>` as implemented by :class:`sklearn.feature_extraction.FeatureHasher` for datasets with categorical variables represented as list of Python dicts or :class:`sklearn.feature_extraction.text.HashingVectorizer` for text documents."
msgstr "\\2. podría ser cualquier forma relevante de extraer características entre los diferentes métodos de :ref:`extracción de características <feature_hashing>` soportados por scikit-learn. Sin embargo, cuando se trabaja con datos que necesitan vectorización y donde el conjunto de características o valores no se conoce de antemano se debe tener un cuidado explícito. Un buen ejemplo es la clasificación de textos donde es probable que se encuentren términos desconocidos durante el entrenamiento. Es posible utilizar un vectorizador con estado si hacer varias pasadas sobre los datos es razonable desde el punto de vista de la aplicación. De lo contrario, se puede aumentar la dificultad utilizando un extractor de características sin estado. Actualmente la forma preferida de hacer esto es utilizar el llamado :ref:`truco hashing<feature_hashing>` como el implementado por :class:`sklearn.feature_extraction.FeatureHasher` para conjuntos de datos con variables categóricas representadas como lista de diccionarios (dicts) de Python o :class:`sklearn.feature_extraction.text.HashingVectorizer` para documentos de texto."

#: ../computing/scaling_strategies.rst:52
msgid "Incremental learning"
msgstr "Aprendizaje incremental"

#: ../computing/scaling_strategies.rst:54
msgid "Finally, for 3. we have a number of options inside scikit-learn. Although not all algorithms can learn incrementally (i.e. without seeing all the instances at once), all estimators implementing the ``partial_fit`` API are candidates. Actually, the ability to learn incrementally from a mini-batch of instances (sometimes called \"online learning\") is key to out-of-core learning as it guarantees that at any given time there will be only a small amount of instances in the main memory. Choosing a good size for the mini-batch that balances relevancy and memory footprint could involve some tuning [1]_."
msgstr "Finalmente, para la 3. tenemos una serie de opciones dentro de scikit-learn. Aunque no todos los algoritmos pueden aprender de forma incremental (es decir, sin ver todas las instancias a la vez), todos los estimadores que implementan la API ``partial_fit`` son candidatos. En realidad, la capacidad de aprender de forma incremental a partir de un minilote de instancias (a veces llamado \"aprendizaje en línea\") es clave para el aprendizaje fuera del núcleo, ya que garantiza que en cualquier momento sólo habrá una pequeña cantidad de instancias en la memoria principal. La elección de un buen tamaño para el mini lote que equilibre la relevancia y la huella de memoria podría implicar algunos ajustes [1]_."

#: ../computing/scaling_strategies.rst:63
msgid "Here is a list of incremental estimators for different tasks:"
msgstr "A continuación se presenta una lista de estimadores incrementales para diferentes tareas:"

#: ../computing/scaling_strategies.rst:70
msgid "Classification"
msgstr "Clasificación"

#: ../computing/scaling_strategies.rst:66
msgid ":class:`sklearn.naive_bayes.MultinomialNB`"
msgstr ":class:`sklearn.naive_bayes.MultinomialNB`"

#: ../computing/scaling_strategies.rst:67
msgid ":class:`sklearn.naive_bayes.BernoulliNB`"
msgstr ":class:`sklearn.naive_bayes.BernoulliNB`"

#: ../computing/scaling_strategies.rst:68
msgid ":class:`sklearn.linear_model.Perceptron`"
msgstr ":class:`sklearn.linear_model.Perceptron`"

#: ../computing/scaling_strategies.rst:69
msgid ":class:`sklearn.linear_model.SGDClassifier`"
msgstr ":class:`sklearn.linear_model.SGDClassifier`"

#: ../computing/scaling_strategies.rst:70
msgid ":class:`sklearn.linear_model.PassiveAggressiveClassifier`"
msgstr ":class:`sklearn.linear_model.PassiveAggressiveClassifier`"

#: ../computing/scaling_strategies.rst:71
msgid ":class:`sklearn.neural_network.MLPClassifier`"
msgstr ":class:`sklearn.neural_network.MLPClassifier`"

#: ../computing/scaling_strategies.rst:74
msgid "Regression"
msgstr "Regresión"

#: ../computing/scaling_strategies.rst:73
msgid ":class:`sklearn.linear_model.SGDRegressor`"
msgstr ":class:`sklearn.linear_model.SGDRegressor`"

#: ../computing/scaling_strategies.rst:74
msgid ":class:`sklearn.linear_model.PassiveAggressiveRegressor`"
msgstr ":class:`sklearn.linear_model.PassiveAggressiveRegressor`"

#: ../computing/scaling_strategies.rst:75
msgid ":class:`sklearn.neural_network.MLPRegressor`"
msgstr ":class:`sklearn.neural_network.MLPRegressor`"

#: ../computing/scaling_strategies.rst:77
msgid "Clustering"
msgstr "Análisis de conglomerados"

#: ../computing/scaling_strategies.rst:77
msgid ":class:`sklearn.cluster.MiniBatchKMeans`"
msgstr ":class:`sklearn.cluster.MiniBatchKMeans`"

#: ../computing/scaling_strategies.rst:78
msgid ":class:`sklearn.cluster.Birch`"
msgstr ":class:`sklearn.cluster.Birch`"

#: ../computing/scaling_strategies.rst:81
msgid "Decomposition / feature Extraction"
msgstr "Descomposición/extracción de características"

#: ../computing/scaling_strategies.rst:80
msgid ":class:`sklearn.decomposition.MiniBatchDictionaryLearning`"
msgstr ":class:`sklearn.decomposition.MiniBatchDictionaryLearning`"

#: ../computing/scaling_strategies.rst:81
msgid ":class:`sklearn.decomposition.IncrementalPCA`"
msgstr ":class:`sklearn.decomposition.IncrementalPCA`"

#: ../computing/scaling_strategies.rst:82
msgid ":class:`sklearn.decomposition.LatentDirichletAllocation`"
msgstr ":class:`sklearn.decomposition.LatentDirichletAllocation`"

#: ../computing/scaling_strategies.rst:86
msgid "Preprocessing"
msgstr "Preprocesamiento"

#: ../computing/scaling_strategies.rst:84
msgid ":class:`sklearn.preprocessing.StandardScaler`"
msgstr ":class:`sklearn.preprocessing.StandardScaler`"

#: ../computing/scaling_strategies.rst:85
msgid ":class:`sklearn.preprocessing.MinMaxScaler`"
msgstr ":class:`sklearn.preprocessing.MinMaxScaler`"

#: ../computing/scaling_strategies.rst:86
msgid ":class:`sklearn.preprocessing.MaxAbsScaler`"
msgstr ":class:`sklearn.preprocessing.MaxAbsScaler`"

#: ../computing/scaling_strategies.rst:88
msgid "For classification, a somewhat important thing to note is that although a stateless feature extraction routine may be able to cope with new/unseen attributes, the incremental learner itself may be unable to cope with new/unseen targets classes. In this case you have to pass all the possible classes to the first ``partial_fit`` call using the ``classes=`` parameter."
msgstr "Para la clasificación, una cosa importante a tener en cuenta es que aunque una rutina de extracción de características sin estado puede ser capaz de hacer frente a los atributos nuevos/no vistos, el propio aprendiz incremental puede ser incapaz de hacer frente a las clases objetivo nuevas/no vistas. En este caso hay que pasar todas las clases posibles a la primera llamada de ``partial_fit`` utilizando el parámetro ``classes=``."

#: ../computing/scaling_strategies.rst:94
msgid "Another aspect to consider when choosing a proper algorithm is that not all of them put the same importance on each example over time. Namely, the ``Perceptron`` is still sensitive to badly labeled examples even after many examples whereas the ``SGD*`` and ``PassiveAggressive*`` families are more robust to this kind of artifacts. Conversely, the latter also tend to give less importance to remarkably different, yet properly labeled examples when they come late in the stream as their learning rate decreases over time."
msgstr "Otro aspecto a tener en cuenta a la hora de elegir un algoritmo adecuado es que no todos dan la misma importancia a cada ejemplo a lo largo del tiempo. En concreto, el ``Perceptron`` sigue siendo sensible a los ejemplos mal etiquetados incluso después de muchos ejemplos, mientras que las familias ``SGD*`` y ``PassiveAggressive*`` son más robustas a este tipo de artefactos. A la inversa, estos últimos también tienden a dar menos importancia a los ejemplos notablemente diferentes, aunque correctamente etiquetados, cuando llegan tarde en el flujo, ya que su tasa de aprendizaje disminuye con el tiempo."

#: ../computing/scaling_strategies.rst:103
msgid "Examples"
msgstr "Ejemplos"

#: ../computing/scaling_strategies.rst:105
msgid "Finally, we have a full-fledged example of :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. It is aimed at providing a starting point for people wanting to build out-of-core learning systems and demonstrates most of the notions discussed above."
msgstr "Por último, tenemos un ejemplo completo de :ref:`sphx_glr_auto_examples_applications_plot_out_of_core_classification.py`. Su objetivo es proporcionar un punto de partida para las personas que quieren construir sistemas de aprendizaje fuera del núcleo y demuestra la mayoría de las nociones discutidas anteriormente."

#: ../computing/scaling_strategies.rst:110
msgid "Furthermore, it also shows the evolution of the performance of different algorithms with the number of processed examples."
msgstr "Además, también muestra la evolución del rendimiento de los distintos algoritmos con el número de ejemplos procesados."

#: ../computing/scaling_strategies.rst:118
msgid "accuracy_over_time"
msgstr "accuracy_over_time"

#: ../computing/scaling_strategies.rst:119
msgid "Now looking at the computation time of the different parts, we see that the vectorization is much more expensive than learning itself. From the different algorithms, ``MultinomialNB`` is the most expensive, but its overhead can be mitigated by increasing the size of the mini-batches (exercise: change ``minibatch_size`` to 100 and 10000 in the program and compare)."
msgstr "Si observamos el tiempo de cálculo de las diferentes partes, vemos que la vectorización es mucho más cara que el aprendizaje en sí. De los diferentes algoritmos, ``MultinomialNB`` es el más caro, pero su sobrecarga se puede mitigar aumentando el tamaño de los mini-lotes (ejercicio: cambiar ``minibatch_size`` a 100 y 10000 en el programa y comparar)."

#: ../computing/scaling_strategies.rst:131
msgid "computation_time"
msgstr "computation_time"

#: ../computing/scaling_strategies.rst:133
msgid "Notes"
msgstr "Notas"

#: ../computing/scaling_strategies.rst:135
msgid "Depending on the algorithm the mini-batch size can influence results or not. SGD*, PassiveAggressive*, and discrete NaiveBayes are truly online and are not affected by batch size. Conversely, MiniBatchKMeans convergence rate is affected by the batch size. Also, its memory footprint can vary dramatically with batch size."
msgstr "Dependiendo del algoritmo, el tamaño del mini lote puede influir en los resultados o no. SGD*, PassiveAggressive* y NaiveBayes discreto son realmente online y no se ven afectados por el tamaño del lote. Por el contrario, la tasa de convergencia de MiniBatchKMeans se ve afectada por el tamaño del lote. Además, su consumo de memoria puede variar drásticamente con el tamaño del lote."

