msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-06 23:25\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/ensemble/plot_bias_variance.po\n"
"X-Crowdin-File-ID: 4554\n"
"Language: es_ES\n"

#: ../auto_examples/ensemble/plot_bias_variance.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_ensemble_plot_bias_variance.py>` to download the full example code or to run this example in your browser via Binder"
msgstr "Haz clic en :ref:`aquí <sphx_glr_download_auto_examples_ensemble_plot_bias_variance.py>` para descargar el código de ejemplo completo o para ejecutar este ejemplo en tu navegador a través de Binder"

#: ../auto_examples/ensemble/plot_bias_variance.rst:23
msgid "Single estimator versus bagging: bias-variance decomposition"
msgstr "Estimador único frente a empaquetado (bagging): descomposición sesgo-varianza"

#: ../auto_examples/ensemble/plot_bias_variance.rst:25
msgid "This example illustrates and compares the bias-variance decomposition of the expected mean squared error of a single estimator against a bagging ensemble."
msgstr "Este ejemplo ilustra y compara la descomposición sesgo-varianza del error cuadrático medio esperado de un único estimador frente a un conjunto de ensemble de empaquetados (bagging)."

#: ../auto_examples/ensemble/plot_bias_variance.rst:28
msgid "In regression, the expected mean squared error of an estimator can be decomposed in terms of bias, variance and noise. On average over datasets of the regression problem, the bias term measures the average amount by which the predictions of the estimator differ from the predictions of the best possible estimator for the problem (i.e., the Bayes model). The variance term measures the variability of the predictions of the estimator when fit over different instances LS of the problem. Finally, the noise measures the irreducible part of the error which is due the variability in the data."
msgstr "En regresión, el error medio cuadrático esperado de un estimador puede descomponerse en términos de sesgo, varianza y ruido. En promedio sobre conjuntos de datos del problema de regresión, el término de sesgo mide la cantidad media en que las predicciones del estimador difieren de las predicciones del mejor estimador posible para el problema (es decir, el modelo de Bayes). El término de varianza mide la variabilidad de las predicciones del estimador cuando se ajusta a diferentes instancias LS del problema. Por último, el ruido mide la parte irreducible del error que se debe a la variabilidad de los datos."

#: ../auto_examples/ensemble/plot_bias_variance.rst:37
msgid "The upper left figure illustrates the predictions (in dark red) of a single decision tree trained over a random dataset LS (the blue dots) of a toy 1d regression problem. It also illustrates the predictions (in light red) of other single decision trees trained over other (and different) randomly drawn instances LS of the problem. Intuitively, the variance term here corresponds to the width of the beam of predictions (in light red) of the individual estimators. The larger the variance, the more sensitive are the predictions for `x` to small changes in the training set. The bias term corresponds to the difference between the average prediction of the estimator (in cyan) and the best possible model (in dark blue). On this problem, we can thus observe that the bias is quite low (both the cyan and the blue curves are close to each other) while the variance is large (the red beam is rather wide)."
msgstr "La figura superior izquierda ilustra las predicciones (en rojo oscuro) de un único árbol de decisión entrenado sobre un conjunto de datos aleatorio LS (los puntos azules) de un problema de regresión 1d de laboratorio. También ilustra las predicciones (en rojo claro) de otros árboles de decisión individuales entrenados sobre otras (y diferentes) instancias aleatorias LS del problema. Intuitivamente, el término de varianza aquí corresponde a la anchura del haz de predicciones (en rojo claro) de los estimadores individuales. Cuanto mayor sea la varianza, más sensibles serán las predicciones de `x` a pequeños cambios en el conjunto de entrenamiento. El término de sesgo corresponde a la diferencia entre la predicción media del estimador (en cian) y el mejor modelo posible (en azul oscuro). En este problema, podemos observar que el sesgo es bastante bajo (tanto la curva cian como la azul están próximas entre sí) mientras que la varianza es grande (el haz rojo es bastante ancho)."

#: ../auto_examples/ensemble/plot_bias_variance.rst:50
msgid "The lower left figure plots the pointwise decomposition of the expected mean squared error of a single decision tree. It confirms that the bias term (in blue) is low while the variance is large (in green). It also illustrates the noise part of the error which, as expected, appears to be constant and around `0.01`."
msgstr "La figura inferior izquierda muestra la descomposición puntual del error cuadrático medio esperado de un solo árbol de decisión. Confirma que el término de sesgo (en azul) es bajo mientras que la varianza es grande (en verde). También ilustra la parte de ruido del error que, como se esperaba, parece ser constante y estar en torno a `0,01`."

#: ../auto_examples/ensemble/plot_bias_variance.rst:56
msgid "The right figures correspond to the same plots but using instead a bagging ensemble of decision trees. In both figures, we can observe that the bias term is larger than in the previous case. In the upper right figure, the difference between the average prediction (in cyan) and the best possible model is larger (e.g., notice the offset around `x=2`). In the lower right figure, the bias curve is also slightly higher than in the lower left figure. In terms of variance however, the beam of predictions is narrower, which suggests that the variance is lower. Indeed, as the lower right figure confirms, the variance term (in green) is lower than for single decision trees. Overall, the bias- variance decomposition is therefore no longer the same. The tradeoff is better for bagging: averaging several decision trees fit on bootstrap copies of the dataset slightly increases the bias term but allows for a larger reduction of the variance, which results in a lower overall mean squared error (compare the red curves int the lower figures). The script output also confirms this intuition. The total error of the bagging ensemble is lower than the total error of a single decision tree, and this difference indeed mainly stems from a reduced variance."
msgstr "Las figuras de la derecha corresponden a los mismos gráficos pero utilizando en su lugar un conjunto de árboles de decisión de ensemble de empaquetados (bagging ensemble). En ambas figuras, podemos observar que el término de sesgo es mayor que en el caso anterior. En la figura superior derecha, la diferencia entre la predicción media (en cian) y el mejor modelo posible es mayor (por ejemplo, nótese el desplazamiento alrededor de `x=2`). En la figura inferior derecha, la curva de sesgo también es ligeramente superior a la de la figura inferior izquierda. Sin embargo, en términos de varianza, el haz de predicciones es más estrecho, lo que sugiere que la varianza es menor. De hecho, como confirma la figura inferior derecha, el término de varianza (en verde) es menor que el de los árboles de decisión simples. En general, la descomposición sesgo-varianza ya no es la misma. La compensación es mejor para el empaquetado (bagging): promediar varios árboles de decisión ajustados en copias bootstrap del conjunto de datos aumenta ligeramente el término de sesgo pero permite una mayor reducción de la varianza, lo que resulta en un error cuadrático medio más bajo (compare las curvas rojas en las figuras inferiores). El resultado del script también confirma esta intuición. El error total del conjunto de bolsas es menor que el error total de un solo árbol de decisión, y esta diferencia se debe principalmente a la reducción de la varianza."

#: ../auto_examples/ensemble/plot_bias_variance.rst:74
msgid "For further details on bias-variance decomposition, see section 7.3 of [1]_."
msgstr "Para más detalles sobre la descomposición sesgo-varianza, véase la sección 7.3 de [1]_."

#: ../auto_examples/ensemble/plot_bias_variance.rst:77
msgid "References"
msgstr "Referencias"

#: ../auto_examples/ensemble/plot_bias_variance.rst:79
msgid "T. Hastie, R. Tibshirani and J. Friedman, \"Elements of Statistical Learning\", Springer, 2009."
msgstr "T. Hastie, R. Tibshirani y J. Friedman, \"Elements of Statistical Learning\", Springer, 2009."

#: ../auto_examples/ensemble/plot_bias_variance.rst:93
msgid "Out:"
msgstr "Salida:"

#: ../auto_examples/ensemble/plot_bias_variance.rst:242
msgid "**Total running time of the script:** ( 0 minutes  1.446 seconds)"
msgstr "**Tiempo total de ejecución del script:** (0 minutos 1.446 segundos)"

#: ../auto_examples/ensemble/plot_bias_variance.rst:264
msgid ":download:`Download Python source code: plot_bias_variance.py <plot_bias_variance.py>`"
msgstr ":download:`Descargar el código fuente de Python: plot_bias_variance.py <plot_bias_variance.py>`"

#: ../auto_examples/ensemble/plot_bias_variance.rst:270
msgid ":download:`Download Jupyter notebook: plot_bias_variance.ipynb <plot_bias_variance.ipynb>`"
msgstr ":download:`Descargar el cuaderno Jupyter: plot_bias_variance.ipynb <plot_bias_variance.ipynb>`"

#: ../auto_examples/ensemble/plot_bias_variance.rst:277
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`Galería generada por Sphinx-Gallery <https://sphinx-gallery.github.io>`_"

