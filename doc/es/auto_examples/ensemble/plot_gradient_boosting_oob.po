msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 00:06\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/ensemble/plot_gradient_boosting_oob.po\n"
"X-Crowdin-File-ID: 2370\n"
"Language: es_ES\n"

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_ensemble_plot_gradient_boosting_oob.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:23
msgid "Gradient Boosting Out-of-Bag estimates"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:25
msgid "Out-of-bag (OOB) estimates can be a useful heuristic to estimate the \"optimal\" number of boosting iterations. OOB estimates are almost identical to cross-validation estimates but they can be computed on-the-fly without the need for repeated model fitting. OOB estimates are only available for Stochastic Gradient Boosting (i.e. ``subsample < 1.0``), the estimates are derived from the improvement in loss based on the examples not included in the bootstrap sample (the so-called out-of-bag examples). The OOB estimator is a pessimistic estimator of the true test loss, but remains a fairly good approximation for a small number of trees."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:37
msgid "The figure shows the cumulative sum of the negative OOB improvements as a function of the boosting iteration. As you can see, it tracks the test loss for the first hundred iterations but then diverges in a pessimistic way. The figure also shows the performance of 3-fold cross validation which usually gives a better estimate of the test loss but is computationally more demanding."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:56
msgid "Out:"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:188
msgid "**Total running time of the script:** ( 0 minutes  4.253 seconds)"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:210
msgid ":download:`Download Python source code: plot_gradient_boosting_oob.py <plot_gradient_boosting_oob.py>`"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:216
msgid ":download:`Download Jupyter notebook: plot_gradient_boosting_oob.ipynb <plot_gradient_boosting_oob.ipynb>`"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_oob.rst:223
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

