msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 05:58\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/ensemble/plot_random_forest_embedding.po\n"
"X-Crowdin-File-ID: 4562\n"
"Language: es_ES\n"

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_ensemble_plot_random_forest_embedding.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:23
msgid "Hashing feature transformation using Totally Random Trees"
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:25
msgid "RandomTreesEmbedding provides a way to map data to a very high-dimensional, sparse representation, which might be beneficial for classification. The mapping is completely unsupervised and very efficient."
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:30
msgid "This example visualizes the partitions given by several trees and shows how the transformation can also be used for non-linear dimensionality reduction or non-linear classification."
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:34
msgid "Points that are neighboring often share the same leaf of a tree and therefore share large parts of their hashed representation. This allows to separate two concentric circles simply based on the principal components of the transformed data with truncated SVD."
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:39
msgid "In high-dimensional spaces, linear classifiers often achieve excellent accuracy. For sparse binary data, BernoulliNB is particularly well-suited. The bottom row compares the decision boundary obtained by BernoulliNB in the transformed space with an ExtraTreesClassifier forests learned on the original data."
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:57
msgid "Out:"
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:158
msgid "**Total running time of the script:** ( 0 minutes  0.794 seconds)"
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:180
msgid ":download:`Download Python source code: plot_random_forest_embedding.py <plot_random_forest_embedding.py>`"
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:186
msgid ":download:`Download Jupyter notebook: plot_random_forest_embedding.ipynb <plot_random_forest_embedding.ipynb>`"
msgstr ""

#: ../auto_examples/ensemble/plot_random_forest_embedding.rst:193
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

