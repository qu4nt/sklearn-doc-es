msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 04:41\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/calibration/plot_calibration.po\n"
"X-Crowdin-File-ID: 4262\n"
"Language: es_ES\n"

#: ../auto_examples/calibration/plot_calibration.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_calibration_plot_calibration.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:23
msgid "Probability calibration of classifiers"
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:25
msgid "When performing classification you often want to predict not only the class label, but also the associated probability. This probability gives you some kind of confidence on the prediction. However, not all classifiers provide well-calibrated probabilities, some being over-confident while others being under-confident. Thus, a separate calibration of predicted probabilities is often desirable as a postprocessing. This example illustrates two different methods for this calibration and evaluates the quality of the returned probabilities using Brier's score (see https://en.wikipedia.org/wiki/Brier_score)."
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:35
msgid "Compared are the estimated probability using a Gaussian naive Bayes classifier without calibration, with a sigmoid calibration, and with a non-parametric isotonic calibration. One can observe that only the non-parametric model is able to provide a probability calibration that returns probabilities close to the expected 0.5 for most of the samples belonging to the middle cluster with heterogeneous labels. This results in a significantly improved Brier score."
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:65
msgid "Out:"
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:186
msgid "**Total running time of the script:** ( 0 minutes  0.431 seconds)"
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:208
msgid ":download:`Download Python source code: plot_calibration.py <plot_calibration.py>`"
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:214
msgid ":download:`Download Jupyter notebook: plot_calibration.ipynb <plot_calibration.ipynb>`"
msgstr ""

#: ../auto_examples/calibration/plot_calibration.rst:221
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

