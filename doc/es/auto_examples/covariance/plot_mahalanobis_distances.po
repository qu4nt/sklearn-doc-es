msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 00:09\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/covariance/plot_mahalanobis_distances.po\n"
"X-Crowdin-File-ID: 2294\n"
"Language: es_ES\n"

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_covariance_plot_mahalanobis_distances.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:23
msgid "Robust covariance estimation and Mahalanobis distances relevance"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:25
msgid "This example shows covariance estimation with Mahalanobis distances on Gaussian distributed data."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:28
msgid "For Gaussian distributed data, the distance of an observation :math:`x_i` to the mode of the distribution can be computed using its Mahalanobis distance:"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:32
msgid "d_{(\\mu,\\Sigma)}(x_i)^2 = (x_i - \\mu)^T\\Sigma^{-1}(x_i - \\mu)"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:36
msgid "where :math:`\\mu` and :math:`\\Sigma` are the location and the covariance of the underlying Gaussian distributions."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:39
msgid "In practice, :math:`\\mu` and :math:`\\Sigma` are replaced by some estimates. The standard covariance maximum likelihood estimate (MLE) is very sensitive to the presence of outliers in the data set and therefore, the downstream Mahalanobis distances also are. It would be better to use a robust estimator of covariance to guarantee that the estimation is resistant to \"erroneous\" observations in the dataset and that the calculated Mahalanobis distances accurately reflect the true organization of the observations."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:48
msgid "The Minimum Covariance Determinant estimator (MCD) is a robust, high-breakdown point (i.e. it can be used to estimate the covariance matrix of highly contaminated datasets, up to :math:`\\frac{n_\\text{samples}-n_\\text{features}-1}{2}` outliers) estimator of covariance. The idea behind the MCD is to find :math:`\\frac{n_\\text{samples}+n_\\text{features}+1}{2}` observations whose empirical covariance has the smallest determinant, yielding a \"pure\" subset of observations from which to compute standards estimates of location and covariance. The MCD was introduced by P.J.Rousseuw in [1]_."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:59
msgid "This example illustrates how the Mahalanobis distances are affected by outlying data. Observations drawn from a contaminating distribution are not distinguishable from the observations coming from the real, Gaussian distribution when using standard covariance MLE based Mahalanobis distances. Using MCD-based Mahalanobis distances, the two populations become distinguishable. Associated applications include outlier detection, observation ranking and clustering."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:70
msgid "See also :ref:`sphx_glr_auto_examples_covariance_plot_robust_vs_empirical_covariance.py`"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:74
msgid "P. J. Rousseeuw. `Least median of squares regression <http://web.ipac.caltech.edu/staff/fmasci/home/astro_refs/LeastMedianOfSquares.pdf>`_. J. Am Stat Ass, 79:871, 1984."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:77
msgid "Wilson, E. B., & Hilferty, M. M. (1931). `The distribution of chi-square. <https://water.usgs.gov/osw/bulletin17b/Wilson_Hilferty_1931.pdf>`_ Proceedings of the National Academy of Sciences of the United States of America, 17, 684-688."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:85
msgid "Generate data"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:87
msgid "First, we generate a dataset of 125 samples and 2 features. Both features are Gaussian distributed with mean of 0 but feature 1 has a standard deviation equal to 2 and feature 2 has a standard deviation equal to 1. Next, 25 samples are replaced with Gaussian outlier samples where feature 1 has a standard devation equal to 1 and feature 2 has a standard deviation equal to 7."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:127
msgid "Comparison of results"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:129
msgid "Below, we fit MCD and MLE based covariance estimators to our data and print the estimated covariance matrices. Note that the estimated variance of feature 2 is much higher with the MLE based estimator (7.5) than that of the MCD robust estimator (1.2). This shows that the MCD based robust estimator is much more resistant to the outlier samples, which were designed to have a much larger variance in feature 2."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:158
msgid "Out:"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:175
msgid "To better visualize the difference, we plot contours of the Mahalanobis distances calculated by both methods. Notice that the robust MCD based Mahalanobis distances fit the inlier black points much better, whereas the MLE based distances are more influenced by the outlier red points."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:231
msgid "Finally, we highlight the ability of MCD based Mahalanobis distances to distinguish outliers. We take the cubic root of the Mahalanobis distances, yielding approximately normal distributions (as suggested by Wilson and Hilferty [2]_), then plot the values of inlier and outlier samples with boxplots. The distribution of outlier samples is more separated from the distribution of inlier samples for robust MCD based Mahalanobis distances."
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:288
msgid "**Total running time of the script:** ( 0 minutes  0.365 seconds)"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:310
msgid ":download:`Download Python source code: plot_mahalanobis_distances.py <plot_mahalanobis_distances.py>`"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:316
msgid ":download:`Download Jupyter notebook: plot_mahalanobis_distances.ipynb <plot_mahalanobis_distances.ipynb>`"
msgstr ""

#: ../auto_examples/covariance/plot_mahalanobis_distances.rst:323
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

