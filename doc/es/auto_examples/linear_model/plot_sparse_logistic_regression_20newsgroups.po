msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-13 01:14\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.po\n"
"X-Crowdin-File-ID: 4646\n"
"Language: es_ES\n"

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_linear_model_plot_sparse_logistic_regression_20newsgroups.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:23
msgid "Multiclass sparse logistic regression on 20newgroups"
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:25
msgid "Comparison of multinomial logistic L1 vs one-versus-rest L1 logistic regression to classify documents from the newgroups20 dataset. Multinomial logistic regression yields more accurate results and is faster to train on the larger scale dataset."
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:30
msgid "Here we use the l1 sparsity that trims the weights of not informative features to zero. This is good if the goal is to extract the strongly discriminative vocabulary of each class. If the goal is to get the best predictive accuracy, it is better to use the non sparsity-inducing l2 penalty instead."
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:36
msgid "A more traditional (and possibly better) way to predict on a sparse subset of input features would be to use univariate feature selection followed by a traditional (l2-penalised) logistic regression model."
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:51
msgid "Out:"
msgstr "Out:"

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:189
msgid "**Total running time of the script:** ( 0 minutes  19.780 seconds)"
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:211
msgid ":download:`Download Python source code: plot_sparse_logistic_regression_20newsgroups.py <plot_sparse_logistic_regression_20newsgroups.py>`"
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:217
msgid ":download:`Download Jupyter notebook: plot_sparse_logistic_regression_20newsgroups.ipynb <plot_sparse_logistic_regression_20newsgroups.ipynb>`"
msgstr ""

#: ../auto_examples/linear_model/plot_sparse_logistic_regression_20newsgroups.rst:224
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

