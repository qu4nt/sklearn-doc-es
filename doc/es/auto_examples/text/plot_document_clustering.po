msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-13 21:15\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/text/plot_document_clustering.po\n"
"X-Crowdin-File-ID: 4192\n"
"Language: es_ES\n"

#: ../auto_examples/text/plot_document_clustering.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_text_plot_document_clustering.py>` to download the full example code or to run this example in your browser via Binder"
msgstr "Haz clic :ref:`aquí <sphx_glr_download_auto_examples_text_plot_document_clustering.py>` para descargar el código completo del ejemplo o para ejecutar este ejemplo en tu navegador a través de Binder"

#: ../auto_examples/text/plot_document_clustering.rst:23
msgid "Clustering text documents using k-means"
msgstr "Análisis de conglomerados en documentos de texto utilizando k-medias(k-means)"

#: ../auto_examples/text/plot_document_clustering.rst:25
msgid "This is an example showing how the scikit-learn can be used to cluster documents by topics using a bag-of-words approach. This example uses a scipy.sparse matrix to store the features instead of standard numpy arrays."
msgstr "Este es un ejemplo que muestra cómo se puede utilizar scikit-learn para agrupar documentos por temas utilizando un enfoque de bolsa de palabras. Este ejemplo utiliza una matriz scipy.sparse para almacenar las características en lugar de arreglos numpy estándar."

#: ../auto_examples/text/plot_document_clustering.rst:29
msgid "Two feature extraction methods can be used in this example:"
msgstr "En este ejemplo se pueden utilizar dos métodos de extracción de características:"

#: ../auto_examples/text/plot_document_clustering.rst:31
msgid "TfidfVectorizer uses a in-memory vocabulary (a python dict) to map the most frequent words to features indices and hence compute a word occurrence frequency (sparse) matrix. The word frequencies are then reweighted using the Inverse Document Frequency (IDF) vector collected feature-wise over the corpus."
msgstr "TfidfVectorizer utiliza un vocabulario en memoria (un diccionario de python) para mapear las palabras más frecuentes a los índices de características y, por tanto, calcular una matriz de frecuencia de aparición de palabras (dispersa). A continuación, las frecuencias de las palabras son reponderadas utilizando el vector de la frecuencia inversa del documento (Inverse Document Frequency, IDF) recogido por las características del corpus."

#: ../auto_examples/text/plot_document_clustering.rst:37
msgid "HashingVectorizer hashes word occurrences to a fixed dimensional space, possibly with collisions. The word count vectors are then normalized to each have l2-norm equal to one (projected to the euclidean unit-ball) which seems to be important for k-means to work in high dimensional space."
msgstr "El HashingVectorizer hace un hash de las ocurrencias de las palabras en un espacio dimensional fijo, posiblemente con colisiones. Los vectores de conteo de palabras se normalizan para que cada uno tenga una norma l2 igual a uno (proyectada al círculo unitario euclidiano), lo que parece ser importante para que k-medias funcione en un espacio de alta dimensión."

#: ../auto_examples/text/plot_document_clustering.rst:42
msgid "HashingVectorizer does not provide IDF weighting as this is a stateless model (the fit method does nothing). When IDF weighting is needed it can be added by pipelining its output to a TfidfTransformer instance."
msgstr "HashingVectorizer no proporciona ponderación IDF ya que es un modelo sin estado (el método de ajuste no hace nada). Cuando se necesita la ponderación IDF se puede añadir mediante un pipeline de su salida a una instancia TfidfTransformer."

#: ../auto_examples/text/plot_document_clustering.rst:46
msgid "Two algorithms are demoed: ordinary k-means and its more scalable cousin minibatch k-means."
msgstr "Se demuestran dos algoritmos: k-medias ordinario y su primo más escalable k-medias de minilote."

#: ../auto_examples/text/plot_document_clustering.rst:49
msgid "Additionally, latent semantic analysis can also be used to reduce dimensionality and discover latent patterns in the data."
msgstr "Además, el análisis semántico latente también puede utilizarse para reducir la dimensionalidad y descubrir patrones latentes en los datos."

#: ../auto_examples/text/plot_document_clustering.rst:52
msgid "It can be noted that k-means (and minibatch k-means) are very sensitive to feature scaling and that in this case the IDF weighting helps improve the quality of the clustering by quite a lot as measured against the \"ground truth\" provided by the class label assignments of the 20 newsgroups dataset."
msgstr "Puede observarse que k-medias (y k-medias de minilote) son muy sensibles al escalamiento de características y que, en este caso, la ponderación IDF ayuda a mejorar la calidad del análisis de conglomerados en gran medida en comparación con la \"verdad fundamental\" proporcionada por las asignaciones de etiquetas de clase del conjunto de datos de 20 grupos de noticias."

#: ../auto_examples/text/plot_document_clustering.rst:57
msgid "This improvement is not visible in the Silhouette Coefficient which is small for both as this measure seem to suffer from the phenomenon called \"Concentration of Measure\" or \"Curse of Dimensionality\" for high dimensional datasets such as text data. Other measures such as V-measure and Adjusted Rand Index are information theoretic based evaluation scores: as they are only based on cluster assignments rather than distances, hence not affected by the curse of dimensionality."
msgstr "Esta mejora no es visible en el Coeficiente de Silueta, que es pequeño para ambos, ya que esta medida parece sufrir el fenómeno llamado \"Concentración de la Medida\" o \"Maldición de la Dimensionalidad\" para conjuntos de datos de alta dimensión como los datos de texto. Otras medidas, como la medida V y el índice de Rand ajustado, son puntuaciones de evaluación basadas en la teoría de la información, ya que sólo se basan en la asignación de conglomerados y no en las distancias, por lo que no se ven afectadas por la maldición de la dimensionalidad."

#: ../auto_examples/text/plot_document_clustering.rst:65
msgid "Note: as k-means is optimizing a non-convex objective function, it will likely end up in a local optimum. Several runs with independent random init might be necessary to get a good convergence."
msgstr "Nota: como k-medias está optimizando una función objetivo no convexa, es probable que termine en un óptimo local. Pueden ser necesarias varias ejecuciones con inicio aleatorio independiente para conseguir una buena convergencia."

#: ../auto_examples/text/plot_document_clustering.rst:76
msgid "Out:"
msgstr "Out:"

#: ../auto_examples/text/plot_document_clustering.rst:306
msgid "**Total running time of the script:** ( 0 minutes  1.189 seconds)"
msgstr "**Tiempo total de ejecución del script:** (0 minutos 1.189 segundos)"

#: ../auto_examples/text/plot_document_clustering.rst:328
msgid ":download:`Download Python source code: plot_document_clustering.py <plot_document_clustering.py>`"
msgstr ":download:`Descargar código fuente de Python: plot_document_clustering.py <plot_document_clustering.py>`"

#: ../auto_examples/text/plot_document_clustering.rst:334
msgid ":download:`Download Jupyter notebook: plot_document_clustering.ipynb <plot_document_clustering.ipynb>`"
msgstr ":download:`Descargar el cuaderno Jupyter: plot_document_clustering.ipynb <plot_document_clustering.ipynb>`"

#: ../auto_examples/text/plot_document_clustering.rst:341
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr "`Galería generada por Sphinx-Gallery <https://sphinx-gallery.github.io>`_"

