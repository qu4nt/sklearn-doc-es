msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-07-13 01:14\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/applications/plot_model_complexity_influence.po\n"
"X-Crowdin-File-ID: 4622\n"
"Language: es_ES\n"

#: ../auto_examples/applications/plot_model_complexity_influence.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_applications_plot_model_complexity_influence.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:23
msgid "Model Complexity Influence"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:25
msgid "Demonstrate how model complexity influences both prediction accuracy and computational performance."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:34
msgid "We will be using two datasets:"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:29
msgid ":ref:`diabetes_dataset` for regression. This dataset consists of 10 measurements taken from diabetes patients. The task is to predict disease progression;"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:32
msgid ":ref:`20newsgroups_dataset` for classification. This dataset consists of newsgroup posts. The task is to predict on which topic (out of 20 topics) the post is written about."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:45
msgid "We will model the complexity influence on three different estimators:"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:37
msgid ":class:`~sklearn.linear_model.SGDClassifier` (for classification data) which implements stochastic gradient descent learning;"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:40
msgid ":class:`~sklearn.svm.NuSVR` (for regression data) which implements Nu support vector regression;"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:43
msgid ":class:`~sklearn.ensemble.GradientBoostingRegressor` (for regression data) which builds an additive model in a forward stage-wise fashion."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:47
msgid "We make the model complexity vary through the choice of relevant model parameters in each of our selected models. Next, we will measure the influence on both computational performance (latency) and predictive power (MSE or Hamming Loss)."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:90
msgid "Load the data"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:92
msgid "First we load both datasets."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:94
msgid "We are using :func:`~sklearn.datasets.fetch_20newsgroups_vectorized` to download 20 newsgroups dataset. It returns ready-to-use features."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:98
msgid "``X`` of the 20 newsgroups dataset is a sparse matrix while ``X`` of diabetes dataset is a numpy array."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:139
msgid "Benchmark influence"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:140
msgid "Next, we can calculate the influence of the parameters on the given estimator. In each round, we will set the estimator with the new value of ``changing_param`` and we will be collecting the prediction times, prediction performance and complexities to see how those changes affect the estimator. We will calculate the complexity using ``complexity_computer`` passed as a parameter."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:194
msgid "Choose parameters"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:196
msgid "We choose the parameters for each of our estimators by making a dictionary with all the necessary values. ``changing_param`` is the name of the parameter which will vary in each estimator. Complexity will be defined by the ``complexity_label`` and calculated using `complexity_computer`. Also note that depending on the estimator type we are passing different data."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:264
msgid "Run the code and plot the results"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:266
msgid "We defined all the functions required to run our benchmark. Now, we will loop over the different configurations that we defined previously. Subsequently, we can analyze the plots obtained from the benchmark: Relaxing the `L1` penalty in the SGD classifier reduces the prediction error but leads to an increase in the training time. We can draw a similar analysis regarding the training time which increases with the number of support vectors with a Nu-SVR. However, we observed that there is an optimal number of support vectors which reduces the prediction error. Indeed, too few support vectors lead to an under-fitted model while too many support vectors lead to an over-fitted model. The exact same conclusion can be drawn for the gradient-boosting model. The only the difference with the Nu-SVR is that having too many trees in the ensemble is not as detrimental."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:358
msgid "Out:"
msgstr "Out:"

#: ../auto_examples/applications/plot_model_complexity_influence.rst:415
msgid "Conclusion"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:417
msgid "As a conclusion, we can deduce the following insights:"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:419
msgid "a model which is more complex (or expressive) will require a larger training time;"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:421
msgid "a more complex model does not guarantee to reduce the prediction error."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:423
msgid "These aspects are related to model generalization and avoiding model under-fitting or over-fitting."
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:429
msgid "**Total running time of the script:** ( 0 minutes  41.995 seconds)"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:451
msgid ":download:`Download Python source code: plot_model_complexity_influence.py <plot_model_complexity_influence.py>`"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:457
msgid ":download:`Download Jupyter notebook: plot_model_complexity_influence.ipynb <plot_model_complexity_influence.ipynb>`"
msgstr ""

#: ../auto_examples/applications/plot_model_complexity_influence.rst:464
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

