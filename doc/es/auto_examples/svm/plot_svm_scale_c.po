msgid ""
msgstr ""
"Project-Id-Version: scikit-learn\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: 2021-04-15 00:10\n"
"Last-Translator: \n"
"Language-Team: Spanish\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=UTF-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"
"Plural-Forms: nplurals=2; plural=(n != 1);\n"
"X-Crowdin-Project: scikit-learn\n"
"X-Crowdin-Project-ID: 450526\n"
"X-Crowdin-Language: es-ES\n"
"X-Crowdin-File: /main/doc/en/auto_examples/svm/plot_svm_scale_c.po\n"
"X-Crowdin-File-ID: 2738\n"
"Language: es_ES\n"

#: ../auto_examples/svm/plot_svm_scale_c.rst:13
msgid "Click :ref:`here <sphx_glr_download_auto_examples_svm_plot_svm_scale_c.py>` to download the full example code or to run this example in your browser via Binder"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:23
msgid "Scaling the regularization parameter for SVCs"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:25
msgid "The following example illustrates the effect of scaling the regularization parameter when using :ref:`svm` for :ref:`classification <svm_classification>`. For SVC classification, we are interested in a risk minimization for the equation:"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:32
msgid "C \\sum_{i=1, n} \\mathcal{L} (f(x_i), y_i) + \\Omega (w)"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:36
msgid "where"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:38
msgid ":math:`C` is used to set the amount of regularization"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:39
msgid ":math:`\\mathcal{L}` is a `loss` function of our samples and our model parameters."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:41
msgid ":math:`\\Omega` is a `penalty` function of our model parameters"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:43
msgid "If we consider the loss function to be the individual error per sample, then the data-fit term, or the sum of the error for each sample, will increase as we add more samples. The penalization term, however, will not increase."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:48
msgid "When using, for example, :ref:`cross validation <cross_validation>`, to set the amount of regularization with `C`, there will be a different amount of samples between the main problem and the smaller problems within the folds of the cross validation."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:53
msgid "Since our loss function is dependent on the amount of samples, the latter will influence the selected value of `C`. The question that arises is `How do we optimally adjust C to account for the different amount of training samples?`"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:58
msgid "The figures below are used to illustrate the effect of scaling our `C` to compensate for the change in the number of samples, in the case of using an `l1` penalty, as well as the `l2` penalty."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:63
msgid "l1-penalty case"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:64
msgid "In the `l1` case, theory says that prediction consistency (i.e. that under given hypothesis, the estimator learned predicts as well as a model knowing the true distribution) is not possible because of the bias of the `l1`. It does say, however, that model consistency, in terms of finding the right set of non-zero parameters as well as their signs, can be achieved by scaling `C1`."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:73
msgid "l2-penalty case"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:74
msgid "The theory says that in order to achieve prediction consistency, the penalty parameter should be kept constant as the number of samples grow."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:79
msgid "Simulations"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:81
msgid "The two figures below plot the values of `C` on the `x-axis` and the corresponding cross-validation scores on the `y-axis`, for several different fractions of a generated data-set."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:85
msgid "In the `l1` penalty case, the cross-validation-error correlates best with the test-error, when scaling our `C` with the number of samples, `n`, which can be seen in the first figure."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:89
msgid "For the `l2` penalty case, the best result comes from the case where `C` is not scaled."
msgstr ""

msgid "Note:"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:94
msgid "Two separate datasets are used for the two different plots. The reason behind this is the `l1` case works better on sparse data, while `l2` is better suited to the non-sparse case."
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:199
msgid "**Total running time of the script:** ( 0 minutes  25.012 seconds)"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:221
msgid ":download:`Download Python source code: plot_svm_scale_c.py <plot_svm_scale_c.py>`"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:227
msgid ":download:`Download Jupyter notebook: plot_svm_scale_c.ipynb <plot_svm_scale_c.ipynb>`"
msgstr ""

#: ../auto_examples/svm/plot_svm_scale_c.rst:234
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

