# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../tutorial/statistical_inference/model_selection.rst:5
msgid "Model selection: choosing estimators and their parameters"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:8
msgid "Score, and cross-validated scores"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:10
msgid ""
"As we have seen, every estimator exposes a ``score`` method that can "
"judge the quality of the fit (or the prediction) on new data. **Bigger is"
" better**."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:22
msgid ""
"To get a better measure of prediction accuracy (which we can use as a "
"proxy for goodness of fit of the model), we can successively split the "
"data in *folds* that we use for training and testing::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:44
msgid "This is called a :class:`KFold` cross-validation."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:49
msgid "Cross-validation generators"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:51
msgid ""
"Scikit-learn has a collection of classes which can be used to generate "
"lists of train/test indices for popular cross-validation strategies."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:54
msgid ""
"They expose a ``split`` method which accepts the input dataset to be "
"split and yields the train/test set indices for each iteration of the "
"chosen cross-validation strategy."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:58
msgid "This example shows an example usage of the ``split`` method."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:71
msgid "The cross-validation can then be performed easily::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:77
msgid ""
"The cross-validation score can be directly calculated using the "
":func:`cross_val_score` helper. Given an estimator, the cross-validation "
"object and the input dataset, the :func:`cross_val_score` splits the data"
" repeatedly into a training and a testing set, trains the estimator using"
" the training set and computes the scores based on the testing set for "
"each iteration of cross-validation."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:83
msgid ""
"By default the estimator's ``score`` method is used to compute the "
"individual scores."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:85
msgid ""
"Refer the :ref:`metrics module <metrics>` to learn more on the available "
"scoring methods."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:91
msgid ""
"`n_jobs=-1` means that the computation will be dispatched on all the CPUs"
" of the computer."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:94
msgid ""
"Alternatively, the ``scoring`` argument can be provided to specify an "
"alternative scoring method."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:101
msgid "**Cross-validation generators**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:108
msgid ":class:`KFold` **(n_splits, shuffle, random_state)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:110
msgid ":class:`StratifiedKFold` **(n_splits, shuffle, random_state)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:112
msgid ":class:`GroupKFold` **(n_splits)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:117
msgid "Splits it into K folds, trains on K-1 and then tests on the left-out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:119
msgid "Same as K-Fold but preserves the class distribution within each fold."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:121
#: ../tutorial/statistical_inference/model_selection.rst:140
msgid "Ensures that the same group is not in both testing and training sets."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:128
msgid ":class:`ShuffleSplit` **(n_splits, test_size, train_size, random_state)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:130
msgid ":class:`StratifiedShuffleSplit`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:132
msgid ":class:`GroupShuffleSplit`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:136
msgid "Generates train/test indices based on random permutation."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:138
msgid ""
"Same as shuffle split but preserves the class distribution within each "
"iteration."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:147
msgid ":class:`LeaveOneGroupOut` **()**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:149
msgid ":class:`LeavePGroupsOut`  **(n_groups)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:151
msgid ":class:`LeaveOneOut` **()**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:157
msgid "Takes a group array to group observations."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:159
msgid "Leave P groups out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:161
msgid "Leave one observation out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:169
msgid ":class:`LeavePOut` **(p)**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:171
msgid ":class:`PredefinedSplit`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:175
msgid "Leave P observations out."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:177
msgid "Generates train/test indices based on predefined splits."
msgstr ""

msgid "**Exercise**"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:184
msgid ""
"On the digits dataset, plot the cross-validation score of a :class:`SVC` "
"estimator with an linear kernel as a function of parameter ``C`` (use a "
"logarithmic grid of points, from 1 to 10)."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:196
msgid "**Solution:** :ref:`sphx_glr_auto_examples_exercises_plot_cv_digits.py`"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:199
msgid "Grid-search and cross-validated estimators"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:202
msgid "Grid-search"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:206
msgid ""
"scikit-learn provides an object that, given data, computes the score "
"during the fit of an estimator on a parameter grid and chooses the "
"parameters to maximize the cross-validation score. This object takes an "
"estimator during the construction and exposes an estimator API::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:227
msgid ""
"By default, the :class:`GridSearchCV` uses a 5-fold cross-validation. "
"However, if it detects that a classifier is passed, rather than a "
"regressor, it uses a stratified 5-fold."
msgstr ""

msgid "Nested cross-validation"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:238
msgid ""
"Two cross-validation loops are performed in parallel: one by the "
":class:`GridSearchCV` estimator to set ``gamma`` and the other one by "
"``cross_val_score`` to measure the prediction performance of the "
"estimator. The resulting scores are unbiased estimates of the prediction "
"score on new data."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:246
msgid ""
"You cannot nest objects with parallel computing (``n_jobs`` different "
"than 1)."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:252
msgid "Cross-validated estimators"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:254
msgid ""
"Cross-validation to set a parameter can be done more efficiently on an "
"algorithm-by-algorithm basis. This is why, for certain estimators, "
"scikit-learn exposes :ref:`cross_validation` estimators that set their "
"parameter automatically by cross-validation::"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:268
msgid ""
"These estimators are called similarly to their counterparts, with 'CV' "
"appended to their name."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:273
msgid "On the diabetes dataset, find the optimal regularization parameter alpha."
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:276
msgid "**Bonus**: How much can you trust the selection of alpha?"
msgstr ""

#: ../tutorial/statistical_inference/model_selection.rst:281
msgid "**Solution:** :ref:`sphx_glr_auto_examples_exercises_plot_cv_diabetes.py`"
msgstr ""

