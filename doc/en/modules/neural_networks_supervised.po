# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/neural_networks_supervised.rst:5
msgid "Neural network models (supervised)"
msgstr ""

#: ../modules/neural_networks_supervised.rst:12
msgid ""
"This implementation is not intended for large-scale applications. In "
"particular, scikit-learn offers no GPU support. For much faster, GPU-"
"based implementations, as well as frameworks offering much more "
"flexibility to build deep learning architectures, see  "
":ref:`related_projects`."
msgstr ""

#: ../modules/neural_networks_supervised.rst:20
msgid "Multi-layer Perceptron"
msgstr ""

#: ../modules/neural_networks_supervised.rst:22
msgid ""
"**Multi-layer Perceptron (MLP)** is a supervised learning algorithm that "
"learns a function :math:`f(\\cdot): R^m \\rightarrow R^o` by training on "
"a dataset, where :math:`m` is the number of dimensions for input and "
":math:`o` is the number of dimensions for output. Given a set of features"
" :math:`X = {x_1, x_2, ..., x_m}` and a target :math:`y`, it can learn a "
"non-linear function approximator for either classification or regression."
" It is different from logistic regression, in that between the input and "
"the output layer, there can be one or more non-linear layers, called "
"hidden layers. Figure 1 shows a one hidden layer MLP with scalar output."
msgstr ""

#: ../modules/neural_networks_supervised.rst:36
msgid "**Figure 1 : One hidden layer MLP.**"
msgstr ""

#: ../modules/neural_networks_supervised.rst:38
msgid ""
"The leftmost layer, known as the input layer, consists of a set of "
"neurons :math:`\\{x_i | x_1, x_2, ..., x_m\\}` representing the input "
"features. Each neuron in the hidden layer transforms the values from the "
"previous layer with a weighted linear summation :math:`w_1x_1 + w_2x_2 + "
"... + w_mx_m`, followed by a non-linear activation function "
":math:`g(\\cdot):R \\rightarrow R` - like the hyperbolic tan function. "
"The output layer receives the values from the last hidden layer and "
"transforms them into output values."
msgstr ""

#: ../modules/neural_networks_supervised.rst:46
msgid ""
"The module contains the public attributes ``coefs_`` and ``intercepts_``."
" ``coefs_`` is a list of weight matrices, where weight matrix at index "
":math:`i` represents the weights between layer :math:`i` and layer "
":math:`i+1`. ``intercepts_`` is a list of bias vectors, where the vector "
"at index :math:`i` represents the bias values added to layer :math:`i+1`."
msgstr ""

#: ../modules/neural_networks_supervised.rst:52
msgid "The advantages of Multi-layer Perceptron are:"
msgstr ""

#: ../modules/neural_networks_supervised.rst:54
msgid "Capability to learn non-linear models."
msgstr ""

#: ../modules/neural_networks_supervised.rst:56
msgid ""
"Capability to learn models in real-time (on-line learning) using "
"``partial_fit``."
msgstr ""

#: ../modules/neural_networks_supervised.rst:60
msgid "The disadvantages of Multi-layer Perceptron (MLP) include:"
msgstr ""

#: ../modules/neural_networks_supervised.rst:62
msgid ""
"MLP with hidden layers have a non-convex loss function where there exists"
" more than one local minimum. Therefore different random weight "
"initializations can lead to different validation accuracy."
msgstr ""

#: ../modules/neural_networks_supervised.rst:66
msgid ""
"MLP requires tuning a number of hyperparameters such as the number of "
"hidden neurons, layers, and iterations."
msgstr ""

#: ../modules/neural_networks_supervised.rst:69
msgid "MLP is sensitive to feature scaling."
msgstr ""

#: ../modules/neural_networks_supervised.rst:71
msgid ""
"Please see :ref:`Tips on Practical Use <mlp_tips>` section that addresses"
" some of these disadvantages."
msgstr ""

#: ../modules/neural_networks_supervised.rst:76
msgid "Classification"
msgstr ""

#: ../modules/neural_networks_supervised.rst:78
msgid ""
"Class :class:`MLPClassifier` implements a multi-layer perceptron (MLP) "
"algorithm that trains using `Backpropagation "
"<http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm>`_."
msgstr ""

#: ../modules/neural_networks_supervised.rst:81
msgid ""
"MLP trains on two arrays: array X of size (n_samples, n_features), which "
"holds the training samples represented as floating point feature vectors;"
" and array y of size (n_samples,), which holds the target values (class "
"labels) for the training samples::"
msgstr ""

#: ../modules/neural_networks_supervised.rst:96
msgid "After fitting (training), the model can predict labels for new samples::"
msgstr ""

#: ../modules/neural_networks_supervised.rst:101
msgid ""
"MLP can fit a non-linear model to the training data. ``clf.coefs_`` "
"contains the weight matrices that constitute the model parameters::"
msgstr ""

#: ../modules/neural_networks_supervised.rst:107
msgid ""
"Currently, :class:`MLPClassifier` supports only the Cross-Entropy loss "
"function, which allows probability estimates by running the "
"``predict_proba`` method."
msgstr ""

#: ../modules/neural_networks_supervised.rst:111
msgid ""
"MLP trains using Backpropagation. More precisely, it trains using some "
"form of gradient descent and the gradients are calculated using "
"Backpropagation. For classification, it minimizes the Cross-Entropy loss "
"function, giving a vector of probability estimates :math:`P(y|x)` per "
"sample :math:`x`::"
msgstr ""

#: ../modules/neural_networks_supervised.rst:120
msgid ""
":class:`MLPClassifier` supports multi-class classification by applying "
"`Softmax <https://en.wikipedia.org/wiki/Softmax_activation_function>`_ as"
" the output function."
msgstr ""

#: ../modules/neural_networks_supervised.rst:124
msgid ""
"Further, the model supports :ref:`multi-label classification "
"<multiclass>` in which a sample can belong to more than one class. For "
"each class, the raw output passes through the logistic function. Values "
"larger or equal to `0.5` are rounded to `1`, otherwise to `0`. For a "
"predicted output of a sample, the indices where the value is `1` "
"represents the assigned classes of that sample::"
msgstr ""

#: ../modules/neural_networks_supervised.rst:143
msgid ""
"See the examples below and the docstring of :meth:`MLPClassifier.fit` for"
" further information."
msgstr ""

#: ../modules/neural_networks_supervised.rst:148
msgid ":ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_training_curves.py`"
msgstr ""

#: ../modules/neural_networks_supervised.rst:149
msgid ":ref:`sphx_glr_auto_examples_neural_networks_plot_mnist_filters.py`"
msgstr ""

#: ../modules/neural_networks_supervised.rst:152
msgid "Regression"
msgstr ""

#: ../modules/neural_networks_supervised.rst:154
msgid ""
"Class :class:`MLPRegressor` implements a multi-layer perceptron (MLP) "
"that trains using backpropagation with no activation function in the "
"output layer, which can also be seen as using the identity function as "
"activation function. Therefore, it uses the square error as the loss "
"function, and the output is a set of continuous values."
msgstr ""

#: ../modules/neural_networks_supervised.rst:160
msgid ""
":class:`MLPRegressor` also supports multi-output regression, in which a "
"sample can have more than one target."
msgstr ""

#: ../modules/neural_networks_supervised.rst:164
msgid "Regularization"
msgstr ""

#: ../modules/neural_networks_supervised.rst:166
msgid ""
"Both :class:`MLPRegressor` and :class:`MLPClassifier` use parameter "
"``alpha`` for regularization (L2 regularization) term which helps in "
"avoiding overfitting by penalizing weights with large magnitudes. "
"Following plot displays varying decision function with value of alpha."
msgstr ""

#: ../modules/neural_networks_supervised.rst:176
msgid "See the examples below for further information."
msgstr ""

#: ../modules/neural_networks_supervised.rst:180
msgid ":ref:`sphx_glr_auto_examples_neural_networks_plot_mlp_alpha.py`"
msgstr ""

#: ../modules/neural_networks_supervised.rst:183
msgid "Algorithms"
msgstr ""

#: ../modules/neural_networks_supervised.rst:185
msgid ""
"MLP trains using `Stochastic Gradient Descent "
"<https://en.wikipedia.org/wiki/Stochastic_gradient_descent>`_, `Adam "
"<https://arxiv.org/abs/1412.6980>`_, or `L-BFGS "
"<https://en.wikipedia.org/wiki/Limited-memory_BFGS>`__. Stochastic "
"Gradient Descent (SGD) updates parameters using the gradient of the loss "
"function with respect to a parameter that needs adaptation, i.e."
msgstr ""

#: ../modules/neural_networks_supervised.rst:192
msgid ""
"w \\leftarrow w - \\eta (\\alpha \\frac{\\partial R(w)}{\\partial w}\n"
"+ \\frac{\\partial Loss}{\\partial w})"
msgstr ""

#: ../modules/neural_networks_supervised.rst:197
msgid ""
"where :math:`\\eta` is the learning rate which controls the step-size in "
"the parameter space search.  :math:`Loss` is the loss function used for "
"the network."
msgstr ""

#: ../modules/neural_networks_supervised.rst:201
msgid ""
"More details can be found in the documentation of `SGD <http://scikit-"
"learn.org/stable/modules/sgd.html>`_"
msgstr ""

#: ../modules/neural_networks_supervised.rst:204
msgid ""
"Adam is similar to SGD in a sense that it is a stochastic optimizer, but "
"it can automatically adjust the amount to update parameters based on "
"adaptive estimates of lower-order moments."
msgstr ""

#: ../modules/neural_networks_supervised.rst:208
msgid "With SGD or Adam, training supports online and mini-batch learning."
msgstr ""

#: ../modules/neural_networks_supervised.rst:210
msgid ""
"L-BFGS is a solver that approximates the Hessian matrix which represents "
"the second-order partial derivative of a function. Further it "
"approximates the inverse of the Hessian matrix to perform parameter "
"updates. The implementation uses the Scipy version of `L-BFGS "
"<https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.fmin_l_bfgs_b.html>`_."
msgstr ""

#: ../modules/neural_networks_supervised.rst:216
msgid ""
"If the selected solver is 'L-BFGS', training does not support online nor "
"mini-batch learning."
msgstr ""

#: ../modules/neural_networks_supervised.rst:221
msgid "Complexity"
msgstr ""

#: ../modules/neural_networks_supervised.rst:223
msgid ""
"Suppose there are :math:`n` training samples, :math:`m` features, "
":math:`k` hidden layers, each containing :math:`h` neurons - for "
"simplicity, and :math:`o` output neurons.  The time complexity of "
"backpropagation is :math:`O(n\\cdot m \\cdot h^k \\cdot o \\cdot i)`, "
"where :math:`i` is the number of iterations. Since backpropagation has a "
"high time complexity, it is advisable to start with smaller number of "
"hidden neurons and few hidden layers for training."
msgstr ""

#: ../modules/neural_networks_supervised.rst:233
msgid "Mathematical formulation"
msgstr ""

#: ../modules/neural_networks_supervised.rst:235
msgid ""
"Given a set of training examples :math:`(x_1, y_1), (x_2, y_2), \\ldots, "
"(x_n, y_n)` where :math:`x_i \\in \\mathbf{R}^n` and :math:`y_i \\in "
"\\{0, 1\\}`, a one hidden layer one hidden neuron MLP learns the function"
" :math:`f(x) = W_2 g(W_1^T x + b_1) + b_2` where :math:`W_1 \\in "
"\\mathbf{R}^m` and :math:`W_2, b_1, b_2 \\in \\mathbf{R}` are model "
"parameters. :math:`W_1, W_2` represent the weights of the input layer and"
" hidden layer, respectively; and :math:`b_1, b_2` represent the bias "
"added to the hidden layer and the output layer, respectively. "
":math:`g(\\cdot) : R \\rightarrow R` is the activation function, set by "
"default as the hyperbolic tan. It is given as,"
msgstr ""

#: ../modules/neural_networks_supervised.rst:245
msgid ""
"g(z)= \\frac{e^z-e^{-z}}{e^z+e^{-z}}\n"
"\n"
msgstr ""

#: ../modules/neural_networks_supervised.rst:248
msgid ""
"For binary classification, :math:`f(x)` passes through the logistic "
"function :math:`g(z)=1/(1+e^{-z})` to obtain output values between zero "
"and one. A threshold, set to 0.5, would assign samples of outputs larger "
"or equal 0.5 to the positive class, and the rest to the negative class."
msgstr ""

#: ../modules/neural_networks_supervised.rst:253
msgid ""
"If there are more than two classes, :math:`f(x)` itself would be a vector"
" of size (n_classes,). Instead of passing through logistic function, it "
"passes through the softmax function, which is written as,"
msgstr ""

#: ../modules/neural_networks_supervised.rst:257
msgid ""
"\\text{softmax}(z)_i = \\frac{\\exp(z_i)}{\\sum_{l=1}^k\\exp(z_l)}\n"
"\n"
msgstr ""

#: ../modules/neural_networks_supervised.rst:260
msgid ""
"where :math:`z_i` represents the :math:`i` th element of the input to "
"softmax, which corresponds to class :math:`i`, and :math:`K` is the "
"number of classes. The result is a vector containing the probabilities "
"that sample :math:`x` belong to each class. The output is the class with "
"the highest probability."
msgstr ""

#: ../modules/neural_networks_supervised.rst:265
msgid ""
"In regression, the output remains as :math:`f(x)`; therefore, output "
"activation function is just the identity function."
msgstr ""

#: ../modules/neural_networks_supervised.rst:268
msgid ""
"MLP uses different loss functions depending on the problem type. The loss"
" function for classification is Cross-Entropy, which in binary case is "
"given as,"
msgstr ""

#: ../modules/neural_networks_supervised.rst:271
msgid ""
"Loss(\\hat{y},y,W) = -y \\ln {\\hat{y}} - (1-y) \\ln{(1-\\hat{y})} + "
"\\alpha ||W||_2^2"
msgstr ""

#: ../modules/neural_networks_supervised.rst:275
msgid ""
"where :math:`\\alpha ||W||_2^2` is an L2-regularization term (aka "
"penalty) that penalizes complex models; and :math:`\\alpha > 0` is a non-"
"negative hyperparameter that controls the magnitude of the penalty."
msgstr ""

#: ../modules/neural_networks_supervised.rst:279
msgid "For regression, MLP uses the Square Error loss function; written as,"
msgstr ""

#: ../modules/neural_networks_supervised.rst:281
msgid ""
"Loss(\\hat{y},y,W) = \\frac{1}{2}||\\hat{y} - y ||_2^2 + "
"\\frac{\\alpha}{2} ||W||_2^2"
msgstr ""

#: ../modules/neural_networks_supervised.rst:286
msgid ""
"Starting from initial random weights, multi-layer perceptron (MLP) "
"minimizes the loss function by repeatedly updating these weights. After "
"computing the loss, a backward pass propagates it from the output layer "
"to the previous layers, providing each weight parameter with an update "
"value meant to decrease the loss."
msgstr ""

#: ../modules/neural_networks_supervised.rst:292
msgid ""
"In gradient descent, the gradient :math:`\\nabla Loss_{W}` of the loss "
"with respect to the weights is computed and deducted from :math:`W`. More"
" formally, this is expressed as,"
msgstr ""

#: ../modules/neural_networks_supervised.rst:296
msgid ""
"W^{i+1} = W^i - \\epsilon \\nabla {Loss}_{W}^{i}\n"
"\n"
msgstr ""

#: ../modules/neural_networks_supervised.rst:300
msgid ""
"where :math:`i` is the iteration step, and :math:`\\epsilon` is the "
"learning rate with a value larger than 0."
msgstr ""

#: ../modules/neural_networks_supervised.rst:303
msgid ""
"The algorithm stops when it reaches a preset maximum number of "
"iterations; or when the improvement in loss is below a certain, small "
"number."
msgstr ""

#: ../modules/neural_networks_supervised.rst:311
msgid "Tips on Practical Use"
msgstr ""

#: ../modules/neural_networks_supervised.rst:313
msgid ""
"Multi-layer Perceptron is sensitive to feature scaling, so it is highly "
"recommended to scale your data. For example, scale each attribute on the "
"input vector X to [0, 1] or [-1, +1], or standardize it to have mean 0 "
"and variance 1. Note that you must apply the *same* scaling to the test "
"set for meaningful results. You can use :class:`StandardScaler` for "
"standardization."
msgstr ""

#: ../modules/neural_networks_supervised.rst:328
msgid ""
"An alternative and recommended approach is to use :class:`StandardScaler`"
" in a :class:`Pipeline`"
msgstr ""

#: ../modules/neural_networks_supervised.rst:331
msgid ""
"Finding a reasonable regularization parameter :math:`\\alpha` is best "
"done using :class:`GridSearchCV`, usually in the range ``10.0 ** "
"-np.arange(1, 7)``."
msgstr ""

#: ../modules/neural_networks_supervised.rst:335
msgid ""
"Empirically, we observed that `L-BFGS` converges faster and with better "
"solutions on small datasets. For relatively large datasets, however, "
"`Adam` is very robust. It usually converges quickly and gives pretty good"
" performance. `SGD` with momentum or nesterov's momentum, on the other "
"hand, can perform better than those two algorithms if learning rate is "
"correctly tuned."
msgstr ""

#: ../modules/neural_networks_supervised.rst:343
msgid "More control with warm_start"
msgstr ""

#: ../modules/neural_networks_supervised.rst:344
msgid ""
"If you want more control over stopping criteria or learning rate in SGD, "
"or want to do additional monitoring, using ``warm_start=True`` and "
"``max_iter=1`` and iterating yourself can be helpful::"
msgstr ""

#: ../modules/neural_networks_supervised.rst:358
msgid ""
"`\"Learning representations by back-propagating errors.\" "
"<https://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf>`_ "
"Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams."
msgstr ""

#: ../modules/neural_networks_supervised.rst:362
msgid ""
"`\"Stochastic Gradient Descent\" <https://leon.bottou.org/projects/sgd>`_"
" L. Bottou - Website, 2010."
msgstr ""

#: ../modules/neural_networks_supervised.rst:364
msgid ""
"`\"Backpropagation\" "
"<http://ufldl.stanford.edu/wiki/index.php/Backpropagation_Algorithm>`_ "
"Andrew Ng, Jiquan Ngiam, Chuan Yu Foo, Yifan Mai, Caroline Suen - "
"Website, 2011."
msgstr ""

#: ../modules/neural_networks_supervised.rst:367
msgid ""
"`\"Efficient BackProp\" <http://yann.lecun.com/exdb/publis/pdf/lecun-"
"98b.pdf>`_ Y. LeCun, L. Bottou, G. Orr, K. Müller - In Neural Networks: "
"Tricks of the Trade 1998."
msgstr ""

#: ../modules/neural_networks_supervised.rst:371
msgid ""
"`\"Adam: A method for stochastic optimization.\" "
"<https://arxiv.org/pdf/1412.6980v8.pdf>`_ Kingma, Diederik, and Jimmy Ba."
" arXiv preprint arXiv:1412.6980 (2014)."
msgstr ""

