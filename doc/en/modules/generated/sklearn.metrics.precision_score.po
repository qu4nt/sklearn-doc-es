# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/generated/sklearn.metrics.precision_score.rst:2
msgid ":mod:`sklearn.metrics`.precision_score"
msgstr ""

#: of sklearn.metrics._classification.precision_score:2
msgid "Compute the precision."
msgstr ""

#: of sklearn.metrics._classification.precision_score:4
msgid ""
"The precision is the ratio ``tp / (tp + fp)`` where ``tp`` is the number "
"of true positives and ``fp`` the number of false positives. The precision"
" is intuitively the ability of the classifier not to label as positive a "
"sample that is negative."
msgstr ""

#: of sklearn.metrics._classification.precision_score:9
msgid "The best value is 1 and the worst value is 0."
msgstr ""

#: of sklearn.metrics._classification.precision_score:11
msgid "Read more in the :ref:`User Guide <precision_recall_f_measure_metrics>`."
msgstr ""

#: of sklearn.metrics._classification.precision_score
msgid "Parameters"
msgstr ""

#: of sklearn.metrics._classification.precision_score:16
msgid "**y_true**"
msgstr ""

#: of
msgid "1d array-like, or label indicator array / sparse matrix"
msgstr ""

#: of sklearn.metrics._classification.precision_score:16
msgid "Ground truth (correct) target values."
msgstr ""

#: of sklearn.metrics._classification.precision_score:19
msgid "**y_pred**"
msgstr ""

#: of sklearn.metrics._classification.precision_score:19
msgid "Estimated targets as returned by a classifier."
msgstr ""

#: of sklearn.metrics._classification.precision_score:31
msgid "**labels**"
msgstr ""

#: of
msgid "array-like, default=None"
msgstr ""

#: of sklearn.metrics._classification.precision_score:22
msgid ""
"The set of labels to include when ``average != 'binary'``, and their "
"order if ``average is None``. Labels present in the data can be excluded,"
" for example to calculate a multiclass average ignoring a majority "
"negative class, while labels not present in the data will result in 0 "
"components in a macro average. For multilabel targets, labels are column "
"indices. By default, all labels in ``y_true`` and ``y_pred`` are used in "
"sorted order."
msgstr ""

#: of sklearn.metrics._classification.precision_score:30
msgid "Parameter `labels` improved for multiclass problem."
msgstr ""

#: of sklearn.metrics._classification.precision_score:37
msgid "**pos_label**"
msgstr ""

#: of
msgid "str or int, default=1"
msgstr ""

#: of sklearn.metrics._classification.precision_score:34
msgid ""
"The class to report if ``average='binary'`` and the data is binary. If "
"the data are multiclass or multilabel, this will be ignored; setting "
"``labels=[pos_label]`` and ``average != 'binary'`` will report scores for"
" that label only."
msgstr ""

#: of sklearn.metrics._classification.precision_score:61
msgid "**average**"
msgstr ""

#: of
msgid ""
"{'micro', 'macro', 'samples', 'weighted', 'binary'}             "
"default='binary'"
msgstr ""

#: of sklearn.metrics._classification.precision_score:40
msgid ""
"This parameter is required for multiclass/multilabel targets. If "
"``None``, the scores for each class are returned. Otherwise, this "
"determines the type of averaging performed on the data:"
msgstr ""

#: of sklearn.metrics._classification.precision_score:45
msgid "``'binary'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_score:45
msgid ""
"Only report results for the class specified by ``pos_label``. This is "
"applicable only if targets (``y_{true,pred}``) are binary."
msgstr ""

#: of sklearn.metrics._classification.precision_score:48
msgid "``'micro'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_score:48
msgid ""
"Calculate metrics globally by counting the total true positives, false "
"negatives and false positives."
msgstr ""

#: of sklearn.metrics._classification.precision_score:51
msgid "``'macro'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_score:51
msgid ""
"Calculate metrics for each label, and find their unweighted mean.  This "
"does not take label imbalance into account."
msgstr ""

#: of sklearn.metrics._classification.precision_score:56
msgid "``'weighted'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_score:54
msgid ""
"Calculate metrics for each label, and find their average weighted by "
"support (the number of true instances for each label). This alters "
"'macro' to account for label imbalance; it can result in an F-score that "
"is not between precision and recall."
msgstr ""

#: of sklearn.metrics._classification.precision_score:61
msgid "``'samples'``:"
msgstr ""

#: of sklearn.metrics._classification.precision_score:59
msgid ""
"Calculate metrics for each instance, and find their average (only "
"meaningful for multilabel classification where this differs from "
":func:`accuracy_score`)."
msgstr ""

#: of sklearn.metrics._classification.precision_score:64
msgid "**sample_weight**"
msgstr ""

#: of
msgid "array-like of shape (n_samples,), default=None"
msgstr ""

#: of sklearn.metrics._classification.precision_score:64
msgid "Sample weights."
msgstr ""

#: of sklearn.metrics._classification.precision_score:68
msgid "**zero_division**"
msgstr ""

#: of
msgid "\"warn\", 0 or 1, default=\"warn\""
msgstr ""

#: of sklearn.metrics._classification.precision_score:67
msgid ""
"Sets the value to return when there is a zero division. If set to "
"\"warn\", this acts as 0, but warnings are also raised."
msgstr ""

#: of sklearn.metrics._classification.precision_score
msgid "Returns"
msgstr ""

#: of sklearn.metrics._classification.precision_score:81
msgid "**precision**"
msgstr ""

#: of
msgid "float (if average is not None) or array of float of shape"
msgstr ""

#: of sklearn.metrics._classification.precision_score:73
msgid ""
"(n_unique_labels,) Precision of the positive class in binary "
"classification or weighted average of the precision of each class for the"
" multiclass task."
msgstr ""

#: of sklearn.metrics._classification.precision_score:86
msgid ":obj:`precision_recall_fscore_support`, :obj:`multilabel_confusion_matrix`"
msgstr ""

#: of sklearn.metrics._classification.precision_score:90
msgid "Notes"
msgstr ""

#: of sklearn.metrics._classification.precision_score:91
msgid ""
"When ``true positive + false positive == 0``, precision returns 0 and "
"raises ``UndefinedMetricWarning``. This behavior can be modified with "
"``zero_division``."
msgstr ""

#: of sklearn.metrics._classification.precision_score:97
msgid "Examples"
msgstr ""

#: ../modules/generated/sklearn.metrics.precision_score.examples:4
msgid "Examples using ``sklearn.metrics.precision_score``"
msgstr ""

#: ../modules/generated/sklearn.metrics.precision_score.examples:15
#: ../modules/generated/sklearn.metrics.precision_score.examples:23
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_precision_recall.py`"
msgstr ""

