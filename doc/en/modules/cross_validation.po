# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../modules/cross_validation.rst:6
msgid "Cross-validation: evaluating estimator performance"
msgstr ""

#: ../modules/cross_validation.rst:10
msgid ""
"Learning the parameters of a prediction function and testing it on the "
"same data is a methodological mistake: a model that would just repeat the"
" labels of the samples that it has just seen would have a perfect score "
"but would fail to predict anything useful on yet-unseen data. This "
"situation is called **overfitting**. To avoid it, it is common practice "
"when performing a (supervised) machine learning experiment to hold out "
"part of the available data as a **test set** ``X_test, y_test``. Note "
"that the word \"experiment\" is not intended to denote academic use only,"
" because even in commercial settings machine learning usually starts out "
"experimentally. Here is a flowchart of typical cross validation workflow "
"in model training. The best parameters can be determined by :ref:`grid "
"search <grid_search>` techniques."
msgstr ""

#: ../modules/cross_validation.rst:32
msgid ""
"In scikit-learn a random split into training and test sets can be quickly"
" computed with the :func:`train_test_split` helper function. Let's load "
"the iris data set to fit a linear support vector machine on it::"
msgstr ""

#: ../modules/cross_validation.rst:45
#, python-format
msgid ""
"We can now quickly sample a training set while holding out 40% of the "
"data for testing (evaluating) our classifier::"
msgstr ""

#: ../modules/cross_validation.rst:60
msgid ""
"When evaluating different settings (\"hyperparameters\") for estimators, "
"such as the ``C`` setting that must be manually set for an SVM, there is "
"still a risk of overfitting *on the test set* because the parameters can "
"be tweaked until the estimator performs optimally. This way, knowledge "
"about the test set can \"leak\" into the model and evaluation metrics no "
"longer report on generalization performance. To solve this problem, yet "
"another part of the dataset can be held out as a so-called \"validation "
"set\": training proceeds on the training set, after which evaluation is "
"done on the validation set, and when the experiment seems to be "
"successful, final evaluation can be done on the test set."
msgstr ""

#: ../modules/cross_validation.rst:72
msgid ""
"However, by partitioning the available data into three sets, we "
"drastically reduce the number of samples which can be used for learning "
"the model, and the results can depend on a particular random choice for "
"the pair of (train, validation) sets."
msgstr ""

#: ../modules/cross_validation.rst:78
msgid ""
"A solution to this problem is a procedure called `cross-validation "
"<https://en.wikipedia.org/wiki/Cross-validation_(statistics)>`_ (CV for "
"short). A test set should still be held out for final evaluation, but the"
" validation set is no longer needed when doing CV. In the basic approach,"
" called *k*-fold CV, the training set is split into *k* smaller sets "
"(other approaches are described below, but generally follow the same "
"principles). The following procedure is followed for each of the *k* "
"\"folds\":"
msgstr ""

#: ../modules/cross_validation.rst:89
msgid "A model is trained using :math:`k-1` of the folds as training data;"
msgstr ""

#: ../modules/cross_validation.rst:90
msgid ""
"the resulting model is validated on the remaining part of the data (i.e.,"
" it is used as a test set to compute a performance measure such as "
"accuracy)."
msgstr ""

#: ../modules/cross_validation.rst:94
msgid ""
"The performance measure reported by *k*-fold cross-validation is then the"
" average of the values computed in the loop. This approach can be "
"computationally expensive, but does not waste too much data (as is the "
"case when fixing an arbitrary validation set), which is a major advantage"
" in problems such as inverse inference where the number of samples is "
"very small."
msgstr ""

#: ../modules/cross_validation.rst:108
msgid "Computing cross-validated metrics"
msgstr ""

#: ../modules/cross_validation.rst:110
msgid ""
"The simplest way to use cross-validation is to call the "
":func:`cross_val_score` helper function on the estimator and the dataset."
msgstr ""

#: ../modules/cross_validation.rst:113
msgid ""
"The following example demonstrates how to estimate the accuracy of a "
"linear kernel support vector machine on the iris dataset by splitting the"
" data, fitting a model and computing the score 5 consecutive times (with "
"different splits each time)::"
msgstr ""

#: ../modules/cross_validation.rst:124
msgid "The mean score and the standard deviation are hence given by::"
msgstr ""

#: ../modules/cross_validation.rst:129
msgid ""
"By default, the score computed at each CV iteration is the ``score`` "
"method of the estimator. It is possible to change this by using the "
"scoring parameter::"
msgstr ""

#: ../modules/cross_validation.rst:139
msgid ""
"See :ref:`scoring_parameter` for details. In the case of the Iris "
"dataset, the samples are balanced across target classes hence the "
"accuracy and the F1-score are almost equal."
msgstr ""

#: ../modules/cross_validation.rst:143
msgid ""
"When the ``cv`` argument is an integer, :func:`cross_val_score` uses the "
":class:`KFold` or :class:`StratifiedKFold` strategies by default, the "
"latter being used if the estimator derives from :class:`ClassifierMixin "
"<sklearn.base.ClassifierMixin>`."
msgstr ""

#: ../modules/cross_validation.rst:148
msgid ""
"It is also possible to use other cross validation strategies by passing a"
" cross validation iterator instead, for instance::"
msgstr ""

#: ../modules/cross_validation.rst:157
msgid ""
"Another option is to use an iterable yielding (train, test) splits as "
"arrays of indices, for example::"
msgstr ""

msgid "Data transformation with held out data"
msgstr ""

#: ../modules/cross_validation.rst:174
msgid ""
"Just as it is important to test a predictor on data held-out from "
"training, preprocessing (such as standardization, feature selection, "
"etc.) and similar :ref:`data transformations <data-transforms>` similarly"
" should be learnt from a training set and applied to held-out data for "
"prediction::"
msgstr ""

#: ../modules/cross_validation.rst:189
msgid ""
"A :class:`Pipeline <sklearn.pipeline.Pipeline>` makes it easier to "
"compose estimators, providing this behavior under cross-validation::"
msgstr ""

#: ../modules/cross_validation.rst:197
msgid "See :ref:`combining_estimators`."
msgstr ""

#: ../modules/cross_validation.rst:203
msgid "The cross_validate function and multiple metric evaluation"
msgstr ""

#: ../modules/cross_validation.rst:205
msgid ""
"The :func:`cross_validate` function differs from :func:`cross_val_score` "
"in two ways:"
msgstr ""

#: ../modules/cross_validation.rst:208
msgid "It allows specifying multiple metrics for evaluation."
msgstr ""

#: ../modules/cross_validation.rst:210
msgid ""
"It returns a dict containing fit-times, score-times (and optionally "
"training scores as well as fitted estimators) in addition to the test "
"score."
msgstr ""

#: ../modules/cross_validation.rst:214
msgid ""
"For single metric evaluation, where the scoring parameter is a string, "
"callable or None, the keys will be - ``['test_score', 'fit_time', "
"'score_time']``"
msgstr ""

#: ../modules/cross_validation.rst:217
msgid ""
"And for multiple metric evaluation, the return value is a dict with the "
"following keys - ``['test_<scorer1_name>', 'test_<scorer2_name>', "
"'test_<scorer...>', 'fit_time', 'score_time']``"
msgstr ""

#: ../modules/cross_validation.rst:221
msgid ""
"``return_train_score`` is set to ``False`` by default to save computation"
" time. To evaluate the scores on the training set as well you need to be "
"set to ``True``."
msgstr ""

#: ../modules/cross_validation.rst:225
msgid ""
"You may also retain the estimator fitted on each training set by setting "
"``return_estimator=True``."
msgstr ""

#: ../modules/cross_validation.rst:228
msgid ""
"The multiple metrics can be specified either as a list, tuple or set of "
"predefined scorer names::"
msgstr ""

#: ../modules/cross_validation.rst:241
msgid ""
"Or as a dict mapping scorer name to a predefined or custom scoring "
"function::"
msgstr ""

#: ../modules/cross_validation.rst:254
msgid "Here is an example of ``cross_validate`` using a single metric::"
msgstr ""

#: ../modules/cross_validation.rst:264
msgid "Obtaining predictions by cross-validation"
msgstr ""

#: ../modules/cross_validation.rst:266
msgid ""
"The function :func:`cross_val_predict` has a similar interface to "
":func:`cross_val_score`, but returns, for each element in the input, the "
"prediction that was obtained for that element when it was in the test "
"set. Only cross-validation strategies that assign all elements to a test "
"set exactly once can be used (otherwise, an exception is raised)."
msgstr ""

#: ../modules/cross_validation.rst:273
msgid "Note on inappropriate usage of cross_val_predict"
msgstr ""

#: ../modules/cross_validation.rst:275
msgid ""
"The result of :func:`cross_val_predict` may be different from those "
"obtained using :func:`cross_val_score` as the elements are grouped in "
"different ways. The function :func:`cross_val_score` takes an average "
"over cross-validation folds, whereas :func:`cross_val_predict` simply "
"returns the labels (or probabilities) from several distinct models "
"undistinguished. Thus, :func:`cross_val_predict` is not an appropriate "
"measure of generalisation error."
msgstr ""

#: ../modules/cross_validation.rst:288
msgid "The function :func:`cross_val_predict` is appropriate for:"
msgstr ""

#: ../modules/cross_validation.rst:285
msgid "Visualization of predictions obtained from different models."
msgstr ""

#: ../modules/cross_validation.rst:286
msgid ""
"Model blending: When predictions of one supervised estimator are used to "
"train another estimator in ensemble methods."
msgstr ""

#: ../modules/cross_validation.rst:290
msgid ""
"The available cross validation iterators are introduced in the following "
"section."
msgstr ""

#: ../modules/cross_validation.rst:295
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_roc_crossval.py`,"
msgstr ""

#: ../modules/cross_validation.rst:296
msgid ":ref:`sphx_glr_auto_examples_feature_selection_plot_rfe_with_cross_validation.py`,"
msgstr ""

#: ../modules/cross_validation.rst:297
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_grid_search_digits.py`,"
msgstr ""

#: ../modules/cross_validation.rst:298
msgid ":ref:`sphx_glr_auto_examples_model_selection_grid_search_text_feature_extraction.py`,"
msgstr ""

#: ../modules/cross_validation.rst:299
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_cv_predict.py`,"
msgstr ""

#: ../modules/cross_validation.rst:300
msgid ":ref:`sphx_glr_auto_examples_model_selection_plot_nested_cross_validation_iris.py`."
msgstr ""

#: ../modules/cross_validation.rst:303
msgid "Cross validation iterators"
msgstr ""

#: ../modules/cross_validation.rst:305
msgid ""
"The following sections list utilities to generate indices that can be "
"used to generate dataset splits according to different cross validation "
"strategies."
msgstr ""

#: ../modules/cross_validation.rst:312
msgid "Cross-validation iterators for i.i.d. data"
msgstr ""

#: ../modules/cross_validation.rst:314
msgid ""
"Assuming that some data is Independent and Identically Distributed "
"(i.i.d.) is making the assumption that all samples stem from the same "
"generative process and that the generative process is assumed to have no "
"memory of past generated samples."
msgstr ""

#: ../modules/cross_validation.rst:319
msgid "The following cross-validators can be used in such cases."
msgstr ""

#: ../modules/cross_validation.rst:323
msgid ""
"While i.i.d. data is a common assumption in machine learning theory, it "
"rarely holds in practice. If one knows that the samples have been "
"generated using a time-dependent process, it is safer to use a :ref"
":`time-series aware cross-validation scheme <timeseries_cv>`. Similarly, "
"if we know that the generative process has a group structure (samples "
"collected from different subjects, experiments, measurement devices), it "
"is safer to use :ref:`group-wise cross-validation <group_cv>`."
msgstr ""

#: ../modules/cross_validation.rst:334
msgid "K-fold"
msgstr ""

#: ../modules/cross_validation.rst:336
msgid ""
":class:`KFold` divides all the samples in :math:`k` groups of samples, "
"called folds (if :math:`k = n`, this is equivalent to the *Leave One Out*"
" strategy), of equal sizes (if possible). The prediction function is "
"learned using :math:`k - 1` folds, and the fold left out is used for "
"test."
msgstr ""

#: ../modules/cross_validation.rst:341
msgid "Example of 2-fold cross-validation on a dataset with 4 samples::"
msgstr ""

#: ../modules/cross_validation.rst:353
msgid ""
"Here is a visualization of the cross-validation behavior. Note that "
":class:`KFold` is not affected by classes or groups."
msgstr ""

#: ../modules/cross_validation.rst:361
msgid ""
"Each fold is constituted by two arrays: the first one is related to the "
"*training set*, and the second one to the *test set*. Thus, one can "
"create the training/test sets using numpy indexing::"
msgstr ""

#: ../modules/cross_validation.rst:372
msgid "Repeated K-Fold"
msgstr ""

#: ../modules/cross_validation.rst:374
msgid ""
":class:`RepeatedKFold` repeats K-Fold n times. It can be used when one "
"requires to run :class:`KFold` n times, producing different splits in "
"each repetition."
msgstr ""

#: ../modules/cross_validation.rst:378
msgid "Example of 2-fold K-Fold repeated 2 times::"
msgstr ""

#: ../modules/cross_validation.rst:394
msgid ""
"Similarly, :class:`RepeatedStratifiedKFold` repeats Stratified K-Fold n "
"times with different randomization in each repetition."
msgstr ""

#: ../modules/cross_validation.rst:400
msgid "Leave One Out (LOO)"
msgstr ""

#: ../modules/cross_validation.rst:402
msgid ""
":class:`LeaveOneOut` (or LOO) is a simple cross-validation. Each learning"
" set is created by taking all the samples except one, the test set being "
"the sample left out. Thus, for :math:`n` samples, we have :math:`n` "
"different training sets and :math:`n` different tests set. This cross-"
"validation procedure does not waste much data as only one sample is "
"removed from the training set::"
msgstr ""

#: ../modules/cross_validation.rst:421
msgid ""
"Potential users of LOO for model selection should weigh a few known "
"caveats. When compared with :math:`k`-fold cross validation, one builds "
":math:`n` models from :math:`n` samples instead of :math:`k` models, "
"where :math:`n > k`. Moreover, each is trained on :math:`n - 1` samples "
"rather than :math:`(k-1) n / k`. In both ways, assuming :math:`k` is not "
"too large and :math:`k < n`, LOO is more computationally expensive than "
":math:`k`-fold cross validation."
msgstr ""

#: ../modules/cross_validation.rst:429
msgid ""
"In terms of accuracy, LOO often results in high variance as an estimator "
"for the test error. Intuitively, since :math:`n - 1` of the :math:`n` "
"samples are used to build each model, models constructed from folds are "
"virtually identical to each other and to the model built from the entire "
"training set."
msgstr ""

#: ../modules/cross_validation.rst:435
msgid ""
"However, if the learning curve is steep for the training size in "
"question, then 5- or 10- fold cross validation can overestimate the "
"generalization error."
msgstr ""

#: ../modules/cross_validation.rst:438
msgid ""
"As a general rule, most authors, and empirical evidence, suggest that 5- "
"or 10- fold cross validation should be preferred to LOO."
msgstr ""

#: ../modules/cross_validation.rst:444
msgid "`<http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html>`_;"
msgstr ""

#: ../modules/cross_validation.rst:445
msgid ""
"T. Hastie, R. Tibshirani, J. Friedman,  `The Elements of Statistical "
"Learning <https://web.stanford.edu/~hastie/ElemStatLearn/>`_, Springer "
"2009"
msgstr ""

#: ../modules/cross_validation.rst:447
msgid ""
"L. Breiman, P. Spector `Submodel selection and evaluation in regression: "
"The X-random case "
"<http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf>`_, "
"International Statistical Review 1992;"
msgstr ""

#: ../modules/cross_validation.rst:449
msgid ""
"R. Kohavi, `A Study of Cross-Validation and Bootstrap for Accuracy "
"Estimation and Model Selection "
"<http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf>`_, "
"Intl. Jnt. Conf. AI"
msgstr ""

#: ../modules/cross_validation.rst:451
msgid ""
"R. Bharat Rao, G. Fung, R. Rosales, `On the Dangers of Cross-Validation. "
"An Experimental Evaluation "
"<https://people.csail.mit.edu/romer/papers/CrossVal_SDM08.pdf>`_, SIAM "
"2008;"
msgstr ""

#: ../modules/cross_validation.rst:453
msgid ""
"G. James, D. Witten, T. Hastie, R Tibshirani, `An Introduction to "
"Statistical Learning <https://www-bcf.usc.edu/~gareth/ISL/>`_, Springer "
"2013."
msgstr ""

#: ../modules/cross_validation.rst:459
msgid "Leave P Out (LPO)"
msgstr ""

#: ../modules/cross_validation.rst:461
msgid ""
":class:`LeavePOut` is very similar to :class:`LeaveOneOut` as it creates "
"all the possible training/test sets by removing :math:`p` samples from "
"the complete set. For :math:`n` samples, this produces :math:`{n \\choose"
" p}` train-test pairs. Unlike :class:`LeaveOneOut` and :class:`KFold`, "
"the test sets will overlap for :math:`p > 1`."
msgstr ""

#: ../modules/cross_validation.rst:467
msgid "Example of Leave-2-Out on a dataset with 4 samples::"
msgstr ""

#: ../modules/cross_validation.rst:486
msgid "Random permutations cross-validation a.k.a. Shuffle & Split"
msgstr ""

#: ../modules/cross_validation.rst:488
msgid ""
"The :class:`ShuffleSplit` iterator will generate a user defined number of"
" independent train / test dataset splits. Samples are first shuffled and "
"then split into a pair of train and test sets."
msgstr ""

#: ../modules/cross_validation.rst:492
msgid ""
"It is possible to control the randomness for reproducibility of the "
"results by explicitly seeding the ``random_state`` pseudo random number "
"generator."
msgstr ""

#: ../modules/cross_validation.rst:496 ../modules/cross_validation.rst:718
msgid "Here is a usage example::"
msgstr ""

#: ../modules/cross_validation.rst:509
msgid ""
"Here is a visualization of the cross-validation behavior. Note that "
":class:`ShuffleSplit` is not affected by classes or groups."
msgstr ""

#: ../modules/cross_validation.rst:517
msgid ""
":class:`ShuffleSplit` is thus a good alternative to :class:`KFold` cross "
"validation that allows a finer control on the number of iterations and "
"the proportion of samples on each side of the train / test split."
msgstr ""

#: ../modules/cross_validation.rst:524
msgid "Cross-validation iterators with stratification based on class labels."
msgstr ""

#: ../modules/cross_validation.rst:526
msgid ""
"Some classification problems can exhibit a large imbalance in the "
"distribution of the target classes: for instance there could be several "
"times more negative samples than positive samples. In such cases it is "
"recommended to use stratified sampling as implemented in "
":class:`StratifiedKFold` and :class:`StratifiedShuffleSplit` to ensure "
"that relative class frequencies is approximately preserved in each train "
"and validation fold."
msgstr ""

#: ../modules/cross_validation.rst:536
msgid "Stratified k-fold"
msgstr ""

#: ../modules/cross_validation.rst:538
msgid ""
":class:`StratifiedKFold` is a variation of *k-fold* which returns "
"*stratified* folds: each set contains approximately the same percentage "
"of samples of each target class as the complete set."
msgstr ""

#: ../modules/cross_validation.rst:542
msgid ""
"Here is an example of stratified 3-fold cross-validation on a dataset "
"with 50 samples from two unbalanced classes.  We show the number of "
"samples in each class and compare with :class:`KFold`."
msgstr ""

#: ../modules/cross_validation.rst:564
msgid ""
"We can see that :class:`StratifiedKFold` preserves the class ratios "
"(approximately 1 / 10) in both train and test dataset."
msgstr ""

#: ../modules/cross_validation.rst:567 ../modules/cross_validation.rst:586
#: ../modules/cross_validation.rst:646 ../modules/cross_validation.rst:734
#: ../modules/cross_validation.rst:836
msgid "Here is a visualization of the cross-validation behavior."
msgstr ""

#: ../modules/cross_validation.rst:574
msgid ""
":class:`RepeatedStratifiedKFold` can be used to repeat Stratified K-Fold "
"n times with different randomization in each repetition."
msgstr ""

#: ../modules/cross_validation.rst:580
msgid "Stratified Shuffle Split"
msgstr ""

#: ../modules/cross_validation.rst:582
msgid ""
":class:`StratifiedShuffleSplit` is a variation of *ShuffleSplit*, which "
"returns stratified splits, *i.e* which creates splits by preserving the "
"same percentage for each target class as in the complete set."
msgstr ""

#: ../modules/cross_validation.rst:596
msgid "Cross-validation iterators for grouped data."
msgstr ""

#: ../modules/cross_validation.rst:598
msgid ""
"The i.i.d. assumption is broken if the underlying generative process "
"yield groups of dependent samples."
msgstr ""

#: ../modules/cross_validation.rst:601
msgid ""
"Such a grouping of data is domain specific. An example would be when "
"there is medical data collected from multiple patients, with multiple "
"samples taken from each patient. And such data is likely to be dependent "
"on the individual group. In our example, the patient id for each sample "
"will be its group identifier."
msgstr ""

#: ../modules/cross_validation.rst:606
msgid ""
"In this case we would like to know if a model trained on a particular set"
" of groups generalizes well to the unseen groups. To measure this, we "
"need to ensure that all the samples in the validation fold come from "
"groups that are not represented at all in the paired training fold."
msgstr ""

#: ../modules/cross_validation.rst:611
msgid ""
"The following cross-validation splitters can be used to do that. The "
"grouping identifier for the samples is specified via the ``groups`` "
"parameter."
msgstr ""

#: ../modules/cross_validation.rst:618
msgid "Group k-fold"
msgstr ""

#: ../modules/cross_validation.rst:620
msgid ""
":class:`GroupKFold` is a variation of k-fold which ensures that the same "
"group is not represented in both testing and training sets. For example "
"if the data is obtained from different subjects with several samples per-"
"subject and if the model is flexible enough to learn from highly person "
"specific features it could fail to generalize to new subjects. "
":class:`GroupKFold` makes it possible to detect this kind of overfitting "
"situations."
msgstr ""

#: ../modules/cross_validation.rst:627
msgid ""
"Imagine you have three subjects, each with an associated number from 1 to"
" 3::"
msgstr ""

#: ../modules/cross_validation.rst:642
msgid ""
"Each subject is in a different testing fold, and the same subject is "
"never in both testing and training. Notice that the folds do not have "
"exactly the same size due to the imbalance in the data."
msgstr ""

#: ../modules/cross_validation.rst:656
msgid "Leave One Group Out"
msgstr ""

#: ../modules/cross_validation.rst:658
msgid ""
":class:`LeaveOneGroupOut` is a cross-validation scheme which holds out "
"the samples according to a third-party provided array of integer groups. "
"This group information can be used to encode arbitrary domain specific "
"pre-defined cross-validation folds."
msgstr ""

#: ../modules/cross_validation.rst:663
msgid ""
"Each training set is thus constituted by all the samples except the ones "
"related to a specific group."
msgstr ""

#: ../modules/cross_validation.rst:666
msgid ""
"For example, in the cases of multiple experiments, "
":class:`LeaveOneGroupOut` can be used to create a cross-validation based "
"on the different experiments: we create a training set using the samples "
"of all the experiments except one::"
msgstr ""

#: ../modules/cross_validation.rst:682
msgid ""
"Another common application is to use time information: for instance the "
"groups could be the year of collection of the samples and thus allow for "
"cross-validation against time-based splits."
msgstr ""

#: ../modules/cross_validation.rst:689
msgid "Leave P Groups Out"
msgstr ""

#: ../modules/cross_validation.rst:691
msgid ""
":class:`LeavePGroupsOut` is similar as :class:`LeaveOneGroupOut`, but "
"removes samples related to :math:`P` groups for each training/test set."
msgstr ""

#: ../modules/cross_validation.rst:694
msgid "Example of Leave-2-Group Out::"
msgstr ""

#: ../modules/cross_validation.rst:711
msgid "Group Shuffle Split"
msgstr ""

#: ../modules/cross_validation.rst:713
msgid ""
"The :class:`GroupShuffleSplit` iterator behaves as a combination of "
":class:`ShuffleSplit` and :class:`LeavePGroupsOut`, and generates a "
"sequence of randomized partitions in which a subset of groups are held "
"out for each split."
msgstr ""

#: ../modules/cross_validation.rst:741
msgid ""
"This class is useful when the behavior of :class:`LeavePGroupsOut` is "
"desired, but the number of groups is large enough that generating all "
"possible partitions with :math:`P` groups withheld would be prohibitively"
" expensive. In such a scenario, :class:`GroupShuffleSplit` provides a "
"random sample (with replacement) of the train / test splits generated by "
":class:`LeavePGroupsOut`."
msgstr ""

#: ../modules/cross_validation.rst:751
msgid "Predefined Fold-Splits / Validation-Sets"
msgstr ""

#: ../modules/cross_validation.rst:753
msgid ""
"For some datasets, a pre-defined split of the data into training- and "
"validation fold or into several cross-validation folds already exists. "
"Using :class:`PredefinedSplit` it is possible to use these folds e.g. "
"when searching for hyperparameters."
msgstr ""

#: ../modules/cross_validation.rst:758
msgid ""
"For example, when using a validation set, set the ``test_fold`` to 0 for "
"all samples that are part of the validation set, and to -1 for all other "
"samples."
msgstr ""

#: ../modules/cross_validation.rst:762
msgid "Using cross-validation iterators to split train and test"
msgstr ""

#: ../modules/cross_validation.rst:764
msgid ""
"The above group cross-validation functions may also be useful for "
"spitting a dataset into training and testing subsets. Note that the "
"convenience function :func:`train_test_split` is a wrapper around "
":func:`ShuffleSplit` and thus only allows for stratified splitting (using"
" the class labels) and cannot account for groups."
msgstr ""

#: ../modules/cross_validation.rst:770
msgid ""
"To perform the train and test split, use the indices for the train and "
"test subsets yielded by the generator output by the `split()` method of "
"the cross-validation splitter. For example::"
msgstr ""

#: ../modules/cross_validation.rst:793
msgid "Cross validation of time series data"
msgstr ""

#: ../modules/cross_validation.rst:795
msgid ""
"Time series data is characterised by the correlation between observations"
" that are near in time (*autocorrelation*). However, classical cross-"
"validation techniques such as :class:`KFold` and :class:`ShuffleSplit` "
"assume the samples are independent and identically distributed, and would"
" result in unreasonable correlation between training and testing "
"instances (yielding poor estimates of generalisation error) on time "
"series data. Therefore, it is very important to evaluate our model for "
"time series data on the \"future\" observations least like those that are"
" used to train the model. To achieve this, one solution is provided by "
":class:`TimeSeriesSplit`."
msgstr ""

#: ../modules/cross_validation.rst:809
msgid "Time Series Split"
msgstr ""

#: ../modules/cross_validation.rst:811
msgid ""
":class:`TimeSeriesSplit` is a variation of *k-fold* which returns first "
":math:`k` folds as train set and the :math:`(k+1)` th fold as test set. "
"Note that unlike standard cross-validation methods, successive training "
"sets are supersets of those that come before them. Also, it adds all "
"surplus data to the first training partition, which is always used to "
"train the model."
msgstr ""

#: ../modules/cross_validation.rst:818
msgid ""
"This class can be used to cross-validate time series data samples that "
"are observed at fixed time intervals."
msgstr ""

#: ../modules/cross_validation.rst:821
msgid ""
"Example of 3-split time series cross-validation on a dataset with 6 "
"samples::"
msgstr ""

#: ../modules/cross_validation.rst:844
msgid "A note on shuffling"
msgstr ""

#: ../modules/cross_validation.rst:846
msgid ""
"If the data ordering is not arbitrary (e.g. samples with the same class "
"label are contiguous), shuffling it first may be essential to get a "
"meaningful cross- validation result. However, the opposite may be true if"
" the samples are not independently and identically distributed. For "
"example, if samples correspond to news articles, and are ordered by their"
" time of publication, then shuffling the data will likely lead to a model"
" that is overfit and an inflated validation score: it will be tested on "
"samples that are artificially similar (close in time) to training "
"samples."
msgstr ""

#: ../modules/cross_validation.rst:855
msgid ""
"Some cross validation iterators, such as :class:`KFold`, have an inbuilt "
"option to shuffle the data indices before splitting them. Note that:"
msgstr ""

#: ../modules/cross_validation.rst:858
msgid "This consumes less memory than shuffling the data directly."
msgstr ""

#: ../modules/cross_validation.rst:859
msgid ""
"By default no shuffling occurs, including for the (stratified) K fold "
"cross- validation performed by specifying ``cv=some_integer`` to "
":func:`cross_val_score`, grid search, etc. Keep in mind that "
":func:`train_test_split` still returns a random split."
msgstr ""

#: ../modules/cross_validation.rst:863
msgid ""
"The ``random_state`` parameter defaults to ``None``, meaning that the "
"shuffling will be different every time ``KFold(..., shuffle=True)`` is "
"iterated. However, ``GridSearchCV`` will use the same shuffling for each "
"set of parameters validated by a single call to its ``fit`` method."
msgstr ""

#: ../modules/cross_validation.rst:867
msgid ""
"To get identical results for each split, set ``random_state`` to an "
"integer."
msgstr ""

#: ../modules/cross_validation.rst:869
msgid ""
"For more details on how to control the randomness of cv splitters and "
"avoid common pitfalls, see :ref:`randomness`."
msgstr ""

#: ../modules/cross_validation.rst:873
msgid "Cross validation and model selection"
msgstr ""

#: ../modules/cross_validation.rst:875
msgid ""
"Cross validation iterators can also be used to directly perform model "
"selection using Grid Search for the optimal hyperparameters of the model."
" This is the topic of the next section: :ref:`grid_search`."
msgstr ""

#: ../modules/cross_validation.rst:882
msgid "Permutation test score"
msgstr ""

#: ../modules/cross_validation.rst:884
msgid ""
":func:`~sklearn.model_selection.permutation_test_score` offers another "
"way to evaluate the performance of classifiers. It provides a "
"permutation-based p-value, which represents how likely an observed "
"performance of the classifier would be obtained by chance. The null "
"hypothesis in this test is that the classifier fails to leverage any "
"statistical dependency between the features and the labels to make "
"correct predictions on left out data. "
":func:`~sklearn.model_selection.permutation_test_score` generates a null "
"distribution by calculating `n_permutations` different permutations of "
"the data. In each permutation the labels are randomly shuffled, thereby "
"removing any dependency between the features and the labels. The p-value "
"output is the fraction of permutations for which the average cross-"
"validation score obtained by the model is better than the cross-"
"validation score obtained by the model using the original data. For "
"reliable results ``n_permutations`` should typically be larger than 100 "
"and ``cv`` between 3-10 folds."
msgstr ""

#: ../modules/cross_validation.rst:899
msgid ""
"A low p-value provides evidence that the dataset contains real dependency"
" between features and labels and the classifier was able to utilize this "
"to obtain good results. A high p-value could be due to a lack of "
"dependency between features and labels (there is no difference in feature"
" values between the classes) or because the classifier was not able to "
"use the dependency in the data. In the latter case, using a more "
"appropriate classifier that is able to utilize the structure in the data,"
" would result in a low p-value."
msgstr ""

#: ../modules/cross_validation.rst:908
msgid ""
"Cross-validation provides information about how well a classifier "
"generalizes, specifically the range of expected errors of the classifier."
" However, a classifier trained on a high dimensional dataset with no "
"structure may still perform better than expected on cross-validation, "
"just by chance. This can typically happen with small datasets with less "
"than a few hundred samples. "
":func:`~sklearn.model_selection.permutation_test_score` provides "
"information on whether the classifier has found a real class structure "
"and can help in evaluating the performance of the classifier."
msgstr ""

#: ../modules/cross_validation.rst:918
msgid ""
"It is important to note that this test has been shown to produce low "
"p-values even if there is only weak structure in the data because in the "
"corresponding permutated datasets there is absolutely no structure. This "
"test is therefore only able to show when the model reliably outperforms "
"random guessing."
msgstr ""

#: ../modules/cross_validation.rst:924
msgid ""
"Finally, :func:`~sklearn.model_selection.permutation_test_score` is "
"computed using brute force and interally fits ``(n_permutations + 1) * "
"n_cv`` models. It is therefore only tractable with small datasets for "
"which fitting an individual model is very fast."
msgstr ""

#: ../modules/cross_validation.rst:931
msgid ":ref:`sphx_glr_auto_examples_feature_selection_plot_permutation_test_for_classification.py`"
msgstr ""

#: ../modules/cross_validation.rst:935
msgid ""
"Ojala and Garriga. `Permutation Tests for Studying Classifier Performance"
" <http://www.jmlr.org/papers/volume11/ojala10a/ojala10a.pdf>`_. J. Mach. "
"Learn. Res. 2010."
msgstr ""

