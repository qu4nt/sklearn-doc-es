# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 11:24-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_auto_examples_miscellaneous_plot_anomaly_comparison.py>`"
" to download the full example code or to run this example in your browser"
" via Binder"
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:23
msgid ""
"Comparing anomaly detection algorithms for outlier detection on toy "
"datasets"
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:25
msgid ""
"This example shows characteristics of different anomaly detection "
"algorithms on 2D datasets. Datasets contain one or two modes (regions of "
"high density) to illustrate the ability of algorithms to cope with "
"multimodal data."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:29
#, python-format
msgid ""
"For each dataset, 15% of samples are generated as random uniform noise. "
"This proportion is the value given to the nu parameter of the OneClassSVM"
" and the contamination parameter of the other outlier detection "
"algorithms. Decision boundaries between inliers and outliers are "
"displayed in black except for Local Outlier Factor (LOF) as it has no "
"predict method to be applied on new data when it is used for outlier "
"detection."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:36
msgid ""
"The :class:`~sklearn.svm.OneClassSVM` is known to be sensitive to "
"outliers and thus does not perform very well for outlier detection. This "
"estimator is best suited for novelty detection when the training set is "
"not contaminated by outliers. That said, outlier detection in high-"
"dimension, or without any assumptions on the distribution of the inlying "
"data is very challenging, and a One-class SVM might give useful results "
"in these situations depending on the value of its hyperparameters."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:44
msgid ""
":class:`~sklearn.covariance.EllipticEnvelope` assumes the data is "
"Gaussian and learns an ellipse. It thus degrades when the data is not "
"unimodal. Notice however that this estimator is robust to outliers."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:48
msgid ""
":class:`~sklearn.ensemble.IsolationForest` and "
":class:`~sklearn.neighbors.LocalOutlierFactor` seem to perform reasonably"
" well for multi-modal data sets. The advantage of "
":class:`~sklearn.neighbors.LocalOutlierFactor` over the other estimators "
"is shown for the third data set, where the two modes have different "
"densities. This advantage is explained by the local aspect of LOF, "
"meaning that it only compares the score of abnormality of one sample with"
" the scores of its neighbors."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:57
msgid ""
"Finally, for the last data set, it is hard to say that one sample is more"
" abnormal than another sample as they are uniformly distributed in a "
"hypercube. Except for the :class:`~sklearn.svm.OneClassSVM` which "
"overfits a little, all estimators present decent solutions for this "
"situation. In such a case, it would be wise to look more closely at the "
"scores of abnormality of the samples as a good estimator should assign "
"similar scores to all the samples."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:65
msgid ""
"While these examples give some intuition about the algorithms, this "
"intuition might not apply to very high dimensional data."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:68
msgid ""
"Finally, note that parameters of the models have been here handpicked but"
" that in practice they need to be adjusted. In the absence of labelled "
"data, the problem is completely unsupervised so model selection can be a "
"challenge."
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:189
msgid "**Total running time of the script:** ( 0 minutes  9.339 seconds)"
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:211
msgid ""
":download:`Download Python source code: plot_anomaly_comparison.py "
"<plot_anomaly_comparison.py>`"
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:217
msgid ""
":download:`Download Jupyter notebook: plot_anomaly_comparison.ipynb "
"<plot_anomaly_comparison.ipynb>`"
msgstr ""

#: ../auto_examples/miscellaneous/plot_anomaly_comparison.rst:224
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

