# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 12:43-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_auto_examples_neural_networks_plot_mnist_filters.py>` "
"to download the full example code or to run this example in your browser "
"via Binder"
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:23
msgid "Visualization of MLP weights on MNIST"
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:25
msgid ""
"Sometimes looking at the learned coefficients of a neural network can "
"provide insight into the learning behavior. For example if weights look "
"unstructured, maybe some were not used at all, or if very large "
"coefficients exist, maybe regularization was too low or the learning rate"
" too high."
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:30
msgid ""
"This example shows how to plot some of the first layer weights in a "
"MLPClassifier trained on the MNIST dataset."
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:33
msgid ""
"The input data consists of 28x28 pixel handwritten digits, leading to 784"
" features in the dataset. Therefore the first layer weight matrix have "
"the shape (784, hidden_layer_sizes[0]).  We can therefore visualize a "
"single column of the weight matrix as a 28x28 pixel image."
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:38
msgid ""
"To make the example run faster, we use very few hidden units, and train "
"only for a very short time. Training longer would result in weights with "
"a much smoother spatial appearance. The example will throw a warning "
"because it doesn't converge, in this case this is what we want because of"
" CI's time constraints."
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:55
msgid "Out:"
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:128
msgid "**Total running time of the script:** ( 0 minutes  54.541 seconds)"
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:150
msgid ""
":download:`Download Python source code: plot_mnist_filters.py "
"<plot_mnist_filters.py>`"
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:156
msgid ""
":download:`Download Jupyter notebook: plot_mnist_filters.ipynb "
"<plot_mnist_filters.ipynb>`"
msgstr ""

#: ../auto_examples/neural_networks/plot_mnist_filters.rst:163
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

#~ msgid "**Total running time of the script:** ( 1 minutes  8.427 seconds)"
#~ msgstr ""

