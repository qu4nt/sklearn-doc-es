# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 12:43-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../auto_examples/ensemble/plot_stack_predictors.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_auto_examples_ensemble_plot_stack_predictors.py>` to "
"download the full example code or to run this example in your browser via"
" Binder"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:23
msgid "Combine predictors using stacking"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:27
msgid ""
"Stacking refers to a method to blend estimators. In this strategy, some "
"estimators are individually fitted on some training data while a final "
"estimator is trained using the stacked predictions of these base "
"estimators."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:31
msgid ""
"In this example, we illustrate the use case in which different regressors"
" are stacked together and a final linear penalized regressor is used to "
"output the prediction. We compare the performance of each individual "
"regressor with the stacking strategy. Stacking slightly improves the "
"overall performance."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:57
msgid "Download the dataset"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:59
msgid ""
"We will use `Ames Housing`_ dataset which was first compiled by Dean De "
"Cock and became better known after it was used in Kaggle challenge. It is"
" a set of 1460 residential homes in Ames, Iowa, each described by 80 "
"features. We will use it to predict the final logarithmic price of the "
"houses. In this example we will use only 20 most interesting features "
"chosen using GradientBoostingRegressor() and limit number of entries "
"(here we won't go into the details on how to select the most interesting "
"features)."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:67
msgid ""
"The Ames housing dataset is not shipped with scikit-learn and therefore "
"we will fetch it from `OpenML`_."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:116
msgid "Make pipeline to preprocess the data"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:118
msgid ""
"Before we can use Ames dataset we still need to do some preprocessing. "
"First, the dataset has many missing values. To impute them, we will "
"exchange categorical missing values with the new category 'missing' while"
" the numerical missing values with the 'mean' of the column. We will also"
" encode the categories with either "
":class:`~sklearn.preprocessing.OneHotEncoder "
"<sklearn.preprocessing.OneHotEncoder>` or "
":class:`~sklearn.preprocessing.OrdinalEncoder "
"<sklearn.preprocessing.OrdinalEncoder>` depending for which type of model"
" we will use them (linear or non-linear model). To facilitate this "
"preprocessing we will make two pipelines. You can skip this section if "
"your data is ready to use and does not need preprocessing"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:197
msgid "Stack of predictors on a single data set"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:199
msgid ""
"It is sometimes tedious to find the model which will best perform on a "
"given dataset. Stacking provide an alternative by combining the outputs "
"of several learners, without the need to choose a model specifically. The"
" performance of stacking is usually close to the best model and sometimes"
" it can outperform the prediction performance of each individual model."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:205
msgid ""
"Here, we combine 3 learners (linear and non-linear) and use a ridge "
"regressor to combine their outputs together."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:208
msgid ""
"Note: although we will make new pipelines with the processors which we "
"wrote in the previous section for the 3 learners, the final estimator "
"RidgeCV() does not need preprocessing of the data as it will be fed with "
"the already preprocessed output from the 3 learners."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:255
msgid "Measure and plot the results"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:257
msgid ""
"Now we can use Ames Housing dataset to make the predictions. We check the"
" performance of each individual predictor as well as of the stack of the "
"regressors."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:261
msgid ""
"The function ``plot_regression_results`` is used to plot the predicted "
"and true targets."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:340
msgid ""
"The stacked regressor will combine the strengths of the different "
"regressors. However, we also see that training the stacked regressor is "
"much more computationally expensive."
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:347
msgid "**Total running time of the script:** ( 0 minutes  24.138 seconds)"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:369
msgid ""
":download:`Download Python source code: plot_stack_predictors.py "
"<plot_stack_predictors.py>`"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:375
msgid ""
":download:`Download Jupyter notebook: plot_stack_predictors.ipynb "
"<plot_stack_predictors.ipynb>`"
msgstr ""

#: ../auto_examples/ensemble/plot_stack_predictors.rst:382
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

#~ msgid "**Total running time of the script:** ( 0 minutes  11.763 seconds)"
#~ msgstr ""

