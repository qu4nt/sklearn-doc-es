# SOME DESCRIPTIVE TITLE.
# Copyright (C) 2007 - 2020, scikit-learn developers (BSD License)
# This file is distributed under the same license as the scikit-learn
# package.
# FIRST AUTHOR <EMAIL@ADDRESS>, 2021.
#
#, fuzzy
msgid ""
msgstr ""
"Project-Id-Version: scikit-learn 0.24\n"
"Report-Msgid-Bugs-To: \n"
"POT-Creation-Date: 2021-03-31 12:43-0400\n"
"PO-Revision-Date: YEAR-MO-DA HO:MI+ZONE\n"
"Last-Translator: FULL NAME <EMAIL@ADDRESS>\n"
"Language-Team: LANGUAGE <LL@li.org>\n"
"MIME-Version: 1.0\n"
"Content-Type: text/plain; charset=utf-8\n"
"Content-Transfer-Encoding: 8bit\n"
"Generated-By: Babel 2.9.0\n"

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:13
msgid ""
"Click :ref:`here "
"<sphx_glr_download_auto_examples_ensemble_plot_gradient_boosting_categorical.py>`"
" to download the full example code or to run this example in your browser"
" via Binder"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:23
msgid "Categorical Feature Support in Gradient Boosting"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:27
msgid ""
"In this example, we will compare the training times and prediction "
"performances of :class:`~ensemble.HistGradientBoostingRegressor` with "
"different encoding strategies for categorical features. In particular, we"
" will evaluate:"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:32
msgid "dropping the categorical features"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:33
msgid "using a :class:`~preprocessing.OneHotEncoder`"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:34
msgid ""
"using an :class:`~preprocessing.OrdinalEncoder` and treat categories as "
"ordered, equidistant quantities"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:36
msgid ""
"using an :class:`~preprocessing.OrdinalEncoder` and rely on the "
":ref:`native category support <categorical_support_gbdt>` of the "
":class:`~ensemble.HistGradientBoostingRegressor` estimator."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:40
msgid ""
"We will work with the Ames Lowa Housing dataset which consists of "
"numerical and categorical features, where the houses' sales prices is the"
" target."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:59
msgid "Load Ames Housing dataset"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:60
msgid ""
"First, we load the ames housing data as a pandas dataframe. The features "
"are either categorical or numerical:"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:84
msgid "Out:"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:101
msgid "Gradient boosting estimator with dropped categorical features"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:102
msgid ""
"As a baseline, we create an estimator where the categorical features are "
"dropped:"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:132
msgid "Gradient boosting estimator with one-hot encoding"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:133
msgid ""
"Next, we create a pipeline that will one-hot encode the categorical "
"features and let the rest of the numerical data to passthrough:"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:161
msgid "Gradient boosting estimator with ordinal encoding"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:162
msgid ""
"Next, we create a pipeline that will treat categorical features as if "
"they were ordered quantities, i.e. the categories will be encoded as 0, "
"1, 2, etc., and treated as continuous features."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:192
msgid "Gradient boosting estimator with native categorical support"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:193
msgid ""
"We now create a :class:`~ensemble.HistGradientBoostingRegressor` "
"estimator that will natively handle categorical features. This estimator "
"will not treat categorical features as ordered quantities."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:197
msgid ""
"Since the :class:`~ensemble.HistGradientBoostingRegressor` requires "
"category values to be encoded in `[0, n_unique_categories - 1]`, we still"
" rely on an :class:`~preprocessing.OrdinalEncoder` to pre-process the "
"data."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:201
msgid ""
"The main difference between this pipeline and the previous one is that in"
" this one, we let the :class:`~ensemble.HistGradientBoostingRegressor` "
"know which features are categorical."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:231
msgid "Model comparison"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:232
msgid ""
"Finally, we evaluate the models using cross validation. Here we compare "
"the models performance in terms of "
":func:`~metrics.mean_absolute_percentage_error` and fit times."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:286
msgid ""
"We see that the model with one-hot-encoded data is by far the slowest. "
"This is to be expected, since one-hot-encoding creates one additional "
"feature per category value (for each categorical feature), and thus more "
"split points need to be considered during fitting. In theory, we expect "
"the native handling of categorical features to be slightly slower than "
"treating categories as ordered quantities ('Ordinal'), since native "
"handling requires :ref:`sorting categories <categorical_support_gbdt>`. "
"Fitting times should however be close when the number of categories is "
"small, and this may not always be reflected in practice."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:296
msgid ""
"In terms of prediction performance, dropping the categorical features "
"leads to poorer performance. The three models that use categorical "
"features have comparable error rates, with a slight edge for the native "
"handling."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:303
msgid "Limitting the number of splits"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:305
msgid ""
"In general, one can expect poorer predictions from one-hot-encoded data, "
"especially when the tree depths or the number of nodes are limited: with "
"one-hot-encoded data, one needs more split points, i.e. more depth, in "
"order to recover an equivalent split that could be obtained in one single"
" split point with native handling."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:311
msgid ""
"This is also true when categories are treated as ordinal quantities: if "
"categories are `A..F` and the best split is `ACF - BDE` the one-hot-"
"encoder model will need 3 split points (one per category in the left "
"node), and the ordinal non-native model will need 4 splits: 1 split to "
"isolate `A`, 1 split to isolate `F`, and 2 splits to isolate `C` from "
"`BCDE`."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:317
msgid ""
"How strongly the models' performances differ in practice will depend on "
"the dataset and on the flexibility of the trees."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:320
msgid ""
"To see this, let us re-run the same analysis with under-fitting models "
"where we artificially limit the total number of splits by both limitting "
"the number of trees and the depth of each tree."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:355
msgid ""
"The results for these under-fitting models confirm our previous "
"intuition: the native category handling strategy performs the best when "
"the splitting budget is constrained. The two other strategies (one-hot "
"encoding and treating categories as ordinal values) lead to error values "
"comparable to the baseline model that just dropped the categorical "
"features altogether."
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:364
msgid "**Total running time of the script:** ( 0 minutes  16.601 seconds)"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:386
msgid ""
":download:`Download Python source code: "
"plot_gradient_boosting_categorical.py "
"<plot_gradient_boosting_categorical.py>`"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:392
msgid ""
":download:`Download Jupyter notebook: "
"plot_gradient_boosting_categorical.ipynb "
"<plot_gradient_boosting_categorical.ipynb>`"
msgstr ""

#: ../auto_examples/ensemble/plot_gradient_boosting_categorical.rst:399
msgid "`Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_"
msgstr ""

#~ msgid "**Total running time of the script:** ( 0 minutes  3.319 seconds)"
#~ msgstr ""

